{"meta":{"title":"原力小站","subtitle":"扎导的原版正联出了吗？","description":"May 4th be with you","author":"Mr.shuan","url":"http://Bithub00.com"},"pages":[{"title":"","date":"2018-04-06T12:26:47.993Z","updated":"2018-04-06T11:53:10.347Z","comments":true,"path":"404.html","permalink":"http://Bithub00.com/404.html","excerpt":"","text":"404"},{"title":"tags","date":"2019-04-03T05:42:34.000Z","updated":"2019-04-03T05:48:03.768Z","comments":false,"path":"tags/index.html","permalink":"http://Bithub00.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Graph Convolutional Matrix Completion[KDD'18]","slug":"GCMC[KDD18]","date":"2021-01-09T13:17:15.588Z","updated":"2021-01-09T13:30:55.861Z","comments":true,"path":"2021/01/09/GCMC[KDD18]/","link":"","permalink":"http://Bithub00.com/2021/01/09/GCMC[KDD18]/","excerpt":"KDD18一篇将图卷积网络用于矩阵补全问题的论文","text":"KDD18一篇将图卷积网络用于矩阵补全问题的论文 解决的问题如何将图卷积网络应用于矩阵补全问题。 具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。如果将评分矩阵转换为一张图，转换方法在下面有进行介绍，这时矩阵补全问题也可以看成图上的边预测问题。要预测用户对一个物品的评分，就是预测图上两个对应顶点之间相连的边的权重。 做法及创新论文通过一个编码器-解码器的架构来实现从已有评分到特征表示再到预测评分的过程。 Bipartite Graph Construction首先是将推荐任务里的评分数据转化为一张图，具体做法是将用户和物品都看作图中的顶点，交互记录看作边，分数作为边的权重，如图所示： Graph Convolutional Encoder上一步所构建的图的输入形式为邻接矩阵$A\\in \\mathbb{R}^{n\\times n}$与图中顶点的特征矩阵$X\\in \\mathbb{R}^{n\\times d}$。编码器在这一步的作用就是得到用户与物品的特征表示$A,X^u,X^v\\rightarrow U,V$。 具体编码时，论文将不同的评分水平分开考虑$r\\in \\{1,2,3,4,5\\}$，我的理解是它们类似于处理图像数据时的多个channel。以一个评分水平$r$为例，说明编码得到特征表示的过程。假设用户$u_i$对电影$v_j$评分为$r$，而这部电影的特征向量为$x_j$，那么这部电影对这个用户特征表示的贡献可以表示为下面的式子(1)，相当于对特征向量进行了一个线性变换。 对当前评分水平下所有评过分的电影进行求和，再对所有评分水平求和拼接，经过一个非线性变换，就得到了用户$u_i$的特征表示$h_{u_i}$，物品的做法相同。 Bilinear Decoder在分别得到用户与物品的特征表示$U$与$V$后，解码器计算出用户对物品评分为$r$的概率，再对每个评分的概率进行求和，得到最终预测的评分。 \\begin{aligned} (P_r)_{ij}&=\\frac{\\exp(u_i^TQ_rv_j)}{\\sum_{s\\in R}\\exp(u_i^TQ_sv_j)} \\\\ \\hat{M}&=\\sum_{r\\in R}rP_r \\end{aligned}数据集Flixster、Douban、YahooMusic、MovieLens","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"Graph Neural Networks for Social Recommendation[WWW'19]","slug":"GraphRec[WWW19]","date":"2020-12-22T03:22:36.248Z","updated":"2020-12-22T03:22:36.248Z","comments":true,"path":"2020/12/22/GraphRec[WWW19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/GraphRec[WWW19]/","excerpt":"WWW19将GNN应用于社会化推荐的一篇论文","text":"WWW19将GNN应用于社会化推荐的一篇论文 解决的问题如何将GNN应用于社会化推荐任务上。 面临的挑战有三点： 在一个社会化推荐任务中，输入的数据包括社会关系图和用户-物品交互图，将两张图的信息都聚合才能得到用户更好的一个表示，而此前的GNN只是在同一张图上对邻域内的信息聚合。 在用户-物品交互图中，顶点与顶点之间的边也包含更多的信息，除了表示是否交互，还能表示用户对一个物品的偏好（喜爱还是厌恶），而此前的GNN只是将边用来表示是否交互。 社会关系图中用户之间的纽带有强有弱，显然地，一个用户更可能与强纽带的其它用户有类似的喜好。如果将所有纽带关系都看成一样，会有偏差。 做法及创新创新： 在不同图(user-user graph和user-item graph)上进行信息传递与聚合 除了捕获user-item间的交互关系，还利用了user对item的评分 用attention机制表示社交关系的重要性，用户纽带的强与弱 整个GraphRec框架由三个部分组成，分别为user modeling、item modeling和rating prediction。其中user modeling用来学习用户的特征表示，学习的方式是两个聚合：item aggregation和social aggregation，类似地item modeling用来学习物品的特征表示，学习的方式是一个聚合：user aggregation。 User Modelingitem aggregation item aggregation的目的是通过用户交互过的物品以及对这些物品的倾向，来学习物品侧的用户特征表示，数学表示为： h_i^I=\\sigma(W·Aggre_{items}(\\{x_{ia},\\forall a\\in C(i)\\})+b)$C(i)$就表示用户交互过的物品的一个集合。这里的$x_{ia}$是一个表示向量，它应该能够同时表示交互关系和用户倾向。论文中的做法是通过一个MLP来结合物品的embedding和倾向的embedding，两者分别用$q_a$和$e_r$表示。倾向的embedding可能很难理解，以五分制评分为例，倾向的embedding表示为$e_r\\in \\mathbb{R}^d$，其中$r\\in \\{1,2,3,4,5\\}$。 x_{ia}=g_v([q_a\\oplus e_r])定义好$x_{ia}$后，下一步就是如何选取聚合函数$Aggre$了。论文中使用的是attention机制，来源于GAT： \\begin{aligned} h_i^I&=\\sigma(W·\\Big\\{\\sum_{a\\in C(i)} \\alpha_{ia}x_{ia}\\Big\\}+b) \\\\ \\alpha_{ia}'&=w_2^T·\\sigma(W_1·[x_{ia}\\oplus p_i]+b_1)+b_2 \\\\ \\alpha_{ia}&=\\frac{\\exp(\\alpha_{ia}')}{\\sum_{a\\in C(i)}\\exp(\\alpha_{ia}')} \\end{aligned}这里的权重$\\alpha_{ia}$考虑了$x_{ia}$和用户$u_i$的embedding $p_i$，使得权重能够与当前用户相关。 social aggregation social aggregation中，同样地使用了attention机制，通过attention机制来选取强纽带的其它用户（表现为聚合时权重更大）并聚合他们的信息，聚合的就是物品侧的用户特征表示。 \\begin{aligned} h_i^S&=\\sigma(W·\\Big\\{\\sum_{o\\in N(i)} \\beta_{io}h_o^I\\Big\\}+b) \\\\ \\beta_{io}'&=w_2^T·\\sigma(W_1·[h_o^I\\oplus p_i]+b_1)+b_2 \\\\ \\beta_{io}&=\\frac{\\exp(\\beta_{io}')}{\\sum_{o\\in N(i)}\\exp(\\beta_{io}')} \\end{aligned}这里跟item aggregation基本一模一样，就不多介绍了。 得到物品侧的用户特征表示$h_i^I$和社交侧的用户特征表示$h_i^S$后，用一个MLP将它们结合，得到用户最终的特征表示： \\begin{aligned} c_1&=[h_i^I\\oplus h_i^S] \\\\ c_2&=\\sigma(W_2·c_1+b_2) \\\\ &······ \\\\ h_i&=\\sigma(W_l·c_{l-1}+b_l) \\end{aligned}Item Modelinguser aggregation Item modeling与User modeling的做法基本一模一样…公式都是一一对应的： \\begin{aligned} f_{jt}&=g_u([p_t\\oplus e_r]) \\\\ z_j&=\\sigma(W·\\Big\\{\\sum_{t\\in B(j)} \\mu_{jt}f_{jt}\\Big\\}+b) \\\\ \\mu_{jt}'&=w_2^T·\\sigma(W_1·[f_{jt}\\oplus q_j]+b_1)+b_2 \\\\ \\mu_{jt}&=\\frac{\\exp(\\mu_{jt}')}{\\sum_{a\\in C(i)}\\exp(\\mu_{jt}')} \\end{aligned}Rating Prediction最后来到评分预测部分，由上面两个部分我们得到了用户特征表示$h_i$与物品特征表示$z_j$，产生评分用的也是一个MLP： \\begin{aligned} g_1&=[h_i\\oplus z_j] \\\\ g_2&=\\sigma(W_2·g_1+b_2) \\\\ &······ \\\\ g_{l-1}&=\\sigma(W_l·g_{l-1}+b_l) \\\\ r_{ij}&=w^T·g_{l-1} \\end{aligned}数据集Ciao、Epinions","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR'19]","slug":"PPNP[ICLR19]","date":"2020-12-22T03:21:34.349Z","updated":"2020-12-22T03:21:34.349Z","comments":true,"path":"2020/12/22/PPNP[ICLR19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/PPNP[ICLR19]/","excerpt":"ICLR19将PageRank与GNN结合以解决GCN层数无法加深的一篇论文","text":"ICLR19将PageRank与GNN结合以解决GCN层数无法加深的一篇论文 解决的问题GCN层数增加后性能反而变差，如何加深GCN的层数。 根据GCN的定义，每一层网络用来捕获一跳邻居的信息，例如一个三层的GCN网络捕获的就是一个顶点三跳邻居以内的信息，而现在如果只能用浅层模型，表示只能捕获有限跳内的邻域信息，而有时候要多几跳才能捕获到有用的信息，例如JK-Net中的例子。 做法及创新这一篇论文的工作其实是接着JK-Net继续往下，在那篇论文中，作者分析了GCN中信息传递这个过程与随机漫步之间的关系，论证了当层数加深之后，GCN会收敛到这个随机漫步的极限分布，而这个极限分布只与图的全局属性有关，没有把随机漫步的起始顶点，或者说是GCN中从邻域中传递和聚合信息的根顶点考虑在内，这么一来，层数加深之后每个顶点聚合出来的样子都差不多，无法区分从而导致性能变差，另一个看待的角度是，因为原始GCN是对所有聚合的信息做平均操作，层数加深之后各个顶点的邻域都变得跟整张图差不多，既然每个顶点的邻域都变得差不多，做的又是平均操作，每个顶点聚合出来的样子就会都差不多。 论文提出的解决办法是引入PageRank的思想，这也是从JK-Net中的结论观察出来的。JK-Net中所说的GCN会收敛到的极限分布的计算方法如下： \\pi_{lim}=\\hat{A}\\pi_{lim}而PageRank的计算方法如下： \\pi_{pr}=A_{rw}\\pi_{pr}其中$A_{rw}=AD^{-1}$，两个计算方法明显地相似，区别在于，PageRank中邻接矩阵$A$没有考虑根顶点自身，而极限分布的计算里$\\hat{A}$是引入了自环的。而Personalized PageRank通过引入自环而考虑了根顶点自身，论文的想法就是将随机漫步的极限分布用Personalized PageRank来代替，它的计算方法为： \\pi_{ppr}(i_x)=(1-\\alpha)\\hat{A}\\pi_{ppr}(i_x)+\\alpha i_x \\\\ \\rightarrow \\pi_{ppr}(i_x)=\\alpha\\Big(I_n-(1-\\alpha)\\hat{A}\\Big)^{-1}i_x其中$i_x$是一个one_hot指示向量，用来从根顶点重新启动。 Personalized PageRank算法的目标是要计算所有节点相对于用户u的相关度。从用户u对应的节点开始游走，每到一个节点都以α的概率停止游走并从u重新开始，或者以1-α的概率继续游走，从当前节点指向的节点中按照均匀分布随机选择一个节点往下游走。这样经过很多轮游走之后，每个顶点被访问到的概率也会收敛趋于稳定，这个时候我们就可以用概率来进行排名了。 相较于原始的GCN模型，现在根顶点$x$对顶点$y$的影响程度$I(x,y)$，变得与$\\pi_{ppr}(i_x)$中的第$y$个元素相关，这个影响程度对于每个根顶点都有不同的取值： \\require{cancel} I(x,y)\\propto \\prod_{ppr}^{(yx)},\\prod_{ppr}^{(yx)}=\\alpha\\Big(I_n-(1-\\alpha)\\hat{A}\\Big)^{-1}\\cancel{I_{n}}PPNP经过上面的铺垫与介绍，论文提出的模型PPNP可以表示为： Z_{PPNP}=\\text{softmax}\\Big(\\alpha\\Big(I_n-(1-\\alpha)\\hat{A}\\Big)^{-1}H\\Big),H_{i,:}=f_{\\theta}(X_i,:)其中$X$为特征矩阵，$f_{\\theta}$是一个参数为$\\theta$的神经网络，用来产生预测类别$H\\in \\mathbb{R}^{n\\times c}$。 由公式和图中都可以看到，PPNP其实是由两部分组成，左边的神经网络与右边的信息传递网络，神经网络部分就类似于在GCN中介绍的，输入顶点特征与图的结构信息（邻接矩阵），输出顶点新的特征表示。信息传递网络部分，在PPNP中通过它来得到预测标签，而原始GCN的做法是$Z_{GCN}=\\text{softmax}(\\hat{A}HW)$，其中$W$是每层网络的参数。 APPNP从前面的构造方式可以看到，矩阵$\\prod_{ppr}$将会有$\\mathbb{R}^{n\\times n}$大小，会带来时间和空间上的复杂度。因此论文提出了一种近似的计算方法APPNP，计算方式如下： \\begin{aligned} Z^{(0)}&=H=f_{\\theta}(X) \\\\ Z^{(k+1)}&=(1-\\alpha)\\hat{A}Z^{(k)}+\\alpha H \\\\ Z^{(K)}&=\\text{softmax}\\Big((1-\\alpha)\\hat{A}Z^{(K-1)}+\\alpha H\\Big) \\end{aligned}其中$K$为信息传递的跳数或者说是随机漫步的步数，$k\\in[0,K-2]$，这样一来就不用构造一个$\\mathbb{R}^{n\\times n}$的矩阵了。（不知道为什么…） 数据集Citeseer、Cora-ML、Pubmed、MS Academic","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"DeepInf - Social Influence Prediction with Deep Learning[KDD'18]","slug":"DeepInf[KDD18]","date":"2020-12-22T03:18:21.401Z","updated":"2020-12-22T03:18:21.401Z","comments":true,"path":"2020/12/22/DeepInf[KDD18]/","link":"","permalink":"http://Bithub00.com/2020/12/22/DeepInf[KDD18]/","excerpt":"KDD18一篇将GNN应用于社交网络中用户影响力预测任务的论文","text":"KDD18一篇将GNN应用于社交网络中用户影响力预测任务的论文 解决的问题如何在图结构的社交数据中预测顶点的影响力。 在图中，给定顶点$v$与它的邻域以及一个时间段，通过对开始时各顶点的状态进行建模，来对结束时顶点$v$的状态进行预测（是否被激活）。 问题定义 邻域：给定图$G=(V,E)$，顶点$v$的邻域定义为$N_v^r=\\{u:d(u,v)\\le r\\}$，是一个顶点集合，不包含顶点$v$自身 中心网络：由邻域中的顶点及边所组成的网络，以$G_v^r$表示 用户行为：以$s_v^t$表示，用户对应于图中的顶点，对于一个时刻$t$，如果顶点$v$有产生动作，例如转发、引用等，则$s_v^t=1$ 给定用户$v$的中心网络、邻域中用户的行为集合$S_v^t=\\{s_i^t:i\\in N_v^r\\}$，论文想解决的问题是，在一段时间$Δt$后，对用户$v$的行为的预测： P(s_v^{t+Δt}|G_v^r,S_v^t)做法及创新数据预处理 数据预处理方面，论文通过带重启的随机漫步来为图中的每个顶点$v$获取固定大小$n$的中心网络$G_v^r$，接着使用$\\text{DeepWalk}$来得到图中顶点的embedding，最后进行归一化。通过这几个步骤对图中的特征进行提取后，论文还进一步添加了几种人工提取的特征，包括用户是否活跃等等： 摘要里说传统的影响力建模方法都是人工提取图中顶点及结构的特征，论文的出发点就是自动学习这种特征表示，结果在预处理的最后还是添加了几种人工提取的特征，这不是自相矛盾吗？ 经过上面的步骤后，最后得到包含所有用户特征的一个特征矩阵$H\\in \\mathbb{R}^{n\\times F}$，每一行$h_i^T$表示一个用户的特征，$F$等同于$\\text{DeepWalk}$长度加上人工特征长度。 影响力计算这一步纯粹是在套GAT的框架，没什么可以说的，计算如下： H'=\\text{GAT}(H)=g(A_{\\text{GAT}}(G)HW^T+b)\\\\ A_{\\text{GAT}}(G)=[a_{ij}]_{n\\times n}其中$W\\in \\mathbb{R}^{F’\\times F}, b\\in \\mathbb{R}^{F’}$是模型的参数，$a_{ij}$的计算在GAT论文的笔记中有记录，不再赘述。 数据集OAG、Digg、Twitter、Weibo","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"KGAT - Knowledge Graph Attention Network for Recommendation[KDD'19]","slug":"KGAT[KDD19]","date":"2020-12-22T03:16:39.766Z","updated":"2020-12-22T03:18:37.048Z","comments":true,"path":"2020/12/22/KGAT[KDD19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/KGAT[KDD19]/","excerpt":"KDD19一篇将知识图谱与GNN融合的论文","text":"KDD19一篇将知识图谱与GNN融合的论文 解决的问题在推荐系统中，如何将用户-物品交互信息与物品自身的属性相结合以做出更好的推荐，从另一个角度来说，即如何融合用户-物品交互图与知识图谱 以上面的图为例，在电影推荐场景中，用户对应于观众，物品对应于电影，实体Entities可以有多种含义，例如导演、演员、电影类别等，对应的就会有多种关系，对应图中的$r_1-r_4$。对于用户$u_1$，协同过滤更关注于他的相似用户，即同样看过$i_1$的$u_4$与$u_5$；而有监督学习方法例如因子分解机等会更关注物品之间的联系，例如$i_1$与$i_2$同样有着属性$e_1$，但它无法进一步建模更高阶的关系，例如图中黄色圈内的用户$u_2$与$u_3$观看了同一个导演$e_1$的电影$i_2$，而这名导演$e_1$又作为演员参演了灰色圈内的电影$i_3$与$i_4$。图中上半部分对应于用户-物品交互图，下半部分对应于知识图谱。 做法及创新 CKG Embedding Layer知识图谱的一般形式可以表示为三元组的集合$\\{(h,r,t)\\}$，表示头实体$h$与尾实体$t$之间有关系$r$，例如$\\text{(Hugh Jackman,ActorOf,Logan)}$表示狼叔是电影罗根的演员，这是一种主动的关系，自然就有逆向的被动关系。而对于用户-物品交互信息来说，通常的表示形式为一个矩阵$R$，$R_{ui}$表示用户$u$与物品$i$的关系，有交互则值为1，否则为0。因此，为了统一两种表示形式，论文中将用户-物品交互信息同样改成三元组的集合$\\text$，这样一来得到的统一后的新图称之为Collaborative Knowledge Graph(CKG)。 第一个步骤是对CKG做embedding，得到图中顶点和边的向量表示形式。论文使用了知识图谱中常用的一个方法$\\text{TransR}$，即对于一个三元组$(h,r,t)$，目标为： e_h^r+e_r\\approx e_t^r其中$e_h,e_t\\in \\mathbb{R}d、e_r\\in \\mathbb{R}k$分别为$h、t、r$的embedding，而$e_h^r,e_t^r$为$e_h、e_t$在$r$所处空间中的投影，损失函数定义为： g(h,r,t)=||W_re_h+e_r-W_re_t||^2_2值越小说明该三元组在知识图谱中更可能存在，即头实体$h$与尾实体$t$之间更可能有关系$r$。经过这一步骤之后，CKG中所有的顶点及边我们都得到了它们的embedding。 Attentive Embedding Propagation Layers第二个步骤直接用的GCN与GAT的想法，在一层embedding propagation layer中，利用图卷积网络在邻域中进行信息传播，利用注意力机制来衡量邻域中各邻居顶点的重要程度。再通过堆叠$l$层来聚合$l$阶邻居顶点的信息。 在每一层中，首先将顶点$h$的邻域以向量形式表示，系数$\\pi(h,r,t)$还会进行$\\text{softmax}$归一化： \\begin{aligned} e_{N_h}&=\\sum_{(h,r,t)\\in N_h}\\pi(h,r,t)e_t \\\\ \\pi(h,r,t)&=(W_re_t)^T\\text{tanh}\\big(W_re_h+e_r\\big) \\end{aligned}通过堆叠$l$层来聚合$l$阶邻居顶点的信息： \\begin{aligned} e_h^{(l)}&=f\\big( e_h^{(l-1)},e_{N_h}^{(l-1)} \\big) \\\\ &=\\text{LeakyReLU}\\big( W_1(e_h+e_{N_h})\\big)+\\text{LeakyReLU}\\big( W_2(e_h\\odot e_{N_h})\\big) \\end{aligned}论文中所使用的聚合函数$f$在GCN与GraphSage的基础上，还额外地引入了第二项中$e_h$与$e_{N_h}$的交互，这使得聚合的过程对于两者之间的相近程度更为敏感，会在更相似的顶点中传播更多的信息。 Model Prediction在得到$L$层embedding propagation layer的表示后，使用JK-Net中的LSTM-attention进行聚合，在通过点积的形式给出预测分数： e_u^*=\\text{LSTM-attention}(e_u^{(0)},e_u^{(L)})\\\\e_i^*=\\text{LSTM-attention()}e_i^{(0)}||\\dots||e_i^{(L)}\\\\ \\hat{y}(u,i)={e_u^*}^Te_i^*数据集Amazon-book、Last-FM、Yelp2018","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"Session-Based Recommendation with Graph Neural Networks[AAAI'19]","slug":"SRGCN[AAAI19]","date":"2020-12-22T03:13:14.168Z","updated":"2020-12-22T03:13:29.818Z","comments":true,"path":"2020/12/22/SRGCN[AAAI19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/SRGCN[AAAI19]/","excerpt":"AAAI19一篇将gated GNN应用于序列推荐任务的论文","text":"AAAI19一篇将gated GNN应用于序列推荐任务的论文 解决的问题在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\\rightarrow b \\rightarrow c$，只考虑$a\\rightarrow b$或者$b\\rightarrow c$ 做法及创新 Session Graph Modeling将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵： 上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\\in \\mathbb{R}^{n\\times 2n}$，其中的一行$A_{s,i:}\\in \\mathbb{R}^{1\\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$： Node Representation Learning论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。 GRU一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate： 直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\\in \\mathbb{R}^{n\\times d}$，上一时刻隐层表示$H_{t-1}\\in \\mathbb{R}^{n\\times h}$，重置门与更新门的输出由下式计算得到： R_t=\\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\\\ Z_t=\\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)式中的$W$与$b$分别为权重与偏置参数。 Reset Gate传统RNN网络的隐式状态更新公式为： H_t=\\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\\tilde{H_t}$称为候选状态： \\tilde{H_t}=\\tanh(X_tW_{xh}+(R_t\\odot{H_{t-1}})W_{hh}+b_h)Update Gate更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\\tilde{H_t}决定$： H_t=Z_t\\odot H_{t-1}+(1-Z_t)\\odot \\tilde{H_t}介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习： 最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。 Session Representation Generation现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\\text{s}=[v_{s,1},v_{s,2},\\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\\in \\mathbb{R}^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。 局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣： s_l=v_n全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\\text{soft-attention}$机制来计算得到： \\begin{aligned} s_g&=\\sum_{i=1}^{n}\\alpha_iv_i\\\\ \\alpha_i&=q^T\\sigma(W_1v_n+W_2v_i+c) \\end{aligned}最后使用一个$\\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量： s_h=W_3[s_l;s_g]Making Recommendation对于一个待推荐物品$v_i\\in V$，计算它在序列$s$中作为下一个被点击物品的概率： \\hat{y_i}=\\text{softmax}(s_h^Tv_i)数据集Yoochoose、Diginetica","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"Representation Learning on Graphs with Jumping Knowledge Networks[ICML'18]","slug":"JK-Net[ICML18]","date":"2020-12-22T03:11:56.918Z","updated":"2020-12-22T03:13:51.003Z","comments":true,"path":"2020/12/22/JK-Net[ICML18]/","link":"","permalink":"http://Bithub00.com/2020/12/22/JK-Net[ICML18]/","excerpt":"ICML18一篇解决GCN层数加深性能反而变差的论文","text":"ICML18一篇解决GCN层数加深性能反而变差的论文 解决的问题当图卷积网络GCN的层数超过两层时模型的表现会变差，这使得GCN只能作为浅层模型使用，且在对邻域节点的信息进行聚合时，即使同样是采用$k$层网络来聚合$k$跳邻居的信息，有着不同局部结构的顶点获得的信息也可能完全不同，以下图为例： 图$(a)$中的顶点位于核心区域，因此采用$4$层网络把几乎整个图的信息都进行聚合了，而不是它的邻域，这会导致过度平滑，而图$(b)$中顶点位于图边缘的一个树状结构中，采取同样的$4$层网络只囊括了一小部分顶点的信息，只有在第$5$层囊括了核心顶点之后才有效地囊括了更多顶点的信息。 所以，对于处于核心区域的顶点，GCN中每多一层即每多一次卷积操作，节点的表达会更倾向全局，这导致核心区域的很多顶点的表示到最后没有区分性。对于这样的顶点应该减少GCN的层数来让顶点更倾向局部从而在表示上可以区分；而处于边缘的顶点，即使更新多次，聚合的信息也寥寥无几，对于这样的顶点应该增加GCN的层数，来学习到更充分的信息。因此，对于不同的顶点应该选取不同的层数，传统做法对于所有顶点都用一个值会带来偏差。 做法及创新理论部分，论文主要讨论的问题是，在一个$k$层的GCN中，顶点$x$对顶点$y$的影响程度，即顶点$x$输入特征的改变，会对顶点$y$在最后一层得到的表示产生多大的变化，也可以说是顶点$y$对于顶点$x$有多敏感。假设输入的特征为$X\\in \\mathbb{R}^{n\\times f}$，输出的预测标签为$Z\\in \\mathbb{R}^{n\\times c}$，其中$n$为图中顶点数目，$c$为类别数目，$f$为特征数目，则这种影响程度可以表示为$I(x,y)=\\sum_i\\sum_j\\frac{\\partial Z_{yi}}{\\partial X_{xj}}$。 更特别地，论文证明了这个影响程度与从顶点$x$开始的$k$步随机漫步的分布有关，如果对$k$取极限$k\\rightarrow \\infty$，则随机漫步的分布会收敛到$P_{lim}(\\rightarrow y)$。详细论证过程可见原文。这说明，结果与随机漫步的的起始顶点$x$没有关系，通过这种方法来得到$x$的邻域信息是不适用的。 另一种说法是，一个$k$层的图卷积网络等同于一个$k$阶的多项式过滤器，其中的系数是预先确定的SDC。这么一个过滤器与随机漫步类似，最终会收敛到一个静态向量，从而导致过度平滑。 实践部分，论文提出JK-Net，通过Layer aggregation来让顶点最后的表示自适应地聚合不同层的信息，局部还是全部，让模型自己来学习： 论文的重点在于最后的Layer aggregation层，可选的三种操作为：Concat、Max-pooing以及LSTM-attn。 Concat 将各层的表示直接拼接在一起，送入Linear层。对于小数据集及结构单一的图这种聚合方式会更好，因为它们不需要顶点在聚合邻域的顶点信息时具有什么自适应性。 Max-pooling 选取各层的表示中包含信息量最多的作为顶点的最终表示，在多层结构中，低层聚合更多局部信息，而高层会聚合更多全局信息，因此对于核心区域内的顶点可能会选取高层表示而边缘顶点选取低层表示。 LSTM-attention 对于各层的表示，attention机制通过计算一个系数$s_v^{(l)}$来表示各层表示的重要性，其中$\\sum_ls_v^{(l)}=1$，顶点最终的表示就是各层表示的一个加权和：$\\sum_ls_v^{(l)}·h_v^{(l)}$。 $s_v^{(l)}$的计算：将$k$层网络各层的表示$h_v^{(1)},\\dots,h_v^{(k)}$输入一个双向LSTM中，同时生成各层$l$的前向LSTM与反向LSTM的隐式特征，分别表示为$f_v^{(l)}、b_v^{(l)}$，拼接后将$|f_v^{(l)}||b_v^{(l)}|$送入一个Linear层，将Linear层的结果进行Softmax归一化操作就得到了系数$s_v^{l}$。 数据集Citeseer、Cora、Reddit、PPI","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"Graph Attention Networks[ICLR'18]","slug":"GAT[ICLR18]","date":"2020-12-22T03:05:16.191Z","updated":"2020-12-22T03:14:01.743Z","comments":true,"path":"2020/12/22/GAT[ICLR18]/","link":"","permalink":"http://Bithub00.com/2020/12/22/GAT[ICLR18]/","excerpt":"ICLR18一篇解决GCN聚合信息时无法区分信息重要性的论文","text":"ICLR18一篇解决GCN聚合信息时无法区分信息重要性的论文 解决的问题如何将attention机制应用于图类型的数据上。 做法及创新图卷积 给定一个含$n$个顶点的图，其中顶点的特征构成的集合为$(\\overrightarrow{h_1},\\overrightarrow{h_2},\\dots,\\overrightarrow{h_n})$，$\\overrightarrow{h_i}\\in \\mathbb{R}^F$且邻接矩阵为$A$。一个图卷积层根据已有的顶点特征和图的结构来计算一个新的特征集合$(\\overrightarrow{h_1’},\\overrightarrow{h_2’},\\dots,\\overrightarrow{h_n’})$，$\\overrightarrow{h_i’}\\in \\mathbb{R}^{F’}$ 每个图卷积层首先会进行特征转换，以特征矩阵$W$表示，$W\\in \\mathbb{R}^{F’\\times F}$它将特征向量线性转换为$\\overrightarrow{g_i}=W\\overrightarrow{h_i}$，再将新得到的特征向量以某种方式进行结合。为了利用邻域的信息，一种典型的做法如下： \\overrightarrow{h_i}'=\\sigma\\bigg(\\sum_{j\\in N_i}\\alpha_{ij}\\overrightarrow{g_j}\\bigg)其中$N_i$表示顶点$i$的邻域（典型的构造方式是选取直接相连的顶点，包括自身），$\\alpha_{ij}$表示顶点$j$的特征对于顶点$i$的重要程度，也可以看成一种权重。 现有的做法都是显式地定义$\\alpha_{ij}$，本文的创新之处在于使用attention机制隐式地定义$\\alpha_{ij}$。所使用的attention机制定义为$a:R^{F’}\\times \\mathbb{R}^{F’} \\rightarrow \\mathbb{R}$，以一个权重向量$\\overrightarrow{a}\\in \\mathbb{R}^{2F’}$表示，对应于论文中的self-attention。 Self-attention 基于顶点的特征计算系数$e_{ij}$ e_{ij}=a(W\\overrightarrow{h_i},W\\overrightarrow{h_j}) 以顶点的邻域将上一步计算得到的系数正则化，这么做能引入图的结构信息： \\begin{aligned} \\alpha_{ij}&=\\text{softmax}_j(e_{ij})=\\frac{\\exp(e_{ij})}{\\sum_{k\\in N_i}\\exp(e_{ik})}\\\\ &=\\frac{\\exp(\\text{LeakyReLU}(\\overrightarrow{a}^T[W\\overrightarrow{h}_i||W\\overrightarrow{h}_j]))}{\\sum_{k\\in N_i}\\exp(\\text{LeakyReLU}(\\overrightarrow{a}^T[W\\overrightarrow{h}_i||W\\overrightarrow{h}_k]))} \\end{aligned} 次序不变性：给定$(i,j),(i,k),(i’,j),(i’,k)$表示两个顶点间的关系，可以为边或自环。$a$为对应的attention系数，如果$a_{ij}&gt;a_{ik}$，则有$a_{i’j}&gt;a_{i’k}$ ​ DeepInf中给出了证明： ​ 将权重向量$\\overrightarrow{a}\\in \\mathbb{R}^{2F’}$重写为$\\overrightarrow{a}=[p^T，q^T]$，则有 e_{ij}=\\text{LeakyReLU}(p^TWh_i+q^TWh_j)​ 由softmax与LeakyReLU的单调性可知，因为$a_{ij}&gt;a_{ik}$，有$q^TWh_j&gt;q^TWh_k$，类似地就可以得到$a_{i’j}&gt;a_{i’k}$。 ​ 这意味着，即使每个顶点都只关注于自己的邻域，但得到的attention系数却具有全局性。 以上一步得到的系数$\\alpha_{ij}$作为顶点$j$的特征对顶点$i$的重要程度，将领域中各顶点的特征做一个线性组合以作为顶点$i$最终输出的特征表示： \\overrightarrow{h_i'}=\\sigma\\bigg(\\sum_{j\\in N_i}\\alpha_{ij}W\\overrightarrow{h_j}\\bigg)Multi-head attention为了稳定self-attention的学习过程，论文引入了multi-head attention，即由$K$个相互独立的self-attention得到各自的特征，再进行拼接： \\overrightarrow{h_i'}=\\Vert_{k=1}^K\\sigma\\bigg(\\sum_{j\\in N_i}\\alpha_{ij}^kW^k\\overrightarrow{h_j}\\bigg)其中$\\alpha_{ij}^k$是第$k$个attention机制$(a^k)$计算出来的正则化系数，$W^k$是对应的将输入进行线性转化的权重矩阵。论文选取的拼接操作为求平均： \\overrightarrow{h_i'}=\\sigma\\bigg(\\frac{1}{K}\\sum_{k=1}^K\\sum_{j\\in N_i}\\alpha_{ij}^kW^k\\overrightarrow{h_j}\\bigg)数据集Cora、Citeseer、Pubmed、PPI","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"Learning Convolutional Neural Networks for Graphs[ICML'16]","slug":"Learning-Convolutional-Neural-Networks-for-Graphs[ICML16]","date":"2020-12-22T03:04:15.061Z","updated":"2020-12-22T03:14:11.624Z","comments":true,"path":"2020/12/22/Learning-Convolutional-Neural-Networks-for-Graphs[ICML16]/","link":"","permalink":"http://Bithub00.com/2020/12/22/Learning-Convolutional-Neural-Networks-for-Graphs[ICML16]/","excerpt":"ICML16一篇将CNN应用到图数据上的论文","text":"ICML16一篇将CNN应用到图数据上的论文 解决的问题卷积神经网络都是应用在图像数据上，如何将它有效地应用于图类型的数据上。 对于图像数据，应用一个卷积神经网络可以看成将receptive field（图中为$3\\times3$）以固定的步长将图像遍历，因为图像中像素点的排列有一定的次序，receptive field的移动顺序总是从上到下，从左到右。这也唯一地决定了receptive field对一个像素点的遍历方式以及它如何被映射到向量空间中。 然而对于图结构数据这种隐式的结构特征很多时候是缺失的，而且当给定不止一张图时，各个图之间的顶点没有必然的联系。因此，在将卷积神经网络应用在图数据上时，需要解决下面两个问题： 决定邻域中顶点的产生次序 计算一个将图映射到向量空间的映射方法 做法及创新论文提出方法的流程如下： Node Sequence Selection从图中选取固定数量$w$的顶点，它类比于图像的宽度，而选出的顶点就是卷积操作中小矩形的中心顶点。$w$就是在这个图上所做的卷积操作的个数。如下图所示，$w=6$，代表需要从图中选择6个顶点做卷积操作。论文中选取顶点的方式为$\\text{DFS}$，关键点在于图标签函数$l$，这个函数的作用是决定选取顶点的次序，可以选区的函数为between centrality与WL算法等等 Neighborhood Assembly选取完顶点后，下一步是为它们构建receptive field，类似于第一张图中的$3\\times3$矩阵。选取的方式为，以顶点$v$为中心，通过$\\text{BFS}$添加领域顶点，直到满足receptive field长度$k$： Graph Normalization在选取了满足数量的邻域顶点后，下一步是通过图标签函数$l$为这些顶点赋予一个次序，目的在于将无序的领域映射为一个有序的向量： Convolutional Architecture最后一步就是应用卷积层提取特征，顶点和边的属性对应于传统图像CNN中的channel： 假设顶点特征的数目为$a_v$，边的特征个数为$a_e$，$w$为选取的顶点个数，$k$为receptive field中的顶点个数，则对于输入的一系列图中的每一个，可以得到两个张量维度分别为$(w,k,a_v)、(w,k,k,a_e)$，可以变换为$(wk,a_v)、(wk^2,a_e)$，其中$a_v$与$a_e$可以看成是传统图像卷积中channel的个数，对它们做一维的卷积操作，第一个的receptive field的大小为$k$，第二个的receptive field的大小为$k^2$。 整体卷积结构： 数据集MUTAG、PTC、NCI、D&amp;D","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"Inductive Representation Learning on Large Graphs[NIPS'17]","slug":"GraphSage[NIPS17]","date":"2020-12-22T03:03:07.830Z","updated":"2020-12-22T03:14:22.897Z","comments":true,"path":"2020/12/22/GraphSage[NIPS17]/","link":"","permalink":"http://Bithub00.com/2020/12/22/GraphSage[NIPS17]/","excerpt":"NIPS17一篇解决GCN不能泛化到未知顶点的论文","text":"NIPS17一篇解决GCN不能泛化到未知顶点的论文 解决的问题对于学习图上顶点的embedding，现有的方法多为直推式学习，学习目标是直接生成当前顶点的embedding，不能泛化到未知顶点上 做法及创新论文提出一种归纳式学习方法GrdaphSAGE，不为每个顶点学习单独的embedding，而是学习一种聚合函数$\\text{AGGREGATE}$，从一个顶点的局部邻域聚合特征信息，为未知的顶点直接生成embedding，因此旧的顶点只要邻域发生变化也能得到一个新的embedding GCN不是归纳式，因为每次迭代会用到整个图的邻接矩阵$A$；而GraphSAGE可以对GCN做了精简，每次迭代只抽样取直接相连的邻居 算法流程 给定顶点$v$及其特征$x_v$,作为它的初始表示$h_v^0=x_v$。 计算邻域向量$h^k_{N(v)}=\\text{AGGREGATE}({h_u^{(k-1)}}, \\forall u\\in N(v))$，当前层顶点的邻居从上一层采样，且邻居个数固定，非所有邻居，这样每个顶点和采样后邻居的个数都相同，可以直接拼成一个batch送到GPU中进行批训练 将邻域向量与自身上一层的表示拼接，通过非线性激活函数$\\sigma$后作为这一层的表示$h_v^k=\\sigma(W^k\\text{CONCAT}(h_v^{(k-1)},h^k_{N(v)})$ 标准化 $h_v^k=h_v^k/||h_v^k||_2$ 聚合函数 MEAN h_v^k=\\sigma(W·\\text{MEAN}(\\{h_v^{k-1}\\}\\cup\\{h_u^{k-1},\\forall u\\in N(v) \\}) LSTM PoolingGraphSAGE采用的max-pooling策略能够隐式地选取领域中重要的顶点： \\text{AGGREGATE}_k^{pool}=\\text{max}(\\{\\sigma(W_{pool}h_u^k + b),\\forall u\\in N(v)\\})数据集BioGRID、Reddit","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"Simplifying Graph Convolutional Networks[PMLR'19]","slug":"SGCN[PMLR19]","date":"2020-12-22T03:01:38.585Z","updated":"2020-12-22T03:14:33.774Z","comments":true,"path":"2020/12/22/SGCN[PMLR19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/SGCN[PMLR19]/","excerpt":"PMLR19一篇简化GCN架构的论文","text":"PMLR19一篇简化GCN架构的论文 解决的问题图卷积网络中可能引入了一些不必要的复杂性及冗余的计算 做法及创新 移除图卷积网络各层之间的非线性关系，合并各层之间的权重矩阵 原始图卷积网络对于一个输入的图，图卷积网络利用多层网络为每个顶点的特征$x_i$学习一个新的特征表示，随即输入一个线性分类器。对第$k$层网络，输入为$H^{(k-1)}$，输出为$H^{(k)}$，其中$H^{(0)}=X$。一个$K$层的图卷积网络等价于对图中每个顶点的特征向量$x_i$应用一个$K$层感知机，不同之处在于顶点的隐层表示local averaging： h_i^{(k)}\\leftarrow \\frac{1}{d_i+1}h_i^{(k-1)}+\\sum^n_{j=1}\\frac{a_{ij}}{\\sqrt{(d_i+1)(d_j+1)}}h_j^{(k-1)}矩阵形式： S=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}其中$A=A+I$，则隐层表示用矩阵的形式表示为： H^{(k)}\\leftarrow SH^{(k-1)}Local averaging：this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes $\\Theta^{(k)}$为第$K$层网络的权重矩阵： H^{(k)}\\leftarrow \\text{ReLU}(H^{(k)}\\Theta^{(k)})$Y\\in \\mathbb{R}^{n\\times C}$，$y_{ic}$表示第$i$个顶点属于类别$C$的概率 Y_{GCN}=\\text{softmax}(SH^{(K-1)}\\Theta^{(K)})简化图卷积网络 在传统的多层感知机中，多层网络可以提高模型的表现力，是因为这样引入了特征之间的层级关系，例如第二层网络的特征是以第一层网络为基础构建的。而在图卷积网络中，这还有另外一层含义，在每一层中顶点的隐层表示都是以一跳的邻居进行平均，经过$K$层之后，一个顶点就能获得$K$跳邻居的特征信息。这类似于在卷积网路中网络的深度提升了特征的receptive field。 保留local averaging，移除了非线性激活函数： Y=\\text{softmax}(S^KX\\Theta^{(1)}\\dots \\Theta^{(K)})其中$S^K$可以预先进行计算，大大减少了模型的训练时间 论文中证明了简化后的图卷积网络等价于谱空间的一个低通滤波器，它通过的低频信号对应于图中平滑后的特征 数据集Cora、Citeseer、Pubmed、Reddit","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"LightGCN - Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR'20]","slug":"LightGCN[SIGIR20]","date":"2020-12-22T03:00:34.998Z","updated":"2020-12-22T03:23:10.944Z","comments":true,"path":"2020/12/22/LightGCN[SIGIR20]/","link":"","permalink":"http://Bithub00.com/2020/12/22/LightGCN[SIGIR20]/","excerpt":"SIGIR20一篇简化GCN架构的论文","text":"SIGIR20一篇简化GCN架构的论文 解决的问题在协同过滤中，图卷积网络中的特征转换与非线性激活对提升模型表现贡献很小，甚至有负面影响。 在半监督顶点分类问题中，每个顶点有充分的语义特征作为输入，例如一篇文章的标题与摘要词，这种情况下加入多层的非线性特征转换能够有助于学习特征。而在协同过滤任务中，每个顶点（用户或商品）没有这么充分的语义特征，因此没有多大的作用。 做法及创新 \\hat{y}_{ui}=e_u^Te_i \\\\ e_u=\\sum_{k=0}^K\\alpha_ke_u^{(k)} \\\\ e_i=\\sum_{k=0}^K\\alpha_ke_u^{(k)} \\\\ e_u^{(k+1)}=\\sum_{i\\in N_u}\\frac{1}{\\sqrt{|N_u||N_i|}}e_i^{(k)} \\\\ e_i^{(k+1)}=\\sum_{i\\in N_i}\\frac{1}{\\sqrt{|N_i||N_u|}}e_u^{(k)} 仅考虑图卷积网络中的neighborhood aggregation，通过在用户-物品交互网络中线性传播来学习用户和物品的embedding，再通过加权和将各层学习的embedding作为最后的embedding 通过减少不必要的架构，相较于NGCF大大减少了需要训练的参数量。唯一需要训练的模型参数是第0层的embedding，即$e_u^{(0)}$与$e_i^{(0)}$，当它们两个给定后，后续层的embedding可以通过传播规则直接进行计算 以加权和的方式结合各层的embedding等价于带自连接的图卷积 \\begin{aligned} E^{(K)}&=(A+I)E^{(K-1)}=(A+I)^KE^{(0)}\\\\ &=C_K^0E^{(0)}+C_K^1AE^{(0)}+C_K^2A^2E^{(0)}+\\dots+C_K^KA^KE^{(0)} \\end{aligned} 模型的可解释性更强，以二层网络为例: e_u^{(2)}=\\sum_{i\\in N_u}\\frac{1}{\\sqrt{|N_u||N_i|}}e_i^{(1)}=\\sum_{i\\in N_u}\\frac{1}{|N_i|}\\sum_{v\\in N_i}\\frac{1}{\\sqrt{|N_u||N_v|}}e_v^{(0)}如果另一个用户$v$与目标用户$u$有关联，则影响以下面的系数表示： c_{v\\rightarrow u}=\\frac{1}{\\sqrt{|N_u||N_v|}}\\sum_{i\\in N_u\\cap N_v}\\frac{1}{|N_i|}可解释为: 共同交互过的物品越多系数越大 $i\\in N_u\\cap N_v$ 物品流行度越低系数越大$\\frac{1}{|N_i|}$ 用户$v$越不活跃系数越大$\\frac{1}{|N_v|}$ 数据集Gowalla、Yelp2018、Amazon-Book","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"Neural Graph Collaborative Filtering[SIGIR'19]","slug":"NGCF[SIGIR19]","date":"2020-12-22T02:59:24.118Z","updated":"2020-12-22T03:19:20.380Z","comments":true,"path":"2020/12/22/NGCF[SIGIR19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/NGCF[SIGIR19]/","excerpt":"SIGIR19年将神经网络与协同过滤结合的一篇论文","text":"SIGIR19年将神经网络与协同过滤结合的一篇论文 解决的问题在现有的推荐模型中，用户和物品的embedding只考虑了它们自身的特征，没有考虑用户-物品的交互信息 做法及创新 \\hat{y}_{NGCF}(u,i)={e^*_u}^Te^*_i \\\\ e^*_u = e_u^{(0)}||\\dotsb||e_u^{(L)} \\\\ e^*_i = e_i^{(0)}||\\dotsb||e_i^{(L)} \\\\ e_u^{(l)}=LeakyReLU(m^{(l)}_{u\\leftarrow u}+\\sum_{i\\in N_u}m^{(l)}_{u\\leftarrow i}) \\\\ \\begin{cases} m^{(l)}_{u\\leftarrow i}=p_{ui}(W_1^{(l)}e_i^{(l-1)}+W_2^{(l)}(e_i^{(l-1)}\\odot e_u^{(l-1)})) \\\\\\\\ m^{(l)}_{u\\leftarrow u}=W_1^{(l)}e_u^{(l-1)} \\end{cases} \\\\ m_{u\\leftarrow i}=\\frac{1}{\\sqrt{|N_u||N_i|}}(W_1e_i+W_2(e_i\\odot e_u)) 通过堆叠$l$层embedding传播层，一个用户（物品）可以获得它的$l$跳邻居所传播的信息，如下图所示，通过这种方法来建模用户-物品交互信息中的高阶connectivity，下图展示的是一个三阶的例子: 传统GCN推荐方法中，message embedding只考虑物品embedding$e_i$，论文中将用户embedding与物品embedding的交互也纳入考虑，解释为“This makes the message dependent on the affinity between $e_i$ and $e_u$, e.g., passing more messages from the similar items.” 两个层面上的dropout：message &amp; node dropout。前者表示在第$l$层传播层中，只有部分信息会对最后的表示有贡献；后者表示在第$l$层传播层中，随机地丢弃一些顶点。 数据集Gowalla、Yelp2018、Amazon-book","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"Semi-Supervised Classification with Graph Convolutional Network [ICLR'17]","slug":"GCN[ICLR17]","date":"2020-12-22T02:57:04.922Z","updated":"2020-12-22T03:26:43.534Z","comments":true,"path":"2020/12/22/GCN[ICLR17]/","link":"","permalink":"http://Bithub00.com/2020/12/22/GCN[ICLR17]/","excerpt":"GCN的原始论文，发表于2017年的ICLR会议","text":"GCN的原始论文，发表于2017年的ICLR会议 解决的问题如何将神经网络应用在图结构数据上？ 问题描述给定以下输入： 图中顶点的特征矩阵$H\\in \\mathbb{R}^{n\\times F}$，其中$n$为顶点数量，$F$为特征数量 图的结构信息，如邻接矩阵$A$ 输出： 图中顶点新的的特征表示$H’\\in \\mathbb{R}^{n\\times F’}$，即 H'=\\text{GCN(H)}=g(AHW^T+b)如果套用神经网络模型，每一层可以用一个非线性函数进行表示： H^{(l+1)}=f(H^{(l)},A)其中$H^{(0)}=X,H^{(L)}=Z$，问题在于如何选取函数$f(.,.)$ 做法及创新对于函数$f(.,.)$的选取，论文中提出了一种可能的函数形式： f(H^{(l)},A)=\\sigma(\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}) \\\\其中$\\hat{A}=A+I$，因为与矩阵$A$相乘表示对于每个顶点，我们对除了自身外所有邻居顶点的特征向量进行求和，因此加上单位矩阵是为了引入自环。而正则化是避免与矩阵$A$相乘改变特征向量的规模。实际在论文中只使用两层网络就达到了很好的效果，表示为： Z_{\\text{GCN}}=\\text{softmax}\\big(\\hat{A'}\\text{ReLU}\\big(\\hat{A'}XW_0\\big)W_1\\big)其中$W_0、W_1$为这两层网络的参数，$\\hat{A’}=\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$，$Z\\in R\\mathbb{R}{n\\times c}$为预测的顶点标签，$c$为类别数目，毕竟论文解决的就是一个分类问题。 更一般地，使用邻域信息的图神经网络形式可以概括为： h_v^{(l)}=\\sigma\\bigg(W_l·\\text{AGGREGATE}\\bigg(\\{h_u^{(l-1)},\\forall u\\in N(v)\\} \\bigg)\\bigg)其中$W_l$是第$l$层网络的权重矩阵，$\\text{AGGREGATE}$是与特定模型相关的聚合函数，$h_v^{(l)}$是顶点$v$在第$l$层的隐层特征表示。论文中只是用了一个两层网络就达到了很好的效果。 将论文所提出的函数改写为上述形式，即为： h_v^{(l)}=\\text{ReLU}\\Big(W_l·\\sum_{u\\in N(v)}(deg(v)deg(u))^{-1/2}h_u^{(l-1)}\\Big)其中$deg(u)$为顶点$u$的度。 AS-GCN中对这篇论文的模型形式描述如下： h_{v_i}^{(l)}=\\sigma\\Big(W_l·\\sum_{j=1}^Na(v_i,u_j)·h_{u_j}^{(l-1)}\\Big),i=1,\\dots,N这里$A=(a(v_i,u_j))\\in \\mathbb{R}^{N\\times N}$对应前面一种写法的正则化邻接矩阵$\\hat{A’}$，表面上看对于顶点$v_i$，需要考虑将图中剩下的所有顶点的上一时刻的隐层表示做加权和，来作为它当前时刻的隐层表示，因为$j$的取值范围为$[1,N]$，$N$就是图中顶点的数量。但实际上，大多数顶点因为与$v_i$并无边相连，所以邻接矩阵中对应的值为0，意味着在加权和中的权重为0，相当于加权和时只会考虑有边相连的顶点，这同样是考虑邻域，只不过跟上面那种写法不同。","categories":[],"tags":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://Bithub00.com/tags/图神经网络/"}]},{"title":"Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR'19]","slug":"GPUIR[SIGIR19]","date":"2020-12-22T02:55:32.621Z","updated":"2020-12-22T03:19:39.414Z","comments":true,"path":"2020/12/22/GPUIR[SIGIR19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/GPUIR[SIGIR19]/","excerpt":"SIGIR19年有关GPU加速的一篇论文","text":"SIGIR19年有关GPU加速的一篇论文 解决的问题FEXIPRO[SIGMOD’17]中的对IPR问题的求解较慢，可以使用GPU进行并行加速。 IPR问题给定一个用户矩阵$Q\\in \\mathbb{R}^{d\\times m}$以及一个物品矩阵$P\\in \\mathbb{R}^{d\\times n}$，对于$Q$中的每一个用户$q$，返回内积$q^TP$中的前k个$q^Tp$对应的物品列表$p$ 做法及创新行文逻辑作者首先画出四个数据集上，SeqScan与FEXIPRO中两个步骤（内积计算与Top-k物品获取）的运行时间占比，发现内积计算占了总开销的90%以上，促使他提出方法加速这一步骤。接下来介绍GPU加速CPU程序的流程，提出了第一个改进方法，即分batch将矩阵送入GPU并行地计算内积。下一步同样地画出它各个步骤的运行时间占比，发现现在top-k物品的获取以及将内积结果从GPU内存复制到CPU内存这两个步骤变成了时间开销的大头。于是顺着分析结果提出了两个改进方法针对性地减小这两个步骤的时间开销。 贡献前后提出了三个改进方法：GPU-IP、GPU-IPR、GPU-IPRO，分别为：GPU-IP：1GPU-IPR：1+2GPU-IRPO：1+2+3 并行计算$Q^TP$，并且提出了一种新的矩阵分割方法以充分利用GPU内存，从而加速内积的计算 给定GPU内存为$M$，各自选取用户矩阵与物品矩阵的子集$Q_s\\in Q,P_s\\in P$使得$Size(Q_s^TP_s)\\le M$，论文的做法是取$Q_s=Q$，通过$Size(Q^TP_s)=M$来选取$P_s$的大小 为每一个用户指定最佳的内积数量$g_s$为1024，从这1024个计算结果中返回top-k，减少了待排序的数据规模 内积数量会严重影响下一步的Bitonic排序的性能。选取的依据是它应该满足每一个线程组的共享内存大小因为它会在GPU缓存层级关系中带来最小的缓存访问延迟(The size of $g_s$ should fit in the sharedmemory of each threads group as it incurs minimum cache access latency in GPU cache hierarchy.) 提出了一种剪枝方法来提前结束计算进程，减少了许多内积计算假设用户$u$与其第$k$大的物品的内积为$S_k$，且$||q||\\cdot||p||\\le S_k$，则有 $q^Tp \\le ||q||\\cdot||p||\\le S_k$，因为目的是得到top-k物品，满足上述不等式的物品已经被排除在top-k之外，不需要送入下一次迭代进行内积计算 使用这种剪枝方法后，在四个数据集的前10次迭代中，分别减少了98.88%、76.61%、88.69%以及1.57%的用户数量。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://Bithub00.com/tags/数据库/"}]},{"title":"ARROW - Approximating Reachability using Random walks Over Web-scale Graphs[ICDE'19]","slug":"ARROW[ICDE19]","date":"2020-12-22T02:54:21.766Z","updated":"2020-12-22T03:18:45.172Z","comments":true,"path":"2020/12/22/ARROW[ICDE19]/","link":"","permalink":"http://Bithub00.com/2020/12/22/ARROW[ICDE19]/","excerpt":"ICDE19年的一篇论文","text":"ICDE19年的一篇论文 解决的问题给定一个有向图$G=(V,E)$以及一系列顶点对$(u,v)$,判断两个顶点之间是否连通，对应两种查询情况： Chained Queries：查询路径上每条边开始于上一条边结束，并且总时间在规定的范围内 Snapshot Queries：对于一个动态变化的图，在给定的时间范围内至少在$c$个快闪图中存在连通 创新之处 以起始顶点和目的顶点各自进行多次固定长度的随机漫步构建两个集合，两个集合的交集非空说明连通 对于随机漫步的长度及次数的选取给出了理论证明2.1 随机漫步的长度：有向图中最长的最短路径的长度，通过10次的深度优先搜索得到2.2 随机漫步的次数：类比于扔球问题，给定$n$个篮子和数量相等的红球与蓝球，需要扔多少个球来保证有一个篮子中同时有红球与蓝球的概率高？红球可以看作起始顶点$u$可以到达的顶点，蓝球可以看作目的顶点$v$可以到达的顶点 模型的一个假设前提是构建的两个反向的随机漫步的平稳分布应尽可能接近，这对应于正向随机漫步选定一个出边的概率与反向随机漫步选定一个入边的概率相等，而这个概率恰等于顶点度的倒数。使用同配性(assortativity)作为这两个平稳分布接近程度的指标。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://Bithub00.com/tags/数据库/"}]},{"title":"论文学习","slug":"论文学习","date":"2020-09-25T00:27:51.106Z","updated":"2021-01-09T13:31:19.013Z","comments":true,"path":"2020/09/25/论文学习/","link":"","permalink":"http://Bithub00.com/2020/09/25/论文学习/","excerpt":"对看过的论文做一个记录","text":"对看过的论文做一个记录 数据库ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]解决的问题给定一个有向图$G=(V,E)$以及一系列顶点对$(u,v)$,判断两个顶点之间是否连通，对应两种查询情况： Chained Queries：查询路径上每条边开始于上一条边结束，并且总时间在规定的范围内 Snapshot Queries：对于一个动态变化的图，在给定的时间范围内至少在$c$个快闪图中存在连通 创新之处 以起始顶点和目的顶点各自进行多次固定长度的随机漫步构建两个集合，两个集合的交集非空说明连通 对于随机漫步的长度及次数的选取给出了理论证明 2.1 随机漫步的长度：有向图中最长的最短路径的长度，通过10次的深度优先搜索得到 2.2 随机漫步的次数：类比于扔球问题，给定$n$个篮子和数量相等的红球与蓝球，需要扔多少个球来保证有一个篮子中同时有红球与蓝球的概率高？红球可以看作起始顶点$u$可以到达的顶点，蓝球可以看作目的顶点$v$可以到达的顶点 模型的一个假设前提是构建的两个反向的随机漫步的平稳分布应尽可能接近，这对应于正向随机漫步选定一个出边的概率与反向随机漫步选定一个入边的概率相等，而这个概率恰等于顶点度的倒数。使用同配性(assortativity)作为这两个平稳分布接近程度的指标。 Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR’19]解决的问题FEXIPRO[SIGMOD’17]中的对IPR问题的求解较慢，可以使用GPU进行并行加速。 IPR问题给定一个用户矩阵$Q\\in \\mathbb{R}^{d\\times m}$以及一个物品矩阵$P\\in \\mathbb{R}^{d\\times n}$，对于$Q$中的每一个用户$q$，返回内积$q^TP$中的前k个$q^Tp$对应的物品列表$p$ 做法及创新行文逻辑作者首先画出四个数据集上，SeqScan与FEXIPRO中两个步骤（内积计算与Top-k物品获取）的运行时间占比，发现内积计算占了总开销的90%以上，促使他提出方法加速这一步骤。接下来介绍GPU加速CPU程序的流程，提出了第一个改进方法，即分batch将矩阵送入GPU并行地计算内积。下一步同样地画出它各个步骤的运行时间占比，发现现在top-k物品的获取以及将内积结果从GPU内存复制到CPU内存这两个步骤变成了时间开销的大头。于是顺着分析结果提出了两个改进方法针对性地减小这两个步骤的时间开销。 贡献前后提出了三个改进方法：GPU-IP、GPU-IPR、GPU-IPRO，分别为：GPU-IP：1GPU-IPR：1+2GPU-IRPO：1+2+3 并行计算$Q^TP$，并且提出了一种新的矩阵分割方法以充分利用GPU内存，从而加速内积的计算 给定GPU内存为$M$，各自选取用户矩阵与物品矩阵的子集$Q_s\\in Q,P_s\\in P$使得$Size(Q_s^TP_s)\\le M$，论文的做法是取$Q_s=Q$，通过$Size(Q^TP_s)=M$来选取$P_s$的大小 为每一个用户指定最佳的内积数量$g_s$为1024，从这1024个计算结果中返回top-k，减少了待排序的数据规模 内积数量会严重影响下一步的Bitonic排序的性能。选取的依据是它应该满足每一个线程组的共享内存大小因为它会在GPU缓存层级关系中带来最小的缓存访问延迟(The size of $g_s$ should fit in the sharedmemory of each threads group as it incurs minimum cache access latency in GPU cache hierarchy.) 提出了一种剪枝方法来提前结束计算进程，减少了许多内积计算假设用户$u$与其第$k$大的物品的内积为$S_k$，且$||q||\\cdot||p||\\le S_k$，则有 $q^Tp \\le ||q||\\cdot||p||\\le S_k$，因为目的是得到top-k物品，满足上述不等式的物品已经被排除在top-k之外，不需要送入下一次迭代进行内积计算 使用这种剪枝方法后，在四个数据集的前10次迭代中，分别减少了98.88%、76.61%、88.69%以及1.57%的用户数量。 图神经网络Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]解决的问题如何将神经网络应用在图结构数据上？ 问题描述给定以下输入： 图中顶点的特征矩阵$H\\in \\mathbb{R}^{n\\times F}$，其中$n$为顶点数量，$F$为特征数量 图的结构信息，如邻接矩阵$A$ 输出： 图中顶点新的的特征表示$H’\\in \\mathbb{R}^{n\\times F’}$，即 H'=\\text{GCN(H)}=g(AHW^T+b)如果套用神经网络模型，每一层可以用一个非线性函数进行表示： H^{(l+1)}=f(H^{(l)},A)其中$H^{(0)}=X,H^{(L)}=Z$，问题在于如何选取函数$f(.,.)$ 做法及创新对于函数$f(.,.)$的选取，论文中提出了一种可能的函数形式： f(H^{(l)},A)=\\sigma(\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}) \\\\其中$\\hat{A}=A+I$，因为与矩阵$A$相乘表示对于每个顶点，我们对除了自身外所有邻居顶点的特征向量进行求和，因此加上单位矩阵是为了引入自环。而正则化是避免与矩阵$A$相乘改变特征向量的规模。实际在论文中只使用两层网络就达到了很好的效果，表示为： Z_{\\text{GCN}}=\\text{softmax}\\big(\\hat{A'}\\text{ReLU}\\big(\\hat{A'}XW_0\\big)W_1\\big)其中$W_0、W_1$为这两层网络的参数，$\\hat{A’}=\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$，$Z\\in R\\mathbb{R}{n\\times c}$为预测的顶点标签，$c$为类别数目，毕竟论文解决的就是一个分类问题。 更一般地，使用邻域信息的图神经网络形式可以概括为： h_v^{(l)}=\\sigma\\bigg(W_l·\\text{AGGREGATE}\\bigg(\\{h_u^{(l-1)},\\forall u\\in N(v)\\} \\bigg)\\bigg)其中$W_l$是第$l$层网络的权重矩阵，$\\text{AGGREGATE}$是与特定模型相关的聚合函数，$h_v^{(l)}$是顶点$v$在第$l$层的隐层特征表示。论文中只是用了一个两层网络就达到了很好的效果。 将论文所提出的函数改写为上述形式，即为： h_v^{(l)}=\\text{ReLU}\\Big(W_l·\\sum_{u\\in N(v)}(deg(v)deg(u))^{-1/2}h_u^{(l-1)}\\Big)其中$deg(u)$为顶点$u$的度。 AS-GCN中对这篇论文的模型形式描述如下： h_{v_i}^{(l)}=\\sigma\\Big(W_l·\\sum_{j=1}^Na(v_i,u_j)·h_{u_j}^{(l-1)}\\Big),i=1,\\dots,N这里$A=(a(v_i,u_j))\\in \\mathbb{R}^{N\\times N}$对应前面一种写法的正则化邻接矩阵$\\hat{A’}$，表面上看对于顶点$v_i$，需要考虑将图中剩下的所有顶点的上一时刻的隐层表示做加权和，来作为它当前时刻的隐层表示，因为$j$的取值范围为$[1,N]$，$N$就是图中顶点的数量。但实际上，大多数顶点因为与$v_i$并无边相连，所以邻接矩阵中对应的值为0，意味着在加权和中的权重为0，相当于加权和时只会考虑有边相连的顶点，这同样是考虑邻域，只不过跟上面那种写法不同。 Neural Graph Collaborative Filtering[SIGIR’19]解决的问题在现有的推荐模型中，用户和物品的embedding只考虑了它们自身的特征，没有考虑用户-物品的交互信息 做法及创新 \\hat{y}_{NGCF}(u,i)={e^*_u}^Te^*_i \\\\ e^*_u = e_u^{(0)}||\\dotsb||e_u^{(L)} \\\\ e^*_i = e_i^{(0)}||\\dotsb||e_i^{(L)} \\\\ e_u^{(l)}=LeakyReLU(m^{(l)}_{u\\leftarrow u}+\\sum_{i\\in N_u}m^{(l)}_{u\\leftarrow i}) \\\\ \\begin{cases} m^{(l)}_{u\\leftarrow i}=p_{ui}(W_1^{(l)}e_i^{(l-1)}+W_2^{(l)}(e_i^{(l-1)}\\odot e_u^{(l-1)})) \\\\\\\\ m^{(l)}_{u\\leftarrow u}=W_1^{(l)}e_u^{(l-1)} \\end{cases} \\\\ m_{u\\leftarrow i}=\\frac{1}{\\sqrt{|N_u||N_i|}}(W_1e_i+W_2(e_i\\odot e_u)) 通过堆叠$l$层embedding传播层，一个用户（物品）可以获得它的$l$跳邻居所传播的信息，如下图所示，通过这种方法来建模用户-物品交互信息中的高阶connectivity，下图展示的是一个三阶的例子: 传统GCN推荐方法中，message embedding只考虑物品embedding$e_i$，论文中将用户embedding与物品embedding的交互也纳入考虑，解释为“This makes the message dependent on the affinity between $e_i$ and $e_u$, e.g., passing more messages from the similar items.” 两个层面上的dropout：message &amp; node dropout。前者表示在第$l$层传播层中，只有部分信息会对最后的表示有贡献；后者表示在第$l$层传播层中，随机地丢弃一些顶点。 数据集Gowalla、Yelp2018、Amazon-book LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]解决的问题在协同过滤中，图卷积网络中的特征转换与非线性激活对提升模型表现贡献很小，甚至有负面影响。 在半监督顶点分类问题中，每个顶点有充分的语义特征作为输入，例如一篇文章的标题与摘要词，这种情况下加入多层的非线性特征转换能够有助于学习特征。而在协同过滤任务中，每个顶点（用户或商品）没有这么充分的语义特征，因此没有多大的作用。 做法及创新 \\hat{y}_{ui}=e_u^Te_i \\\\ e_u=\\sum_{k=0}^K\\alpha_ke_u^{(k)} \\\\ e_i=\\sum_{k=0}^K\\alpha_ke_u^{(k)} \\\\ e_u^{(k+1)}=\\sum_{i\\in N_u}\\frac{1}{\\sqrt{|N_u||N_i|}}e_i^{(k)} \\\\ e_i^{(k+1)}=\\sum_{i\\in N_i}\\frac{1}{\\sqrt{|N_i||N_u|}}e_u^{(k)} 仅考虑图卷积网络中的neighborhood aggregation，通过在用户-物品交互网络中线性传播来学习用户和物品的embedding，再通过加权和将各层学习的embedding作为最后的embedding 通过减少不必要的架构，相较于NGCF大大减少了需要训练的参数量。唯一需要训练的模型参数是第0层的embedding，即$e_u^{(0)}$与$e_i^{(0)}$，当它们两个给定后，后续层的embedding可以通过传播规则直接进行计算 以加权和的方式结合各层的embedding等价于带自连接的图卷积 \\begin{aligned} E^{(K)}&=(A+I)E^{(K-1)}=(A+I)^KE^{(0)}\\\\ &=C_K^0E^{(0)}+C_K^1AE^{(0)}+C_K^2A^2E^{(0)}+\\dots+C_K^KA^KE^{(0)} \\end{aligned} 模型的可解释性更强，以二层网络为例: e_u^{(2)}=\\sum_{i\\in N_u}\\frac{1}{\\sqrt{|N_u||N_i|}}e_i^{(1)}=\\sum_{i\\in N_u}\\frac{1}{|N_i|}\\sum_{v\\in N_i}\\frac{1}{\\sqrt{|N_u||N_v|}}e_v^{(0)}如果另一个用户$v$与目标用户$u$有关联，则影响以下面的系数表示： c_{v\\rightarrow u}=\\frac{1}{\\sqrt{|N_u||N_v|}}\\sum_{i\\in N_u\\cap N_v}\\frac{1}{|N_i|}可解释为: 共同交互过的物品越多系数越大 $i\\in N_u\\cap N_v$ 物品流行度越低系数越大$\\frac{1}{|N_i|}$ 用户$v$越不活跃系数越大$\\frac{1}{|N_v|}$ 数据集Gowalla、Yelp2018、Amazon-Book Simplifying Graph Convolutional Networks[PMLR’19]解决的问题图卷积网络中可能引入了一些不必要的复杂性及冗余的计算 做法及创新 移除图卷积网络各层之间的非线性关系，合并各层之间的权重矩阵 原始图卷积网络对于一个输入的图，图卷积网络利用多层网络为每个顶点的特征$x_i$学习一个新的特征表示，随即输入一个线性分类器。对第$k$层网络，输入为$H^{(k-1)}$，输出为$H^{(k)}$，其中$H^{(0)}=X$。一个$K$层的图卷积网络等价于对图中每个顶点的特征向量$x_i$应用一个$K$层感知机，不同之处在于顶点的隐层表示local averaging： h_i^{(k)}\\leftarrow \\frac{1}{d_i+1}h_i^{(k-1)}+\\sum^n_{j=1}\\frac{a_{ij}}{\\sqrt{(d_i+1)(d_j+1)}}h_j^{(k-1)}矩阵形式： S=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}其中$A=A+I$，则隐层表示用矩阵的形式表示为： H^{(k)}\\leftarrow SH^{(k-1)}Local averaging：this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes $\\Theta^{(k)}$为第$K$层网络的权重矩阵： H^{(k)}\\leftarrow \\text{ReLU}(H^{(k)}\\Theta^{(k)})$Y\\in \\mathbb{R}^{n\\times C}$，$y_{ic}$表示第$i$个顶点属于类别$C$的概率 Y_{GCN}=\\text{softmax}(SH^{(K-1)}\\Theta^{(K)})简化图卷积网络 在传统的多层感知机中，多层网络可以提高模型的表现力，是因为这样引入了特征之间的层级关系，例如第二层网络的特征是以第一层网络为基础构建的。而在图卷积网络中，这还有另外一层含义，在每一层中顶点的隐层表示都是以一跳的邻居进行平均，经过$K$层之后，一个顶点就能获得$K$跳邻居的特征信息。这类似于在卷积网路中网络的深度提升了特征的receptive field。 保留local averaging，移除了非线性激活函数： Y=\\text{softmax}(S^KX\\Theta^{(1)}\\dots \\Theta^{(K)})其中$S^K$可以预先进行计算，大大减少了模型的训练时间 论文中证明了简化后的图卷积网络等价于谱空间的一个低通滤波器，它通过的低频信号对应于图中平滑后的特征 数据集Cora、Citeseer、Pubmed、Reddit Inductive Representation Learning on Large Graphs[NIPS’17]解决的问题对于学习图上顶点的embedding，现有的方法多为直推式学习，学习目标是直接生成当前顶点的embedding，不能泛化到未知顶点上 做法及创新论文提出一种归纳式学习方法GrdaphSAGE，不为每个顶点学习单独的embedding，而是学习一种聚合函数$\\text{AGGREGATE}$，从一个顶点的局部邻域聚合特征信息，为未知的顶点直接生成embedding，因此旧的顶点只要邻域发生变化也能得到一个新的embedding GCN不是归纳式，因为每次迭代会用到整个图的邻接矩阵$A$；而GraphSAGE可以对GCN做了精简，每次迭代只抽样取直接相连的邻居 算法流程 给定顶点$v$及其特征$x_v$,作为它的初始表示$h_v^0=x_v$。 计算邻域向量$h^k_{N(v)}=\\text{AGGREGATE}({h_u^{(k-1)}}, \\forall u\\in N(v))$，当前层顶点的邻居从上一层采样，且邻居个数固定，非所有邻居，这样每个顶点和采样后邻居的个数都相同，可以直接拼成一个batch送到GPU中进行批训练 将邻域向量与自身上一层的表示拼接，通过非线性激活函数$\\sigma$后作为这一层的表示$h_v^k=\\sigma(W^k\\text{CONCAT}(h_v^{(k-1)},h^k_{N(v)})$ 标准化 $h_v^k=h_v^k/||h_v^k||_2$ 聚合函数 MEAN h_v^k=\\sigma(W·\\text{MEAN}(\\{h_v^{k-1}\\}\\cup\\{h_u^{k-1},\\forall u\\in N(v) \\}) LSTM PoolingGraphSAGE采用的max-pooling策略能够隐式地选取领域中重要的顶点： \\text{AGGREGATE}_k^{pool}=\\text{max}(\\{\\sigma(W_{pool}h_u^k + b),\\forall u\\in N(v)\\})数据集BioGRID、Reddit Learning Convolutional Neural Networks for Graphs[ICML’16]解决的问题卷积神经网络都是应用在图像数据上，如何将它有效地应用于图类型的数据上。 对于图像数据，应用一个卷积神经网络可以看成将receptive field（图中为$3\\times3$）以固定的步长将图像遍历，因为图像中像素点的排列有一定的次序，receptive field的移动顺序总是从上到下，从左到右。这也唯一地决定了receptive field对一个像素点的遍历方式以及它如何被映射到向量空间中。 然而对于图结构数据这种隐式的结构特征很多时候是缺失的，而且当给定不止一张图时，各个图之间的顶点没有必然的联系。因此，在将卷积神经网络应用在图数据上时，需要解决下面两个问题： 决定邻域中顶点的产生次序 计算一个将图映射到向量空间的映射方法 做法及创新论文提出方法的流程如下： Node Sequence Selection从图中选取固定数量$w$的顶点，它类比于图像的宽度，而选出的顶点就是卷积操作中小矩形的中心顶点。$w$就是在这个图上所做的卷积操作的个数。如下图所示，$w=6$，代表需要从图中选择6个顶点做卷积操作。论文中选取顶点的方式为$\\text{DFS}$，关键点在于图标签函数$l$，这个函数的作用是决定选取顶点的次序，可以选区的函数为between centrality与WL算法等等 Neighborhood Assembly选取完顶点后，下一步是为它们构建receptive field，类似于第一张图中的$3\\times3$矩阵。选取的方式为，以顶点$v$为中心，通过$\\text{BFS}$添加领域顶点，直到满足receptive field长度$k$： Graph Normalization在选取了满足数量的邻域顶点后，下一步是通过图标签函数$l$为这些顶点赋予一个次序，目的在于将无序的领域映射为一个有序的向量： Convolutional Architecture最后一步就是应用卷积层提取特征，顶点和边的属性对应于传统图像CNN中的channel： 假设顶点特征的数目为$a_v$，边的特征个数为$a_e$，$w$为选取的顶点个数，$k$为receptive field中的顶点个数，则对于输入的一系列图中的每一个，可以得到两个张量维度分别为$(w,k,a_v)、(w,k,k,a_e)$，可以变换为$(wk,a_v)、(wk^2,a_e)$，其中$a_v$与$a_e$可以看成是传统图像卷积中channel的个数，对它们做一维的卷积操作，第一个的receptive field的大小为$k$，第二个的receptive field的大小为$k^2$。 整体卷积结构： 数据集MUTAG、PTC、NCI、D&amp;D Graph Attention Networks[ICLR’18]解决的问题如何将attention机制应用于图类型的数据上。 做法及创新图卷积 给定一个含$n$个顶点的图，其中顶点的特征构成的集合为$(\\overrightarrow{h_1},\\overrightarrow{h_2},\\dots,\\overrightarrow{h_n})$，$\\overrightarrow{h_i}\\in \\mathbb{R}^F$且邻接矩阵为$A$。一个图卷积层根据已有的顶点特征和图的结构来计算一个新的特征集合$(\\overrightarrow{h_1’},\\overrightarrow{h_2’},\\dots,\\overrightarrow{h_n’})$，$\\overrightarrow{h_i’}\\in \\mathbb{R}^{F’}$ 每个图卷积层首先会进行特征转换，以特征矩阵$W$表示，$W\\in \\mathbb{R}^{F’\\times F}$它将特征向量线性转换为$\\overrightarrow{g_i}=W\\overrightarrow{h_i}$，再将新得到的特征向量以某种方式进行结合。为了利用邻域的信息，一种典型的做法如下： \\overrightarrow{h_i}'=\\sigma\\bigg(\\sum_{j\\in N_i}\\alpha_{ij}\\overrightarrow{g_j}\\bigg)其中$N_i$表示顶点$i$的邻域（典型的构造方式是选取直接相连的顶点，包括自身），$\\alpha_{ij}$表示顶点$j$的特征对于顶点$i$的重要程度，也可以看成一种权重。 现有的做法都是显式地定义$\\alpha_{ij}$，本文的创新之处在于使用attention机制隐式地定义$\\alpha_{ij}$。所使用的attention机制定义为$a:R^{F’}\\times \\mathbb{R}^{F’} \\rightarrow \\mathbb{R}$，以一个权重向量$\\overrightarrow{a}\\in \\mathbb{R}^{2F’}$表示，对应于论文中的self-attention。 Self-attention 基于顶点的特征计算系数$e_{ij}$ e_{ij}=a(W\\overrightarrow{h_i},W\\overrightarrow{h_j}) 以顶点的邻域将上一步计算得到的系数正则化，这么做能引入图的结构信息： \\begin{aligned} \\alpha_{ij}&=\\text{softmax}_j(e_{ij})=\\frac{\\exp(e_{ij})}{\\sum_{k\\in N_i}\\exp(e_{ik})}\\\\ &=\\frac{\\exp(\\text{LeakyReLU}(\\overrightarrow{a}^T[W\\overrightarrow{h}_i||W\\overrightarrow{h}_j]))}{\\sum_{k\\in N_i}\\exp(\\text{LeakyReLU}(\\overrightarrow{a}^T[W\\overrightarrow{h}_i||W\\overrightarrow{h}_k]))} \\end{aligned} 次序不变性：给定$(i,j),(i,k),(i’,j),(i’,k)$表示两个顶点间的关系，可以为边或自环。$a$为对应的attention系数，如果$a_{ij}&gt;a_{ik}$，则有$a_{i’j}&gt;a_{i’k}$ ​ DeepInf中给出了证明： ​ 将权重向量$\\overrightarrow{a}\\in \\mathbb{R}^{2F’}$重写为$\\overrightarrow{a}=[p^T，q^T]$，则有 e_{ij}=\\text{LeakyReLU}(p^TWh_i+q^TWh_j)​ 由softmax与LeakyReLU的单调性可知，因为$a_{ij}&gt;a_{ik}$，有$q^TWh_j&gt;q^TWh_k$，类似地就可以得到$a_{i’j}&gt;a_{i’k}$。 ​ 这意味着，即使每个顶点都只关注于自己的邻域，但得到的attention系数却具有全局性。 以上一步得到的系数$\\alpha_{ij}$作为顶点$j$的特征对顶点$i$的重要程度，将领域中各顶点的特征做一个线性组合以作为顶点$i$最终输出的特征表示： \\overrightarrow{h_i'}=\\sigma\\bigg(\\sum_{j\\in N_i}\\alpha_{ij}W\\overrightarrow{h_j}\\bigg)Multi-head attention为了稳定self-attention的学习过程，论文引入了multi-head attention，即由$K$个相互独立的self-attention得到各自的特征，再进行拼接： \\overrightarrow{h_i'}=\\Vert_{k=1}^K\\sigma\\bigg(\\sum_{j\\in N_i}\\alpha_{ij}^kW^k\\overrightarrow{h_j}\\bigg)其中$\\alpha_{ij}^k$是第$k$个attention机制$(a^k)$计算出来的正则化系数，$W^k$是对应的将输入进行线性转化的权重矩阵。论文选取的拼接操作为求平均： \\overrightarrow{h_i'}=\\sigma\\bigg(\\frac{1}{K}\\sum_{k=1}^K\\sum_{j\\in N_i}\\alpha_{ij}^kW^k\\overrightarrow{h_j}\\bigg)数据集Cora、Citeseer、Pubmed、PPI Representation Learning on Graphs with Jumping Knowledge Networks[ICML’18]解决的问题当图卷积网络GCN的层数超过两层时模型的表现会变差，这使得GCN只能作为浅层模型使用，且在对邻域节点的信息进行聚合时，即使同样是采用$k$层网络来聚合$k$跳邻居的信息，有着不同局部结构的顶点获得的信息也可能完全不同，以下图为例： 图$(a)$中的顶点位于核心区域，因此采用$4$层网络把几乎整个图的信息都进行聚合了，而不是它的邻域，这会导致过度平滑，而图$(b)$中顶点位于图边缘的一个树状结构中，采取同样的$4$层网络只囊括了一小部分顶点的信息，只有在第$5$层囊括了核心顶点之后才有效地囊括了更多顶点的信息。 所以，对于处于核心区域的顶点，GCN中每多一层即每多一次卷积操作，节点的表达会更倾向全局，这导致核心区域的很多顶点的表示到最后没有区分性。对于这样的顶点应该减少GCN的层数来让顶点更倾向局部从而在表示上可以区分；而处于边缘的顶点，即使更新多次，聚合的信息也寥寥无几，对于这样的顶点应该增加GCN的层数，来学习到更充分的信息。因此，对于不同的顶点应该选取不同的层数，传统做法对于所有顶点都用一个值会带来偏差。 做法及创新理论部分，论文主要讨论的问题是，在一个$k$层的GCN中，顶点$x$对顶点$y$的影响程度，即顶点$x$输入特征的改变，会对顶点$y$在最后一层得到的表示产生多大的变化，也可以说是顶点$y$对于顶点$x$有多敏感。假设输入的特征为$X\\in \\mathbb{R}^{n\\times f}$，输出的预测标签为$Z\\in \\mathbb{R}^{n\\times c}$，其中$n$为图中顶点数目，$c$为类别数目，$f$为特征数目，则这种影响程度可以表示为$I(x,y)=\\sum_i\\sum_j\\frac{\\partial Z_{yi}}{\\partial X_{xj}}$。 更特别地，论文证明了这个影响程度与从顶点$x$开始的$k$步随机漫步的分布有关，如果对$k$取极限$k\\rightarrow \\infty$，则随机漫步的分布会收敛到$P_{lim}(\\rightarrow y)$。详细论证过程可见原文。这说明，结果与随机漫步的的起始顶点$x$没有关系，通过这种方法来得到$x$的邻域信息是不适用的。 另一种说法是，一个$k$层的图卷积网络等同于一个$k$阶的多项式过滤器，其中的系数是预先确定的SDC。这么一个过滤器与随机漫步类似，最终会收敛到一个静态向量，从而导致过度平滑。 实践部分，论文提出JK-Net，通过Layer aggregation来让顶点最后的表示自适应地聚合不同层的信息，局部还是全部，让模型自己来学习： 论文的重点在于最后的Layer aggregation层，可选的三种操作为：Concat、Max-pooing以及LSTM-attn。 Concat 将各层的表示直接拼接在一起，送入Linear层。对于小数据集及结构单一的图这种聚合方式会更好，因为它们不需要顶点在聚合邻域的顶点信息时具有什么自适应性。 Max-pooling 选取各层的表示中包含信息量最多的作为顶点的最终表示，在多层结构中，低层聚合更多局部信息，而高层会聚合更多全局信息，因此对于核心区域内的顶点可能会选取高层表示而边缘顶点选取低层表示。 LSTM-attention 对于各层的表示，attention机制通过计算一个系数$s_v^{(l)}$来表示各层表示的重要性，其中$\\sum_ls_v^{(l)}=1$，顶点最终的表示就是各层表示的一个加权和：$\\sum_ls_v^{(l)}·h_v^{(l)}$。 $s_v^{(l)}$的计算：将$k$层网络各层的表示$h_v^{(1)},\\dots,h_v^{(k)}$输入一个双向LSTM中，同时生成各层$l$的前向LSTM与反向LSTM的隐式特征，分别表示为$f_v^{(l)}、b_v^{(l)}$，拼接后将$|f_v^{(l)}||b_v^{(l)}|$送入一个Linear层，将Linear层的结果进行Softmax归一化操作就得到了系数$s_v^{l}$。 数据集Citeseer、Cora、Reddit、PPI Session-Based Recommendation with Graph Neural Networks[AAAI’19]解决的问题在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\\rightarrow b \\rightarrow c$，只考虑$a\\rightarrow b$或者$b\\rightarrow c$ 做法及创新 Session Graph Modeling将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵： 上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\\in \\mathbb{R}^{n\\times 2n}$，其中的一行$A_{s,i:}\\in \\mathbb{R}^{1\\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$： Node Representation Learning论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。 GRU一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate： 直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\\in \\mathbb{R}^{n\\times d}$，上一时刻隐层表示$H_{t-1}\\in \\mathbb{R}^{n\\times h}$，重置门与更新门的输出由下式计算得到： R_t=\\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\\\ Z_t=\\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)式中的$W$与$b$分别为权重与偏置参数。 Reset Gate传统RNN网络的隐式状态更新公式为： H_t=\\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\\tilde{H_t}$称为候选状态： \\tilde{H_t}=\\tanh(X_tW_{xh}+(R_t\\odot{H_{t-1}})W_{hh}+b_h)Update Gate更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\\tilde{H_t}决定$： H_t=Z_t\\odot H_{t-1}+(1-Z_t)\\odot \\tilde{H_t}介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习： 最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。 Session Representation Generation现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\\text{s}=[v_{s,1},v_{s,2},\\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\\in \\mathbb{R}^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。 局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣： s_l=v_n全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\\text{soft-attention}$机制来计算得到： \\begin{aligned} s_g&=\\sum_{i=1}^{n}\\alpha_iv_i\\\\ \\alpha_i&=q^T\\sigma(W_1v_n+W_2v_i+c) \\end{aligned}最后使用一个$\\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量： s_h=W_3[s_l;s_g]Making Recommendation对于一个待推荐物品$v_i\\in V$，计算它在序列$s$中作为下一个被点击物品的概率： \\hat{y_i}=\\text{softmax}(s_h^Tv_i)数据集Yoochoose、Diginetica KGAT: Knowledge Graph Attention Network for Recommendation[KDD’19]解决的问题在推荐系统中，如何将用户-物品交互信息与物品自身的属性相结合以做出更好的推荐，从另一个角度来说，即如何融合用户-物品交互图与知识图谱 以上面的图为例，在电影推荐场景中，用户对应于观众，物品对应于电影，实体Entities可以有多种含义，例如导演、演员、电影类别等，对应的就会有多种关系，对应图中的$r_1-r_4$。对于用户$u_1$，协同过滤更关注于他的相似用户，即同样看过$i_1$的$u_4$与$u_5$；而有监督学习方法例如因子分解机等会更关注物品之间的联系，例如$i_1$与$i_2$同样有着属性$e_1$，但它无法进一步建模更高阶的关系，例如图中黄色圈内的用户$u_2$与$u_3$观看了同一个导演$e_1$的电影$i_2$，而这名导演$e_1$又作为演员参演了灰色圈内的电影$i_3$与$i_4$。图中上半部分对应于用户-物品交互图，下半部分对应于知识图谱。 做法及创新 CKG Embedding Layer知识图谱的一般形式可以表示为三元组的集合$\\{(h,r,t)\\}$，表示头实体$h$与尾实体$t$之间有关系$r$，例如$\\text{(Hugh Jackman,ActorOf,Logan)}$表示狼叔是电影罗根的演员，这是一种主动的关系，自然就有逆向的被动关系。而对于用户-物品交互信息来说，通常的表示形式为一个矩阵$R$，$R_{ui}$表示用户$u$与物品$i$的关系，有交互则值为1，否则为0。因此，为了统一两种表示形式，论文中将用户-物品交互信息同样改成三元组的集合$\\text$，这样一来得到的统一后的新图称之为Collaborative Knowledge Graph(CKG)。 第一个步骤是对CKG做embedding，得到图中顶点和边的向量表示形式。论文使用了知识图谱中常用的一个方法$\\text{TransR}$，即对于一个三元组$(h,r,t)$，目标为： e_h^r+e_r\\approx e_t^r其中$e_h,e_t\\in \\mathbb{R}d、e_r\\in \\mathbb{R}k$分别为$h、t、r$的embedding，而$e_h^r,e_t^r$为$e_h、e_t$在$r$所处空间中的投影，损失函数定义为： g(h,r,t)=||W_re_h+e_r-W_re_t||^2_2值越小说明该三元组在知识图谱中更可能存在，即头实体$h$与尾实体$t$之间更可能有关系$r$。经过这一步骤之后，CKG中所有的顶点及边我们都得到了它们的embedding。 Attentive Embedding Propagation Layers第二个步骤直接用的GCN与GAT的想法，在一层embedding propagation layer中，利用图卷积网络在邻域中进行信息传播，利用注意力机制来衡量邻域中各邻居顶点的重要程度。再通过堆叠$l$层来聚合$l$阶邻居顶点的信息。 在每一层中，首先将顶点$h$的邻域以向量形式表示，系数$\\pi(h,r,t)$还会进行$\\text{softmax}$归一化： \\begin{aligned} e_{N_h}&=\\sum_{(h,r,t)\\in N_h}\\pi(h,r,t)e_t \\\\ \\pi(h,r,t)&=(W_re_t)^T\\text{tanh}\\big(W_re_h+e_r\\big) \\end{aligned}通过堆叠$l$层来聚合$l$阶邻居顶点的信息： \\begin{aligned} e_h^{(l)}&=f\\big( e_h^{(l-1)},e_{N_h}^{(l-1)} \\big) \\\\ &=\\text{LeakyReLU}\\big( W_1(e_h+e_{N_h})\\big)+\\text{LeakyReLU}\\big( W_2(e_h\\odot e_{N_h})\\big) \\end{aligned}论文中所使用的聚合函数$f$在GCN与GraphSage的基础上，还额外地引入了第二项中$e_h$与$e_{N_h}$的交互，这使得聚合的过程对于两者之间的相近程度更为敏感，会在更相似的顶点中传播更多的信息。 Model Prediction在得到$L$层embedding propagation layer的表示后，使用JK-Net中的LSTM-attention进行聚合，在通过点积的形式给出预测分数： e_u^*=\\text{LSTM-attention}(e_u^{(0)},e_u^{(L)})\\\\e_i^*=\\text{LSTM-attention()}e_i^{(0)}||\\dots||e_i^{(L)}\\\\ \\hat{y}(u,i)={e_u^*}^Te_i^*数据集Amazon-book、Last-FM、Yelp2018 DeepInf: Social Influence Prediction with Deep Learning[KDD’18]解决的问题如何在图结构的社交数据中预测顶点的影响力。 在图中，给定顶点$v$与它的邻域以及一个时间段，通过对开始时各顶点的状态进行建模，来对结束时顶点$v$的状态进行预测（是否被激活）。 问题定义 邻域：给定图$G=(V,E)$，顶点$v$的邻域定义为$N_v^r=\\{u:d(u,v)\\le r\\}$，是一个顶点集合，不包含顶点$v$自身 中心网络：由邻域中的顶点及边所组成的网络，以$G_v^r$表示 用户行为：以$s_v^t$表示，用户对应于图中的顶点，对于一个时刻$t$，如果顶点$v$有产生动作，例如转发、引用等，则$s_v^t=1$ 给定用户$v$的中心网络、邻域中用户的行为集合$S_v^t=\\{s_i^t:i\\in N_v^r\\}$，论文想解决的问题是，在一段时间$Δt$后，对用户$v$的行为的预测： P(s_v^{t+Δt}|G_v^r,S_v^t)做法及创新数据预处理 数据预处理方面，论文通过带重启的随机漫步来为图中的每个顶点$v$获取固定大小$n$的中心网络$G_v^r$，接着使用$\\text{DeepWalk}$来得到图中顶点的embedding，最后进行归一化。通过这几个步骤对图中的特征进行提取后，论文还进一步添加了几种人工提取的特征，包括用户是否活跃等等： 摘要里说传统的影响力建模方法都是人工提取图中顶点及结构的特征，论文的出发点就是自动学习这种特征表示，结果在预处理的最后还是添加了几种人工提取的特征，这不是自相矛盾吗？ 经过上面的步骤后，最后得到包含所有用户特征的一个特征矩阵$H\\in \\mathbb{R}^{n\\times F}$，每一行$h_i^T$表示一个用户的特征，$F$等同于$\\text{DeepWalk}$长度加上人工特征长度。 影响力计算这一步纯粹是在套GAT的框架，没什么可以说的，计算如下： H'=\\text{GAT}(H)=g(A_{\\text{GAT}}(G)HW^T+b)\\\\ A_{\\text{GAT}}(G)=[a_{ij}]_{n\\times n}其中$W\\in \\mathbb{R}^{F’\\times F}, b\\in \\mathbb{R}^{F’}$是模型的参数，$a_{ij}$的计算在GAT论文的笔记中有记录，不再赘述。 数据集OAG、Digg、Twitter、Weibo Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR’19]解决的问题GCN层数增加后性能反而变差，如何加深GCN的层数。 根据GCN的定义，每一层网络用来捕获一跳邻居的信息，例如一个三层的GCN网络捕获的就是一个顶点三跳邻居以内的信息，而现在如果只能用浅层模型，表示只能捕获有限跳内的邻域信息，而有时候要多几跳才能捕获到有用的信息，例如JK-Net中的例子。 做法及创新这一篇论文的工作其实是接着JK-Net继续往下，在那篇论文中，作者分析了GCN中信息传递这个过程与随机漫步之间的关系，论证了当层数加深之后，GCN会收敛到这个随机漫步的极限分布，而这个极限分布只与图的全局属性有关，没有把随机漫步的起始顶点，或者说是GCN中从邻域中传递和聚合信息的根顶点考虑在内，这么一来，层数加深之后每个顶点聚合出来的样子都差不多，无法区分从而导致性能变差，另一个看待的角度是，因为原始GCN是对所有聚合的信息做平均操作，层数加深之后各个顶点的邻域都变得跟整张图差不多，既然每个顶点的邻域都变得差不多，做的又是平均操作，每个顶点聚合出来的样子就会都差不多。 论文提出的解决办法是引入PageRank的思想，这也是从JK-Net中的结论观察出来的。JK-Net中所说的GCN会收敛到的极限分布的计算方法如下： \\pi_{lim}=\\hat{A}\\pi_{lim}而PageRank的计算方法如下： \\pi_{pr}=A_{rw}\\pi_{pr}其中$A_{rw}=AD^{-1}$，两个计算方法明显地相似，区别在于，PageRank中邻接矩阵$A$没有考虑根顶点自身，而极限分布的计算里$\\hat{A}$是引入了自环的。而Personalized PageRank通过引入自环而考虑了根顶点自身，论文的想法就是将随机漫步的极限分布用Personalized PageRank来代替，它的计算方法为： \\pi_{ppr}(i_x)=(1-\\alpha)\\hat{A}\\pi_{ppr}(i_x)+\\alpha i_x \\\\ \\rightarrow \\pi_{ppr}(i_x)=\\alpha\\Big(I_n-(1-\\alpha)\\hat{A}\\Big)^{-1}i_x其中$i_x$是一个one_hot指示向量，用来从根顶点重新启动。 Personalized PageRank算法的目标是要计算所有节点相对于用户u的相关度。从用户u对应的节点开始游走，每到一个节点都以α的概率停止游走并从u重新开始，或者以1-α的概率继续游走，从当前节点指向的节点中按照均匀分布随机选择一个节点往下游走。这样经过很多轮游走之后，每个顶点被访问到的概率也会收敛趋于稳定，这个时候我们就可以用概率来进行排名了。 相较于原始的GCN模型，现在根顶点$x$对顶点$y$的影响程度$I(x,y)$，变得与$\\pi_{ppr}(i_x)$中的第$y$个元素相关，这个影响程度对于每个根顶点都有不同的取值： \\require{cancel} I(x,y)\\propto \\prod_{ppr}^{(yx)},\\prod_{ppr}^{(yx)}=\\alpha\\Big(I_n-(1-\\alpha)\\hat{A}\\Big)^{-1}\\cancel{I_{n}}PPNP经过上面的铺垫与介绍，论文提出的模型PPNP可以表示为： Z_{PPNP}=\\text{softmax}\\Big(\\alpha\\Big(I_n-(1-\\alpha)\\hat{A}\\Big)^{-1}H\\Big),H_{i,:}=f_{\\theta}(X_i,:)其中$X$为特征矩阵，$f_{\\theta}$是一个参数为$\\theta$的神经网络，用来产生预测类别$H\\in \\mathbb{R}^{n\\times c}$。 由公式和图中都可以看到，PPNP其实是由两部分组成，左边的神经网络与右边的信息传递网络，神经网络部分就类似于在[GCN](#Semi-Supervised Classification with Graph Convolutional Network [ICLR'17])中介绍的，输入顶点特征与图的结构信息（邻接矩阵），输出顶点新的特征表示。信息传递网络部分，在PPNP中通过它来得到预测标签，而原始GCN的做法是$Z_{GCN}=\\text{softmax}(\\hat{A}HW)$，其中$W$是每层网络的参数。 #### APPNP 从前面的构造方式可以看到，矩阵$\\prod_{ppr}$将会有$\\mathbb{R}^{n\\times n}$大小，会带来时间和空间上的复杂度。因此论文提出了一种近似的计算方法APPNP，计算方式如下： $$ \\begin{aligned} Z^{(0)}&=H=f_{\\theta}(X) \\\\ Z^{(k+1)}&=(1-\\alpha)\\hat{A}Z^{(k)}+\\alpha H \\\\ Z^{(K)}&=\\text{softmax}\\Big((1-\\alpha)\\hat{A}Z^{(K-1)}+\\alpha H\\Big) \\end{aligned} $$ 其中$K$为信息传递的跳数或者说是随机漫步的步数，$k\\in[0,K-2]$，这样一来就不用构造一个$\\mathbb{R}^{n\\times n}$的矩阵了。（不知道为什么...） ### 数据集 Citeseer、Cora-ML、Pubmed、MS Academic ## Graph Neural Networks for Social Recommendation[WWW'19] ### 解决的问题 如何将GNN应用于社会化推荐任务上。 面临的挑战有三点： 1. 在一个社会化推荐任务中，输入的数据包括社会关系图和用户-物品交互图，将两张图的信息都聚合才能得到用户更好的一个表示，而此前的GNN只是在同一张图上对邻域内的信息聚合。 2. 在用户-物品交互图中，顶点与顶点之间的边也包含更多的信息，除了表示是否交互，还能表示用户对一个物品的偏好（喜爱还是厌恶），而此前的GNN只是将边用来表示是否交互。 3. 社会关系图中用户之间的纽带有强有弱，显然地，一个用户更可能与强纽带的其它用户有类似的喜好。如果将所有纽带关系都看成一样，会有偏差。 ### 做法及创新 创新： * 在不同图(user-user graph和user-item graph)上进行信息传递与聚合 * 除了捕获user-item间的交互关系，还利用了user对item的评分 * 用attention机制表示社交关系的重要性，用户纽带的强与弱 整个GraphRec框架由三个部分组成，分别为user modeling、item modeling和rating prediction。其中user modeling用来学习用户的特征表示，学习的方式是两个聚合：item aggregation和social aggregation，类似地item modeling用来学习物品的特征表示，学习的方式是一个聚合：user aggregation。 #### User Modeling ##### item aggregation item aggregation的目的是通过用户交互过的物品以及对这些物品的倾向，来学习物品侧的用户特征表示，数学表示为： h_i^I=\\sigma(W·Aggre_{items}(\\{x_{ia},\\forall a\\in C(i)\\})+b)$C(i)$就表示用户交互过的物品的一个集合。这里的$x_{ia}$是一个表示向量，它应该能够同时表示交互关系和用户倾向。论文中的做法是通过一个MLP来结合物品的embedding和倾向的embedding，两者分别用$q_a$和$e_r$表示。倾向的embedding可能很难理解，以五分制评分为例，倾向的embedding表示为$e_r\\in \\mathbb{R}^d$，其中$r\\in \\{1,2,3,4,5\\}$。 x_{ia}=g_v([q_a\\oplus e_r])定义好$x_{ia}$后，下一步就是如何选取聚合函数$Aggre$了。论文中使用的是attention机制，来源于GAT： \\begin{aligned} h_i^I&=\\sigma(W·\\Big\\{\\sum_{a\\in C(i)} \\alpha_{ia}x_{ia}\\Big\\}+b) \\\\ \\alpha_{ia}'&=w_2^T·\\sigma(W_1·[x_{ia}\\oplus p_i]+b_1)+b_2 \\\\ \\alpha_{ia}&=\\frac{\\exp(\\alpha_{ia}')}{\\sum_{a\\in C(i)}\\exp(\\alpha_{ia}')} \\end{aligned}这里的权重$\\alpha_{ia}$考虑了$x_{ia}$和用户$u_i$的embedding $p_i$，使得权重能够与当前用户相关。 social aggregation social aggregation中，同样地使用了attention机制，通过attention机制来选取强纽带的其它用户（表现为聚合时权重更大）并聚合他们的信息，聚合的就是物品侧的用户特征表示。 $$ \\begin{aligned} h_i^S&=\\sigma(W·\\Big\\{\\sum_{o\\in N(i)} \\beta_{io}h_o^I\\Big\\}+b) \\\\ \\beta_{io}'&=w_2^T·\\sigma(W_1·[h_o^I\\oplus p_i]+b_1)+b_2 \\\\ \\beta_{io}&=\\frac{\\exp(\\beta_{io}')}{\\sum_{o\\in N(i)}\\exp(\\beta_{io}')} \\end{aligned} $$ 这里跟item aggregation基本一模一样，就不多介绍了。 得到物品侧的用户特征表示$h_i^I$和社交侧的用户特征表示$h_i^S$后，用一个MLP将它们结合，得到用户最终的特征表示： $$ \\begin{aligned} c_1&=[h_i^I\\oplus h_i^S] \\\\ c_2&=\\sigma(W_2·c_1+b_2) \\\\ &······ \\\\ h_i&=\\sigma(W_l·c_{l-1}+b_l) \\end{aligned} $$ #### Item Modeling ##### user aggregation Item modeling与User modeling的做法基本一模一样...公式都是一一对应的： $$ \\begin{aligned} f_{jt}&=g_u([p_t\\oplus e_r]) \\\\ z_j&=\\sigma(W·\\Big\\{\\sum_{t\\in B(j)} \\mu_{jt}f_{jt}\\Big\\}+b) \\\\ \\mu_{jt}'&=w_2^T·\\sigma(W_1·[f_{jt}\\oplus q_j]+b_1)+b_2 \\\\ \\mu_{jt}&=\\frac{\\exp(\\mu_{jt}')}{\\sum_{a\\in C(i)}\\exp(\\mu_{jt}')} \\end{aligned} $$ #### Rating Prediction 最后来到评分预测部分，由上面两个部分我们得到了用户特征表示$h_i$与物品特征表示$z_j$，产生评分用的也是一个MLP： $$ \\begin{aligned} g_1&=[h_i\\oplus z_j] \\\\ g_2&=\\sigma(W_2·g_1+b_2) \\\\ &······ \\\\ g_{l-1}&=\\sigma(W_l·g_{l-1}+b_l) \\\\ r_{ij}&=w^T·g_{l-1} \\end{aligned} $$ ### 数据集 Ciao、Epinions ## Graph Convolutional Matrix Completion[KDD'18] ### 解决的问题 如何将图卷积网络应用于矩阵补全问题。 具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。如果将评分矩阵转换为一张图，转换方法在下面有进行介绍，这时矩阵补全问题也可以看成图上的边预测问题。要预测用户对一个物品的评分，就是预测图上两个对应顶点之间相连的边的权重。 ### 做法及创新 论文通过一个编码器-解码器的架构来实现从已有评分到特征表示再到预测评分的过程。 Bipartite Graph Construction首先是将推荐任务里的评分数据转化为一张图，具体做法是将用户和物品都看作图中的顶点，交互记录看作边，分数作为边的权重，如图所示： Graph Convolutional Encoder上一步所构建的图的输入形式为邻接矩阵$A\\in \\mathbb{R}^{n\\times n}$与图中顶点的特征矩阵$X\\in \\mathbb{R}^{n\\times d}$。编码器在这一步的作用就是得到用户与物品的特征表示$A,X^u,X^v\\rightarrow U,V$。 具体编码时，论文将不同的评分水平分开考虑$r\\in \\{1,2,3,4,5\\}$，我的理解是它们类似于处理图像数据时的多个channel。以一个评分水平$r$为例，说明编码得到特征表示的过程。假设用户$u_i$对电影$v_j$评分为$r$，而这部电影的特征向量为$x_j$，那么这部电影对这个用户特征表示的贡献可以表示为下面的式子(1)，相当于对特征向量进行了一个线性变换。 对当前评分水平下所有评过分的电影进行求和，再对所有评分水平求和拼接，经过一个非线性变换，就得到了用户$u_i$的特征表示$h_{u_i}$，物品的做法相同。 Bilinear Decoder在分别得到用户与物品的特征表示$U$与$V$后，解码器计算出用户对物品评分为$r$的概率，再对每个评分的概率进行求和，得到最终预测的评分。 \\begin{aligned} (P_r)_{ij}&=\\frac{\\exp(u_i^TQ_rv_j)}{\\sum_{s\\in R}\\exp(u_i^TQ_sv_j)} \\\\ \\hat{M}&=\\sum_{r\\in R}rP_r \\end{aligned}数据集Flixster、Douban、YahooMusic、MovieLens 知识图谱","categories":[],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"d2l学习笔记","slug":"d2l","date":"2020-08-25T04:07:04.464Z","updated":"2020-09-29T07:23:19.267Z","comments":true,"path":"2020/08/25/d2l/","link":"","permalink":"http://Bithub00.com/2020/08/25/d2l/","excerpt":"Dive into Deep Learning学习笔记","text":"Dive into Deep Learning学习笔记 d2l学习记录第二章 Preliminaries 用with torch.no_grad()将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。 grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。 在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor。 view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。 在测试模型时，我们为了拿到更加确定性的结果，一般需要关闭dropout。1234if isinstance(net, torch.nn.Module): net.eval() # 评估模式, 这会关闭dropout acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() net.train() # 改回训练模式 第三章 Linear Neural Networkscross-entropy loss给定含n个样本的数据集$\\{X,Y\\}$，我们可以通过将模型的预测标签与实际标签进行比较来查看模型的效果，即最大化$P(Y|X)=\\prod_{i=1}^nP(y^{(i)}|x^{(i)})$，根据最大似然准则，这等同于最小化负的对数似然函数： -\\log P(Y|X)=\\sum_{i=1}^{n}-\\log P(y^{(i)}|x^{(i)})=\\sum_{i=1}^{n}l(y^{(i)}|\\hat{y}^{(i)})假设数据集中共有q个类别，则给定真实标签$y$与模型预测标签$\\hat{y}$，损失函数l为所谓的交叉熵： l(y,\\hat{y})=-\\sum^q_{j=1}y_j\\log\\hat{y_j}代入softmax的表达式： \\begin{aligned} l(y,\\hat{y})&= -\\sum_{j=1}^qy_j\\log \\frac{\\exp(o_j)}{\\sum^q_{k=1}\\exp(o_k)} \\\\ &= \\sum^q_{j=1}y_j\\log \\sum^q_{k=1}\\exp (o_k)-\\sum^q_{k=1}y_j\\log \\exp (o_j) \\\\ &= \\log \\sum^q_{k=1}\\exp (o_k)-\\sum^q_{j=1}y_jo_j \\end{aligned}softmax可以看作是一个单层神经网络： 将损失函数对任意一个$o_j$求导，得到： \\partial_{o_j}l(y,\\hat{y}) = \\frac{\\exp(o_j)}{\\sum^q_{k=1}\\exp(o_k)} - y_j = softmax(o)_j-y_j这表示模型预测标签与真实标签之间的差距。在任意指数族模型中，对数似然函数的梯度都是这样的形式。 对交叉熵的形式做更深入的理解，给定一个分布p，信息论指出，该分布的熵为 H[p]=\\sum_j-p(j)\\log p(j)给定一个需要压缩的数据流，如果数据流中的数据是完全一致的，则预测下一个数据和对数据进行压缩是非常简单的。当我们不能准确预测下一个数据j，预测结果与实际结果之间的差别会带来所谓的surprisal，用数学形式表达为$\\log \\frac{1}{P(j)}=-\\log P(j)$。The cross-entropy from p to q, denoted H(p,q), is the expected surprisal of an observer with subjective probabilities p. 第五章 Deep Learning Computation 如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。 12345678910111213class MyModel(nn.Module): def __init__(self, **kwargs): super(MyModel, self).__init__(**kwargs) self.weight1 = nn.Parameter(torch.rand(20, 20)) self.weight2 = torch.rand(20, 20) def forward(self, x): passn = MyModel()for name, param in n.named_parameters(): print(name)weight1 共享参数，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的。 1234567891011121314151617181920class FancyMLP(nn.Module): def __init__(self, **kwargs): super(FancyMLP, self).__init__(**kwargs) self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不可训练参数（常数参数） self.linear = nn.Linear(20, 20) def forward(self, x): x = self.linear(x) # 使用创建的常数参数，以及nn.functional中的relu函数和mm函数 x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1) # 复用全连接层。等价于两个全连接层共享参数 x = self.linear(x) # 控制流，这里我们需要调用item函数来返回标量进行比较 while x.norm().item() &gt; 1: x /= 2 if x.norm().item() &lt; 0.8: x *= 10 return x.sum() 如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。所以在自定义含模型参数的层时，我们应该将参数定义成Parameter。 PyTorch中保存和加载训练模型推荐保存和加载state_dict。state_dict是一个从参数名称隐射到参数Tesnor的字典对象。1234567891011121314151617181920class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.hidden = nn.Linear(3, 2) self.act = nn.ReLU() self.output = nn.Linear(2, 1) def forward(self, x): a = self.act(self.hidden(x)) return self.output(a)net = MLP()net.state_dict()OrderedDict([('hidden.weight', tensor([[ 0.2448, 0.1856, -0.5678], [ 0.2030, -0.2073, -0.0104]])), ('hidden.bias', tensor([-0.3117, -0.4232])), ('output.weight', tensor([[-0.4556, 0.4084]])), ('output.bias', tensor([-0.3573]))])# 只有具有可学习参数的层(卷积层、线性层等)才有state_dict中的条目。优化器(optim)也有一个state_dict，其中包含关于优化器状态以及所使用的超参数的信息。 第八章 Recurrent Neural Networks scatter() 一般可以用来对标签进行 one-hot 编码 1234567891011121314# self[i][ index[i][j][k] ][k] = src[i][j][k] # if dim == 1class_num = 10batch_size = 4label = torch.LongTensor(batch_size, 1).random_() % class_num#tensor([[6],# [0],# [3],# [2]])torch.zeros(batch_size, class_num).scatter_(1, label, 1)#tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],# [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],# [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],# [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]) 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。 另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。 torch.view等方法操作需要连续的Tensor，transpose、permute 操作虽然没有修改底层一维数组，但是新建了一份Tensor元信息，并在新的元信息中的 重新指定 stride。torch.view 方法约定了不修改数组本身，只是使用新的形状查看数据。如果我们在 transpose、permute 操作后执行 view，Pytorch 会报错。 Tensor多维数组底层实现是使用一块连续内存的1维数组（行优先顺序存储，下文描述），Tensor在元信息里保存了多维数组的形状，在访问元素时，通过多维度索引转化成1维数组相对于数组起始位置的偏移量即可找到对应的数据。某些Tensor操作（如transpose、permute、narrow、expand）与原Tensor是共享内存中的数据，不会改变底层数组的存储，但原来在语义上相邻、内存里也相邻的元素在执行这样的操作后，在语义上相邻，但在内存不相邻，即不连续了（is not contiguous）。 第十一章 Optimization Algorithms动量法如果同一位置上，目标函数在竖直方向($x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大，则给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。 动量法看作是对最近$1/(1−γ)$个时间步的$x_t$值的加权平均，而且离当前时间步$t$越近的$x_t$值获得的权重越大。动量法在每个时间步的自变量更新量近似于将最近1/(1−γ)个时间步的普通更新量（即学习率乘以梯度）做了指数加权移动平均后再除以1−γ。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。 AdaGrad和RMSProp如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$s_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。 不同于AdaGrad算法里状态变量$s_t$是截至时间步$t$所有小批量随机梯度$g_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。$s_t$所以可以看作是最近1/(1−γ)个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。 Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。 第十四章 自然语言处理 求类比词问题可以定义为：对于类比关系中的4个词 $a:b::c:d$，给定前3个词$a、b和c，求d$。设词$w$的词向量为$vec(w)$，求类比词的思路是，搜索与$vec(c)+vec(b)-vec(a)$的结果向量最相似的词向量。","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://Bithub00.com/tags/深度学习/"}]},{"title":"CUDA学习笔记","slug":"CUDA学习记录","date":"2020-07-15T13:04:08.623Z","updated":"2020-10-10T01:22:12.231Z","comments":true,"path":"2020/07/15/CUDA学习记录/","link":"","permalink":"http://Bithub00.com/2020/07/15/CUDA学习记录/","excerpt":"CUDA编程的学习笔记以及踩过的一些坑","text":"CUDA编程的学习笔记以及踩过的一些坑 CUDACUDA程序的执行流程： 分配host内存，并进行数据初始化； 分配device内存，并从host将数据拷贝到device上； 调用CUDA的核函数在device上完成指定的运算； 将device上的运算结果拷贝到host上； 释放device和host上分配的内存。 上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，核函数用global符号声明，它在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，而网格又可以分为很多线程块（block），一个线程块里面包含很多线程。这构成了两层结构。 CUDA内存模型每个线程有自己的私有本地内存（Local Memory），而每个线程块（Block）有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory） 如何使用CUDA对程序进行优化 使用共享内存减少全局内存读取次数；从常量内存（Constant Memory）中读取数据相较于全局内存会更节省内存带宽，因为常量内存是缓存的，从相同的地址中连续读取数据不会带来额外的内存读取开销。 流并行；通过多个流（stream）进行操作时，应该将所做的操作尽可能广地安排在各个流中，而不是尽可能地深，例如将内存拷贝和计算并行，其中的原理是：同一时刻cuda程序可以同时进行一个计算和数据传输。 合并访问；线程块内相邻的线程访问相邻的数据。因为半束(16)线程的访问16字节的数据可以合为一个访问指令。 使用零复制、锁页内存；零复制（zero copy）是一种特殊形式的内存映射，它允许你将host内存直接映射到device内存空间上。其实就是device可以通过直接内存访问（direct memory access，DMA）方式来访问host的锁页内存。 现代操作系统都支持虚拟内存，操作系统实现虚拟内存的主要方法就是通过分页机制。将内存中暂时不使用的内容换出到外存（硬盘等大容量存储）上，从而腾出空间存放将要调入内存的信息。这样，系统好像为用户提供了一个比实际内存大得多的存储器，称为虚拟存储器。锁页（page-locked host memory）就是将内存页面标记为不可被操作系统换出的内存。所以device可以使用页面的物理地址直接访问内存（DMA），从而避免从外存到内存的复制操作。在GPU上分配的内存默认都是锁页内存，这是因为GPU不支持将内存交换到磁盘上。在CPU上分配的内存默认都是可分页。 重构时的问题及解决 论文中涉及的数据结构较为复杂，在使用GPU进行并行计算时需要先将数据复制到GPU上，复制的步骤十分繁琐，容易出现内存访问出错的问题。 CUDA6.0引入了统一内存的概念，大大简化了数据从CPU传到GPU这一步骤，只需要在host端分配好空间，写入数据，直接作为参数传入device端的核函数即可，CUDA会自动完成复制工作。 c++中的std在CUDA中不被支持，例如std::vector、std::sort等都无法使用。 CUDA提供了Thrust库作为替代，提供了sort、host_vector、device_vector等，然而这些都是host端的实现，在device端无法调用。 Thrust作为host端的实现在device端无法调用，原数据结构中的类成员有vector，在device端无法访问其中的数据。 在后续CUDA版本中，thrust::sort可以在device端运行，只需加入参数thrust::sort(thrust::device, XX, XX)。vector在device函数中的访问仍不被支持，因此将原类成员vector改成Edge **二维指针数组形式，vector读取完数据后，再复制到二维指针数组中，此时device端就可以访问其中的数据了。 调试时nvcc编译带上-g参数，使用cuda-gdb运行程序，可以在cu文件中设置断点，发生段错误后可以使用where命令来查明是哪一行导致的错误。 addKernel&lt;&lt;&gt;&gt;(devA, devB, devC);这里的&lt;&lt;&gt;&gt;的意思是，调用函数的时候，开出60个线程格，每个线程格包含501个线程。在global函数中通过代码int i = threadIdx.x + blockIdx.x*blockDim.x;得到当前线程是第几个线程。 Host调用完kernel函数需要进行线程同步,而在kernel或global函数只需要在必要的地方__syncthreads()即可: 这时dev_b会保存一个指向GPU内存的指针,但是CPU是无法访问GPU上面的数据,所以利用这个指针做任何获取数据或者赋值都是会出错的,理解这点对传递数据很重要 12int *dev_b = 0cudaMalloc((void**)&amp;dev_b, size * sizeof(int)); 实用问答： https://stackoverflow.com/questions/14790999/how-to-pass-a-c-class-with-array-of-pointers-to-cuda https://stackoverflow.com/questions/9309195/copying-a-struct-containing-pointers-to-cuda-device 统一内存 教你一步步写一个cuda path tracer：cuda与类 CUDA值得注意的特性(坑) https://stackoverflow.com/questions/11874667/cuda-allocation-of-an-array-of-structs-inside-a-struct https://forums.developer.nvidia.com/t/how-to-cudamalloc-two-dimensional-array/4042/9 https://stackoverflow.com/questions/40682163/cuda-copy-inherited-class-object-to-device https://stackoverflow.com/questions/16024087/copy-an-object-to-device https://stackoverflow.com/questions/6929626/cuda-copy-to-array-within-array-of-objects https://stackoverflow.com/questions/5510715/thrust-inside-user-written-kernels https://stackoverflow.com/questions/14284964/cuda-how-to-allocate-memory-for-data-member-of-a-class https://forums.developer.nvidia.com/t/how-to-measure-total-time-for-cpu-and-gpu/28234/2","categories":[],"tags":[{"name":"CUDA","slug":"CUDA","permalink":"http://Bithub00.com/tags/CUDA/"}]},{"title":"常用操作","slug":"实用技巧","date":"2020-06-08T02:31:17.355Z","updated":"2020-11-19T07:52:49.795Z","comments":true,"path":"2020/06/08/实用技巧/","link":"","permalink":"http://Bithub00.com/2020/06/08/实用技巧/","excerpt":"用本文记录平时遇到的一些问题和解决方法，自己试过成功的","text":"用本文记录平时遇到的一些问题和解决方法，自己试过成功的 Ubuntu 修改Ubuntu的apt-get源为国内镜像源 CSDN Ubuntu16.04安装python3.5同时保留python2.7 askubuntu 修改Ubuntu的pip源为阿里源 CSDN win10安装ubuntu第一次开启时黑屏：netsh winsock reset知乎 动态查看GPU资源使用情况：watch -d -n 0.5 nvidia-smi Python将conda环境加入jupyterlab假设conda中现有一个虚拟环境star，现在想把它作为kernel加入到jupyterlab中，注意虚拟环境与kernel并不是同步的，即使激活star这个虚拟环境之后打开jupyterlab，使用的kernel仍然是默认的python环境，如果要使用star里面安装的各种版本的库，需要进行如下步骤： conda activate star conda install ipykernel #安装 ipykernel库 ipython kernel install —user —name=star # —user表示当前用户 jupyter-lab #启动jupyterlab 远程使用jupyterlab有时如果需要利用服务器上的GPU资源来跑一些模型，就需要在服务器上运行jupyterlab，具体步骤如下： 首先建立ssh连接，将usr、remote_server、port_num替换 12&gt; ssh -f usr@remote_server -N -L port_num:localhost:port_num&gt; &gt; 如果需要，输入登录密码。之后再进行一次ssh连接登陆远程服务器 12&gt; ssh -X usr@remote_server&gt; &gt; 运行jupyterlab，port_num与刚才选择的要保持一致，不与常见端口冲突即可，例如选择当天的日期1116等 1234567&gt; jupyter-lab --no-browser --port=port_num&gt; &gt; To access the notebook, open this file in a browser:&gt; file:///xxxxxxxxxxxxxxxxxxxxx.html&gt; Or copy and paste one of these URLs:&gt; http://localhost:port_num/?token=jupyterlab_token&gt; &gt; 复制cmd窗口最后一行的地址，在本地浏览器中打开，就可以使用服务器的资源来运行jupyterlab了。 json.decoder.JSONDecodeError Expecting value line 1column 1 (char 0)报错解决解决方法将判断条件由 1if response.status_code == 200: 改为1if response.content: Pycharm from xx import出错错误描述使用Pycharm的时候，使用from引入自己模块报错： 原因pycharm不会将当前文件目录自动加入sourse_path，右键源代码目录make_directory as–&gt;sources root将当前工作的文件夹加入就行了 解决方案 Selenium TypeError init() takes 2 positional arguments but 3 were given_解决方案错误描述执行以下代码时会报错： 12345678wait = WebDriverWait(browser, 10)input = wait.until( EC.presence_of_element_located ( By.ID, 'search' )) PyCharm会提示：TypeError: init() takes 2 positional arguments but 3 were given。 问题来源在expected conditions.py中找到presence_of_element_located类 1234567891011class presence_of_element_located(object): \"\"\" An expectation for checking that an element is present on the DOM of a page. This does not necessarily mean that the element is visible. locator - used to find the element returns the WebElement once it is located \"\"\" def __init__(self, locator): self.locator = locator def __call__(self, driver): return _find_element(driver, self.locator) 1234EC.presence_of_element_located ( By.ID, 'search' ) 这里如果不加小括号，相当于输入了三个参数:self、By.ID、’search’，而presence_of_element_located类的_init_方法取的是两个参数self、locator，其中locator调用的是一个tuple(元组)，所以添加小括号后将(By.ID, ‘search’)作为一个整体对应于一个参数 解决方案添加小括号 1234EC.presence_of_element_located ( (By.ID, 'search' ) ) 参考资料 stackoverflow","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://Bithub00.com/tags/Ubuntu/"}]},{"title":"计量经济学课程笔记","slug":"计量经济学","date":"2019-11-18T09:17:46.871Z","updated":"2019-11-18T12:54:25.298Z","comments":true,"path":"2019/11/18/计量经济学/","link":"","permalink":"http://Bithub00.com/2019/11/18/计量经济学/","excerpt":"本篇博客记录计量经济学课程学习过程中的笔记","text":"本篇博客记录计量经济学课程学习过程中的笔记 OLS估计模型假设 SLR.1 被解释变量y与解释变量x之间的关系满足 y=\\beta_0+\\beta_1x+u SLR.2 $(x_1,y_1),…,(x_n,y_n)是(x,y)的一组随机样本$ SLR.3 $x_1,…,x_n$不完全相同 SLR.4 随机误差满足$E(u|x)=0$ SLR.5 同方差假设 Var(u|x)=\\sigma^2参数估计假设样本$(x_1,y_1)…(x_n,y_n)$近似分布在一条直线两侧，模型为 y_i=\\beta_0+\\beta_1x_i+u_i直观上看最佳的拟合应该使数据点都尽量靠近所拟合的直线，转化为数学表达式就是对于每个样本点的实际观测值与拟合值之间的差$\\hat{u_i}=y_i-\\hat{y_i}$应尽可能小，其中$\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i$为拟合值。为了避免符号导致求和时进行相互抵消，使用残差平方和来表示实际观测值与拟合值之间的差： S(\\hat{\\beta_0},\\hat{\\beta_1})=\\sum_{i=1}^{n}\\hat{u_i}^2=\\sum_{i=1}^{n}(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)^2则最小二乘法的目的就是最小化这个残差平方和来估计参数$\\hat{\\beta_0}$和$\\hat{\\beta_1}$。记$S(\\hat{\\beta_0},\\hat{\\beta_1})=\\sum(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)^2$，则目标变成求解这个二元函数的最小值点，由微积分中求函数最值的办法进行求导： \\begin{cases} \\frac{\\partial S(\\hat{\\beta_0},\\hat{\\beta_1})}{\\partial\\hat{\\beta_0}} = 0 \\\\\\\\ \\frac{\\partial S(\\hat{\\beta_0},\\hat{\\beta_1})}{\\partial\\hat{\\beta_1}} = 0 \\end{cases}得到如下方程组： \\begin{cases} \\sum_{i=1}^n(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)=0 \\\\\\\\ \\sum_{i=1}^n(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)x_i=0 \\end{cases}由第一个式子得到： \\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1\\bar{x}}将它代入第二个式子，得到： \\hat{\\beta_1}=\\frac{\\sum x_iy_i-n\\bar{x}\\bar{y}}{\\sum x_i^2-n(\\bar{x})^2}=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}=\\frac{S_{xy}}{S_x^2}这样我们就得到了两个未知参数的回归估计。 拟合结果对于简单线性模型: y_i=\\beta_0+\\beta_1x_i+u_i通过最小二乘法得到了一个拟合模型： \\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i自然而然地，我们想知道这个模型的拟合效果如何，解释变量在多大程度上解释了被解释变量？总的平方和： TSS=\\sum_{i=1}^n(y_i-\\bar{y})^2解释平方和： ESS=\\sum_{i=1}^n(\\hat{y_i}-\\bar{y})^2残差平方和： RSS=\\sum_{i=1}^n(y_i-\\hat{y})^2三者的关系为： \\begin{aligned} TSS=\\sum_{i=1}^n(y_i-\\bar{y})^2 &=\\sum_{i=1}^n(y_i-\\hat{yi}+\\hat{yi}-\\bar{y})^2 \\\\ &= RSS+ESS+2\\sum_{i=1}^n\\hat{u_i}(\\hat{yi}-\\bar{y}) \\\\ &= RSS+ESS \\end{aligned}判定系数定义为： R^2=\\frac{ESS}{TSS}=1-\\frac{RSS}{TSS}可以表明解释变量多大程度上解释了被解释变量。 期望与方差OLS估计的期望：记离差$d_i=x_i-\\bar{x}$，$TSS_x=\\sum_{i=1}^n(x_i-\\bar{x})^2$，有 \\sum^n_{i=1}d_i=0，\\sum^n_{i=1}d_ix_i=\\sum^n_{i=1}d_i^2=TSS_x于是有 \\hat{\\beta_1}=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}=\\frac{1}{TSS_x}\\sum^n_{i=1}d_iy_i代入$y_i=\\beta_0+\\beta_1x_i+u_i$，得： \\hat{\\beta_1}=\\beta_1+\\frac{1}{TSS_x}\\sum_{i=1}^nd_iu_i所以 E(\\hat{\\beta_1}|x_1,...,x_n)=\\beta_1+\\frac{1}{TSS_x}\\sum_{i=1}^nd_iE(u_i)=\\beta_1即OLS估计是无偏的，$\\hat{\\beta_0}$的证明同理 OLS估计的方差：前面已经得到$\\hat{\\beta_1}=\\beta_1+\\frac{1}{TSS_x}\\sum_{i=1}^nd_iu_i$，则 Var(\\hat{\\beta_1}|x_1,...,x_n)=\\frac{1}{TSS_x^2}\\sum^n_{i=1}d_i^2Var(u_i)=\\frac{\\sigma^2}{TSS_x}同理 Var(\\hat{\\beta_0}|x_1,...,x_n)=\\frac{\\sum^n_{i=1}x_i^2}{n\\sum^n_{i=1}(x_i-\\bar{x})^2}\\sigma^2则OLS估计方差依赖于三个因素： 随机误差的方差 自变量的方差 样本量","categories":[],"tags":[{"name":"数学","slug":"数学","permalink":"http://Bithub00.com/tags/数学/"}]},{"title":"Laplace先验与L1正则","slug":"Laplace与L1正则","date":"2019-11-18T02:38:55.731Z","updated":"2019-11-18T03:31:08.144Z","comments":true,"path":"2019/11/18/Laplace与L1正则/","link":"","permalink":"http://Bithub00.com/2019/11/18/Laplace与L1正则/","excerpt":"本篇博客探讨一下为什么说L1正则等同于Laplace先验","text":"本篇博客探讨一下为什么说L1正则等同于Laplace先验 Laplace分布首先介绍什么是Laplace分布，它的概率密度函数如下： f(x|\\mu,b)=\\frac{1}{2b}e^{-\\frac{|x-\\mu|}{b}}分布的图像如下： 由图可以看出，Laplace分布集中在$\\mu$附件，b越小数据分布越集中 Laplace先验导出L1正则导出之前，需要用到最大似然的知识： 首先我们假设现在需要从一些样本点$(x_1,y_1)···(x_N,y_N)$中来估计参数 $\\beta$，假设输出$y$与输入$x$之间线性相关，并且受噪声$\\epsilon$影响: y_n=\\beta x_n+\\epsilon这里的$\\epsilon$服从均值为0，方差为$\\sigma^2$的高斯分布，这个式子还可以写成： \\begin{aligned} f(X)&=\\sum_i(x_i\\beta_i)+\\epsilon \\\\ &=X\\theta^T+\\epsilon \\end{aligned}其中$X=(x_1,x_2…x_n)$，对数据集$(X_i,Y_i)$，用$X_i$得到$Y_i$的概率是$Y_i\\backsim N(f(X_i),\\sigma^2)$： P(Y_i|X_i,\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{\\lVert f(X_i)-Y_i\\rVert^2}{2\\sigma^2}}假设数据集中每对数据$(X_i,Y_i)$都是独立的，那么对于数据集来说由X得到Y的概率是： P(Y|X,\\theta)=\\prod_i\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{\\lVert f(X_i)-Y_i\\rVert^2}{2\\sigma^2}}显然，我们的目标是使这个概率值最大，最大时的参数$\\theta$就是我们需要的参数： \\begin{aligned} \\theta^* &= argmax_\\theta(\\prod_i\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{\\lVert f(X_i)-Y_i\\rVert^2}{2\\sigma^2}}) \\\\ &= argmax_\\theta(-\\frac{1}{2\\sigma^2}\\sum_i\\lVert f(X_i)-Y_i\\rVert^2 + \\sum_iln(\\sigma \\sqrt{2\\pi})) \\\\ &= argmin_\\theta(\\sum_i \\lVert f(X_i)-Y_i\\rVert^2) \\end{aligned}可以看到，这就是最小二乘法。在最大似然估计中，假设了$\\theta$是均匀分布的，$p(\\theta)=常数$，同这篇博客高斯先验分布与L2正则一样，我们要最大化如下的后验估计： \\begin{aligned} \\theta^* &= argmax_\\theta(\\prod_i P(Y_i|X_i,\\theta)\\prod_i P(\\theta_i)) \\\\ &= argmin_\\theta(\\sum_i \\lVert f(X_i)-Y_i\\rVert^2+\\sum_i ln(P(\\theta_i))) \\end{aligned}将第一节中的Laplace分布代入$P(\\theta_i)$，我们得到： \\theta^*=argmin_\\theta(\\sum_i \\Vert f(X_i)-Y_i\\rVert^2+\\lambda\\sum_i|\\theta_i|)这就是L1正则。","categories":[],"tags":[{"name":"数学","slug":"数学","permalink":"http://Bithub00.com/tags/数学/"}]},{"title":"毕业设计","slug":"毕业设计","date":"2019-11-04T06:51:55.936Z","updated":"2019-11-04T07:27:57.125Z","comments":true,"path":"2019/11/04/毕业设计/","link":"","permalink":"http://Bithub00.com/2019/11/04/毕业设计/","excerpt":"以本文记录自己毕业设计中的学习过程。","text":"以本文记录自己毕业设计中的学习过程。 NRLMF与GRNMF首先是老师让我看的两篇论文 GRNMF：a graph regularized nonnegative matrix factorization method for identifying microRNA-disease associations NRLMF：Neighborhood Regularized Logistic Matrix Factorization for Drug-Target Interaction Prediction 以下是自己的一点笔记： \\min_{W,H}\\lVert Y-WH^T \\rVert^2_F两篇论文共同点：都利用了样本点的邻域信息，GRNMF为miRNA-disease，NRLMF为drug-target，而两篇论文都选取了前k个相似的样本而不是全部相似样本，目的是为了避免引入噪声。使用[0,1]区间连续取值的相似度而不是0/1的二值变量，相似度越大的两个样本它们各自的特征向量应该越相似。 GRNMF模型所使用的数据集没有提供现成的相似度，因此论文的一个创新点在于计算miRNA和disease的相似度矩阵，其中disease使用的是根据现有的有向无环图结构类型的数据进行计算，而miRNA是通过引入gene作为中介来计算，使用的是基因网络数据HumanNet。 在得到目标式进行优化之前，GRNMF的另一个创新之处在于对输入的数据矩阵Y先利用刚刚得到的相似度来进行更新，想法同NRLMF一样，也是认为没有观测值的miRNA或disease会造成精度的损失。这里同样是根据相似性去取一个k近邻，但与NRLMF不同之处在于不是更新特征向量，而是对输入矩阵逐行（miRNA）、逐列（disease）进行更新分别得到矩阵$Y_m$和$Y_d$。 Y_m(m_q) = \\frac{1}{Q_m}\\textstyle\\sum_{i=1}^Kw_iY(m_i)对不存在观测值即$Y_{ij}$=0的数据用$\\frac{Y_m + Y_d}{2}$来进行更新。接下来就是构建目标式，同时引入邻域信息。 GRNMF中的邻域 \\lambda_m\\sum_{i,p=1}^n\\lVert w_i-w_p \\rVert^2S_{ip}^{m*} S_{ij}^{m*}=X_{ij}^mS_{ij}^m X_{ij} = \\begin{cases} 1, i\\in N(m_j) \\And j\\in N(M_i) \\And m_i,m_j\\in C \\\\\\\\ 0, i\\notin N(m_j) \\And j\\notin N(M_i) \\And m_i,m_j\\notin C \\\\\\\\ \\frac 1 2, otherwise \\end{cases}GRNMF中利用了两种领域信息，第一种由ClusterOne算法产生，算法的结果有分成多少簇，哪些miRNA在同一簇内；第二种由k近邻产生，衡量是否近邻的相似度由论文提出的方法进行计算，miRNA和target分别用不同的方法计算。 NRLMF论文的目标是建模计算某个药物会对某个目标产生作用的概率值，概率的计算使用的是矩阵分解后得到的特征向量，药品和目标分别对应u和v p_{ij} = \\frac{exp(u_iv_j^T)}{1 + exp(u_iv_j^T)}论文的一个创新之处在于，对于已经得到实验验证的样本，模型中会赋予更高的权重。对于样本出现的概率使用的是似然函数进行估计，同时对特征矩阵U和V假设高斯先验分布，均值为0，方差为$\\sigma^2​$。可以证明这等同于引入L2正则。优化的目标式就等同于最大化取对数后的似然函数。 (6)式推导： \\begin{aligned} p(U,V|Y,\\sigma^2_d,\\sigma^2_t) &\\propto p(Y|U,V)p(U|\\sigma^2_d)p(V|\\sigma^2_t) \\\\ logp(U,V|Y,\\sigma^2_d,\\sigma^2_t) &= log(\\prod^m_{i=1}\\prod^n_{j=1}p_{ij}^{cy_{ij}}(1-p_{ij})^{(1-y_{ij})}\\prod_{i=1}^m\\frac{1} {\\sqrt{2\\pi} \\sigma_d}e^{-\\frac{u_i^2}{2\\sigma_d^2}}\\prod_{j=1}^n\\frac{1} {\\sqrt{2\\pi} \\sigma_t}e^{-\\frac{v_j^2}{2\\sigma_t^2}} \\\\ &=\\sum_{i=1}^m\\sum_{j=1}^n[cy_{ij}logp_{ij}+(1-y_{ij})log(1-p_{ij})]+\\sum_{i=1}^m[log(\\frac{1}{\\sqrt{2\\pi}\\sigma_d})-\\frac{u_i^2}{2\\sigma_d^2}]\\\\+\\sum_{j=1}^n[log(\\frac{1}{\\sqrt{2\\pi}\\sigma_t})&-\\frac{v_j^2}{2\\sigma_t^2}] \\\\ &=\\sum_{i=1}^m\\sum_{j=1}^ncy_{ij}[u_iv_j^T-log(1+e^{u_iv_j^T})]+(y_{ij}-1)log(1+e^{u_iv_j^T})-\\frac{1}{2\\sigma_d^2}\\sum_{i=1}^m\\lVert u_i \\rVert^2 \\\\-\\frac{1}{2\\sigma_t^2}\\sum_{j=1}^n\\lVert v_j \\rVert^2&+\\sum_{i=1}^mlog(\\frac{1}{\\sqrt{2\\pi}\\sigma_d}+\\sum_{j=1}^nlog(\\frac{1}{\\sqrt{2\\pi}\\sigma_t}) \\\\ &=\\sum_{i=1}^m\\sum_{j=1}^n[cy_{ij}u_iv_j^T-(cy_{ij}-y_{ij}+1)log(1+e^{u_iv_j^T})]-\\frac{1}{2\\sigma_d^2}\\sum_{i=1}^m\\lVert u_i \\rVert^2 \\\\-\\frac{1}{2\\sigma_t^2}\\sum_{j=1}^n\\lVert v_j \\rVert^2 &+ C \\end{aligned}NRLMF中的邻域 \\frac \\alpha 2\\sum_{i=1}^m\\sum_{\\mu=1}^na_{i\\mu}\\lVert u_i-u_\\mu\\rVert^2_F a_{i\\mu} = \\begin{cases} s_{i\\mu}^d, if d_{\\mu} \\in N(d_i)\\\\\\\\ 0, otherwise \\end{cases} \\\\目标式同GRNMF基本一致，同样是通过引入类似$a_{i\\mu}$来引入邻域信息，不同在于NRLMF没有利用其他算法产生的簇作为额外的邻域信息，只是利用了数据集中提供的drug和target的相似度，构建k近邻来作为邻域信息。 然而NRLMF在另外一个地方又使用了邻域的信息，因为在drug-target的数据集中存在某个药物没有任何治疗目标的情况，也就是说某一行全为0，论文中认为这种情况下进行矩阵分解所得到的特征向量会有很大误差，因此通过药物和药物之间的相似度，来利用其它存在观测值的药物的特征向量取一个k近邻来代替，以提高精度。 \\tilde{u} = \\begin{cases} u_i, &\\text{if } d_i \\in D^+ \\\\\\\\ \\frac{1}{\\textstyle\\sum_{\\mu \\in N^+(d_i)S_{i\\mu}^d}}\\textstyle\\sum_{\\mu \\in N^+(d_i)}s_{i\\mu}^du_{\\mu}, &\\text{if } d_i \\in D^- \\end{cases}证明高斯先验分布与L2正则的等价性首先我们假设现在需要从一些样本点$(x_1,y_1)···(x_N,y_N)$中来估计参数 $\\beta$，假设输出$y$与输入$x$之间线性相关，并且受噪声$\\epsilon$影响: y_n=\\beta x_n+\\epsilon这里的$\\epsilon$服从均值为0，方差为$\\sigma^2$的高斯分布，问题转换为如下的似然估计式子： \\prod_{n=1}^N N(y_n|\\beta x_n, \\sigma^2)接下来我们引入高斯先验分布$N(\\beta|0, \\frac{1}{\\lambda})$，这里的$\\lambda​$是一个正数值，把这个先验分布与上面的似然估计相结合，我们得到： \\prod_{n=1}^N N(y_n|\\beta x_n, \\sigma^2)N(\\beta|0, \\frac{1}{\\lambda})对这个式子取对数，并且对常数进行化简，因为它们不影响优化的结果，我们得到： \\sum_{n=1}^N-\\frac{1}{\\sigma^2}(y_n-\\beta x_n)^2-\\lambda \\beta^2+const根据最大似然的原则，我们需要最大化上面这个式子，这时就可以看出来为什么说高斯先验分布与L2正则等价了。 接下来是引入拉普拉斯约束来对特征向量进行优化，论文的另一个创新之处在于在约束中加入了邻域的信息，如上一节所讲。之后便是对目标函数进行优化得到迭代式以进行特征向量的更新。 问题 NRLMF(13)式的求导结果 可优化的地方GRNMF 利用邻域信息时，GRNMF只是用了一种聚类方法ClusterOne进行聚类，如果用多种聚类方法，多个结果中都被聚在同一类中认为可信度更高会不会更好？ NRLMF 如果像GRNMF一样不仅利用相似度得到邻域信息，还用其它聚类算法的聚类结果作为邻域信息会不会更好？","categories":[],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"}]},{"title":"矩阵求导和迹","slug":"矩阵求导和迹","date":"2019-10-21T12:22:04.624Z","updated":"2019-10-21T12:33:10.957Z","comments":true,"path":"2019/10/21/矩阵求导和迹/","link":"","permalink":"http://Bithub00.com/2019/10/21/矩阵求导和迹/","excerpt":"看到的一篇很好的介绍矩阵求导和迹的文章，mark下来","text":"看到的一篇很好的介绍矩阵求导和迹的文章，mark下来","categories":[],"tags":[{"name":"数学","slug":"数学","permalink":"http://Bithub00.com/tags/数学/"}]},{"title":"高斯先验分布与L2正则","slug":"高斯先验与L2正则","date":"2019-10-21T08:48:26.354Z","updated":"2019-10-21T12:22:33.264Z","comments":true,"path":"2019/10/21/高斯先验与L2正则/","link":"","permalink":"http://Bithub00.com/2019/10/21/高斯先验与L2正则/","excerpt":"本篇博客探讨一下为什么说L2正则等同于高斯先验分布","text":"本篇博客探讨一下为什么说L2正则等同于高斯先验分布 首先我们假设现在需要从一些样本点$(x_1,y_1)···(x_N,y_N)$中来估计参数 $\\beta$，假设输出$y$与输入$x$之间线性相关，并且受噪声$\\epsilon$影响: y_n=\\beta x_n+\\epsilon这里的$\\epsilon$服从均值为0，方差为$\\sigma^2$的高斯分布，问题转换为如下的似然估计式子： \\prod_{n=1}^N N(y_n|\\beta x_n, \\sigma^2)接下来我们引入高斯先验分布$N(\\beta|0, \\frac{1}{\\lambda})$，这里的$\\lambda$是一个正数值，把这个先验分布与上面的似然估计相结合，我们得到： \\prod_{n=1}^N N(y_n|\\beta x_n, \\sigma^2)N(\\beta|0, \\frac{1}{\\lambda})对这个式子取对数，并且对常数进行化简，因为它们不影响优化的结果，我们得到： \\sum_{n=1}^N-\\frac{1}{\\sigma^2}(y_n-\\beta x_n)^2-\\lambda \\beta^2+const根据最大似然的原则，我们需要最大化上面这个式子，这时就可以看出来为什么说高斯先验分布与L2正则等价了。","categories":[],"tags":[{"name":"数学","slug":"数学","permalink":"http://Bithub00.com/tags/数学/"}]},{"title":"K-means总结","slug":"kmeans总结","date":"2019-09-23T06:15:16.807Z","updated":"2019-09-23T11:36:29.364Z","comments":true,"path":"2019/09/23/kmeans总结/","link":"","permalink":"http://Bithub00.com/2019/09/23/kmeans总结/","excerpt":"k-means1.kmeans简介1.1聚类算法（clustering Algorithms）介绍聚类是一种无监督学习—对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。","text":"k-means1.kmeans简介1.1聚类算法（clustering Algorithms）介绍聚类是一种无监督学习—对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。 聚类算法可以分为原型聚类（k均值算法（k-means）、学习向量量化、（Learning Vector Quantization -LVQ）、高斯混合聚类（Mixture-of-Gaussian），密度聚类（DBSCAN），层次聚类（AGNES）等。 2.kmeans原理详解k-means是一种常见的聚类算法，也叫k均值或k平均。通过迭代的方式，每次迭代都将数据集中的各个点划分到距离它最近的簇内，这里的距离即数据点到簇中心的距离。 kmean步骤： 随机初始化k个簇中心坐标 计算数据集内所有点到k个簇中心的距离，并将数据点划分近最近的簇 更新簇中心坐标为当前簇内节点的坐标平均值 重复2、3步骤直到簇中心坐标不再改变（收敛了） 优缺点及改进算法 优点：效率高、适用于大规模数据集 缺点 改进 描述 k值的确定 ISODATA 当属于某个簇的样本数过少时把这个簇去除，当属于某个簇的样本数过多、分散程度较大时把这个簇分为两个子簇 对奇异点敏感 k-median 中位数代替平均值作为簇中心 只能找到球状群 GMM 以高斯分布考虑簇内数据点的分布 分群结果不稳定 k-means++ 初始的聚类中心之间的相互距离要尽可能的远 算法十问k值的选取 1234567k-means算法要求事先知道数据集能分为几群，主要有两种方法定义k。elbow method通过绘制k和损失函数的关系图，选拐点处的k值。经验选取人工据经验先定几个k，多次随机初始化中心选经验上最适合的。通常都是以经验选取，因为实际操作中拐点不明显，且elbow method效率不高。 K-means算法中初始点的选择对最终结果的影响 1k-means选择的初始点不同获得的最终分类结果也可能不同，随机选择的中心会导致K-means陷入局部最优解。 为什么在计算k-means之前要将数据点在各维度上归一化 12因为数据点各维度的量级不同。举个栗子，最近正好做完基于RFM模型的会员分群，每个会员分别有R（最近一次购买距今的时长）、F（来店消费的频率）和M（购买金额）。如果这是一家奢侈品商店，你会发现M的量级（可能几万元）远大于F（可能平均10次以下），如果不归一化就算k-means，相当于F这个特征完全无效。如果我希望能把常客与其他顾客区别开来，不归一化就做不到。 k-means不适用哪些数据 121. 数据特征极强相关的数据集，因为会很难收敛（损失函数是非凸函数），一般要用kernal k-means，将数据点映射到更高维度再分群。2. 数据集可分出来的簇密度不一，或有很多离群值（outliers），这时候考虑使用密度聚类。 k-means 中常用的距离度量 1K-means中比较常用的距离度量是欧几里得距离和余弦相似度。 K-means是否会一直陷入选择质心的循环停不下来（为什么迭代次数后会收敛）？ 从k-means的第三步我们可以看出，每回迭代都会用簇内点的平均值去更新簇中心，所以最终簇内的平方误差和（SSE, sum of squared error）一定最小。 平方误差和的公式如下： L(X) = \\sum_{i=1}^{k}{\\sum_{j\\in C_i}{(x_{ij}-\\bar{x_i})^2}}聚类和分类区别 121. 产生的结果相同（将数据进行分类）2. 聚类事先没有给出标签（无监督学习） 如何对k-means聚类效果进行评估 回到聚类的定义，我们希望得到簇内数据相似度尽可能地大，而簇间相似度尽可能地小。常见的评估方式： 名称 公式 含义 如何比较 sum of squares within clusters(SSW) $\\sum_{i=1}^{K}{ \\parallel x_i-c_{l_i} \\parallel ^2}$ 所有簇内差异之和 越小越好 sum of squares between clusters(SSB) $\\sum_{i=1}^{K}{n_i \\parallel c_i-\\bar{x} \\parallel ^2}$ 簇心与簇内均值差异的加权和 越大越好 Calinski-Harabasz $\\frac{\\frac{SSB}{K-1}}{\\frac{SSW}{N-K}}$ 簇间距离和簇内距离之比（除数是惩罚项，因为SSW下降地比较快） 越大越好 Ball&amp;Hall $\\frac{SSW}{K}$ 几乎同SSW 越小越好 Dunn’s index $\\frac{\\min_{i=1}^M{\\min_{j=i+1}^M{d(c_i, c_j)}}}{\\max_{k=1}^M{diam(c_k)}}$ $where d(c_i, c_j)=$ $\\min_{x \\in c_i, x’ \\in c_j}{\\parallel x-x’ \\parallel}^2$ $diam(c_k)=$ $\\max_{x, x’ \\in c_k}{\\parallel x-x’ \\parallel}^2$ 本质上也是簇间距离和簇内距离之比 越大越好 另一个常见的方法是画图，将不同簇的数据点用不同颜色表示。这么做的好处是最直观，缺点是无法处理高维的数据，它最多能展示三维的数据集。如果维数不多也可以做一定的降维处理（PCA）后再画图，但会损失一定的信息量。 聚类算法几乎没有统一的评估指标，可能还需要根据聚类目标想评估方式，如对会员作分群以后，我想检查分得的群体之间是否确实有差异，这时候可以用MANOVA计算，当p值小于0.01说明分群合理。 K-means中空聚类的处理 1如果所有的点在指派步骤都未分配到某个簇，就会得到空簇。如果这种情况发生，则需要某种策略来选择一个替补质心，否则的话，平方误差将会偏大。一种方法是选择一个距离当前任何质心最远的点。这将消除当前对总平方误差影响最大的点。另一种方法是从具有最大SEE的簇中选择一个替补的质心。这将分裂簇并降低聚类的总SEE。如果有多个空簇，则该过程重复多次。另外编程实现时，要注意空簇可能导致的程序bug。 參考資料 Mann A K, Kaur N. Review paper on clustering techniques[J]. Global Journal of Computer Science and Technology, 2013. https://blog.csdn.net/hua111hua/article/details/86556322 REZAEI M. Clustering validation[J].","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://Bithub00.com/tags/机器学习/"}]},{"title":"协同过滤","slug":"协同过滤总结","date":"2019-09-23T02:52:32.259Z","updated":"2019-09-23T11:39:50.126Z","comments":true,"path":"2019/09/23/协同过滤总结/","link":"","permalink":"http://Bithub00.com/2019/09/23/协同过滤总结/","excerpt":"协同过滤(collaborative filtering)直观解释协同过滤是推荐算法中最常用的算法之一，它根据user与item的交互，发现item之间的相关性，或者发现user之间的相关性，进行推荐。比如你有位朋友看电影的爱好跟你类似，然后最近新上了《调音师》，他觉得不错，就会推荐给你，这是最简单的基于user的协同过滤算法（user-based collaboratIve filtering），还有一种是基于item的协同过滤算法（item-based collaborative filtering），比如你非常喜欢电影《当幸福来敲门的时候》，那么观影系统可能会推荐一些类似的励志片给你，比如《风雨哈佛路》等。如下主要分析user-based，item-based同理。","text":"协同过滤(collaborative filtering)直观解释协同过滤是推荐算法中最常用的算法之一，它根据user与item的交互，发现item之间的相关性，或者发现user之间的相关性，进行推荐。比如你有位朋友看电影的爱好跟你类似，然后最近新上了《调音师》，他觉得不错，就会推荐给你，这是最简单的基于user的协同过滤算法（user-based collaboratIve filtering），还有一种是基于item的协同过滤算法（item-based collaborative filtering），比如你非常喜欢电影《当幸福来敲门的时候》，那么观影系统可能会推荐一些类似的励志片给你，比如《风雨哈佛路》等。如下主要分析user-based，item-based同理。 导图 核心公式 符号定义$r_{u,i}$：user $u$ 对 item $i$ 的评分$\\bar{r}_{u}$：user $u$ 的平均评分$P_{a,b}$：用户$a,b$都有评价的items集合 核心公式 item-based CF 邻域方法预测公式 \\operatorname{Pred}(u, i)=\\overline{r}_{u}+\\frac{\\sum_{j \\in S_{i}}\\left(\\operatorname{sim}(i, j) \\times r_{u, j}\\right)}{\\sum_{j \\in S_{i}} \\operatorname{sim}(i, j)} 偏差优化目标 \\min _{b} \\sum_{(u, i) \\in K}\\left(r_{(u, i)}-\\mu-b_{u}-b_{i}\\right)^{2}其中$(u，i) \\in K$表示所有的评分，$\\mu$总评分均值，$b_u$为user $u$的偏差，$b_i$为item $i$ 的偏差。 加入正则项后的Funk SVD 优化公式 \\min _{u v} \\sum_{(u, i) \\in k n o w n}\\left(r_{u,i}-u_{u} v_{i}\\right)+\\lambda\\left(|u|^{2}+|v|^{2}\\right)其中$u_u$为user $u$的偏好，即为user特征矩阵$U$的第$u$行，$v_i$为item $i$的特征，即为特征矩阵$V$的第$i$列 注意要点 相似度与距离之间的关系 距离越大，相似度越小；距离越小，相似度越高。即在求解最大相似度的时候可以转为求解最小距离。 在协同过滤中，常用的相似度函数有哪些，简要说明 杰卡德相似度（Jaccard similarity）公式： sim_{jaccard}(u_{1}, u_{2})=\\frac{ \\text {items} \\text { bought by } u_{1}\\ and\\ u_{2}}{ \\text { items bought by } u_{1}\\ or\\ u_{2}}适用于二元情况，即定性情况，比如买或者没买，喜欢或者不喜欢，在数据稀疏的情况，可以转为二元应用。 余弦相似度公式： \\operatorname{sim}(u_{1}, u_{2})=\\frac{r_{u_{1}} \\cdot r_{u_{2}}}{\\left|r_{u_{1}}\\right|_{2}|r_{u_{2}}|_{2}}=\\frac{\\sum_{i \\in P_{u_1,u_2}} r_{u_{1}, i} r_{u_{2}, i}}{\\sqrt{\\sum_{i \\in P_{u_1}} r_{u_{1},i}^{2}} \\sqrt{\\sum_{i \\in P_{u_2}}r_{u_{2},i}^{2}}}考虑不同用户的评价范围不一样，比如乐天派一般评分范围一般会高于悲观的人，会将评分进行去中心化再进行计算，即 修正余弦相似度，公式变为 \\operatorname{sim}(u_{1}, u_{2})=\\frac{r_{u_{1}} \\cdot r_{u_{2}}}{\\left|r_{u_{1}}\\right|_{2}|r_{u_{2}}|_{2}}=\\frac{\\sum_{i \\in P_{u_1,u_2}} (r_{u_{1}, i}-{\\bar{r}_{u_{1}}}) (r_{u_{2}, i}-\\bar{r}_{u_2})}{\\sqrt{\\sum_{i \\in P_{u_1}} (r_{u_{1},i}-\\bar{r}_{u_{1}})^{2}} \\sqrt{\\sum_{i \\in P_{u_2}}(r_{u_{2},i}-\\bar{r}_{u_{2}})^{2}}}适用于定量情况，比如评分场景，要求数据具有一定的稠密度。注意如果计算一个评价很少电影的用户与一个评价很多电影的用户会导致相似度为0. 皮尔森相关系数公式： \\operatorname{sim}(u_1, u_2)=\\frac{\\sum_{i \\in P_{u_1.u_2}}\\left(r_{u_1, i}-\\overline{r}_{u_1}\\right)\\left(r_{u_2, i}-\\overline{r}_{u_2}\\right)}{\\sqrt{\\sum_{i \\in P_{u_1.u_2}}\\left(r_{u_1, i}-\\overline{r}_{u_1}\\right)^{2}} \\sqrt{\\sum_{i \\in P_{u_1.u_2}}\\left(r_{u_2, i}-\\overline{r}_{u_2}\\right)^{2}}}皮尔森系数跟修正的余弦相似度几乎一致，两者的区别在于分母上，皮尔逊系数的分母采用的评分集是两个用户的共同评分集（就是两个用户都对这个物品有评价），而修正的余弦系数则采用两个用户各自的评分集。 $L_{p}-norms$公式： sim(u_1,u_2) =\\frac{1}{ \\sqrt[p]{| r_{u_1}-r_{u_2} |^p}+1}$p$取不同的值对应不同的距离公式，空间距离公式存在的不足这边也存在。对数值比较敏感。 有了相似度测量后，那么基于邻域的推荐思路是怎样的呢？过滤掉被评论较少的items以及较少评价的users，然后计算完users之间的相似度后，寻找跟目标user偏好既有大量相同的items，又存在不同的items的近邻几个users(可采用K-top、阈值法、聚类等方式)，然后进行推荐。步骤如下：(1) 选择：选出最相似几个用户，将这些用户所喜欢的物品提取出来并过滤掉目标用户已经喜欢的物品(2) 评估：对余下的物品进行评分与相似度加权(3) 排序：根据加权之后的值进行排序(4) 推荐：由排序结果对目标用户进行推荐 协同过滤算法具有特征学习的特点，试解释原理以及如何学习 特征学习：把users做为行，items作为列，即得评分矩阵$R_{m,n}=[r_{i,j}]$，通过矩阵分解的方式进行特征学习，即将评分矩阵分解为$R=U_{m,d}V_{d,n}$，其中$U_{m,d}$为用户特征矩阵，$V_{d,n}$表示items特征矩阵，其中$d$表示对items进行$d$个主题划分。举个简单例子，比如看电影的评分矩阵划分后，$U$中每一列表示电影的一种主题成分，比如搞笑、动作等，$V$中每一行表示一个用户的偏好，比如喜欢搞笑的程度，喜欢动作的程度，值越大说明越喜欢。这样，相当于，把电影进行了主题划分，把人物偏好也进行主题划分，主题是评分矩阵潜在特征。 学习方式:SVD分解式为 R_{m,n}=U_{m,m}\\Sigma_{m,n}V_{n,n}^T 其中$U$为user特征矩阵，$\\Sigma$为权重矩阵体现对应特征提供的信息量，$V$为item特征矩阵。同时可通过SVD进行降维处理，如下 奇异值分解的方式，便于处理要目标user（直接添加到用户特征矩阵的尾部即可），然而要求评分矩阵元素不能为空，因此需要事先进行填充处理，同时由于user和item的数量都比较多，矩阵分解的方式计算量大，且矩阵为静态的需要随时更新，因此实际中比较少用。 Funk SVD， Funk SVD 是去掉SVD的$\\Sigma$成分，优化如下目标函数，可通过梯度下降法，得到的$U,V$矩阵 J=\\min _{u v} \\sum_{(u, i) \\in k n o w n}\\left(r_{u,i}-u_{u} v_{i}\\right)+\\lambda\\left(|u|^{2}+|v|^{2}\\right)Funk SVD 只要利用全部有评价的信息，不需要就空置进行处理，同时可以采用梯度下降法，优化较为方便，较为常用。 有了user特征信息和item特征信息，就可用$u_{u} v_{i}$对目标用户进行评分预测，如果目标用户包含在所计算的特征矩阵里面的话。针对于新user、新item，协同过滤失效。 如何简单计算user偏差以及item偏差？ b_u=\\frac{1}{|I_u|}\\sum_{i \\in I_u}(r_{u,i}-\\mu) \\\\ b_i=\\frac{1}{|U_i|}\\sum_{u \\in U_i}(r_{u,i}-b_u-\\mu) 如何选择协同过滤算法是基于user还是基于item一般，谁的量多就不选谁。然而基于user的会给推荐目标带来惊喜，选择的范围更为宽阔，而不是基于推荐目标当前的相似item。因此如果要给推荐目标意想不到的推荐，就选择基于user的方式。可以两者结合。 协同过滤的优缺点 缺点： (1)稀疏性—— 这是协同过滤中最大的问题，大部分数据不足只能推荐比较流行的items，因为很多人只有对少量items进行评价，而且一般items的量非常多，很难找到近邻。导致大量的user木有数据可推荐（一般推荐比较流行的items），大量的item不会被推荐 (2)孤独用户——孤独user具有非一般的品味，难以找到近邻，所以推荐不准确 (3)冷启动——只有大量的评分之后，才能协同出较多信息，所以前期数据比较少，推荐相对不准确；而如果没有人进行评分，将无法被推荐 (4)相似性——协同过滤是与内容无关的推荐，只根据用户行为，所以倾向于推荐较为流行的items。 优点： (1)不需要领域知识，存在users和items的互动，便能进行推荐 (2)简单易于理解 (3)相似度计算可预计算，预测效率高 协同过滤与关联规则的异同关联规则是不考虑tems或者使用它们的users情况下分析内容之间的关系，而协同过滤是不考虑内容直接分析items之间的关系或者users之间的关系。两者殊途同归均能用于推荐系统，但是计算方式不同。 实践中的一些注意点(1) 过滤掉被评价较少的items(2) 过滤掉评价较少的users(3) 可用聚类方式缩小搜索空间，但是面临找不到相同偏好的用户（如果用户在分界点，被误分的情况），这种方式通过缩小搜索空间的方式优化协同过滤算法(4) 使用的时候，可以考虑时间范围，偏好随着时间的改变会改变 面试真题使用协同过滤算法之前，数据应该如何进行预处理？协同过滤的方式有哪些？如何通过相似度计算设计协同过滤推荐系统？请谈谈你对协同过滤中特征学习的理解？如何将协同过滤用于推荐系统？FUNK SVD相对于SVD有哪些优势？如何求解FUNK SVD？请描述下协同过滤的优缺点？","categories":[],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"},{"name":"机器学习","slug":"机器学习","permalink":"http://Bithub00.com/tags/机器学习/"},{"name":"协同过滤","slug":"协同过滤","permalink":"http://Bithub00.com/tags/协同过滤/"}]},{"title":"算法基本思想","slug":"算法基本思想","date":"2019-09-21T02:02:42.483Z","updated":"2019-09-21T02:06:07.789Z","comments":true,"path":"2019/09/21/算法基本思想/","link":"","permalink":"http://Bithub00.com/2019/09/21/算法基本思想/","excerpt":"算法设计常见的5种基本技巧，有贪婪算法、分治算法、动态规划、随机化算法和回溯算法。","text":"算法设计常见的5种基本技巧，有贪婪算法、分治算法、动态规划、随机化算法和回溯算法。 贪婪算法虽然每次的选择都是局部最优，当在算法结束的时候，其期望是全局最优才是正确的。不过有时，在不同条件与要求下时，最优解的答案可能不止有一个或不一样，而贪婪算法也可以得出一个近似的答案。 分治算法分治算法不是简单的递归，而是将大的问题递归解决较小的问题，然后从子问题的解构建原问题的解。比如，快速排序和归并排序算分治算法，而图的递归深度搜索和二叉树的递归遍历则不是分治算法的运用。 动态规划动态规划与分治算法的区别是，两种算法同样是将较大的问题分解成较小问题，而动态规划对这些较小的问题并不是对原问题明晰的分割，其中一部分是被重复求解的，因此动态规划将较小问题的解记录下来，使得在处理较大问题的时候，可以不用重复去处理较小的问题，而是直接利用所记录的较小问题的答案来求解。 随机化算法随机化算法的一个应用是在快速排序中对枢纽元素的选择，使得算法的运行时间不止依赖于特定的输入，还而且还依赖于所出现的随机数。虽然一个随机化算法的最坏情形运行时间常常与非随机化算法的最坏情形运行时间相同，但两者还是有区别的，好的随机化算法没有坏的输入，而只有坏的随机数。 回溯算法回溯算法相当于穷举搜索的巧妙实现，对比蛮力的穷举搜索，回溯算法可以对一些不符合要求的或者是重复的情况进行裁剪，不再对其进行搜索，以减少搜索的工作量提高效率。比如，在图运用回溯算法的深度优先搜索遍历中，会对已搜索遍历过的顶点进行标记，避免下次的回溯搜索中对再次出现的该顶点进行重复遍历。","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"http://Bithub00.com/tags/算法/"}]},{"title":"一亩三分地题库+新手上路答案","slug":"一亩三分地","date":"2019-09-11T11:13:30.290Z","updated":"2019-11-15T07:10:12.449Z","comments":true,"path":"2019/09/11/一亩三分地/","link":"","permalink":"http://Bithub00.com/2019/09/11/一亩三分地/","excerpt":"1.一亩三分地 新手上路 网站规则 - 满分5大米(适用于所有用户)2.一亩三分地 考试中心 满分95大米(只适用于2019年之后注册的新人)3.一亩三分地 每日答题 题库","text":"1.一亩三分地 新手上路 网站规则 - 满分5大米(适用于所有用户)2.一亩三分地 考试中心 满分95大米(只适用于2019年之后注册的新人)3.一亩三分地 每日答题 题库 网站规则 - 满分5大米 一亩三分地发帖可以选择内容用hide进行隐藏。隐藏方式：[hide=200]隐藏的内容[/hide] 下面哪个选项hi‍‌‌‌‌‌‌‌‍‍‍‌‌‌‍‌‌‌‌‌de正确。 (1.0 分)A. 两人第一次华山论剑，争的是荣名与[hide=188]《九阴真经》[/hide]；B. 第二次在桃花岛过招，是为了郭靖与[hide=188]欧阳克[\\hide]争婚。C. 第三次海上相斗，生死只隔一线，但(hide=188)洪七公(\\ hide)手下尚自容让；D. 现下第四次恶战，[hide188]才是各出全力[/hide]，再无半点留情。E. 洪七公伸指疾点欧阳锋背心[hide=188]“凤尾穴”(/hide)，要迫他松手。 为什么一亩三分地除了租房广告找室友、学有飞友工友、本地版聚等少数版块之外，绝大多数板块都不允许拉群？ (1.0 分)A. 地里信息是公开的，全部回复每个人都可以看到。而群里的信息，作为新入群的同学是无法看到历史消息的，并且无法查找B. 公开自己的微信，将来可能会被人人肉或者骚扰C. 地里的信息永远存在，有些群很可能不活跃甚至不存在了，里面的讨论也就消失了D. 以上全都理解并接受 在下面哪些板块里留微信号等各种联系方式，事后可以要求版主删除？ (1.0 分)A. 如果发在求职类板块里，可以删除，我的隐私你得尊重啊B. 如果发在留学类板块里，可以删除，毕竟我年少不懂事嘛C. 如果发在非正事、无聊帖子里，可以删除，毕竟没啥营养啊D. 绝大多数板块禁止留微信、拉群，如果你非要发出来，那就永远不删。另外，私下联系建议发站内信。 为什么我们不鼓励用谐音或者各种拐弯抹角的说法来指代公司或者学校名称？比如：“湾区某元音开头和结尾公司” (1.0 分)A. 也许有什么顾虑吧B. 这样不好玩C. 写成这样子，别人看不懂，也搜不到。如果别人也这样写，你也看不懂、搜不到。信息没法分享和交流。D. 不知道 下面哪个说法是错误的？ (1.0 分)A. 连续超过14天不登录，每天扣一个大米，直到大米数=100B. 抖包袱版看帖，不会消耗积分C. 可以消耗积分更改用户名（网站右上角设置-&gt;个人资料-&gt;更改用户名）D. 看到干货帖子和回复，给作者加分，不会消耗我的积分E. 每日签到、每日答题都可以拿到积分奖励F. 手机app里也可以每日签到，好方便！G. 给地里官方app五星好评，可以拿加分奖励: www.1point3acres.com/bbs/thread-446981-1-1.htmlH. 在手工加分的帖子里，多次回复骗取积分，会被系统检测到，积分扣除+额外扣分！I. 网站上方导航栏 -&gt; 道具中心，可以兑换匿名卡，把自己的帖子匿名。 网站规则 - 满分95大米 一亩三分地里有哪些方面的信息？ (5.0 分)A. 美国大学各专业录取信息B. 美国和中国公司面试题目C. 买房买车租‍‌‌‌‌‌‌‌‍‍‍‌‌‌‍‌‌‌‌‌房等生活信息D. 移民签证H1B等信息E. 公开课、刷题、做项目F. 八卦我司版：美国公司评价（各公司员工发帖）G. 抖包袱版：各公司工资offer的详细数目H. 缘来如此版可以发帖征友I. 职场达人：中美职场和职业发展话题J. TOEFL/GRE考试信息 一亩三分地有积分限制，请戳这里阅读《攒积分和消耗积分完全指南》然后回答：下面哪些说法正确？ (10.0 分)A. 认真做考卷，很容易拿到满分B. 每日签到（网站右上方），奖励1颗大米C. 每日答题（网站右上方），答对了奖励1颗大米，答错了消耗1颗D. 设置头像，奖励一颗大米（只能获得一次）E. 验证邮箱，奖励一颗大米（只能获得一次）F. 管理员每日选择2-3个干货帖子全站置顶，大家会奖励给你大米G. 分享面经、录取信息、工资数目等干货，或者积极参与讨论H. 积极参与论坛各种活动，比如给地里官方app五星好评I. 《攒积分和消耗积分完全指南》里有更多获取积分方法的说明J. 积分变更记录在网站右上角设置-&gt;积分里能找到（2019年3月2日之前的部分记录不显示，之后的全显示） 一亩三分地发帖可以选择内容进行隐藏。举个例子：黄蓉哭了一会，抽抽噎噎的道：“我听爹爹说过，洪老前辈有一套武功，当真是天下无双、古今独步，甚至全真教的王重阳也忌惮三分，叫做……叫做……咦，我怎么想不起来啦，明明刚才我还记得的，我想求他教你，这套拳法叫做……叫做……”其实她哪里知道，全是信口胡吹。洪七公在树顶上听她苦苦思索，实在忍不住了，喝道：“叫做‘[hide=200]降龙十八掌[/hide]’！”说着一跃而下。上述段落里，“降龙十八掌”五个字被隐藏，只有积分&gt;=200分的用户才能看到。隐藏方式：[hide=200]隐藏的内容[/hide] 注意不要添加多余的空格！第二个hide前面的斜线别弄反方向！下面隐藏的内容哪个被188分正确隐藏？真相只有一个！ (10.0 分)A. 柯南的名字是[hide=188]工藤新一[\\hide]B. 柯南的名字是[hide]工藤新一[/hide=188]C. 柯南的名字是[hide=188]工藤新一[/hide]D. 柯南的名字是[hide=188]工藤新一[\\ hide]E. 柯南的名字是[hide=188]工藤新一{\\hide}F. 柯南的名字是(hide=188)工藤新一(\\hide) 在论坛发slack群、qq群、微信群等任何站外讨论方式，会如何？ (10.0 分)A. 如果发在本地版聚、租房找室友、飞友学友工友版块，是允许的B. 如果发在求职、面经、申请类板块里，都会被删帖扣分C. 举报这些群，可能得到加分奖励D. 以上都正确 下面哪种情况，管理员会按照你的要求，进行删帖？ (10.0 分)A. 问了问题，得到了答案，然后我过河拆桥，删帖让其他人看不到B. 尽管地里不允许，但是我到处留微信号，然后说隐私暴露要求删帖C. 发帖赚到了积分，看到了有权限设置的内容，然后反悔D. 这些情况全都不删帖！ 你的一亩三分地账号很宝贵，在别的网站，考完托福你就离开了，申请完了你也不会再去。但是在一亩三分地，拿到录取以后，你找实习要回来，找全职工作要回来，申请OPT要回来，等H1b签证和绿卡要回来，甚至工作几年后要跳槽你也要回来看求职信息 - 随着地里的壮大，你会经常回来。下面哪些方法可以保护你的账号？ (10.0 分)A. 给账号设置复杂密码，并且只在一亩三分地使用该密码。B. 把地里的邮件放入白名单以免被判定为垃圾邮件而导致收不到密码重设等重要信息（地里从来不发广告）。C. 绑定微信。如果账号有异常，会被系统要求扫码登录，而且扫码登录比输入密码方便。D. 管理好用来注册账号的电子邮箱，如果连邮箱都没了，那就惨了。。。E. 别那么多有的没的，我爱咋地就咋地。 下面哪些发帖行为，值得鼓励和倡导？ (10.0 分)A. 随便发就是了，反正有版主和管理员给调整B. 读一下各个版块的置顶信息，了解网站规则。C. 发帖之前，先浏览有哪些版块，帖子要发在对应的板块里。D. 帖子设置回复可见，大家必须回复才能看到，然后一堆人发“顶”E. 帖子标题最好能用一句话概括主要内容，方便大家阅读F. 地里很多帖子有分类设置，方便大家查找，发帖确保分类信息设置合理G. 很多帖子有权限设置，我积分不够，到处发“看不到啊”、“积分不够啊”，水掉论坛 新人积分不够。下面哪些做法正确？ (5.0 分)A. 分享干货、签到答题、参加活动等，争取早日攒够积分。B. 到处发帖抱怨积分不够、看不到，没准会有帮助呢。C. 想各种办法绕开积分限制，比如要求楼主私下把信息发给我D. 注册个小号，相互加分（嘿嘿，地里有后台系统检测哦） 下面哪些方式，可以获得积分奖励？ (5.0 分)A. 分享干货，无论是录取信息、面经题目、工资数据，还是各种生活经验。B. 在帖子里给别人加分，告诉对方给我加回来。C. 认真参与网站里的讨论，贡献我的看法和信息D. 没有分享干货，但是可以缠着别人要积分E. 干货帖子被全站置顶或者公众号推出，至少拿100分，很多帖子拿到200+F. 我有信息可以分享，但是大家来给我加分吧，谁加了分我私下里发给你 一亩三分地里可以购买VIP通行证，位于网站上方导航栏 -&gt; VIP。关于VIP，下面哪些说法正确？ (10.0 分)A. VIP可以瞬间解锁地里有权限的帖子，获得阅读权限B. VIP下载附件不扣积分，不受权限限制C. VIP用户可以自由搜索，不扣积分D. VIP是“免责金牌”，在地里不受规则约束，可以为所欲为E. VIP获得的只是阅读帖子权限，网站有的功能仍需积分（可以攒大米或者购买蓝莓）F. VIP收入用来支付一亩三分地运营和发展的必要花费；我的打赏能支持一亩三分地做的更好 下面哪种行为，在地里会被扣光积分，甚至封号？ (10.0 分)A. 戾气十足、人身攻击B. 种种手段恶意刷分C. 软硬广告或导流吸粉D. 造谣或者发钓鱼贴E. 多人共享账号F. 违反版规，胡乱跨版发帖，到处留邮箱或微信号G. 购买了VIP通行证，作为付费用户，违反网站规则。 每日答题题库【题目】 公司之间级别对应，如下哪个是错误的？Google T5 = Facebook E5/6Uber Sr II = Lyft T6Amazon L7 = Facebook E6✓ Facebook L6 = Facebook E6 【题目】一亩三分地发帖可以选择内容用 hide 进行隐藏。隐藏方式：[hide=200]隐藏的内容[/hide] 下面哪个选项 hide 正确。✓ A. 两人第一次华山论剑，争的是荣名与[hide=188]《九阴真经》[/hide]；B. 第二次在桃花岛过招，是为了郭靖与[hide=188]欧阳克[\\hide]争婚。C. 第三次海上相斗，生死只隔一线，但(hide=188)洪七公(\\ hide)手下尚自容让；D. 现下第四次恶战，[hide188]才是各出全力[/hide]，再无半点留情。E. 洪七公伸指疾点欧阳锋背心[hide=188]“凤尾穴”(/hide)，要迫他松手。 【题目】为什么一亩三分地除了租房广告找室友、学有飞友工友、本地版聚等少数版块之外，绝大多数板块都不允许拉群？A. 地里信息是公开的，全部回复每个人都可以看到。而群里的信息，作为新入群的同学是无法看到历史消息的，并且无法查找B. 公开自己的微信，将来可能会被人人肉或者骚扰C. 地里的信息永远存在，有些群很可能不活跃甚至不存在了，里面的讨论也就消失了✓ D. 以上全都理解并接受 【题目】在下面哪些板块里留微信号等各种联系方式，事后可以要求版主删除？A. 如果发在求职类板块里，可以删除，我的隐私你得尊重啊B. 如果发在留学类板块里，可以删除，毕竟我年少不懂事嘛C. 如果发在非正事、无聊帖子里，可以删除，毕竟没啥营养啊✓ D. 绝大多数板块禁止留微信、拉群，如果你非要发出来，那就永远不删。另外，私下联系建议发站内信。 【题目】为什么我们不鼓励用谐音或者各种拐弯抹角的说法来指代公司或者学校名称？ 比如：“湾区某元音开头和结尾公司”，你能猜到是哪家吗？A. 也许有什么顾虑吧B. 这样不好玩✓ C. 写成这样子，别人看不懂，也搜不到。如果别人也这样写，你也看不懂、搜不到。信息没法分享和交流。D. 不知道 【题目】下面哪个说法是错误的？A. 连续超过 14 天不登录，每天扣一个大米，直到大米数=100✓ B. 抖包袱版看帖，不会消耗积分C. 可以消耗积分更改用户名（网站右上角设置-&gt;个人资料-&gt;更改用户名）D. 看到干货帖子和回复，给作者加分，不会消耗我的积分 E. 每日签到、每日答题都可以拿到积分奖励F. 手机 app 里也可以每日签到，好方便！G. 给地里官方 app 五星好评，可以拿加分奖励: www.1point3acres.com/bbs/thread-446981-1-1.html H. 在手工加分的帖子里，多次回复骗取积分，会被系统检测到，积分扣除+额外扣分！I. 网站上方导航栏 -&gt; 道具中心，可以兑换匿名卡，把自己的帖子匿名。 【题目】 下面哪个情况，不会消耗你的积分？超过 14 天不登录使用论坛搜索下载附件✓ 看到干货帖子和精华回复，给作者加分！ 【题目】 下面哪个州，没有 income tax?✓ NevadaNew YorkNebraskaMassachusetts 【题目】 下面哪个州，有 state income taxTennesseeAlaskaWashington✓ Mississippi 【题目】 求内推如何作死？一下子叫好多人给内推同一家公司别人回复慢了就抱怨简历上撒谎✓ 这些都会作死 【题目】 下面哪种方法，可以妥妥拿到积分？上传头像每日签到（需绑定微信）分享干货✓ 这些全都可以 【题目】 回答别人的私信提问还需要消耗我 5 大米怎么办？✓ 直接在版面回答，这样大家都能看见 【题目】 下面哪种行为，在地里会被扣光积分，甚至封号？✓ 这些全都会 【题目】一亩三分地发帖可以用 hide 语法隐藏内容。下面哪个写法正确？✓ 柯南的名字是[hide=200]工藤新一[/hide]柯南的名字是[hide=200]工藤新一[\\hide]柯南的名字是[hide=200]工藤新一[hide]柯南的名字是[hide=200]工藤新一[/hide=200] 【题目】 在 Linkedin 上求内推如何作死看也不看对方情况，直接扔简历要求内推，国人必须帮助国人啊写模板内容要求内推，不论男女都叫学姐也不自我介绍，就要求对方介绍公司情况✓ 这些都会作死 【题目】一亩三分地鼓励如何发面经？遇到有人留邮箱，私下发面经的，点举报积分隐藏[hide==188]内容[/hide]✓ 以上都正确 【题目】 下面哪个大学在华盛顿州？Washington University✓ University of WashingtonGeorge Washington UniversityWashington College 【题目】下面哪个大学不在 Virginia/DC 附近✓ Washington and Jefferson CollegeTrinity Washington UniversityGeorge Washington UniversityWashington and Lee University 【题目】 下面哪个州，对公司友好，所以吸引了美国很多公司注册？加利福尼亚✓ 特拉华佛罗里达纽约 【题目】 下面哪个州，有 state income taxSouth DakotaWyoming✓ North DakotaTennessee 【题目】 下面哪个州，没有 state income taxNew YorkNew Jersey✓ New HampshireNew Mexico 【题目】 下面哪个州，没有 state income tax?✓ FloridaGeorgiaHawaiiIdahoda 【题目】 下面哪个州，没有 state income tax?Alabama✓ AlaskaArizonaArkansas 【题目】下面哪个州冬天最暖和？Minnesota✓ OklahomaMichiganMassachusetts 【题目】下面哪个大学实际上不存在？University of California, San FranciscoUniversity of Massachusetts, Dartmouth✓ University of Michigan, Twin CityUniversity of Nevada, Las Vegas 【题目】下面哪所大学所在城市不是波士顿？✓ Boston CollegeBerklee College Of MusicNortheastern UniversityBoston University 【题目】下面哪个说法错误？伊利诺伊大学在芝加哥有校区✓ 芝加哥是美国著名的雨城美国西北大学在芝加哥有校区芝加哥 skydeck 上可以看到四个州 【题目】 Which company is the largest single✓site employer in the US?WalmartFordCostco✓ Disney World 【题目】 下面哪种方法，可以妥妥拿到积分？分享干货上传头像每日签到（需绑定微信）✓ 这些全都可以 【题目】 下面哪家公司的总部不在西雅图亚马逊阿拉斯加航空公司星巴克✓ 波音 【题目】 给论坛 ios 或者安卓手机应用留评价如何获取 50 大米？留 5 星评价截屏作为证据上传到第一个大区的”官方开发版“✓ 以上步骤都需要 【题目】 地里发帖可以隐藏内容。假如要设置 200 积分以上才可以看到，下面哪个语法正确？[hide]想要隐藏的内容[/hide][hide=200 ]想要隐藏的内容[/hide]✓ [hide=200]想要隐藏的内容[/hide][hide=200]想要隐藏的内容[hide] 【题目】 地里面经数目最多的是哪家公司？FacebookGoogle✓ AmazonUber 【题目】 Negotiate 工资的时候，哪种做法有利于得到更大的包裹？拿地里抖包袱版的工资数字要对方 match直接告诉对方自己目前薪酬，让对方看着良心办开一个天价，谈不拢就散伙✓ 精读地里谈工资宝典，知己知彼，百战不殆 【题目】 which state is University of Miami located?CaliforniaNevada✓ FloridaOhio 【题目】 下面哪个城市没有 SUNY（纽约州立大学）校区？AlbanyBuffalo✓ FultonStony Brook 【题目】 下面哪个州里有 Disney World？✓ FloridaNew YorkNorth CarolinaTexas 【题目】 下面哪所大学所在城市不是波士顿？✓ MITBoston UniversityNortheastern UniversityEmerson College 【题目】 关于旧金山市中心描述，下面哪个不正确？走路得看着路，很多流浪汉，地上屎尿一不小心会踩上车里一定不要放东西，但即使不放，也可能被砸车玻璃Uber/Airbnb/Pinterest/Twitter 等著名科技公司都在 SOMA 区✓ 旧金山创业公司很多，被称为“硅谷” 【题目】 一亩三分地是哪年创立的？✓ 2009201120132015 【题目】 下面哪个州在美国西海岸VirginiaNorthDakotaMaine✓ Washington 【题目】 which state is University of Miami located?Ohio✓ FloridaNevadaCalifornia 【题目】 加州大学有多个分校，下面哪个成立时间最短？UC Davis✓ UC MercedUC RiversideUC Santa Cruz 【题目】 下面哪个专业，不是 STEM，OPT 没法延期？会计学以前不是，现在很多学校 stem 获批数据科学EECS✓ 教育学 【题目】 哪种选校策略最合理？按照排名高低选，谁高谁就好交给中介选，反正不想操心所有学校都申，蒙中哪个算哪个✓ 根据自己下一步职业和学业目标，参考地里数据和成功率，认真斟酌 【题目】 一亩三分地是谁创立的？✓ Warald俞敏洪李大辉徐小平 【题目】 下面几个州，哪个离美国首都最远？MarylandDelaware✓ North CarolinaVirginia 【题目】 地里数据科学类职位面经放在在什么版最合理？数据科学版美国面经版数据科学分类✓ 数科面经版找工求职版 【题目】 下面哪个公司总部在圣地亚哥？✓ QualcommAMDNvidiaNetflix 【题目】 下面哪种情况，管理员会按照你的要求，进行删帖？问了问题，得到了答案，然后我过河拆桥，删帖让其他人看不到发帖赚到了积分，看到了有权限设置的内容，然后反悔尽管地里不允许，但是我到处留微信号，然后说隐私暴露要求删帖✓ 这些情况全都不删帖！ 【题目】 Miami University 在哪个城市Miami, FloridaLas Vegas, Nevada✓ Oxford, OhioLos Angeles, California 【题目】 想找室友或者当房东，帖子发在哪里？✓ 租房广告|找室友版房地产版生活版面经版 【题目】 在论坛发 slack 群，qq 群，微信群，任何站外讨论方式，会如何？如果发在求职面经大区，申请大区，都会被删帖扣分举报这些群，可能得到加分奖励✓ 以上都正确如果发在版聚，或者本地版块，是允许的 【题目】 下面哪类版块，可以拉群，而且不会被警告扣分？录取结果汇报求职、面经内推✓ 学友工友、找室友或者版聚本地 【题目】下面哪个说法错误？雪城大学尽管在纽约州，但是离纽约城很远！✓ 中国驻纽约领事馆位于法拉盛中国城，周围全是好吃的！哥伦比亚大学离纽约中央公园很近纽约州立大学石溪分校学费很便宜 【题目】 下面哪个学术会议不是机器学习领域的？CVPRICMLSIGKDD✓ ICSE 【题目】 下面哪个童话故事不是安徒生写的✓ 尼尔斯骑鹅旅行记冰雪女王卖火柴的小女孩国王的新装 【题目】 下面哪个作家是英国人？✓Charles DickensErnest HemingwayVictor HugoAlexander Pushkin 【题目】 income tax on wages✓North DakotaSouth DakotaWyomingTeness… 【题目】 下面哪个machine learning 的模型不是supervisedLogistic regression✓ClusteringSVMDecision Tree 【题目】 Apollo 11是哪一年登月的？1969 【题目】 下面哪个公司的streaming service不是会员subscription付费模式运营的？✓tubi 【题目】 著名篮球运动员姚明效力的NBA球队是休斯敦火箭队。取名“ 火箭队”是因为休斯敦是美国著名的?钢城汽车城✓ 宇航工业城电影城 【题目】 音乐家贝多芬出生于哪国？✓ 德国法国意大利英国 【题目】 下面哪个Ivy League，离东海岸最远？BrownDartmouthPrinceton✓ Cornell 【题目】 美国哪个州没有夏令时？南达科他州爱荷华州✓亚利桑那州阿肯色州 【题目】 下面哪部作品是喜剧？麦克白李尔王✓仲夏夜之梦哈姆雷特 【题目】 下面哪个公司总部不在湾区？google✓snapchatfacebookApple 【题目】 下面哪所纽约高校坐落于中央公园附近？Fordham UniversityNew York UniversityNew York Institute of Technology✓Columbia University","categories":[],"tags":[{"name":"一亩三分地","slug":"一亩三分地","permalink":"http://Bithub00.com/tags/一亩三分地/"}]},{"title":"托福备考","slug":"托福备考","date":"2019-09-09T03:02:28.587Z","updated":"2019-09-16T06:24:25.265Z","comments":true,"path":"2019/09/09/托福备考/","link":"","permalink":"http://Bithub00.com/2019/09/09/托福备考/","excerpt":"8月25号香港荃湾考试局托福首考，9月4号出分102成功分手，R29L28S22W23，没有报过补习班，分享自己复习的经验，希望能帮助大家","text":"8月25号香港荃湾考试局托福首考，9月4号出分102成功分手，R29L28S22W23，没有报过补习班，分享自己复习的经验，希望能帮助大家 前言我也会把当初有帮助的知乎回答贴在这里，我自认为自己的写作准备方法是有效的，只是可能准备时间不是很充分没发挥出来，话不多说进入正题:我把自己积累的素材和格言等都放在github上，有需要的话可以自己获取：托福个人积累素材及格言等 阅读阅读向来是中国考生的强项，我这次最好的单项也是阅读。我觉得最主要是平时要多读点英文材料，例如时事评论或者论文，然后考试时注意时间的把控，就没什么大问题了，当然也可以按照传统的复习方法使用托福模考软件多做tpo 听力八月份的托福改革，减少了一篇听力lecture，好处是考试时不会这么累，因为托福的听力长度一般比较长，而且偏学术，需要全程保持专注和紧张，同时因为开考的时间不一致，可能你在听力时别人在考口语，会造成影响。坏处是题目量减少之后容错率降低，拿高分会变难一点，不过也不用顾虑太多，多练习才是王道。还有一点就是，网上说新的改革里加入了英国口音，我没怎么留意，然后说听力变成了1.2倍速度，我觉得这个不实，感觉和平时练的tpo语速是一样的，不要听信网上的说法给自己制造焦虑。我复习的方法来自知乎，觉得还是挺有效的，如果只是单纯做tpo然后对答案其实并不会提升自己的听力水平，这里贴出来给大家参考： 如何快速提高托福听力水平？ - 梁跃的回答 - 知乎 口语口语部分在八月改革之后改动很大，这次考试我也确认了网上的说法，即第一部分和第五部分是直接删掉了，因为是第一次考托福，考口语的时候我比较紧张，主要是考试界面的那个倒计时增添了我的紧张，总怕说不完，教训是平时复习的时候应该多掐表按照时间来练习，复习方法来自豆瓣：托福口语备考 写作写作的确需要长期积累才能出效果，我个人觉得自己的复习方法是正确的，这次的分数23也还行，可能需要积累到一定程度才会出效果。那就是对某些话题，直接上google看本土人士是怎么写的，不要去背很多范文，因为托福写作毕竟也会跟以前的考试进行查重比对，而且很多范文说实在也是我们国人写的，怎么都没有外国人地道，我拿一篇举例，旅游的好处： 11 Reasons Why Travel Makes You a Happier Person &#8195;I feel happy when I’m gaining new experiences and insights, and challenging my boundaries. Travel is the perfect catalyst for happiness, as it has allowed me to experience the natural, cultural and man-made wonders of the world. Being in foreign lands, it also continuously forces me to step out of my comfort zone - a great confidence-builder. Travel expands our capacity for wonder, joy and appreciation of the amazing diversity on our lovely planet. FIND YOUR SELF-CONFIDENCE BY DEALING WITH UNEXPECTED SITUATIONS&#8195;There comes a time when everyone must deal with an unexpected situation when they’re on the road. Even if you plan your trip to the letter, things can take a surprise turn. Whatever happens, there is a way around the problem and knowing that you can deal with these situations is a big boost to self-confidence and therefore your happiness. HAPPINESS IS INFECTIOUS&#8195;When locals are happy, smiling and friendly, it has an immediate knock-on effect. I found the people of Thailand and Laos to be notably friendly and cheerful, despite the relative poorness of these countries and the former in particular having a very recent traumatic history. When faced with those big beaming smiles, it’s hard to be annoyed at the hassling you might experience at busy sites like Angkor Wat; putting that knee-jerk irritation to one side instantly lifts your mood and is a good habit to take home. BEING AWAY MAKES YOU APPRECIATE FAMILY AND HOME&#8195;Being away from things we often take for granted — family, close friends, home — makes us appreciate them more. Calling home isn’t a chore, but something to look forward to: no one enjoys listening to your envy-inducing travel stories more than your parents, so it’s the perfect excuse to wax lyrical about whatever place with which you’ve just fallen in love. YOU MAKE NEW FRIENDS&#8195;It’s much easier to make new friends on the road than it is at home, where people are less inclined to chat to strangers on a bus or strike up conversation in a bar. When people are away from home, there seem to be less boundaries to cross and making friends becomes much easier, whether it’s a local curious to know where you’ve come from or a fellow traveler keen to have someone with whom to enjoy a beer or share a taxi. Social interactions make us happier and increasing our social circle means that we’re talking more and meeting different, interesting people, which hopefully means we’re learning more, too. DETOX FROM SOCIAL MEDIA &#8195;Social media can be used for both good and bad, but it's healthy for everyone to have a break from the internet every once in a while. Wi-fi is so prevalent that it's hard to turn off and you can quite often find yourself tuning out whatever amazing place you're in with your face in your phone, checking Twitter, scrolling through your Facebook feed, checking your emails... stop. Turn it off. Better yet, find somewhere with no reception and no wi-fi so that you don't have a choice. It's liberating and allows you to better enjoy the 'here and now', which nicely ties into the following point. GETTING SOME ‘YOU’ TIME&#8195;Traveling gives us breathing space that is often lost in our usual day-to-day existence. Having a moment to take advantage of peace and quiet and to simply ‘be’ allows us to let go of stress and tension and just enjoy being in the moment — a key focus of meditation and a practice you can take home with you. If you’re traveling with a partner, it’s a chance to spend time with only each other for company, which is a thought that probably shouldn’t fill you with dread. EDUCATION, EDUCATION, EDUCATION&#8195;Whether it’s learning a new skill such as cooking Thai food or learning a new language, travel presents ways in which we can further our knowledge and education. Learning makes our brains more active, which psychologists have found increases our level of happiness - particularly when learning something we find enjoyable. GET A VITAMIN D BOOST&#8195;Whilst it’s a bit of myth that you need to be on a sun-lounger for twelve hours to feel the full effects of vitamin D (20 minutes of exposure to sunlight is enough), there’s no doubt that in the same way that the cold and dark of winter makes us unhappy (feeling the effects of seasonal affective disorder or SAD), sunshine and warmth generally put us in a much better mood. A beach break is a great way to relax and enjoy the health benefits of a warm climate. Admittedly, this is more of a short-term boost, but a healthy glow makes everyone feels better and lasts for a few weeks after your trip is over. YOU’RE MORE INTERESTING&#8195;You don’t need to be a ‘travel bore’ to have a few interesting stories to tell. Traveling throws up a lot of bizarre, funny and sometimes serious situations that relating back to people will make you — at least — feel interesting. Making someone laugh is an easy way to instantly bump up your self-esteem, so hold on to those embarrassing memories — no matter how much they might make you cringe. NEW EXPERIENCES GIVE US MOMENTS TO REMEMBER&#8195;For most people, travelling is about the new experiences. I will never forget that moment of awe when I stood watching the sunlight leak out around the ancient temple of Chiang Mai in Thailand at sunrise, the sky turned a striking shade of violet: it was one of the most extraordinary sights I’ve ever seen. Recalling memories of happiness can sustain a feeling of contentment long after the moment has passed, and new experiences are memories that can stick with you forever. THE EFFECTS OF TRAVELING AREN’T JUST SHORT-TERM&#8195;Aside from making you happier in the short-term, traveling can make you a much more contented, happy and relaxed person in the long run, too. Of course, most travel enthusiasts are constantly planning their next trip, but when we’re at home or past a point of being able to jet off whenever we like, past travels leave us with the memories and personal skills - such as confidence, broad-mindedness, friends and a more worldly perspective — that make people happy. And that’s why travel makes you a happier person. 标红的那些部分是我个人认为很地道的表达，而且关于旅行如何让我们更幸福文章里就列举了11个观点，很多观点都很新颖，如果我们自己来写旅行，能写到多少个观点，而且有多少个大家想的都是一样的？所以我觉得这样子积累对写作会很有帮助。至于素材积累，我自己使用的网站有： techcrunch Debate 更多的时候是在google上搜索特定话题的时候，直接看靠前面的搜索结果，因为好文章不一定都能集中在同一个网站 结语不管什么复习方法，其实多练习才是王道，认定一个复习方法持之以恒就可以了，不存在什么最好最高效的方法，同时要有自己的主见，不要知乎上说什么就信什么。祝愿大家早日和托福分手","categories":[],"tags":[{"name":"托福","slug":"托福","permalink":"http://Bithub00.com/tags/托福/"}]},{"title":"C++刷题","slug":"C++刷题","date":"2019-09-01T08:30:55.972Z","updated":"2020-11-14T02:04:46.051Z","comments":true,"path":"2019/09/01/C++刷题/","link":"","permalink":"http://Bithub00.com/2019/09/01/C++刷题/","excerpt":"C++刷题记录","text":"C++刷题记录 题目集一C++面向对象程序设计50道编程题 Problem 1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;using namespace std;class Fract&#123; int num, den;public: Fract(int a = 0, int b = 1) &#123; num = a; den = b; &#125; int ged(int m, int n); Fract add(Fract f); void show();&#125;;int Fract::ged(int m, int n)&#123; int k = 0; if(m &gt;= n) k = n; else k = m; for(; k &gt;= 1; k--) &#123; if(m % k == 0 &amp;&amp; n % k == 0) break; &#125; return k;&#125;Fract Fract::add(Fract f)&#123; Fract ff; int v; v = ged(f.den, den); int vv = den / v * f.den; int uu = vv / den * num + vv / f.den * f.num; int cc = ged(vv, uu); if (cc != 1) &#123;vv = vv / cc; uu = uu / cc;&#125; ff.den = vv; ff.num = uu; return ff;&#125;void Fract::show()&#123; cout &lt;&lt; num &lt;&lt; '/' &lt;&lt; den &lt;&lt; endl;&#125;int main()&#123; Fract f1(1,5), f2(7,20), f3; f3 = f1.add(f2); f3.show(); return 0;&#125; Problem 21234567891011121314151617181920212223242526272829303132333435363738394041#include&lt;iostream&gt;using namespace std;class ARRAY&#123; float a[10], b[10];public: ARRAY(float t[10]) &#123; for(int i = 0; i &lt; 10; i++) a[i] = t[i]; &#125; void process() &#123; for(int i = 0; i &lt; 10; i++) &#123; int pre = (i - 1) % 10; if(pre &lt; 0) pre = 10 + pre; int aft = (i + 1) % 10; b[i] = (a[pre] + a[i] + a[aft]) / 3; &#125; &#125; void print() &#123; for(int i = 0; i &lt; 10; i++) cout &lt;&lt; a[i] &lt;&lt; ' '; cout &lt;&lt; endl; for(int i = 0; i &lt; 10; i++) cout &lt;&lt; b[i] &lt;&lt; ' '; &#125;&#125;;int main()&#123; float aa[10] = &#123;0,3,6,9,12,15,18,21,24,27&#125;; ARRAY v(aa); v.process(); v.print(); return 0;&#125; Problem 31234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;class ID&#123; char s[18], x[11]; int w[17];public: ID(char *str)&#123; int i; for(i = 0; i &lt; 18; i++) s[i] = '0'; for(i = 0; i &lt; strlen(str); i++)&#123; s[i] = str[i]; &#125; char x1[] = &#123;'1','0','X','9','8','7','6','5','4','3','2'&#125;; strcpy(x,x1); int w1[] = &#123;7,9,10,5,8,4,2,1,6,3,7,9,10,5,8,4,2&#125;; for(i = 0; i &lt; 17; i++) w[i] = w1[i]; &#125; void fun()&#123; int i; for(i = 16; i &gt; 7; i--) s[i] = s[i - 2]; s[6] = '1'; s[7] = '9'; int sum = 0; for(i = 0; i &lt; 17; i++)&#123; int temp = s[i] - '0'; sum += temp * w[i]; &#125; int index = sum % 11; s[17] = x[index]; &#125; void print()&#123; cout &lt;&lt; s &lt;&lt; endl; &#125;&#125;;int main()&#123; char *str = \"340524800101001\"; ID id(str); id.fun(); id.print();&#125; Problem 412345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;class String&#123; char *str1, *str2; char *str;public: String(char *s1, char* s2)&#123; str1=new char[strlen(s1)+1]; strcpy(str1,s1); str2=new char[strlen(s2)+1]; strcpy(str2,s2); str=new char[strlen(s1)+strlen(s2)+1]; strcpy(str,s1);strcat(str,s2); &#125; void del()&#123; char *ptr1 = str; char *ptr2 = str; while(*ptr1)&#123; if(*ptr1 != ' ')&#123; *ptr2 = *ptr1; ptr2++; &#125; ptr1++; &#125; *ptr2 = '\\0'; &#125; void _sort()&#123; char *ptr1 = str; char *ptr2, temp; while(*ptr1)&#123; for(ptr2 = ptr1; *ptr2; ptr2++)&#123; if(*ptr1 &gt; *ptr2)&#123; temp = *ptr1; *ptr1 = *ptr2; *ptr2 = temp; &#125; &#125; ptr1++; &#125; &#125; void show()&#123; cout &lt;&lt; \"str1: \" &lt;&lt; str1 &lt;&lt; endl; cout &lt;&lt; \"str2: \" &lt;&lt; str2 &lt;&lt; endl; cout &lt;&lt; \"str: \" &lt;&lt; str &lt;&lt; endl; &#125; ~String()&#123; free(str1); free(str2); free(str); &#125;&#125;;int main()&#123; char *s1 = \"db a\"; char *s2 = \"4 1\"; String str(s1, s2); str.del(); str.show(); str._sort(); str.show(); return 0;&#125; Problem 5123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cstring&gt;#include&lt;string&gt;#include&lt;math.h&gt;using namespace std;class Array&#123; int *p,k; float s;public: Array(int *ptr, int n)&#123; k = n; p = new int[k]; for(int i = 0; i &lt; k; i++) p[i] = ptr[i]; s = 0; &#125; int fun(int n); void sum(); void show(); ~Array()&#123; delete []p; &#125;&#125;;int Array::fun(int n)&#123; if(n == 0 || n == 1) return 0; int i; for(i = 2; i &lt; n; i++) &#123; if(n % i == 0) return 0; &#125; return 1;&#125;void Array::sum()&#123; float sum = 0; int length = 0; for(int i = 0; i &lt; k; i++)&#123; if(fun(p[i]))&#123; sum += p[i]; length++; &#125; &#125; s = sum / length;&#125;void Array::show()&#123; cout &lt;&lt; k &lt;&lt; endl; int i; for(i = 0; i &lt; k; i++)&#123; cout &lt;&lt; p[i] &lt;&lt; '\\t'; if(i % 4 ==0 &amp;&amp; i != 0) cout &lt;&lt; endl; &#125; cout &lt;&lt; s;&#125;int main()&#123; int ptr[] = &#123;5,2,7,4,8,23,65,1,40&#125;; int n = 9; Array arr(ptr, n); arr.sum(); arr.show(); return 0;&#125; Problem 6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;class STR&#123; char s1[80], s2[80]; char s3[160];public: STR(char a[], char b[])&#123; char *ptr1 = s1; char *ptr2 = s2; while(*a)&#123; *ptr1 = *a; ptr1++; a++; &#125; *ptr1 = '\\0'; while(*b)&#123; *ptr2 = *b; ptr2++; b++; &#125; *ptr2 = '\\0'; &#125; void consort(); void show();&#125;;void STR::consort()&#123; strcpy(s3, s1); strcat(s3, s2); char *ptr1 = s3, *ptr2, temp; while(*ptr1)&#123; for(ptr2 = ptr1; *ptr2; ptr2++)&#123; if(*ptr2 &lt; *ptr1)&#123; temp = *ptr2; *ptr2 = *ptr1; *ptr1 = temp; &#125; &#125; ptr1++; &#125;&#125;void STR::show()&#123; cout &lt;&lt; s1 &lt;&lt; endl; cout &lt;&lt; s2 &lt;&lt; endl; cout &lt;&lt; s3 &lt;&lt; endl;&#125;int main()&#123; char a[] = \"pear\"; char b[] = \"apple\"; STR str(a, b); str.consort(); str.show(); return 0;&#125; Problem 71234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include&lt;iostream&gt;using namespace std;class RECT&#123;protected: double x,y;public: RECT(double x1, double y1)&#123; x = x1; y = y1; &#125; virtual double area()&#123; double are = x * y; return are; &#125; double peri()&#123; double per = 2 * (x + y); return per; &#125; virtual int isSquare()&#123; if(x == y) return 1; else return 0; &#125;&#125;;class CUB:public RECT&#123; double height;public: CUB(double x1, double y1, double h):RECT(x1,y1)&#123; height = h; &#125; double volume(); double area(); int isSquare();&#125;;double CUB::volume()&#123; double square = RECT::area(); double v = square * height; return v;&#125;double CUB::area()&#123; double square = RECT::area(); double result = 2 * square + peri() * height; return result;&#125;int CUB::isSquare()&#123; if(RECT::isSquare()) return x == height; else return 0;&#125;int main()&#123; double a,b,c; cin&gt;&gt;a&gt;&gt;b&gt;&gt;c; CUB cu(a,b,c); RECT *re; re=&amp;cu; cout&lt;&lt;\"长方体的体积为：\"&lt;&lt;cu.volume()&lt;&lt;endl; cout&lt;&lt;\"长方体的表面积为：\"&lt;&lt;re-&gt;area()&lt;&lt;endl; if(re-&gt;isSquare()) cout&lt;&lt;\"该长方体是正方体\\n\"; else cout&lt;&lt;\"该长方体不是正方体\\n\"; return 0;&#125; Problem 81234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;using namespace std;class ARR&#123; int n; int a[100];public: ARR(int x[], int size)&#123; n = size; int i; for(i = 0; i &lt; n; i++) a[i] = x[i]; &#125; void change(); void show();&#125;;void ARR::change()&#123; int i = 0, j = n - 1; for(; i &lt; j; i++)&#123; while(a[i] &lt; 0) &#123; if(i == j) break; i++; &#125; while(a[j] &gt; 0) &#123; if(i == j) break; j--; &#125; int temp = a[i]; a[i] = a[j]; a[j] = temp; &#125;&#125;void ARR::show()&#123; for(int i = 0; i &lt; n; i++)&#123; cout &lt;&lt; a[i] &lt;&lt; ' '; &#125; cout &lt;&lt; endl;&#125;int main()&#123; int b[10] = &#123;1,-3,-1,3,2,4,-4,5,-5,-2&#125;; ARR arr(b, 10); arr.show(); arr.change(); arr.show();&#125; Problem 9123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include&lt;iostream&gt;using namespace std;class Base&#123;protected: char name[8]; int num;public: Base()&#123; cout &lt;&lt; \"请输入姓名：\"; cin &gt;&gt; name; &#125; void print()&#123; cout &lt;&lt; name &lt;&lt; endl; &#125; virtual int isGood() = 0;&#125;;class Student:public Base&#123;public: Student()&#123; cout &lt;&lt; \"请输入成绩：\"; cin &gt;&gt; num; &#125; int isGood()&#123; if(num &gt; 90) return 1; else return 0; &#125;&#125;;class Teacher:public Base&#123;public: Teacher()&#123; cout &lt;&lt; \"请输入论文数：\"; cin &gt;&gt; num; &#125; int isGood()&#123; if(num &gt; 3) return 1; else return 0; &#125;&#125;;int main()&#123; cout &lt;&lt; \"学生情况：\" &lt;&lt; endl; Student s[2]; cout &lt;&lt; \"老师情况：\" &lt;&lt; endl; Teacher t[2]; Base *p; int i; cout &lt;&lt; \"优秀学生：\" &lt;&lt; endl; for(i = 0, p = s; i &lt; 2; i++)&#123; if(p-&gt;isGood()) p-&gt;print(); p++; &#125; cout &lt;&lt; \"优秀老师：\" &lt;&lt; endl; for(i = 0, p = t; i &lt; 2; i ++)&#123; if(p-&gt;isGood()) p-&gt;print(); p++; &#125;&#125; Problem 10123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;using namespace std;#define M 4class Array&#123; int b[M][M];public: Array(int (*p)[M]); void operator+(); friend void operator-(Array &amp;b); void print();&#125;;Array::Array(int (*p)[M])&#123; for(int i = 0; i &lt; M; i++) for(int j = 0; j &lt; M; j++) b[i][j] = p[i][j];&#125;void Array::operator+()&#123; int t[M][M]; for(int i = 0; i &lt; M; i++) for(int j = 0; j &lt; M; j++) t[i][j] = b[j][M-1-i]; for(int i = 0; i &lt; M; i++) for(int j = 0; j &lt; M; j++) b[i][j] = t[i][j];&#125;void operator-(Array &amp;b)&#123; int t[M][M]; for(int i = 0; i &lt; M; i++) for(int j = 0; j &lt; M; j++) t[i][j] = b.b[M-1-j][i]; for(int i = 0; i &lt; M; i++) for(int j = 0; j &lt; M; j++) b.b[i][j] = t[i][j];&#125;void Array::print()&#123; for(int i = 0; i &lt; M; i++) &#123; for(int j = 0; j &lt; M; j++) &#123; cout &lt;&lt; b[i][j] &lt;&lt; '\\t'; &#125; cout &lt;&lt; endl; &#125; cout &lt;&lt; '\\n';&#125;int main()&#123; int a[][M]=&#123;1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16&#125;; Array arr(a); arr.print(); +arr; arr.print(); -arr; arr.print();&#125; Problem 111234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;string&gt;using namespace std;class String_Integer&#123; char *s;public: String_Integer(char *str); operator int()&#123; char *ptr = s; int num = 0; while(*ptr)&#123; if(*ptr &gt;= '0' &amp;&amp; *ptr &lt;= '9') &#123; num = num*10 + *ptr - '0'; &#125; ptr++; &#125; return num; &#125; void show(); ~String_Integer();&#125;;String_Integer::String_Integer(char *str)&#123; s = new char(strlen(str) + 1); strcpy(s, str);&#125;void String_Integer::show()&#123; char *ptr = s; while(*ptr)&#123; cout &lt;&lt; *ptr; ptr++; &#125; cout &lt;&lt; endl;&#125;String_Integer::~String_Integer()&#123; delete []s;&#125;int main()&#123; char *s = \"ab12 3c00d45ef\"; String_Integer str(s); str.show(); int n = str; cout &lt;&lt; \"输出的整数为：\" &lt;&lt; n;&#125; Problem 121234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include&lt;iostream&gt;using namespace std;class SET&#123; int *a; int len;public: SET(int *p, int n); int operator ==(int m); friend int operator ==(SET &amp;s1, SET &amp;s2); void print(); ~SET();&#125;;SET::SET(int *p, int n)&#123; len = n; a = new int(len); for(int i = 0; i &lt; len; i++) a[i] = p[i];&#125;int SET::operator ==(int m)&#123; for(int i = 0; i &lt; len; i++) if(m == a[i]) return 1; return 0;&#125;int operator ==(SET &amp;s1, SET &amp;s2)&#123; if(s1.len != s2.len) return 0; else &#123; for(int i = 0; i &lt; s1.len; i++) if(!(s2 == s1.a[i])) return 0; &#125; return 1;&#125;void SET::print()&#123; for(int i = 0; i &lt; len; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; '\\t'; &#125; cout &lt;&lt;endl;&#125;SET::~SET()&#123; delete []a;&#125;int main()&#123; int a[]=&#123;1,2,3,4,5&#125;,b[]=&#123;1,2,3,4,5&#125;,c[]=&#123;1,2,3,4,5,6&#125;,d[]=&#123;1,3,5,7,9&#125;; SET s1(a,5),s2(b,5),s3(c,6),s4(d,5); cout&lt;&lt;\"a:\\t\";s1.print(); cout&lt;&lt;\"b:\\t\";s2.print(); cout&lt;&lt;\"c:\\t\";s3.print(); cout&lt;&lt;\"d:\\t\";s4.print(); if(s1==s2)cout&lt;&lt;\"a==b\\n\"; else cout&lt;&lt;\"a!=b\\n\"; if(s1==s3)cout&lt;&lt;\"a==c\\n\"; else cout&lt;&lt;\"a!=c\\n\"; if(s1==s4)cout&lt;&lt;\"a==d\\n\"; else cout&lt;&lt;\"a!=d\\n\";&#125; Problem 1312345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cstring&gt;#include&lt;string&gt;using namespace std;class STR&#123; char *s;public: STR(char *p = 0); STR&amp; operator =(STR &amp;str); friend STR&amp; operator +=(STR &amp;str1, STR &amp;str2); void print(); ~STR();&#125;;STR::STR(char *p)&#123; if(p == 0) s = 0; else &#123; s = new char(strlen(p) + 1); strcpy(s, p); &#125;&#125;STR&amp; STR::operator =(STR &amp;str)&#123; if(s) delete []s; s = new char[strlen(str.s) + 1]; strcpy(s, str.s); return *this;&#125;STR&amp; operator +=(STR &amp;str1, STR &amp;str2)&#123; char *p = new char[strlen(str1.s) + strlen(str2.s) + 1]; strcpy(p, str1.s); strcat(p, str2.s); if(str1.s) delete []str1.s; str1.s = new char[strlen(p) + 1]; strcpy(str1.s, p); return str1;&#125;void STR::print()&#123; cout &lt;&lt; s &lt;&lt; endl;&#125;STR::~STR()&#123; delete []s;&#125;int main()&#123; STR str1(\"Shenzhen\"),str2(\" University\"),str3; str1.print(); str2.print(); str3=str1+=str2; str3.print();&#125; Problem 141234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include&lt;iostream&gt;using namespace std;#define pi 3.14class container&#123;protected: int radius;public: container(int n) &#123;radius = n;&#125; virtual double square() = 0; virtual double volume() = 0;&#125;;class cube:public container&#123;public: cube(int n):container(n)&#123; &#125; double square() &#123; int s = 6 * radius * radius; return s; &#125; double volume() &#123; double v = radius * radius * radius; return v; &#125;&#125;;class sphere:public container&#123;public: sphere(int n):container(n)&#123; &#125; double square() &#123; double s = 4 * radius * radius * pi; return s; &#125; double volume() &#123; double v = (4 / 3.0) * radius * radius * radius; return v; &#125;&#125;;class cylinder:public container&#123; int height;public: cylinder(int n, int h):container(n)&#123; height = h;&#125; double square() &#123; double s = 2 * radius * radius * pi + 2 * pi * radius * height; return s; &#125; double volume() &#123; double v = radius * radius * pi * height; return v; &#125;&#125;;int main()&#123; int radius = 2; int height = 4; container *base; cube c(radius); sphere s(radius); cylinder cy(radius, height); base = &amp;c; cout &lt;&lt; \"正方体表面积为：\" &lt;&lt; base-&gt;square() &lt;&lt; endl;; cout &lt;&lt; \"正方形体积为：\" &lt;&lt; base-&gt;volume() &lt;&lt;endl; base = &amp;s; cout &lt;&lt; \"球体表面积为：\" &lt;&lt; base-&gt;square() &lt;&lt; endl; cout &lt;&lt; \"球体体积为：\" &lt;&lt; base-&gt;volume() &lt;&lt;endl; base = &amp;cy; cout &lt;&lt; \"圆柱体表面积为：\" &lt;&lt; base-&gt;square() &lt;&lt;endl; cout &lt;&lt; \"圆柱体体积为：\" &lt;&lt; base-&gt;volume() &lt;&lt; endl;&#125; 题目集二王道论坛计算机机试指南 贪心算法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;iomanip&gt;using namespace std;int main()&#123; int M, N; cin &gt;&gt; M &gt;&gt; N; while(M != - 1 &amp;&amp; N != -1) &#123; int storage = M; float result = 0.0; int J[N], F[N]; float rate[N]; for(int i = 0; i &lt; N; i++) &#123; cin &gt;&gt; J[i] &gt;&gt; F[i]; rate[i] = float(J[i]) / F[i]; &#125; while(storage != 0) &#123; int index = 0; float max = 0.0; for(int i = 0; i &lt; N; i++) &#123; if(rate[i] &gt; max) &#123; index = i; max = rate[i]; &#125; &#125; if(storage &gt; F[index] &amp;&amp; F[index] != 0) &#123; result += J[index]; storage -= F[index]; rate[index] = 0; &#125; else &#123; result += storage * rate[index]; storage = 0; &#125; &#125; cout &lt;&lt; fixed &lt;&lt; setprecision(3) &lt;&lt; result &lt;&lt; endl; cin &gt;&gt; M &gt;&gt; N; &#125;&#125; 括号匹配12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;#include&lt;stack&gt;#include&lt;cstring&gt;using namespace std;int main()&#123;// char str[100];// cin &gt;&gt; str; char str[] = \")(rttyy())sss)(\"; stack&lt;char&gt; S; stack&lt;int&gt; index; int len = strlen(str); int i; char result[len]; for(i = 0; i &lt; len; i++) result[i] = ' '; for(i = 0; i &lt; len; i++) &#123; if(str[i] == '(')&#123; S.push(str[i]); index.push(i); &#125; else if(str[i] == ')') &#123; if(S.empty())&#123; result[i] = '?'; &#125; else&#123; S.pop(); index.pop(); &#125; &#125; &#125; result[i] = '\\0'; while(!S.empty())&#123; int top = index.top(); index.pop(); S.pop(); result[top] = '$'; &#125; cout &lt;&lt; result &lt;&lt; endl;&#125; 简易计数器[中缀表达式-&gt;后缀表达式]12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include&lt;iostream&gt;#include&lt;stack&gt;#include&lt;cstring&gt;#include&lt;stdio.h&gt;using namespace std;int priority(const char symbol)&#123; int grade = 0; switch(symbol)&#123; case '+': grade = 4;break; case '-': grade = 4;break; case '*': grade = 5;break; case '/': grade = 5;break; &#125; return grade;&#125;float calculate(const float a, const char symbol, const float b)&#123; float result = 0.0; switch(symbol)&#123; case '+': result = a + b;break; case '-': result = a - b;break; case '*': result = a * b;break; case '/': result = float(a) / b;break; &#125; cout &lt;&lt; a &lt;&lt; ' ' &lt;&lt; symbol &lt;&lt; ' ' &lt;&lt; b &lt;&lt; endl; return result;&#125;int main()&#123; char str[] = \"4+2*5-6/3\"; stack&lt;char&gt; result; stack&lt;char&gt; S; int i; for(i = 0; i &lt; strlen(str); i++)&#123; if(str[i] &gt;= '0' &amp;&amp; str[i] &lt;= '9') result.push(str[i]); else if(str[i] == '+' || str[i] == '-' || str[i] == '*' || str[i] == '/')&#123; while(true)&#123; if(S.empty() || S.top() == '(')&#123; S.push(str[i]); break; &#125; else if(priority(str[i]) &gt; priority(S.top()))&#123; S.push(str[i]); break; &#125; else&#123; char top = S.top(); result.push(top); S.pop(); &#125; &#125; &#125; else&#123; if(str[i] == '(') S.push(str[i]); else&#123; while(S.top() != '(')&#123; char top = S.top(); result.push(top); S.pop(); &#125; S.pop(); &#125; &#125; &#125; while(!result.empty())&#123; char top = result.top(); S.push(top); result.pop(); &#125; stack&lt;float&gt; C; while(!S.empty())&#123; char top = S.top(); S.pop(); if(top &gt;= '0' &amp;&amp; top &lt;= '9')&#123; C.push(top - '0'); &#125; else&#123; float b = C.top(); C.pop(); float a = C.top(); C.pop(); float result = calculate(a, top, b); C.push(result); &#125; &#125; float num = C.top(); cout &lt;&lt; \"计算结果：\" &lt;&lt; num &lt;&lt; endl;&#125; 哈夫曼树1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;stdio.h&gt;using namespace std;int main()&#123; priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; &gt; Q; int n; while(scanf(\"%d\", &amp;n) != EOF)&#123; while(!Q.empty()) Q.pop(); for(int i = 0; i &lt; n; i++)&#123; int x; cin &gt;&gt; x; Q.push(x); &#125; int ans = 0; while(Q.size() &gt; 1)&#123; int a = Q.top(); Q.pop(); int b = Q.top(); Q.pop(); ans += a + b; Q.push(a + b); &#125; cout &lt;&lt; ans &lt;&lt; endl; &#125;&#125; 二叉树遍历[前序+中序-&gt;后序]1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;stdio.h&gt;using namespace std;typedef struct TNode &#123; char data; TNode *lchild, *rchild;&#125;*Tree;char preOrder[] = \"FDXEAG\", inOrder[] = \"XDEFAG\";Tree createTree()&#123; Tree T = new TNode; T-&gt;lchild = T-&gt;rchild = NULL; return T;&#125;void postOrder(Tree T)&#123; if(T-&gt;lchild != NULL) postOrder(T-&gt;lchild); if(T-&gt;rchild != NULL) postOrder(T-&gt;rchild); cout &lt;&lt; T-&gt;data;&#125;Tree BuildTree(int s1, int e1, int s2, int e2)&#123; Tree T = createTree(); T-&gt;data = preOrder[s1]; int rootIdx; for(int i = 0; i &lt; strlen(inOrder); i++) if(inOrder[i] == preOrder[s1])&#123; rootIdx = i; break; &#125; if(rootIdx != s2)&#123; //左子树非空 T-&gt;lchild = BuildTree(s1 + 1, s1 + (rootIdx - s2), s2, rootIdx - 1); &#125; if(rootIdx != e2)&#123; //右子树非空 T-&gt;rchild = BuildTree(s1 + (rootIdx - s2) + 1, e1, rootIdx + 1, e2); &#125; return T;&#125;int main()&#123; while(scanf(\"%s\", preOrder) != EOF)&#123; scanf(\"%s\", inOrder); int len_pre = strlen(preOrder), len_in = strlen(inOrder); Tree T = BuildTree(0, len_pre - 1, 0, len_in - 1); postOrder(T); cout &lt;&lt; endl; &#125;&#125; 二叉排序树12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;iostream&gt;#include&lt;stdio.h&gt;using namespace std;typedef struct TNode&#123; int data; TNode *lchild, *rchild;&#125;*Tree;void preOrder(Tree T)&#123; if(T != NULL)&#123; cout &lt;&lt; T-&gt;data &lt;&lt; \" \"; preOrder(T-&gt;lchild); preOrder(T-&gt;rchild); &#125;&#125;void inOrder(Tree T)&#123; if(T != NULL)&#123; inOrder(T-&gt;lchild); cout &lt;&lt; T-&gt;data &lt;&lt; \" \"; inOrder(T-&gt;rchild); &#125;&#125;void postOrder(Tree T)&#123; if(T != NULL)&#123; postOrder(T-&gt;lchild); postOrder(T-&gt;rchild); cout &lt;&lt; T-&gt;data &lt;&lt; \" \"; &#125;&#125;Tree Insert(Tree T, int X)&#123; if(!T)&#123; T = new TNode; T-&gt;lchild = T-&gt;rchild = NULL; T-&gt;data = X; &#125; if(X &gt; T-&gt;data) T-&gt;rchild = Insert(T-&gt;rchild, X); else if(X &lt; T-&gt;data) T-&gt;lchild = Insert(T-&gt;lchild, X); return T;&#125;int main()&#123; int n; while(scanf(\"%d\", &amp;n) != EOF)&#123; Tree T = NULL; for(int i = 0; i &lt; n; i++)&#123; int num; scanf(\"%d\", &amp;num); T = Insert(T, num); &#125; preOrder(T); cout &lt;&lt; endl; inOrder(T); cout &lt;&lt; endl; postOrder(T); cout &lt;&lt; endl; &#125;&#125; 同一二叉搜索树序列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;using namespace std;typedef struct TNode&#123; int data; TNode *lchild, *rchild;&#125;*Tree;char *str;char str1[40], str2[40];int *size;int size1, size2;Tree Insert(Tree T, int X)&#123; if(T == NULL)&#123; T = new TNode; T-&gt;data = X; T-&gt;lchild = T-&gt;rchild = NULL; &#125; if(X &lt; T-&gt;data) T-&gt;lchild = Insert(T-&gt;lchild, X); else if(X &gt; T-&gt;data) T-&gt;rchild = Insert(T-&gt;rchild, X); return T;&#125;void preOrder(Tree T)&#123; if(T != NULL)&#123; str[(*size)++] = T-&gt;data + '0'; preOrder(T-&gt;lchild); preOrder(T-&gt;rchild); &#125;&#125;void inOrder(Tree T)&#123; if(T != NULL)&#123; inOrder(T-&gt;lchild); str[(*size)++] = T-&gt;data + '0'; inOrder(T-&gt;rchild); &#125;&#125;void postOrder(Tree T)&#123; if(T != NULL)&#123; postOrder(T-&gt;lchild); postOrder(T-&gt;rchild); str[(*size)++] = T-&gt;data + '0'; &#125;&#125;int main()&#123; int n; cin &gt;&gt; n; while(n != 0)&#123; char originTree[20]; cin &gt;&gt; originTree; for(int i = 0; i &lt; n; i++)&#123; char compareTree[20]; cin &gt;&gt; compareTree; size1 = 0; size = &amp;size1; str = str1; Tree T1 = NULL; for(int j = 0; j &lt; strlen(originTree); j++) T1 = Insert(T1, originTree[j] - '0'); preOrder(T1); inOrder(T1); str1[size1] = '\\0'; if(strlen(compareTree) != strlen(originTree)) cout &lt;&lt; \"NO\" &lt;&lt; endl; else&#123; size2 = 0; size = &amp;size2; str = str2; Tree T2 = NULL; for(int j = 0; j &lt; strlen(compareTree); j++) T2 = Insert(T2, compareTree[j] - '0'); preOrder(T2); inOrder(T2); str2[size2] = '\\0'; puts(strcmp(str1, str2) == 0 ? \"YES\" : \"NO\"); &#125; &#125; cin &gt;&gt; n; &#125;&#125; 最大公约数123456789101112131415161718192021222324252627#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;int gcd(int a, int b)&#123; if(b == 0) return a; else gcd(b, a % b);&#125;int main()&#123; int a, b; while(scanf(\"%d %d\", &amp;a, &amp;b) != EOF)&#123; cout &lt;&lt; gcd(a, b) &lt;&lt; endl;; while(a != 0 &amp;&amp; b != 0)&#123; int temp = a; a = b; b = temp % b; &#125; if(a == 0) cout &lt;&lt; b &lt;&lt; endl; else cout &lt;&lt; a &lt;&lt; endl; &#125;&#125; 最小公倍数123456789101112131415161718192021#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;int gcd(int a, int b)&#123; if(b == 0) return a; else gcd(b, a % b);&#125;int main()&#123; int a, b; while(scanf(\"%d %d\", &amp;a, &amp;b) != EOF)&#123; int c = gcd(a, b); int l = a / c * b; cout &lt;&lt; l &lt;&lt; endl; &#125;&#125; 列出连通集123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#include&lt;iostream&gt;#include&lt;queue&gt;using namespace std;#define Vertex int#define WeightType int#define MaxVertexNum 1000bool visited[MaxVertexNum];typedef struct GNode&#123; int Gv, Ge; int vexs[MaxVertexNum]; WeightType G[MaxVertexNum][MaxVertexNum];&#125;*MGraph;typedef struct ENode&#123; Vertex v1, v2;&#125;*Edge;MGraph BuildGraph(int VertexNum)&#123; MGraph Graph = new GNode; Graph-&gt;Gv = VertexNum; Graph-&gt;Ge = 0; Vertex v, w; for(v = 0; v &lt; Graph-&gt;Gv; v++) for(w = 0; w &lt; Graph-&gt;Gv; w++) Graph-&gt;G[v][w] = 0; for(int i = 0; i &lt; Graph-&gt;Gv; i++) Graph-&gt;vexs[i] = i; return Graph;&#125;void Insert(MGraph g, Edge e)&#123; g-&gt;G[e-&gt;v1][e-&gt;v2] = 1; g-&gt;G[e-&gt;v2][e-&gt;v1] = 1;&#125;void DFS(MGraph g, int i)&#123; visited[i] = true; cout &lt;&lt; ' ' &lt;&lt; g-&gt;vexs[i]; for(int j = 0; j &lt; g-&gt;Gv; j++) if(g-&gt;G[i][j] == 1 &amp;&amp; !visited[j]) DFS(g, j);&#125;void DFSTraverse(MGraph g)&#123; for(int i = 0; i &lt; g-&gt;Gv; i++) visited[i] = false; for(int i = 0; i &lt; g-&gt;Gv; i++)&#123; if(!visited[i])&#123; cout &lt;&lt; \"&#123;\"; DFS(g, i); cout &lt;&lt; \" &#125;\\n\"; &#125; &#125;&#125;void BFSTraverse(MGraph g)&#123; int i, j; queue&lt;int&gt; q; for(i = 0; i &lt; g-&gt;Gv; i++) visited[i] = false; for(i = 0; i &lt; g-&gt;Gv; i++)&#123; if(!visited[i])&#123; visited[i] = true; cout &lt;&lt; \"&#123;\"; cout &lt;&lt; ' ' &lt;&lt; g-&gt;vexs[i]; q.push(i); while(!q.empty()) &#123; int k = q.front(); q.pop(); for(j = 0; j &lt; g-&gt;Gv; j++) &#123; if(g-&gt;G[k][j] == 1 &amp;&amp; !visited[j]) &#123; visited[j] = true; cout &lt;&lt; ' ' &lt;&lt; g-&gt;vexs[j]; q.push(j); &#125; &#125; &#125; cout &lt;&lt; \" &#125;\\n\"; &#125; &#125;&#125;int main()&#123; int N; cin &gt;&gt; N; MGraph G = BuildGraph(N); cin &gt;&gt; G-&gt;Ge; if(G-&gt;Ge != 0)&#123; for(int i = 0; i &lt; G-&gt;Ge; i++)&#123; Edge e = new ENode; cin &gt;&gt; e-&gt;v1 &gt;&gt; e-&gt;v2; Insert(G, e); &#125; &#125; DFSTraverse(G); BFSTraverse(G);&#125; 计算智能课程分油问题123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;stack&gt;#include&lt;cstring&gt;using namespace std;typedef struct&#123; int now[3]; int step; int pre;&#125;state;//当前状态，包括油瓶现容量以及上一步操作int capcacity[3] = &#123;10, 7, 3&#125;; //油瓶的容量int visit[11][8][4]; //记录当前的操作，避免重复上一步操作vector&lt;state&gt; path;//记录找到答案时的操作int index = -1, n = 1;//f记录当前操作的序号int bfs(state &amp;S, int i, int j)&#123;//使用广度优先搜索求解 if(i == j) return 0; if(S.now[i] == 0) return 0; if(S.now[j] == capcacity[j]) return 0; if(S.now[i] + S.now[j] &lt;= capcacity[j])&#123;//倒油 S.now[j] = S.now[j] + S.now[i]; S.now[i] = 0; &#125;else&#123; S.now[i] = S.now[i] - (capcacity[j] - S.now[j]); S.now[j] = capcacity[j]; &#125; if(visit[S.now[0]][S.now[1]][S.now[2]]) return 0; else &#123; visit[S.now[0]][S.now[1]][S.now[2]] = 1; S.pre = index; path.push_back(S); n++; &#125; S.step++; return 1;&#125;void print(int i)&#123; stack&lt;state&gt; output; while(i != -1)&#123;//倒序输出 state S; S.now[0] = path[i].now[0]; S.now[1] = path[i].now[1]; S.now[2] = path[i].now[2]; output.push(S); i = path[i].pre; &#125; printf(\"A\\tB\\tC\\n\"); while(!output.empty())&#123; state S = output.top(); printf(\"%d\\t%d\\t%d\\n\", S.now[0], S.now[1], S.now[2]); output.pop(); &#125;&#125;int main()&#123; int end[3] = &#123;5, 5, 0&#125;; int i, j, flag, answer; queue&lt;state&gt;q; flag = 0; memset(visit, 0, sizeof(visit)); state start;//初始状态 start.now[0] = 10; start.now[1] = 0; start.now[2] = 0; start.step = 0; q.push(start); start.pre = -1; path.push_back(start); while(!q.empty())&#123; state S = q.front(); index++; q.pop(); if(S.now[0] == end[0] &amp;&amp; S.now[1] == end[1])&#123;//得到目标状态 flag = 1; answer = index; S.pre = answer; path.push_back(S); break; &#125; for(i = 0; i &lt; 3; i++)&#123; for(j = 0; j &lt; 3; j++)&#123; state _S = S; if(bfs(_S, i, j)) q.push(_S); &#125; &#125; &#125; print(answer); return 0;&#125; Leetcodebithub00/Leetcode","categories":[],"tags":[{"name":"C++","slug":"C","permalink":"http://Bithub00.com/tags/C/"}]},{"title":"阅读点滴","slug":"读书笔记","date":"2019-08-27T11:41:18.469Z","updated":"2020-10-20T11:34:46.728Z","comments":true,"path":"2019/08/27/读书笔记/","link":"","permalink":"http://Bithub00.com/2019/08/27/读书笔记/","excerpt":"这里摘录自己看过的书里受启发的一些句子以及自己的所想","text":"这里摘录自己看过的书里受启发的一些句子以及自己的所想 《扫起落叶好过冬》— 林达 林奇堡：私刑的发源地对于民众激情的过度赞美是危险的。“人民”和“暴民”之间，并没有一条不可逾越的鸿沟。在失控的人群中，“人民法庭”随意蹂躏和消灭一个生命的情况，就很容易发生。聚集的人群在心理上一旦放任自己，就容易在处置罪犯的借口下，忽略个体生命，放大自己的权力。程序自然会迅速简化，刑罚必然就日趋严峻。而契约规定的严格的司法程序正是对草菅人命的有效约束。 阿米绪人的故事一个社会要发动成千上百的人并不难，要达到多数人的一致也不难，难的是公正善待只有百分之几的少数。有时候，少数显得如此人微言轻，他们的生死存亡是如此地微不足道，可是，能否保证这微乎其微地少数得到公平地善待，恰恰是检验文明和人道的试金石，也是决定能否长治久安的一个关键。 一个历史学家和他的小镇我们在自己短暂的一生里，所看到的社会、所看到的人和人之间的关系，和历史长河里大时段大范围里呈现的图景是有所不同的，甚至会有很大的差别。短暂一生里，更多的机会是看到了人性之恶，是令人失望的现实。即使是在我们的上半辈子，我们也看到不知有多少人是怀着对人类、对国家、对社会的彻底绝望离开这个世界的。这样的事情，自古以来不知发生了多少。只有在读历史的时候，你能在纸页间经历几百年几千年，你才能看到进步、改善，你才会庆幸自己生活在此时此刻。【我们为什么要读历史】 各有一番风景美国宪法之父詹姆斯·麦迪逊认为，政治之所以经常败坏人性，和人们在政治中拉帮结派有关。他认为，单独的个人都有一定得道德要求。独立的个人，须对自己的行为主张负责，更容易有道德心。但是，当一些人结成宗派，就会人为制造虚幻正义，把个人的自私，在虚幻正义下掩盖起来，互相提供行为正当性的保证。所以，小宗派的道德水准，通常低于个人道德。单个好人，会在拉帮结派中放任自己的私心，甚至做出坏事来。 科尔曼报告它向国会证明，教育机会的平等，不仅要从教育的投入来考察，即考察学生能够获得的公共教育资源，而且更重要的是，要从教育的结果来考察，因为正是对受教育结果的期望，影响了学生的自我评估，决定了学生的学习状态，也造成了种族、肤色、宗教等因素下，弱势人群教育机会的实质不平等。 两千年前那个叫西塞罗的老头儿两千年前的时候，罗马人已经有了人类的初始民主，民主决定的法律，总不算错了吧？西塞罗想想觉得还是不对，假如大众通过一项法律说，现在可以抢劫了，难道就真的能出去抢东西了吗？西塞罗琢磨着，人应该有一种“本性”的东西，它不会“屈从愚氓者的意见和命令”。于是他就找到如伊甸园里那种人的最初状态去了。一旦进了伊甸园，我发现西塞罗还是很有道理。仔细打量的话，人和人之间，真的就有非常近似的那一部分。只要这么一想就明白了：所有的人，都有一些绝对不愿意发生在自己身上的事情。比如说，只要是个正常人，就没人愿意自己被杀被抢的，没人愿意别人骑在自己头上作威作福的，这才是人“自然本源”的状态。人要维护自己这样的生存状态，就是维护人的“自然权利”，这权利与生俱来。就刚才那简单的几个“不愿意”，已经隐含了生命的权利、平等的权利、人身自由的权利等等。维护自然权利的法，就是自然法。所以另一个比西塞罗还要早的罗马老头儿狄摩西尼说，“每一种法律都是另一种发现”。法律不是胡编乱造、随心所欲的，正义的法律是对自然法的发现。【柏拉图的理型论】 陪审团已经作出了判决法制制度最关键的一步，就是这个文化本身必须建立起绝对尊重司法的传统。因为论“硬件”来说，司法分支是最弱的一环，它在相当程度上是必须依靠社会共识来维持的。宪政国家的产生，就其历史发展来说，是一个社会依据其长期的经验，首先得出对司法之崇高地位的认可。缺少这种文化上的认可，司法是很容易被破坏的。一旦陪审团宣布被告无罪，任何人，即使是总统和最高法院大法官，都没有权力改变。假如不是这样，那就将是一个打不开的死结，就会引出打着伸张正义旗号的民众私刑，就会走向暴力和血腥。假如不是这样，司法就失去权威，整个法制制度都将崩溃。新奥尔良私刑事件中的意大利裔受难者们，用他们的鲜血，用他们被民众的子弹打得残碎的躯体，为后代美国人重申了非常简单却至为重要的道理，这就是今日美国人在法庭大门口经常听到的话：“陪审团已经作出了判决，我们的制度要求我们，必须尊重陪审团。 非法之法不是法实际上，美国的领袖们对立法议会的警惕非常容易理解。读读权利法案那短短十条就明白，它要保障的，是民众个人的权利，是一个一个具体的、分散的、个人的权利，听起来是一回事，只是局部和整体的关系，在实际生活中却根本就是两回事。因为所谓人民的权利，在组成政府的时候，就已经委托给了政府，变成了政府之权力。建国领袖们所担心的，是作为这些个人之集合体的人民，通过他们选出的代表，在具体事务上，侵犯一部分民众的个人权利。多数的暴政和绝对个人专权的暴政，可以在顷刻间转换。美国的建国领袖和同时代的法国革命者不同的是，在他们看来，“多数”并不天然地蕴含着“正确”，多数民众对少数人地镇压，并没有想象中地合理性。所以，对当年的宪法起草者来说，保障民众的个人权利，即使是保障少数人甚至一个人的权利，和防止暴政，特别是多数的暴政，就是同一回事。 星期日早晨的谋杀案哪怕再合理的推论，也不足以定罪。定罪必须有超越“合理怀疑”的确凿无疑的证据。所以，被告的辩护律师们常常说的一句话是：”合理怀疑“是我们的救星。在一般人看来，假如被告被发现有强烈的作案动机，应该对被告是不利的，但是在律师看来，远非如此。因为作案动机的存在，通常会引出人们逻辑合理的推论。就可能在这种强烈的逻辑力量下忽略证据，甚至自然而然地就以推理取代证据。这个时候，距离辩护取胜、被告被开释，也就不远了。在这里经常出现的、对种族问题的此类不确切描述，起因于人们尤其是知识阶层，有很强烈的、要表达自己对弱者深切同情，以及要挺身为底层代言的倾向。这是知识阶层由来已久、经久不息的一个情结。这也恰恰旁证了知识阶层和底层事实上的本质差异。这种差异给知识阶层带来越多的不安，他们产生这种表达的意向就越趋强烈。无疑，贫穷与恶劣的生活状态导致罪恶。可是，对这张联系的探究，应该引出的是社会学意义上的如何消除贫困、消灭罪恶根源的研究和行动，而不是对已经结出的罪恶之果表达泛滥的同情，不论这个恶果是个别的罪犯或是群体的暴民。道理很简单：任何罪行都是有受害者的。而知识阶层假如放弃面对犯罪行为的道德立场，甚至提供过分的借口和“理解”，不仅无助于弱势群体自身的演进，甚至可能将他们带入更为危险的困惑和歧途。 汉娜的手提箱人类历史有大量的负面经验，即使是在和平时期，每个国家也都有大量负面的现实。人们需要历史的传承，汲取历史的教训，需要面对现实。而与此同时，作为儿童和青少年教育，又要警惕大规模的心理伤害。悲和愤等感情，是正义感的基础，可是一旦过度，很容易走向极端，产生对理性的摒弃。历史教育的目的，是带来一个健康的社会，让新的一代有幸福的生活、健康的心态。他们应该是幽默、睿智、快乐、自尊、富有想象力的一代，而不是一代悲壮的愤怒青年。 哪怕在奥斯维辛，绘画依然是美丽的历史学家在摸索的，多是粗大的社会走向之脉络；文学艺术在细细解剖的，却是人们在不由自主中刻意藏匿的内心。在一定程度上，后者是理解前者必不可少的依托；前者又是后者无可离弃的基本背景。 《野火集》的启示从中华文化圈里出来，很多人能够做到文字优美、内容正确、逻辑严密，可是，也许是我们习惯了这一文化中“士”的特殊位置，即使理解平等的意义，对自己的定位定调往往还是会“开低走高”。做社会批判时，会忘记自己也是社会的一员。批判的烈度越大，自我的位置就不断上升，不能持之以恒地维护和读者对话的平等。因为在我们的文化中，历来缺少平等地概念，我们自己地“低调”往往只是理智的产物，而不是本能的反应。 听一次演讲后的随想他们大多是没有名气的，甚至是被误解的。然而，假如没有他们在一片需要改良的土地上默默耕耘，而只有燃野火者，那么当春天来临，也未见得就能够快快长出健康丰润的树苗来。 《刀锋》— 毛姆 你不觉得或许他正在追求一个深藏在一片未知的云朵中的理想——就像一位天文学家寻找某个仅仅通过数学计算才能知道其存在的星球吗？ 我们谁也不能两次涉足于同一条河流，然而，河水流去，随即流来的河水依旧沁人心脾。 《娱乐至死》— 尼尔·波兹曼 我时常想起萧伯纳著名的诗句，理智的人适应环境，而世上所有的进步都依赖不理智的人。麦克卢汉是不理智的，兰斯是不理智的，尼尔也是不理智的。因为这样，所有美好的事才发生了。 根据柏拉图的观点，我们应该把焦点放在人类会话的形式上，而且假定我们会话的形式对于要表达的思想有重大的影响，而且容易表达出来的思想自然会成为文化的组成部分。 犹太人的上帝存在于文字中，或者通过文字而存在，这需要人们进行最精妙的抽象思考。运用图像是亵渎神袛的表现，这样就防止了新的上帝进入。 随着印刷术影响的减退，政治、宗教、教育和任何其他构成公共事务的领域都要改变其内容，并且用最适用于电视的表达方式去重新定义。 如果我们能够意识到，我们创造的每一种工具都蕴含着超越其自身的意义，那么理解这些隐喻就会容易多了。例如，有人指出，12世纪眼镜的发明不仅使矫正视力成为可能，而且还暗示了人类可以不必把天赋或者缺陷视为最终的命运。 马克思《德意志意识形态》：如果印刷机存在，这世上是否还可能有《伊利亚特》？他反问道：”有了印刷机，那些吟唱、传说和思考难道还能继续吗？这些史诗存在的必备条件难道不会消失吗？“ 道格拉斯：“在讨论这些问题的时候，沉默比掌声更得体，我希望你们能够用自己的评判力、理解力和良知来听我的演讲，而不是用你们的激情或热情。”……，对于读者更是如此，因为作者并不是一直值得信任的。他们撒谎他们陷入迷茫，他们过于笼统，他们滥用逻辑甚至常识。读者对此必须有备而来，用知识武装好自己。 这并不是说，印刷术时代发表的文字就一定都是真实的。文字不能保证内容的真实性，而是形成一个语境，让人们可以问“这是真的还是假的”。 “阐释年代”：所有成熟话语所拥有的特征，都被偏爱阐释的印刷术发扬光大：富有逻辑的复杂思维，高度的理性和秩序，对于自相矛盾的憎恶，超常的冷静和客观以及等待受众反应的耐心。 《瓦尔登湖》：我们匆匆地建起了从缅因州通往德克萨斯州的磁性电报，但是它们可能并没有什么重要的东西需要交流……我们满腔热情地在大西洋下开通隧道，把新旧世界拉近几个星期，但是到达美国人耳朵里的第一条新闻可能却是阿德雷德公主得了百日咳。 从此，来路不明、读者对象不定的新闻开始横扫整个国家。战争、犯罪、交通事故、火灾和水灾——大多是阿德莱德公主得百日咳新闻的社会版本和政治版本——开始成为所谓“今日新闻”的主要内容。……电报使脱离语境的信息合法化，也就是说，信息的价值不再取决于其在社会和政治对策和行动中所起的运用，而是取决于它是否新奇有趣。电报把信息变成了一种商品，一种可以置用处或意义于不顾而进行买卖的东西。 行为最终产生的结果并不能成为行为最初的目的；人本性就是自私的，只有判断他行为中私己成分的多与少。 马歇尔·麦克卢汉所说的”后视镜“思维：认为一种新媒介只是旧媒介的延伸和扩展，比如汽车只是速度更快的马，电灯是功率更大的蜡烛。他们犯的错误就是完全误解了电视如何重新定义公众话语的意义。 宗教改革运动的发起者马丁·路德：如果每个家庭的餐桌上都有上帝的文字，基督徒就不需要教皇来为他们释义了。 萧伯纳第一次看见百老汇和四十二街上夜间闪烁的霓虹灯时发表的评论：如果你不识字，这些灯光无疑是美丽的。 思考不是表演艺术，而电视需要的是表演艺术…….人们看的以及想要看的是有动感的画面—-成千上万的图片，稍纵即逝却斑斓夺目。正是电视本身的这种性质决定了他必须舍弃思想，来迎合人们都视觉快感的需求，适应娱乐业的发展。 和他们一样，你要把注意力放在如何最大限度地实现节目的娱乐价值上。例如，你要为节目选择一个音乐主题。所有的电视新闻节目的开始、结束或者中间都要插入一段音乐。我发现很少有美国人会觉得这样的做法奇怪，这足以证明严肃的公众话语和娱乐之间存在的分界线已经荡然无存…….制造一种情绪，为娱乐提供一个主题，提醒观众使用一个合适的情绪。 假信息并不意味着错误的信息，而是意味着使人产生误解的信息——没有依据、毫无关联、支离破碎或流于表面的信息——这些信息让人产生错觉，以为自己知道了很多事实，其实却离事实的真相越来越远。当新闻被包装成一种娱乐形式时，它提供给观众的是娱乐而不是信息。 电视信息的娱乐化：节目中加入音乐引导观众情绪 | 新闻通常不超过1分钟，几乎不可能报导一个完整的严肃性新闻 | 紧跟着播放的一系列广告会在瞬间消解新闻的重要性 | 电视屏幕上的图像源源不断出现，不留有时间供观众思考 | 播音员播报任何新闻时保持一种固定不变的、得体的热情，不为播报的新闻内容所影响 | 播音员的容貌须得体，以及讲述者的可信度决定了事件的真实性。 不论是释迦牟尼、摩西、耶稣还是穆罕默德、路德，从来没有那个伟大的宗教领袖会给人们他们想要的东西，他们给的是人们应该具备的东西。但电视传教士的不成文规则是：“只有给观众他们想要的东西，你才可以得到市场占有率。” 真正的危险不在于宗教已经成为电视节目的内容，而在于电视节目可能会成为宗教的内容。 那些经营电视的人从来没有限制我们获得信息，而是不断扩大我们获得信息的途径。也就是说，通过制造大量的娱乐性的、无目的性、无意义的信息，来使真正应该被引起注意的新闻被淹没其中。不采用限制的手段却达到了使特定的信息在操纵下无法流通。 杜威《经验与教育》：也许人们对于教育最大的错误认识是，一个人学会的只有他当时正在学习的东西。其实，伴随学习的过程形成持久的态度········也许比拼写课或地理历史课更为重要·······因为这些态度才是在未来发挥重要作用的东西。 有两种方法可以让文化精神枯萎，一种是奥威尔式的——文化成为一个监狱，另一种是赫胥黎式的——文化成为一种滑稽戏。 奥威尔预言的世界比赫胥黎预言的世界更容易辨认，也更有理由去反对。我们的生活经历已经能够让我们认识监狱，并且知道在监狱大门即将关上的时候要奋力反抗。在弥尔顿、培根、伏尔泰、歌德和杰弗逊这些前辈的精神的激励下，我们一定会拿起武器保卫和平。但是，如果我们没有听到痛苦的哭声呢？谁会拿起武器去反对娱乐？ 那些提出这些问题的人是得出和我一样的答案还是和马歇尔·麦克卢汉一样的答案并不重要，能够提出答案就行了，提出了问题就是破除了禁忌。 《如彗星划过夜空》— 林达 这是一件非常有意思的事情，人类文明可能在一部分人中间先创造出来，而它被称为是“文明”的原因之一，就是它有能力超越自身利益的局限，有了抽象的人道、人权的思维，而且，还在设计“制度”，保障这样的权益。 华盛顿将军：“先生们，请等我戴上眼镜。这么些年，我的头发白了，眼神也不济了。”在华盛顿将军心中，“枪杆子”只是带来了追求自由的一个可能。唯有民众的授权，才是政府权力的合法来源。 欠债的农夫一多，就群起要求州议会通过立法，允许他们缓偿债务，要求州里加印纸币，还要求立法强立债权人接受纸币作为还款…….欠债的农夫们走投无路，开始造反，最震动的一次冲突是美国历史上有名的谢思暴动。 熟悉英国议会制度的代表们，在费城会议中也设立了全体委员会，作用却不一样。它只是一个矛盾的缓冲设置。这时的表决结果，只相当于委员会推荐方案，是对可行方案的试探。 今天人们在讨论的民主，往往都是指政治制度。为什么要去牵扯出“民主是一种思维方式”或者非政治领域的“生活方式”呢？我想，政治制度，其实时需要一个相应的社会文明程度去配合的。在民主制度自然生成的国家，是文明的土壤长出了这颗制度之树，而不是相反。 费城制宪会议的另一个规则，是一条条地分别表决提案，任何一条通过之后，都可以再返回来提出异议，要求重新表决。 民主意识的一个重要来源是人性的觉醒，从而自然地引发出对底层悲惨状况的同情和不平，进而为他们争取权益。正由于这种同情大多发自有比较优越的社会地位、文明程度较高的阶层，或者说发自知识阶层，因此他们的民主意识从起源来说，都是带着原罪负担的。他们非常容易进入的一个误区，就是会不由自主地要美化底层，以平衡自己的原罪意识。他们会在表达对底层苦难同情的时候，在赞美底层的时候，表现得煽情和夸张，以支撑自己的道德感。他们中的一部分，会要求竭力降低自己的文明水准甚至在行为上表现出反文明和粗俗…….这种倾向符合道德出发点的原始冲动。从法国大革命对平民杀贵族的支持，到一代代的民粹倾向，直至现代美国走到极端的“政治正确”，都是源于同样的出发点。在原罪负担之下，承担原罪感的人群往往是不自信的，他们需要他人对自己做出道德上的肯定。结果就是以过激的平民认同和平民倾向，来达到心理和道德需求上的平衡。 所以，我以为知识阶层的所谓道德勇气，一部分应该是表现在对强权的批判上，但更为困难更难做到的，是表现在他不迎合、不取悦于民众上。前者是很容易理解也相对更容易做到的。可是，只有非常少的人，能够有智慧有勇气做到，对强权和民众，都保持应有的独立和批判。这和他是否同情弱者，是否保护弱势人群，其实是两回事。 这是一个自然形成的利益诉求。个人在人群中非常弱下，可能被强者吃掉。他因此需要一个社会保护层，有法律抵挡强人，有地方可以申诉，有政府力量的保护，如同在身上加一个保护性的外壳。因此，人的联合、政府组织自然形成。 麦迪逊认为：结派会导致人的道德水平下降。个人作为个人行动的时候，都会对自己有一定的道德要求，有人之常情，有恻隐之心，会自觉地压抑人性中自私和恶的一面。可是一群人结成一派行动的时候，就会互相提供行为的正当性，提供派别内部的互相暗示，自我道德要求就会下降，甚至做出在一个人的时候不会做的坏事。 只有获得充分信息的持批判态度的大众意见，能够保护民主政府的价值体系。所以，警觉的、无所不晓的、自由的新闻界本身，对实现宪法第一修正案的目的是最为重要的。他说：“没有一个自由的、获得了充分信息的新闻界，就不可能有脱离蒙昧的人民。” 在这里，人们仍然有一种信念，他们相信，他们可能要走一段弯路，可能有一段倒退，可是任何威胁只能阻碍人们追求自由的道路，却不可能堵死它。也许今夜没有星辰，可是，他们相信，在云霭之上，仍然有群星在太空闪亮。 《美丽新世界》— 赫胥黎 直到最后，孩童的心灵就是这些暗示，而这些暗示的总和也就是孩童的心灵。不仅仅是孩童的心灵而已。成年人的心灵亦复如此······终其一生而不渝。那用来判断、希冀和下决心的心灵——都由这些暗示所组成。而所有这些暗示都是我们的暗示！ 长久的追悔，是最可厌的一种情绪，这是所有道德家都同意的。如果你犯了错，就忏悔、努力改正，争取下回做好就是了。绝对不要沉溺在自己的错失里。在污泥中打滚可不是最好的净身方法。 《乌合之众》— 古斯塔夫·勒庞 群体不善推理，却急于采取行动。他们目前的组织赋予了他们巨大的力量。我们目睹其诞生的那些教条，很快也会具有旧式教条的威力，也就是说，不容讨论的专横武断的力量。群众的神权就要取代国王的神权了。 只有环境的单一性，才能造成明显的性格单一性。我曾在其它著作中指出，一切精神结构都包含着各种性格的可能性，环境的突变就会使这种可能性表现出来。这解释了法国国民公会中最野蛮的成员为何原来都是些谦和的公民。 他很难约束自己不产生这样的念头：群体是个无名氏，因此也不必承担责任。这样一来，总是约束着个人的责任感便彻底消失了。 群体在智力上总是低于孤立的个人，但是从感情及其激起的行动这个角度看，群体可以比个人表现得更好或更差，这全看环境如何。一切取决于群体所接受的暗示具有什么性质……刺激群体的因素多种多样，群体总是屈从于这些刺激，因此群体也极为多变。群体很容易做出刽子手的举动，同样也很容易慷慨赴义。 孤立的个人很清楚，在孤身一人时，他不能焚烧宫殿或洗劫商店，即使受到这样做的诱惑，他也很容易抵制这种诱惑。但是在成为群体的一员时，他就会意识到人数赋予他的力量，这足以让他生出杀人劫掠的念头，并且会立刻屈从于这种诱惑。 群体中的某个人对真相的第一次歪曲，是传染性暗示过程的起点……被派往寻找失散巡洋舰的护航舰“贝勒·波拉号”上，执勤兵突然发出了有一艘遇难船只的信号。而下船去营救落难士兵的官员们发现，那只不过是几根长满树叶的树枝。在这个实例中，可以清楚地看到我们已经解释过的集体幻觉的作用机制。一方面，我们看到一个在期待中观望的群体；另一方面，是执勤者发出海上有遇难船只的信号这样一个暗示。 群体推理的特点，是把彼此不同、只在表面上相似的事物搅在一起，而且立刻把具体的事物普遍化。知道如何操纵群体的人，给他们提供的也正是这种论证。包含一系列环节的逻辑论证，对群体来说完全是不可理解的。对于演讲家来说，20本滔滔不绝的长篇论证，尽管它们是认真思考的产物，还不如几句对群众有号召力的口号。 拿破仑对国会说：“我通过改宗天主教，终止了旺代战争；通过变成一个穆斯林教徒，在埃及站住了脚；通过成为一名信奉教皇至上的人，赢得了意大利神父的支持。如果我去统治一个犹太人的国家，我也会重修所罗门的神庙。” 持政府和帝国的具体工作就是用新的名称把大多数过去的制度重新包装一遍，这就是说，用新名称代替那些能够让群众想起不利形象的名称，因为它们的新鲜能防止这种联想。如商号和行会的税款变成了执照费等…名称的威力如此强大，如果选择得当，它足以使最可恶的事情改头换面，变得能被民众所接受。 让人们怀抱着那些希望和幻想吧，不然他们是活不下去的。这就是存在着诸神、英雄和诗人的原因。科学承担起这一任务已有50年的时间，但是在渴望理想的心灵里，科学是有所欠缺的，因为它不敢作出过于慷慨的承诺，因为它不能撒谎。 然而群众无论付出多大的代价，他们必须拥有自己的幻想，于是他们便像趋光的昆虫一样，本能地转向那些迎合他们需要的巧舌如簧者。 群众从来就没有渴望过真理，面对那些不合口味的证据，他们会拂袖而去，假如谬论对他们有诱惑力，他们更容易崇拜谬论。凡是能向他们供应幻觉的，也可以很容易地成为他们的主人。 要让群体相信什么，首先得搞清楚让他们兴奋的感情，并且装出自己也有这种感情的样子……那些知道如何影响他们的演说家，总是诉诸他们的感情而不是他们的理性。逻辑定律对群体不起作用……若演讲者遵循的是自己的思路而不是听众的思路，仅仅这一个事实就会使他不可能产生任何影响。 一个人占据着某种位置，拥有一定的财富或头衔，仅仅这些事实，就能使他享有名望，不管他本人多么没有价值。帕斯卡尔十分正确地指出，法袍和假发使法官必不可少的行头。没了这些东西，他们的权威就会损失一半。 巴特农神庙按其现存的状态，不过是一堆非常没有意思的破败废墟，但是它的巨大名望却使它看起来不是那个样子，而是与所有的历史记忆联系在一起。 各民族一直清楚获得普遍信念的好处，他们本能地知道，这种信念的消失是他们衰败的信号。使罗马人能够征服世界的信念，是他们对罗马的狂热崇拜；当这种信念寿终正寝时，罗马也注定衰亡。 每个时代的人都是在一个由相似的传统、意见和习惯组成的基本环境中成长，他们不能摆脱这些东西的桎梏。人的行为首先是受他们的信念支配，也受由这些信念所形成的习惯支配。 利用密谋可以推翻一个暴君，而反对牢固的信念又有什么可资利用？人类所知道的唯一真正的暴君，历来就是他们对死人的怀念或他们为自己编织出来的幻觉……躺在坟墓深处的摩西、佛祖、耶稣和默罕穆德，对人类实行着更深刻的转制统治。 至于过去引导意见的报业，就像政府一样，它在群众势力面前也变得屈尊俯就。当然，它仍然有相当大的影响，然而这不过是因为它只一味反映群众的意见及其不断的变化。报业既然成了仅仅提供信息的部门，它便放弃了让人接受某种观念或学说的努力。它在公众思想的变化中随波逐流，出于竞争的必要，它也只能这样做，因为它害怕失去自己的读者。最有价值的新闻被夹在各种轻松话题、社会见闻和金融谎言之间。 人们的意见还大致存在着一般趋势，它们的产生是因为接受了一些基本的信仰，只根据某人是个君主制的拥护者这一事实，即可断定他持有某些明确的历史观和科学观；只根据某人是共和主义者，便可以说他有着完全相反的观点。 候选人可以毫无惧色地承诺最重要地改革。这些夸张能够产生巨大的效果，但它们对未来并没有约束力，因为这需要不断地进行观察，而选民绝对不想为这事操心，他并不想知道自己支持的候选人在实行他所赞成的竞选纲领上走了多远，虽然他以为正是这个纲领使他的选择有了保证。 《达摩流浪者》— 杰克·凯鲁亚克 在钢铁工厂和飞机场遍布的美国，会出现这样一号人物，更是奇上加奇。有贾菲这样的人在，表示着世界还不算太没有希望。我为此而感到高兴。我全身的肌肉都酸痛得要死，而肚子也饿得要命，不过，能够坐在这里和另一个充满热情的年轻人为这个世界祷告，这件事所带给我的安抚，就足以胜过一千个吻和一千句柔情话。 一想到这个，我就不禁噗嗤一声笑了出来。我想不出来，除了露宿、攀火车和做自己想做的事情以外，还有什么生活是值得过的，难道是在精神病院里和其它一百个病人聚精会神地盯着电视看吗？ 你知道吗，我常常开着这辆大东西，在俄亥俄和洛杉矶之间没命地跑来跑去，而我跑一趟的钱，说不定要比你当流浪汉一辈子能赚的还要多。但你不必工作，不需要多少钱，却可以享受人生。到你是你聪明还是我聪明，我实在说不上来。 你和我都不是那种愿意为了过优裕的生活而践踏别人的人。我们的理想是找一个安静的地方，永远为所有的有情祷告，而只要等我们都变得够强壮，就可以付诸实行。 《僧侣与哲学家》— 让-弗朗索瓦·何维勒 用同样的方式，我们正眼去看一个念头，追溯它的起源，一样找不到任何具体的东西。当你发现这一点的那一刹那，那个念头就会消失掉。这称为“通过认识念头的本质来解放它”，意思是要认出念头的“空性”。我们一旦解放一个念头，就不会产生连环作用，反而像是从天空中飞过去的一只鸟，消失了却不留痕迹。 一个不存在的自我其实没有办法真正被“毁灭”，但是我们可以认出它的不存在性。我们想毁灭的就是那个幻觉，那个错误，那原来并非存在的东西。关于这一点经常有如下的隐喻：如果你进到一个房间，在阴暗的光线下看到一条绳子，以为是一条蛇，你会害怕，你会想逃走，或者想拿一根棍棒把它赶跑。这时候一个人突然把灯打开，你马上看到了，那根本不是一条蛇。事实上，在这个过程之中，什么都没有发生；你也没有“消灭”那条蛇，因为它从来就没有存在过，你只不过是驱除了一个幻觉。 我们需要再度分辨科学的种类，一方面是历史科学、心灵科学和人文科学，另外一边是“硬”的科学。后者得到的证据是任何人，不管他的意见是什么，都必须接受的。而前者不断累积见证，增加真理的可能性，但是有一个永远属于“绝对确定”的门槛，是没有办法跨越的。 我向你保证，我们无法让一位新几内亚热带雨林的居民相信百分之一的科学发现。我们说话的对象必须有一个相称的心理架构才行。我们必须许多年用一种特定的方式来教育他。同样地，没有打开自己的心来研究修行的人士，也不可能接受这样的成果。这也是需要教育的。“硬”的科学，也就是需要可复制性证据的科学，目标其实不是要解决形而上的问题，也不是要让生命有意义，而是希望用最准确的方式来描述物质世界，如果现实可以被浓缩成物质，意识只不过是神经系统的一个作用，这只是在替科学的运作下一个定义。修行的生物也有它的规则，修行在心中所带出的深沉信仰力量，可以和物质世界进行的实验相当。从一个纯粹修行的观点来观察心的本性，所带来的确定性等同于观察一个因地心引力而坠落的物体。 如果内省作为一种科学方法在西方心理学中已经失败而且被抛弃了，那是因为使用它的人没有适当的工具来进行实验。他们完全没有训练，完全没有心灵领域的知识，也完全不知道如何让心平静的技巧，让心背后的本性能够被观察出来。这像一个人用一个不稳定的电表来测电，最后得到的结论是：电是无法测量的。心灵技巧的学习需要精进，你不能一挥手就说不算，就因为它们不属于西方世界的主流思想。怀疑是容易理解的，但是缺乏兴趣，或不愿去印证另外一种方式这才是难以理解的。事实上，反过来说，也会遭遇同样的问题：我认识的一些西藏人绝对不愿意相信人类已经登陆了月球！ 褊狭有两种主要形态：第一种是当人们没有深入自己宗教的深层意义时，不用纯正的方式实践它，反而拿它作为一面镜子，用来煽动地方性、民族性或国家性的情绪；第二种就是诚恳实行自己宗教的人们深信所信仰的真理，到他们认为可以用任何方式把这些信仰强加到别人身上的地步，因为他们认为这样做是在帮助别人的情况。 我们对现象世界的觉知是通过感觉器官，再由一串意识的刹那来感受这些器官送出来的资讯，并加以诠释。于是我们并不在觉知真正的世界，我们觉知的只不过是在我们意识中反映出来的意向。 如果一个囚犯想要释放和他同样受苦的伙伴，他必须先想办法挣脱自己的枷锁……如果要有能力帮助众生，你所教导的就是你所实行的。 丹增仁波切经常说：“我来西方的目的并不是为了要多创造一两个佛教徒，而是要分享我的经验，关于佛教这几世纪所发展出来的智慧。”他每一次演讲完都会说：“如果你举得我说的任何话有用，请用它，不然就忘掉它吧！”他甚至劝告到其他国家旅行的西藏喇嘛们不要过度强调佛教的教义，而要以一个人的身份对另外一个人提供自己的经验。 佛教谈到三种懒惰。第一种很简单，就是把所有时间用在吃饭和睡觉上。第二种就是告诉自己，“像我这样的人绝对不可能达到完美”。佛教的观点中，这种懒惰会让你觉得就算努力也没有意义，你永远无法达到任何心灵上的成就，这种懒惰让自己灰心，反而令自己试都不去试。第三种，也是在这里最切题的，就是把生命浪费在次等重要的工作上，永远不去面对最精要的问题，所有时间都花在解决次要问题上，一个个接一个，在一个永无止境的顺序中，像湖上的涟漪一样。你告诉自己，当你完成了这件或那件事情之后，你会开始寻找你生命的意义。 佛说：“不要因为对我的尊敬而相信我的教导。检查它，让你自己重新发掘出真理。”他又说：“我已经为你指引出道路，要不要走下去是你的事。”佛陀的教导就像一本导游手册，解释他自己走向智慧的道路。真正要成为一个佛教徒，我们必须皈依佛陀，并不是要把他当做一个神，而是把他当作一个导游，以及证悟的象征。同时我们要皈依他的教导，就是佛法，但佛法不是教条，而是一条道路。 不要只顾着梯子，要记得你要爬向哪里。 《魔鬼经济学》 值得注意的是，这些网站仅仅是将报价罗列了出来，并未参与保险的销售。因此，它们所经手的并非保险，而是信息……三K党的机密情报被爆料给广播和报纸媒体后，申请入会人数开始减少，机要信息如内部的等级分级制度及阶层名称也成为了供人讥讽的笑料……最高法院大法官路易斯·D·布兰戴斯曾写道：“据说阳光是最好的杀菌剂。” 信息就是互联网的货币。作为一种媒介，互联网行之有效地将信息从占有方转移到需求方。通常，一如定期人寿保险费的例子，信息以极端零碎的方式存在。在此情况下，互联网就像一块巨大的马蹄形磁铁，将沉入大海的绣花针一根根收集起来。互联网有一项成就，就连最热心积极的消费者保护团体也望尘莫及：它大幅缩小了专家与公众之间的信息差。 报纸上的言论听起来更加顺耳——犯罪率的下降要归功于新型的治安策略、灵活的枪支管制和良好的经济形势。我们越来越习惯于从我们触手可及的事物上寻找因果联系，而忽视年代久远、难以理解的现象。我们尤其迷信短期可见的原因，多数时候，这种推断都是正确的。但探讨因果关系时，这种一概而论的思维却常常存在陷阱。 这其实是个滑坡谬论——看似毫无意义的个人行为积少成多，就会产生客观的影响。并不是说有选民认识到自己的选票无法左右选举结果而不去投票，会使得选举不复存在。滑坡谬论，是一种非形式谬论，使用连串的因果推论，却夸大了每个环节的因果强度，因此得到不合理的结论。 那说90年代的经济繁荣导致了犯罪率的下降，似乎也顺理成章。然而，二者存在相关关系，并不等同于一者导致了另一者。相关关系仅表示，两个因素——姑且称之为X和Y——之间存在某种关系，但你无从判断孰因孰果。 《通往维根码头之路》— 乔治·奥威尔 这个地方开始让我抑郁。不仅仅是因为那些灰尘，异味，和令人生厌的食物，更是由于那种一潭死水又毫无意义的衰退的感觉，那些在地底下工作的人散发出的味道如爬虫一般，那是一种被禁锢在充满污秽的劳作和刻薄的抱怨的泥沼之中，一圈一圈兜兜转转无法消散的气息。像布鲁克夫妇这样的人，最让人忍无可忍的是他们对于同一件事情一遍又一遍发着牢骚的方式。这让你觉得他们并不是活生生的人，而像是某种神神叨叨反复呢喃胡言乱语的幽魂。最后布鲁克夫人那千篇一律、自怜自艾的话语永远都是以“这真的太难了，不是吗？”结尾，一遍又一遍地重复着。 北方商人抱着其可恶的“要么成功要么滚蛋”的哲学理念成为十九世纪的主导力量，并至今阴魂不散地统治着我们。这是一类受阿诺德·本涅特启发的人——一类从半个克朗起家，到头来赢得五万英镑的人，他们最可吹嘘之处就是赚了钱之后变得比以前更粗鲁。他们唯一的优点就是会赚钱。我们被教导要推崇这样的人，仅仅是因为他们尽管心胸狭窄、卑鄙、愚昧、贪婪又粗鲁，却有“胆识”，他“成功”了。 接下来，你要面对的是西方社会阶级差异的真正秘密……下等人臭，这就是我们被教导的。没有什么喜恶之感能像生理感受这般切肤。种族厌恶、宗教憎恨、教育差异、性格、智力、甚至道德观的差异都可以被跨越，唯有生理上的排斥无从化解。 但是你的愿景苍白无力，除非你能抓紧它所牵扯进来的东西，即一个你不得不面对的现实，想要废除阶级差异，等于废除一部分的自我。像我这样，一个典型的中产阶级的一员，叫我说出我想让阶级差异消亡殆尽简直易如反掌。但我所有的行之所至，思之所及都来源于这阶级特质。我所有的观念，我的善与恶、悲与喜、笑与怒、美与丑，本质上都是中产阶级所拥有的概念；我读书、穿衣、饮食的品味，我的幽默，我的礼节，甚至我的一言一行、一平一仄，都是一种在社会阶层中自抬身价的特殊举措。当我意识到这点，我也同样意识到了我在无产阶级背后所给予他们的鼓励和平等的徒负虚名；若我果真想和他接触，我必须彻底改头换面，以至于最终从前的那个自我荡然无存，无人能辨。这里牵涉到的不仅仅是改善工人阶级的生活条件，也不仅仅是避免愚蠢的自命不凡的姿态，而是彻底抛弃中上层阶级的生活态度。 《人性的枷锁》— 毛姆 自己读完的感受：【艺术性地夸大欲望的力量，将人的本性暴露得一览无遗】 他明明愿意放弃一切来再和罗斯成为朋友的。他恨自己和罗斯吵架，现在他看到自己让他痛苦了他觉得十分抱歉。但是刚刚那一刻仿佛不是自己了，就好像魔鬼抓住了他，强迫着他违背自己的意愿对罗斯说一些尖酸刻薄的话，尽管他现在恨不得立刻冲去和罗斯握手言和。但是他报仇雪恨的渴望太强烈了。他想要为自己曾经受到的痛苦和侮辱报复罗斯。这是他的骄傲：当然他也知道这很蠢，因为罗斯根本就不会在乎，而他自己却会因此二痛苦。 等你长大一点儿你就会明白要让这个世界变成一个尚可容忍的生活场所，第一件事情就是要承认人类的自私是不可避免的。你要求别人对你无私实际上就很荒谬，因为这就是在要求他们为了你而牺牲自己的欲望。他们凭什么要那么做呢？如果你甘心接受世界上每个人都是为了自己而活的，你反而就会对别人要求变少了。那么他们也不会让你失望，而你也会更加宽恕地看待他们。人们在世上只追求一件事情——那就是他们自己的愉悦。 菲利普突然意识到了他一直在欺骗自己：并不是自我牺牲逼得他想到了结婚，而是对于一位妻子一个家和爱情的渴望；而现在这一切似乎都在他的手上溜走了，他感到十分的绝望。他想要那些，比世界上任何东西都想要。他有什么好在乎西班牙和西班牙的那些城市呢，什么科尔德瓦，什么多莱多，什么莱昂；那些缅甸的宝塔和南海群岛的环礁湖又对他有什么意义呢？此时此地就是美洲啊。在他看来他以前的所有人生都在追随着其他人通过言语或者文字灌输给他的想法，他从来就没有过自己心底的渴望。他的轨迹永远是被他认为应该做的事情所动摇，而不是被他的整颗心想要做的事情所动摇。他现在不耐烦的把这些所有的一切都抛弃了。他永远都活在未来，而当下永远，永远地都从他的指尖溜走。他自己的想法？他想到了他想要从纷繁复杂又毫无意义地生活中编出一个美丽而复杂的图案来的愿望：他不也见过了那些最简单的图案了吗？那种一个人出生、工作、结婚、生子然后死亡的图案，那不也是最完美的图案吗？屈服于幸福也许就是承认失败，但是这种失败却比千百次的成功都更好啊 《沉默的大多数》— 王小波 假设有某君思想高尚，我是十分敬佩的；可是如果你因此想把我的脑子挖出来扔掉，换上他的，我绝不肯。人既然活着，就有权保证他思想的连续性，到死方休。假如我全盘接受，无异于请那些善良的思想母鸡到我脑子里下蛋，而我总不肯相信，自己的脖子上方，原来是长了一座鸡窝……假设我相信上帝，并且正在为善恶不分而苦恼，我就会请求上帝让我聪明到足以明辨是非的程度，而绝不会请他让我愚蠢到让人家给我灌输善恶标准的程度。假若上帝要我负起灌输的任务，我就要请求他让我在此项任务和下地狱中作一选择，并且我坚定不移的决心是：选择后者。 假如要我举出一生最善良的时刻，那我就要举出刚当知青时，当时我一心想要解放全人类，丝毫也没有想到自己。同时我也要承认，当时我愚蠢得很，所以不仅没干成什么事情，反而染上了一身病，丢盔卸甲地讨回城里。现在我认为，愚蠢是一种极大的痛苦；降低人类的智能，乃是一种最大的罪孽。所以，以愚蠢教人，那是善良的人所能犯下的最严重地罪孽。从这个意义上说，我们决不可对善人放松警惕。 中国的人文知识分子，有种以天下为己任的使命感，总觉得自己该搞出些给老百姓当信仰的东西。这种想法的古怪之处在于，他们不仅是想当牧师、想当神学家，还想当上帝（中国话不叫上帝，叫“圣人”）。可惜的是，老百姓该信什么，信到哪种程度，你说了并不算哪，这是令人遗憾的。还有一条不令人遗憾，但却要命：你自己也是老百姓；所以弄得不好，就会自己屙屎自己吃。 我对国学的看法是：这种东西实在厉害。最可怕之处就在那个“国”字。顶着这个字，谁还敢有不同意见？这种套子套上脖子，想把它再扯下来是枉然的；否则也不至于套了好几千年。它的诱人之处也在这个“国”字，抢到这个制高点，就可以压制一切不同意见；所以它对一切想在思想领域里巧取豪夺的不良分子都有莫大的诱惑力。你说它是史学也好，哲学也罢，我都不反对——倘若此文对正经史学家哲学家有了得罪之处，我深表歉意——但你不该否认它有成为棍子的潜力。想当年，像姚文元之类的思想流氓拿阶级斗争当棍子，打死打伤了无数人。现在有人又在造一根漂亮棍子。它实在太漂亮了，简直是完美无缺。我怀疑除了落进思想流氓手中变成一种凶器之外，它还能有什么用场。鉴于有这种危险，我建议大家都不要做上帝梦，也别做圣人梦，以免头上鲜血淋漓。 作为一个知识分子，我对信念的看法是：人活在世上，自会形成信念。对我本人来说，学习自然科学、阅读文学作品、看人文科学的书籍，乃至旅行、恋爱，无不有助于形成我的信念，构造我的价值观。一种学问、一本书，假如不对我的价值观发生作用（姑不论其大小，我要求它是有作用的），就不值得一学，不值得一看。有一个公开的秘密就是：任何一个知识分子，只要他有了成就，就会形成自己的哲学、自己的信念。托尔斯泰是这样，维纳也是这样。到目前为止，我还看不出自己有要死的迹象，所以不想最终皈依什么——这块地方我给自己留着，它将是我一生事业的终结之处，我的精神墓地。不断地学习和追求，这可是人生在世最有趣的事啊，要把这件趣事从生活中去掉，倒不如把我给阉了……你有种美好的信念，我很尊重，但要硬塞给我，我就不那么乐意：打个粗俗的比方，你的把把不能代替我的把把，更不能代替天下人的把把啊。这种看法会遭到反对，你会说：有些人就是笨，老也形不成信念，也管不了自己，就这么浑浑噩噩地活着，简直是种灾难！所以，必须有种普遍适用的信念，我们给它加点压力，灌到他们脑子里！你倒说说看，这再不叫意识形态，什么叫意识形态？ 有关理性，哲学家有很多讨论，但根据我的切身体会，它的关键是：凡不可信的东西就不信，像我姥姥当年对待亩产三十万斤粮的态度，就叫做有理性。但这一点有时候不容易做到，因为会导致悲观和消极，从理性和乐观两样东西里选择理性颇不容易。理性就像贞操，失去了就不会再有；只要碰上了开心的事，乐观还会回来的。不过这一点很少有人注意到。从逻辑上说，从一个错误的前提什么都能推出来；从实际上看，一个扯谎的人什么都能编出来。所以假如你失去了理性，就会遇到大量令人诧异的新鲜事物，从此迷失在万花筒里，直到碰上了钉子。假如不是遇到了林彪事件，我至今还以为自己真能保卫毛主席哩。 我上大学时，有一次我的数学教授在课堂上讲到：我现在所教的数学，你们也许一生都用不到，但我还要教，因为这些知识是好的，应该让你们知道。这位老师的胸襟之高远，使我终生佩服。 我认为像我这样的人不在少数：我们热爱艺术、热爱科学，认为它们是崇高的事业，但是不希望这些领域里的事同我为人处世的态度、我对别人的责任、我的爱憎感情发生关系，更不愿因此触犯社会的禁忌。这是因为，这两个方面不在一个论域里，而且后一个论域比前者要严重。打个比方，我像本世纪初年的一个爪哇土著人，此种人生来勇敢，不畏惧战争，但是更重视清洁。换言之，生死和清洁两个领域里，他们更看重后者；因为这个缘故，他们敢于面对枪林弹雨猛冲，却不敢朝着秽物冲杀。荷兰殖民军和他们作战时，就把屎橛子劈面掷去，使他们望风而逃。当我和别人讨论文化问题时，我以为自己的审美情趣、文化修养在经受挑战，这方面的反对意见就如飞来的子弹，不能使我惧怕；而道德方面的非难就如飞来的粪便那样使我胆寒。我的意思当然不是说现在文化的领域是个屎橛纷飞的场所，臭气熏天——决不是的；我只是说，它还有让我胆寒的气味。所以，假如有人以这种态度论争，我要做的第一件事，就是逃到安全距离之外，然后再好言相劝：算了吧，何必呢？ 前年夏天，我到外地开一个会——在此声明，我很少去开会，这个会议的伙食标准也不高——看到一位男会友穿了一件文化衫，上面用龙飞凤舞的笔迹写着一串英文：OK, Let’s pee！总的来说，这个口号让人振奋，因为它带有积极、振奋的语调，这正是我们都想听到的。但是这个pee是什么意思不大明白，我觉得这个字念起来不大对头。回来一查，果不出我所料，是尿尿的意思。搞明白了全句的意思，我就觉得这话不那么激动人心了。众所周知，我们已过了要人催尿的年龄，在小便这件事上无须别人的鼓励。我提到这件事，不是要讨论如何小便的问题，而是想指出，在做一件事之前，首先要弄明白是在干什么，然后再决定是不是需要积极和振奋。 现在有人说，同性恋是一种社会丑恶现象，我反对这种说法，但不想在此详加讨论——我的看法是，同性恋是指一些人和他们的生活，说人家是种社会现象很不郑重。我要是说女人是种社会现象，大家以为如何？——我只想转述一位万事通先生在澡堂里对这个问题发表的宏论，他说：“同性恋那是外国的高级玩意儿，我们这里有些人就会赶时髦……这艾滋病也不是谁想得就配得的！”在他说这些话时，我的一位调查对象就在一边坐着。后者告诉我说，他的同性恋倾向是与生俱来的。他既不是想赶时髦，也不是想得艾滋病。他还认为，生为一个同性恋者，是世间最沉重的事。 我想，假如这位万事通先生知道这一切，也不会对同性恋作出轻浮、赶时髦这样的价值评判，除非他对自己说出的话是对是错也不关心。我举这个例子是想说明：伦理道德的论域也和其他论域一样，你也需要先明白有关事实才能下结论，而并非像某些人想象的那样，只要你是个好人，或者说，站对了立场，一切都可以不言自明。不管你学物理也好，学数学也罢，都得想破了脑袋，才能得到一点成绩；假设有一个领域，你在其中想都不用想就能得到大批的成绩，那倒是很开心的事。不过，假如我有了这样的感觉，一定要先去看看心理医生。 现在可以说，孔孟程朱我都读过了。虽然没有很钻进去，但我也怕钻进去就爬不出来。如果说，这就是中华文化遗产的主要部分，那我就要说，这点东西太少了，拢共就是人际关系里那么一点事，再加上后来的阴阳五行。这么多读书人研究了两千年，实在太过分。我们知道，旧时的读书人都能把四书五经背得烂熟，随便点出两个字就能知道它在书中什么地方。这种钻研精神虽然可佩，这种做法却十足是神经病。显然，会背诵爱因斯坦原著，成不了物理学家；因为真正的学问不在字句上，而在于思想。就算文科有点特殊性，需要背诵，也到不了这个程度。因为“文革”里我也背过毛主席语录，所以以为，这个调调我也懂——说是诵经念咒，并不过分。 “二战”期间，有一位美国将军深入敌后，不幸被敌人堵在了地窖里，敌人在头上翻箱倒柜，他的一位随行人员却咳嗽起来。将军给了随从一块口香糖让他嚼，以此来压制咳嗽。但是该随从嚼了一会儿，又伸手来要，理由是：这一块太没味道。将军说：没味道不奇怪，我给你之前已经嚼了两个钟头了！我举这个例子是要说明，四书五经再好，也不能几千年地念；正如口香糖再好吃，也不能换着人地嚼。当然，我没有这样地念过四书，不知道其中的好处。有人说，现代的科学、文化，林林总总，尽在儒家的典籍之中，只要你认真钻研。这我倒是相信的，我还相信那块口香糖再嚼下去，还能嚼出牛肉干的味道，只要你不断地嚼。我个人认为，我们民族最重大的文化传统，不是孔孟程朱，而是这种钻研精神。过去钻研四书五经，现在钻研《红楼梦》。我承认，我们晚生一辈在这方面差得很远，但也未尝不是一件好事。四书也好，《红楼梦》也罢，本来只是几本书，却硬要把整个大千世界都塞在其中。我相信世界不会因此得益，而是因此受害。 任何一门学问，即便内容有限而且已经不值得钻研，但你把它钻得极深极透，就可以挟之以自重，换言之，让大家都佩服你；此后假如再有一人想挟这门学问以自重，就必须钻得更深更透。此种学问被无数的人这样钻过，会成个什么样子，实在难以想象。那些钻进去的人会成个什么样子，更是难以想象。古宅闹鬼，树老成精，一门学问最后可能变成一种妖怪。就说国学吧，有人说它无所不包，到今天还能拯救世界，虽然我很乐意相信，但还是将信将疑。 拉封丹寓言里，有一则《大山临盆》，内容如下：大山临盆，天为之崩，地为之裂，日月星辰，为之无光。房倒屋坍，烟尘滚滚，天下生灵，死伤无数……最后生下了一只耗子。中国的人文学者弄点学问，就如大山临盆一样壮烈。当然，我说的不只现在，而且有过去，还有未来。 大概把对自己所治之学的狂热感情视作学问本身乃是一种常见的毛病，不独中国人犯，外国人也要犯。…躲在人文学科的领域之内，享受自满自足的大快乐，在目前还是可以的；不过要有人养。在自然科学里就不行：这世界上每年都有人发明永动机，但谁也不能因此发财。 我知道，这哲人王也不是谁想当就能当，他必须是品格高洁之士，而且才高八斗，学富五车。在此我举中国古代的哲人王为例——这只是为了举例方便，毫无影射之意——孔子是圣人，也很有学问。夏礼、周礼他老人家都能言之。但假如他来打量我，我就要抱怨说：甭管您会什么礼，千万别来打量我。再举孟子为例，他老人家善养浩然之气，显然是品行高洁，但我也要抱怨道：您养正气是您的事，打量我干什么？这两位老人家的学养再好，总不能构成侵犯我的理由。特别是，假如学养的目的是要打量人的话，我对这种学养的性质是很有看法的。比方说，朱熹老夫子格物、致知，最后是为了齐家、治国、平天下。因为本人不姓朱，还可以免于被齐，被治和被平总是免不了的。假如这个逻辑可以成立，生活就是很不安全的。很可能在我不知道的地方，有一位我全然不认识的先生在努力地格、致，只要他功夫到家，不管我乐意不乐意，也不管他打算怎样下手，我都要被治和平，而且根本不知自己会被修理成什么模样。 顺便说一句，有些话只有哲人才能说得出来，比如尼采说：到女人那里去不要忘了带上鞭子。我要替女人说上一句：我们招谁惹谁了。至于这类疯话气派很大，我倒是承认的。总的来说，哲人王藐视人类，比牢头禁子有过之无不及。主张信任哲人王的人会说：只有藐视人类的人才能给人类带来更大利益。我又要说：只有这种人才能给人类带来最大的祸害。从常理来说，倘若有人把你当做了nothing，你又怎能信任他们？ 让我们像奥威尔一样，想想什么是一加一等于二，七十年代对于大多数中国人来说，是个极痛苦的年代。很多年轻人作出了巨大的自我牺牲，而且这种牺牲毫无价值。想清楚了这些事，我们再来谈谈崇高的问题。就七十年代这个例子来说，我认为崇高有两种：一种是当时的崇高，领导上号召我们到农村去吃苦，说这是一种光荣。还有一种崇高是现在的崇高，忍受了这些痛苦、作出了自我牺牲之后，我们自己觉得这是崇高的。我觉得这后一种崇高比较容易讲清楚。弗洛伊德对受虐狂有如下的解释：假如人生活在一种无力改变的痛苦之中，就会转而爱上这种痛苦，把它视为一种快乐，以便使自己好过一些。对这个道理稍加推广，就会想到：人是一种会自己骗自己的动物。我们吃了很多无益的苦，虚掷了不少年华，所以有人就想说，这种经历是崇高的。这种想法可以使他自己好过一些，所以它有些好作用。很不幸的是它还有些坏作用：有些人就据此认为，人必须吃一些无益的苦、虚掷一些年华，用这种方法来达到崇高。这种想法不仅有害，而且是有病。 说到吃苦、牺牲，我认为它是负面的事件。吃苦必须有收益，牺牲必须有代价，这些都属一加一等于二的范畴。我个人认为，我在七十年代吃的苦、作出的牺牲是无价值的，所以这种经历谈不上崇高；这不是为了贬低自己，而是为了对现在和未来发生的事件有个清醒的评价。逻辑学家指出，从正确的前提能够推导出正确的结论，但从一个错误的前提就什么都能够推导出来。把无价值的牺牲看做崇高，也就是接受了一个错误的前提。此后你就会什么鬼话都能说出口来，什么不可信的事都肯信。 人有权拒绝一种虚伪的崇高，正如他有权拒绝下水去捞一根稻草。假如这是对的，就对营造或提倡社会伦理的人提出了更高的要求：不能只顾浪漫煽情，要留有余地；换言之，不能够只讲崇高，不讲道理。举例来说，孟子发明了一种伦理学，说亲亲敬长是人的良知良能，孝敬父母、忠君爱国是人间的大义。所以，臣民向君父奉献一切，就是崇高之所在。孟子的文章写得很煽情，让我自愧不如，他老人家要是肯去作诗，就是中国的拜伦；只可惜不讲道理。臣民奉献了一切之后，靠什么活着？再比方说，在七十年代，人们说，大公无私就是崇高之所在。为公前进一步死，强过了为私后退半步生。这是不讲道理的：我们都死了，谁来干活呢？在煽情的伦理流行之时，人所共知的虚伪无所不在；因为照那些高调去生活，不是累死就是饿死——高调加虚伪才能构成一种可行的生活方式。 《廊桥遗梦》上演之前，有几位编辑朋友要我去看，看完给他们写点小文章。现在电影都演过去了，我还没去看。这倒不是故作清高，主要是因为围绕着《廊桥遗梦》有种争论，使我觉得很烦，结果连片子都懒得看了。有些人说，这部小说在宣扬婚外恋，应该批判。还有人说，这部小说恰恰是否定婚外恋的，所以不该批判。于是，《廊桥遗梦》就和“婚外恋”焊在一起了。我要是看了这部电影，也要对婚外恋作一评判，这是我所讨厌的事情。 在一生的黄金时代里，我们没有欣赏到别的东西，只看了八个戏。现在有人说，这些戏都是伟大的作品，应该列入经典作品之列，以便流传到千秋万代。这对我倒是种安慰——如前所述，这些戏到底有多好我也不知道，你怎么说我就怎么信，但我也有点怀疑，怎么我碰到的全是经典？就说《红色娘子军》吧，作曲的杜鸣心先生显然是位优秀的作曲家，但他毕竟不是柴可夫斯基，……芭蕾和京剧我不懂，但概率论我是懂的。这辈子碰上了八个戏，其中有两个是芭蕾舞剧，居然个个是经典，这种运气好得让人起疑。根据我的人生经验，假如你遇到一种可疑的说法，这种说法对自己又过于有利，这种说法准不对，因为它是编出来自己骗自己的。 作品里的艺术性，或则按事急从权的原则，最低限度地出现；或则按得到最高格调的原则，合理地搭配。比如说，径直去写男女之爱，得分为一，搭配成革命的爱情故事，就可以得到一百零一分。不管怎么说，最后总要得到高大全。我反对把一切统一到格调上，这是因为它会把整个生活变成一种得分游戏。一个得分游戏不管多么引人入胜，总不能包容全部生活，包容艺术，何况它根本就没什么意思。假如我要写什么，我就根本不管它格调不格调，正如谈恋爱时我决不从爱祖国谈起。 所谓幽闭类型的小说，有这么个特征：那就是把囚笼和噩梦当做一切来写。或者当媳妇，被人烦；或者当婆婆，去烦人；或者自怨自艾；或者顾影自怜；总之，是在不幸之中品来品去。这种想法我很难同意。我原是学理科的，学理科的不承认有牢不可破的囚笼，更不信有摆不脱的噩梦；人生唯一的不幸就是自己的无能……由此得出结论，要努力去做事情，拼命地想问题，这才是自己的救星。 总而言之，当一种现象（不管是社会现象还是文学现象）开始贫了的时候，就该兜头给它一瓢凉水。要不然它还会贫下去，就如美国人说的，散发出屁眼气味——我是福尔斯先生热烈的拥护者。我总觉得文学的使命就是制止整个社会变得无趣……当然，你要说福尔斯是反色情的义士，我也没什么可说的。你有权利把任何有趣的事往无趣处理解。 如果说贫穷是种生活方式，捡垃圾和挑大粪只是这种方式的契机。生活方式像一个曲折漫长的故事，或者像一座使人迷失的迷宫。很不幸的是，任何一种负面的生活都能产生很多乱七八糟的细节，使它变得蛮有趣的；人就在这种趣味中沉沦下去，从根本上忘记了这种生活需要改进。用文化人类学的观点来看，这些细节加在一起，就叫做“文化”。有人说，任何一种文化都是好的，都必须尊重。就我们谈的这个例子来说，我觉得这解释不对。在萧伯纳的《英国佬的另一个岛》里，有一位年轻人这么说他的穷父亲：“一辈子都在弄他的那片土、那只猪；结果自己也变成了一片土、一只猪。”要是一辈子都这么兴冲冲地弄一堆垃圾、一桶屎，最后自己也会变成一堆垃圾、一桶屎。所以，我觉得总要想出些办法，别和垃圾、大粪直接打交道才对。 他老人家还说，须知参差多态，乃是幸福的本源。反过来说，呆板无趣就是不幸福——正是这句话使我对他有了把握。一般来说，主张扼杀有趣的人总是这么说的：为了营造至善，我们必须做出这种牺牲。但却忘记了让人们活着得到乐趣，这本身就是善；因为这点小小的疏忽，至善就变成了至恶…… 我常听人说：这世界上哪有那么多有趣的事情。人对现实世界有这种评价、这种感慨，恐怕不能说是错误的。问题就在于应该做点什么。这句感慨是个四通八达的路口，所有的人都到达过这个地方，然后在此分手。有些人去开创有趣的事业，有些人去开创无趣的事业。前者以为，既然有趣的事不多，我们才要做有趣的事。后者经过这一番感慨，就自以为知道了天命，此后板起脸来对别人进行说教。我以为自己是前一种人，我写作的起因就是：既然这世界上有趣的书是有限的，我何不去试着写几本——至于我写成了还是没写成，这是另一个问题，我很愿意就这后一个问题进行讨论，但很不愿有人就头一个问题来和我商榷。 对我自己来说，心胸是我在生活中想要达到的最低目标。某件事有悖于我的心胸，我就认为它不值得一做；某个人有悖于我的心胸，我就觉得他不值得一交；某种生活有悖于我的心胸，我就会以为它不值得一过。罗素先生曾言，对人来说，不加检点的生活，确实不值得一过。我同意他的意见：不加检点的生活，属于不能接受的生活之一种。人必须过他可以接受的生活，这恰恰是他改变一切的动力。 知识另有一种作用，它可以使你生活在过去、未来和现在，使你的生活变得更充实、更有趣。这其中另有一种境界，非无知的人可解。不管有没有直接的好处，都应该学习——持这种态度来求知更可取。大概是因为我曾独自一人度过了求知非法的长夜，所以才有这种想法……当然，我这些说明也未必能服人。反对我的人会说，就算你说的属实，但我就愿意只生活在现时现世！我就愿意得些能见得到的好处！有用的我学，没用的我不学，你能奈我何？……假如执意这样放纵自己，也就难以说服。罗素曾经说：对于人来说，不加检点的生活，确实不值得一过。他的本意恰恰是劝人不要放弃求知这一善行。抱着封闭的态度来生活，活着真的没什么意思。 有一种说法是这样的：人在年轻时，心气总是很高的，最后总要向现实投降。我刚刚过了44岁生日，在这个年龄上给自己做结论似乎还为时过早。但我总觉得，我这一生决不会向虚无投降。我会一直战斗到死。 《美丽新世界》— 奥尔德斯·赫胥黎 完全组织化的社会、科学式的等级体制、以系统培育泯灭自由意志、通过定期服用化学药物产生快感而接受奴役、利用夜间睡眠教育灌输正统理念——这些事情将会发生 《一九八四》的寓言所描写的社会是一个几乎完全通过惩罚和对惩罚的恐惧实施控制的社会。而在我所想的世界里，惩罚并不经常发生，而且大体上很温和。由政府实施的近乎完美的控制是通过多种多样的几乎非暴力的身心控制、基因标准化和对合乎要求的行为进行系统性的强化而实现的。 科学或许可以被定义为“将多样性归纳为统一性”。它的宗旨是通过忽略个别事件的独特性，专注于事件之间的共性 城市生活是匿名性的，也是抽象的。人们彼此之间的关系不是基于完整的人格，而是经济功能的体现，当他们不工作时，就只是不负责任的贪图享乐的人。过着这样的生活，个体会感受到孤单和无足轻重。他们的存在失去了任何意义。 在生物学的意义上，人有一定的群居性，但不是彻底的社会性动物——他更像是一头狼或一头大象，而不是一只蜜蜂或一只蚂蚁。原始形态的人类社会根本不像蚂蚁穴或白蚁窝，他们只是组成了族群而已。做一个简单的类比，文明是从原始的族群转变到社会性昆虫式的有机体的过程。 《美丽新世界》所描述的社会是一个世界统一的国家，战争已经被消除，统治者的首要目标是不惜一切代价让被统治者不制造麻烦。这一点他们通过合法化一定程度的性自由（通过废除家庭）以及其他手段，基本上保证生活在美丽新世界里的人不会有任何形式的毁灭性（或创造性）的情感压力。在《一九八四》里，权力欲通过施加痛苦而得到满足，而在《美丽新世界》里，则是通过给予快乐，但同样是对人的侮辱。 在西方，它们正构成威胁，将维护个体自由和民主制度的理性宣传淹没在无关紧要的琐事的海洋中。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://Bithub00.com/tags/读书/"}]},{"title":"word转pdf图片避免压缩","slug":"pdf","date":"2019-08-20T03:24:15.386Z","updated":"2019-08-20T03:48:48.633Z","comments":true,"path":"2019/08/20/pdf/","link":"","permalink":"http://Bithub00.com/2019/08/20/pdf/","excerpt":"平时生活中经常会需要将word转换成pdf，比如个人简历，然而直接使用word的另存为pdf的话，文字部分虽然清晰，但会造成图像的失真。","text":"平时生活中经常会需要将word转换成pdf，比如个人简历，然而直接使用word的另存为pdf的话，文字部分虽然清晰，但会造成图像的失真。如果是简历的话自己的照片就糊成一团了，这第一印象就不好了，即使是用网上所说的调整什么压缩选项也没用，该失真还是失真，今天记录一种可行的方法，需要用到Adobe pdf软件，相信很多人都有安装。 首先，在编辑好的word中点击文件-打印，打印机选择Adobe pdf： 接着，点击打印机属性，默认设置这里选择下拉菜单的第一项印刷质量，然后点击编辑： 左边列表中选择“图像”，然后将“彩色图像”这一区域的采样和压缩选项关闭： 这里的300可以自己设置，数值越大图像质量越好，当然转换的pdf文件也就越大 可以选择左下角的另存为保存设置，方便下次使用，或者直接点击确定，也会提示你进行保存： 保存完后，回到刚刚的窗口，在默认设置中选择刚刚保存的设置，点击确定，然后在打印页面选择打印就可以了，这时候输出的pdf图像就不会失真 虽然图像不会失真，但是pdf文件往往也会变大了很多，不便于邮件发送和查看，这时候推荐一个pdf压缩的网站：ilovepdf，对自己的pdf进行压缩就可以了","categories":[],"tags":[{"name":"pdf","slug":"pdf","permalink":"http://Bithub00.com/tags/pdf/"}]},{"title":"RUC与AOC","slug":"ROC与AUC","date":"2019-08-07T07:38:52.547Z","updated":"2019-08-07T12:00:22.361Z","comments":true,"path":"2019/08/07/ROC与AUC/","link":"","permalink":"http://Bithub00.com/2019/08/07/ROC与AUC/","excerpt":"用本文记录自己对ROC曲线与AUC值的学习和理解","text":"用本文记录自己对ROC曲线与AUC值的学习和理解 ROC曲线定义ROC曲线与AUC值常被用来评价一个二分类器的好坏，以下内容出自项亮《推荐系统实践》的“分类问题”一节： 对于二类分类问题常用的评价指标是精确率(precision)和召回率(recall)，通常以关注的类为正类，其他类为负类，分类器可将实例分成正类(positive)和负类(negative)，预测时会出现4种情况： TP (True Positice)——将正类预测为正类数 FN (False Negative)——将正类预测为负类数 FP (False Positive)——将负类预测为正类数 TN (True Negative)——将负类预测为负类数 精确率定义为 P =\\frac{TP}{TP + FP}召回率定义为 R =\\frac{TP}{TP + FN} 而ROC曲线与上面的定义有关，FPR(False Positive Rate)为x轴，它以TPR(True Positive Rate)为y轴，它们的定义分别为： \\begin{aligned} TPR & = \\frac{TP}{TP+FN} \\\\ FPR & =1-\\frac{TN}{TN+FP} \\\\ & = \\frac{FP}{TN+FP} \\end{aligned}可以看到，TPR与召回率的定义是一样的，直观理解是： 横坐标：伪正类率(False positive rate， FPR)，预测为正但实际为负的样本占所有负例样本的比例； 纵坐标：真正类率(True positive rate， TPR)，预测为正且实际为正的样本占所有正例样本的比例。 其实ROC的曲线的横坐标和纵坐标是没有相关性的，不能把ROC曲线当成函数曲线来分析，应该把它看成无数个点，每个点都代表一个分类器，其横纵坐标代表了这个分类器的性能。为了更好的理解ROC曲线，引入ROC空间的概念： A,B,C,C’为四个分类器，指标如下： 其中C’的性能最好，B的准确率为0.5，几乎是随即分类，图中左上角的点为完美分类，它代表所有的分类完全正确，分类为1的点完全正确，分类为0的点没有错误。在一个二分类模型中，分类器给出每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0)，阈值最小时，对应坐标点(1,1) 典型的ROC曲线： 理想情况下，TPR应该接近1，FPR应该接近0。ROC曲线上的每一个点对应于一个阈值，对于一个分类器，每个阈值下会有一个TPR和FPR。比如阈值最大时，TP=FP=0，对应于原点；阈值最小时，TN=FN=1，对应于右上角的点(1,1)。 绘制ROC曲线对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组FPR和TPR结果，而要得到一个曲线，我们实际上需要一系列FPR和TPR的值，如何得到呢？办法就是从阈值下手。分类器的一个重要功能是“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本）。假如我们已经得到了所有样本的概率输出（属于正样本的概率），现在的问题是如何改变阈值从而得到不同的PPR和TPR结果。根据每个测试样本属于正样本的概率值从大到小排序，下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。 接下来从高到低，依次将“Score”值作为阈值，当测试样本属于正样本的概率大于或等于这个阈值时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的阈值，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图： 当我们将阈值设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当阈值取值越多，ROC曲线越平滑。其实并不一定要得到每个测试样本是正样本的概率值，只要得到这个分类器对该测试样本的“评分值”即可（评分值并不一定在(0,1)区间）。评分越高，表示分类器越肯定地认为这个测试样本是正样本，而且同时使用各个评分值作为阈值。 AUC值定义AUC (Area Under Curve) 被定义为ROC曲线下的面积，这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。将AUC值看成一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。从AUC判断分类器（预测模型）优劣的标准： AUC = 1，完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 AUC &lt; 0.5，比随机猜测还差；对预测结果取反之后就优于随机猜测。 三种AUC值示例： 为什么用ROC曲线既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比： 在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。 参考 维基百科 孔明的博客 ROC曲线-阈值评价标准 简书 博客园 知乎问答","categories":[],"tags":[{"name":"评价指标","slug":"评价指标","permalink":"http://Bithub00.com/tags/评价指标/"}]},{"title":"推荐系统学习记录","slug":"推荐系统","date":"2019-08-05T11:40:06.163Z","updated":"2019-11-18T11:39:31.396Z","comments":true,"path":"2019/08/05/推荐系统/","link":"","permalink":"http://Bithub00.com/2019/08/05/推荐系统/","excerpt":"用本文记录学习推荐系统的过程，以及一些实用的资料与资源。","text":"用本文记录学习推荐系统的过程，以及一些实用的资料与资源。 推荐系统数据集 UCSD github 科研数据共享列表 基于隐式反馈数据的推荐系统【理论及python实践】 Yifan Hu,Yehuda Koren,Chris Volinsky.Collaborative Filtering for Implicit Feedback Datasets[J].IEEE International Conference on Data Mining，2008 前言隐式反馈(Implict)数据就是用户的行为数据，包括点击，浏览和停留等，它不像评分和点赞一样直观地表示了用户的喜好。实际情况中像评分这种显式数据往往很难获得，因为它输入用户额外的进行操作，而像点击这种隐式反馈的数据，是随着用户的行为自然产生的，不需要额外的获取成本，因此实际情况中隐式反馈的数据规模要远大于显式反馈，因此很有必要研究基于隐式反馈数据的推荐系统 隐式反馈数据的特征： 没有负样本。不同于评分，用户可以通过打低分来表达对某个物品的厌恶，我们只能通过点击猜测用户可能对某个物品有偏好，而不能通过没有点击来说明用户不喜欢，可能只是他还没接触过这个物品。处理显式数据时，缺失的评分项可以当作缺失值处理，而处理隐式数据时，为了避免只得到正向反馈，必须要对数据整体进行分析。 隐含很多的噪声数据。例如用户购买某个物品，不代表他一定喜欢这个物品，可能只是作为礼物或者其它原因，而给一个物品打高分可以很大程度上表示他偏好这个用品。 数值含义不同。在显式数据里，数值的含义代表偏好程度，如评分；而隐式数据里，数值代表置信度，往往表现为行为的频率，例如观看次数等等。频率越高，我们越能确定它与用户的偏好相关联，而不是一个偶然性情况。 评价指标不同。传统的推荐系统可以使用如均方根误差(Mean Square Error)来判断预测的好坏，而隐式数据上会有不同。 符号定义$r_{ui}$定义为用户$u$对物品$i$的一次观测值，例如购买物品的次数或浏览网页的次数，在电视节目推荐中，它代表完整观看某个节目的次数，0.7表示观看了节目的70%。区别于显式数据将空缺数据当作缺失值处理，我们将缺失值置为0，表示没有此观测记录。 隐语义模型隐语义模型通过用户特征向量$x_u\\in R^f$和物品特征向量$y_i\\in R^f$来进行预测，预测评分用上面两个向量的内积形式表示：$\\hat{r}_{ui}=x_u^Ty_i$，具体细节可见另一篇博客的矩阵分解引入部分。目标函数表示为： min_{x_*,y_*}\\sum_{r_{ui}\\ is\\ known}(r_{ui}-x_u^Ty_i)^2+\\lambda(\\mid\\mid x_u \\mid\\mid^2+\\mid\\mid y_i \\mid\\mid^2)公式中的$\\lambda$作为正则化参数，用于约束模型，参数的求解常使用随机梯度下降。然而这种方法在应用于隐式反馈数据时需要做调整 论文提出的模型首先引入一个二值变量$p_{ui}$来标识用户对物品是否产生过行为如购买、观看等等： p_{ui} = \\begin{cases} 1, &r_{ui} > 0\\\\ 0, &r_{ui} = 0 \\end{cases}注意，$p_{ui}=1$只是暗示了用户喜欢某个物品的可能，而且$r_{ui}$的数值越大应该表示喜欢的可能性越大，因此再引入变量表示这种可能性： c_{ui}=1+\\alpha r_{ui}引入$c_{ui}$后，对于每一个观测值，我们有一个最小的可能性1，随着观测值的增大，可能性也在逐渐增大，这里的可能性与概率的含义不同，论文中将常数$\\alpha$设为40我们的目标是为每一个用户和物品分别得到一个用户特征向量$x_u\\in R^f$和物品特征向量$y_i\\in R^f$，则预测值可以通过两个向量的内积表示：$p_{ui}=x^T_uy_i$，即预测用户是否会对某个物品产生行为，则目标函数可表示为： min_{x_*,y_*}L(X,Y)=min_{x_*,y_*}\\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\\lambda(\\sum_u\\mid\\mid x_u \\mid\\mid^2+\\sum_i\\mid\\mid y_i \\mid\\mid^2)同理，第二项为约束项，防止过拟合于训练数据。下一步就是根据目标函数进行求解，因为原始表达式计算复杂度太高，输入的数据集为$m\\ast n$的矩阵，m为用户数，n为物品数，规模很容易达到百万级，需要进行相应的优化。我们引入交替最小二乘法(Alternate Least Sqaures)， 目标函数需要优化用户和物品两个维度，可以先固定物品这个维度，对用户唯独进行优化，我的推导过程（论文只给了结果表达式）：对$x_u$求导，可得： \\begin{aligned} \\frac{\\partial L}{\\partial x_u} & = -2\\sum_i(p_{ui}-x^T_uy_i)y_ic_{ui}+2\\lambda x_u \\\\ & = -2\\sum_i(p_{ui}-y_i^Tx_u)y_ic_{ui} + 2\\lambda x_u \\\\ & = -2Y^TC_up(u)+2Y^TC_uYx_u+2\\lambda x_u \\end{aligned}其中$Y_{n\\ast f}$为含有所有物品特征向量的矩阵，f类似隐语义模型中的隐含特征，可见矩阵分解引入中的解释。令求导结果为0，则有： \\begin{aligned} Y^TC_uYx_u+\\lambda Ix_u & = Y^TC_uP_u \\\\ x_u & = (Y^TC_uY+\\lambda I)^{-1}Y^TC_up(u) \\end{aligned}观察表达式，计算复杂度的瓶颈在于计算$Y^TC^uY$这一部分，计算每一个用户的时间复杂度为$O(f^2n)$，我们使用线性代数的知识进行简单的变形：$Y^TC^uY=Y^TY+Y^T(C^u-I)Y$，其中$Y^TY$与用户u无关，可以在迭代开始时预先计算，而$Y^T(C^u-I)Y$中的$C^u-I$只有$n_u$个非零值，其中$n_u$是用户u$r_{ui}&gt;0$的个数，很明显$n_u\\ll n$。同样地，$C^up(u)$也只含有$n_u$个非零值，则此时对于单个用户$x_u$的计算复杂度为$O(f^2n_u+f^3)$，其中$O(f^3)$求逆的时间复杂度，总共有m个用户，所以总的时间复杂度为$O(f^2N+f^3m)$，N为总的非零值的个数，隐含特征f的个数通常设置为$20-200$。则$y_i$的表达式同理： y_i=(X^TC^iX+\\lambda I)^{-1}X^TC^ip(i)实践理解完论文框架后，使用一个数据集进行实践，自己构建一个基于隐式反馈数据的推荐系统。数据集来源于UCI大学的机器学习资源库，它包含了一个位于英国的网上零售商时间跨度为八个月的所以购买记录，数据集中包含了InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomID Country这些字段。 数据预处理 下载数据并读取12345678910111213141516retail_data = pd.read_excel('Online Retail.xlsx')print(retail_data.info())&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 541909 entries, 0 to 541908Data columns (total 8 columns):InvoiceNo 541909 non-null objectStockCode 541909 non-null objectDescription 540455 non-null objectQuantity 541909 non-null int64InvoiceDate 541909 non-null datetime64[ns]UnitPrice 541909 non-null float64CustomerID 406829 non-null float64Country 541909 non-null objectdtypes: datetime64[ns](1), float64(2), int64(1), object(4)memory usage: 33.1+ MB 可以看到，除了CustomerID这一字段外，其它字段几乎没有缺失值，但如果CustomerID未知，我们就无法知道购买记录中是谁购买了某个商品，因此需要将CustomerID未知的记录去除：12345678910111213141516cleaned_retail = retail_data.loc[pd.isnull(retail_data.CustomerID) == False]print(cleaned_retail.info())&lt;class 'pandas.core.frame.DataFrame'&gt;Int64Index: 406829 entries, 0 to 541908Data columns (total 8 columns):InvoiceNo 406829 non-null objectStockCode 406829 non-null objectDescription 406829 non-null objectQuantity 406829 non-null int64InvoiceDate 406829 non-null datetime64[ns]UnitPrice 406829 non-null float64CustomerID 406829 non-null float64Country 406829 non-null objectdtypes: datetime64[ns](1), float64(2), int64(1), object(4)memory usage: 27.9+ MB 这样所有的购买记录都可以对应到唯一的顾客了。接下来我们生成一个商品的描述表，方便查看某个商品编号对应的商品是什么：12345678910item_lookup = cleaned_retail[['StockCode', 'Description']].drop_duplicates()item_lookup['StockCode'] = item_lookup.StockCode.astype(str)print(item_lookup.info()) StockCode Description0 85123A WHITE HANGING HEART T-LIGHT HOLDER1 71053 WHITE METAL LANTERN2 84406B CREAM CUPID HEARTS COAT HANGER3 84029G KNITTED UNION FLAG HOT WATER BOTTLE4 84029E RED WOOLLY HOTTIE WHITE HEART. 接下来需要做的是 对于某个用户，将他购买的相同物品的数目进行求和 123cleaned_retail['CustomerID'] = cleaned_retail.CustomerID.astype(int)cleaned_retail = cleaned_retail[['StockCode', 'Quantity', 'CustomerID']]grouped_cleaned = cleaned_retail.groupby(['CustomerID', 'StockCode']).sum().reset_index() 将求和结果中的0置为1，数目为0的情况往往是顾客进行了退货，这种情况我们也当作顾客购买了商品，而不是当作没购买的情况处理 1grouped_cleaned.Quantity.loc[grouped_cleaned.Quantity == 0] = 1 去除求和结果中小于0的记录 1grouped_purchased = grouped_cleaned.query('Quantity &gt; 0') 建立ALS算法中输入的数据矩阵 123456789customers = list(np.sort(grouped_purchased.CustomerID.unique())) products = list(grouped_purchased.StockCode.unique())quantity = list(grouped_purchased.Quantity)rows = grouped_purchased.CustomerID.astype('category', categories=customers).cat.codescols = grouped_purchased.StockCode.astype('category', categories=products).cat.codespurchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))&lt;4338x3664 sparse matrix of type '&lt;class 'numpy.int64'&gt;' with 266723 stored elements in Compressed Sparse Row format&gt; 构建的输入矩阵维度是$4338\\ast 3664$，其中有266723个非空值。 划分数据集接下来需要把输入数据划分为训练集和测试集，传统的划分方式如图所示： 然而这种方式对于推荐系统是不适用的，因为矩阵分解时需要用上所有的用户-物品数据，更好的方法是随机隐藏输入矩阵中的某些观测值，将隐藏好的矩阵作为训练数据，将完整的矩阵作为测试矩阵，来判断推荐的物品用户是否会购买。 为了对比推荐的效果，我们可以和另一种推荐方法作对比，即只推荐最流行的物品。 划分数据集123456789101112131415import randomdef make_train(ratings, pct_test): test_set = ratings.copy() test_set[test_set != 0] = 1 training_set = ratings.copy() nonzero_inds = training_set.nonzero() nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) random.seed(0) num_samples = int(np.ceil(pct_test * len(nonzero_pairs))) samples = random.sample(nonzero_pairs, num_samples) user_inds = [index[0] for index in samples] item_inds = [index[1] for index in samples] training_set[user_inds, item_inds] = 0 training_set.eliminate_zeros() return training_set, test_set, list(set(user_inds)) ALS算法下一步就是实现论文所用的ALS算法，具体数学公式见上面的推导： ALS算法12345678910111213141516171819202122232425262728293031323334def implicit_weighted_ALS(training_set, lambda_val=0.1, alpha=40, iterations=10, rank_size=20, seed=0): conf = (alpha * training_set) num_user = conf.shape[0] num_item = conf.shape[1] rstate = np.random.RandomState(seed) X = sparse.csr_matrix(rstate.normal(size=(num_user, rank_size))) Y = sparse.csr_matrix(rstate.normal(size=(num_item, rank_size))) X_eye = sparse.eye(num_user) Y_eye = sparse.eye(num_item) lambda_eye = lambda_val * sparse.eye(rank_size) for iter_step in range(iterations): yTy = Y.T.dot(Y) xTx = X.T.dot(X) for u in range(num_user): conf_samp = conf[u, :].toarray() pref = conf_samp.copy() pref[pref != 0] = 1 CuI = sparse.diags(conf_samp, [0]) yTCuIY = Y.T.dot(CuI).dot(Y) yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T) X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu) for i in range(num_item): conf_samp = conf[:, i].T.toarray() pref = conf_samp.copy() pref[pref != 0] = 1 CiI = sparse.diags(conf_samp, [0]) xTCiIX = X.T.dot(CiI).dot(X) xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T) Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi) return X, Y.T 我们可以举一个例子来看看效果：123user_vecs, item_vecs = implicit_weighted_ALS(product_train, lambda_val = 0.1, alpha = 15, iterations = 1, rank_size = 20)print(user_vecs[0,:].dot(item_vecs).toarray()[0,:5])[ 0.00644811, -0.0014369 , 0.00494281, 0.00027502, 0.01275582 ] 对第一个用户来说，前五个物品中第五个物品的得分最高，因此会被用于推荐，这只是一次迭代的结果，迭代多次效果会更好，但原始ALS的算法计算过程太慢，我们需要对它加速，可以使用github上star数上千的python ALS加速版本，所用时间要少得多：1234import implictalpha = 15product_train, product_test, product_users_altered = make_train(purchases_sparse, pct_test=0.2)user_vecs, item_vecs = implicit.alternating_least_squares((product_train * alpha).astype('double'), factors=20, regularization=0.1, iterations=50) 可以直观地感觉到计算速度大大加快了，用兴趣的可以去github上看看源代码 效果评估数据集有了，推荐系统也搭好了，那下一步就是对我们的推荐系统进行评估，看看它的表现。在划分训练集和测试集时，有这么一步：1test_set[test_set != 0] = 1 于是推荐就变成了一个二分类问题，购买1或不购买0，这时就可以引入分类系统的评测指标：ROC(Receiver Operating Characteristic)曲线与AUC(Area Under the Curve)值。它的介绍可以看另一篇博客ROC与AUC。接下来是编写一个函数计算AUC值以进行比较： AUC1234from sklearn import metricsdef auc_score(predictions, test): fpr, tpr, thresholds = metrics.roc_curve(test, predictions) return metrics.auc(fpr, tpr) 基于上面这个函数，我们为训练集中每一个被隐藏了至少一条记录的用户计算AUC值，随后求平均值并与“推荐最流行物品”的策略进行比较：12345678910111213141516171819def calc_mean_auc(training_set, altered_users, predictions, test_set): store_auc = [] popularity_auc = [] pop_items = np.array(test_set.sum(axis=0)).reshape(-1) item_vecs = predictions[1] for user in altered_users: training_row = training_set[user, :].toarray().reshape(-1) zero_inds = np.where(training_row == 0) user_vec = predictions[0][user, :] pred = user_vec.dot(item_vecs).toarray()[0, zero_inds].reshape(-1) actual = test_set[user, :].toarray()[0, zero_inds].reshape(-1) pop = pop_items[zero_inds] store_auc.append(auc_score(pred, actual)) popularity_auc.append(auc_score(pop, actual)) return float('%.3f' % np.mean(store_auc)), float('%.3f' % np.mean(popularity_auc))print(calc_mean_auc(product_train, product_users_altered,[sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], product_test))(0.87, 0.814) 从结果可以看出，使用论文提出的推荐模型的效果是比单纯推荐最流行物品的效果要好的。 实例观察AUC值的大小比较还是比较抽象不够直观，下一步选取一个用户查看他购买过的物品，以及模型所推荐的物品来直观地感受推荐的效果。首先编写一个函数，根据customer_id来获得他所购买的商品的一个列表：123456789101112def get_items_purchased(customer_id, mf_train, customers_list, products_list, item_lookup): cust_ind = np.where(customers_list == customer_id)[0][0 purchased_ind = mf_train[cust_ind,:].nonzero()[1] prod_codes = products_list[purchased_ind] return item_lookup.loc[item_lookup.StockCode.isin(prod_codes)]customers_arr = np.array(customers)products_arr = np.array(products)print(get_items_purchased(12346, product_train, customers_arr, products_arr, item_lookup)) StockCode Description61619 23166 MEDIUM CERAMIC TOP STORAGE JAR 根据结果显示，这位顾客曾经购买了一个中等大小的陶瓷罐用于装东西，我们的推荐系统会推荐什么样的物品给他呢？123456789101112131415161718192021222324252627282930313233343536from sklearn.preprocessing import MinMaxScalerdef rec_items(customer_id, mf_train, user_vecs, item_vecs, customer_list, item_list, item_lookup, num_items=10): cust_ind = np.where(customer_list == customer_id)[0][0] pref_vec = mf_train[cust_ind, :].toarray() pref_vec = pref_vec.reshape(-1) + 1 pref_vec[pref_vec &gt; 1] = 0 rec_vector = user_vecs[cust_ind, :].dot(item_vecs.T) min_max = MinMaxScaler() rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1, 1))[:, 0] recommend_vector = pref_vec * rec_vector_scaled product_idx = np.argsort(recommend_vector)[::-1][:num_items] rec_list = [] for index in product_idx: code = item_list[index] rec_list.append([code, item_lookup.Description.loc[item_lookup.StockCode == code].iloc[0]]) codes = [item[0] for item in rec_list] descriptions = [item[1] for item in rec_list] final_frame = pd.DataFrame(&#123;'StockCode': codes, 'Description': descriptions&#125;) return final_frame[['StockCode', 'Description']]print(rec_items(12353, product_train, user_vecs, item_vecs, customers_arr,products_arr, item_lookup, num_items=10)) StockCode Description0 23167 SMALL CERAMIC TOP STORAGE JAR1 23165 LARGE CERAMIC TOP STORAGE JAR2 22963 JAM JAR WITH GREEN LID3 23294 SET OF 6 SNACK LOAF BAKING CASES4 22980 PANTRY SCRUBBING BRUSH5 23296 SET OF 6 TEA TIME BAKING CASES6 23293 SET OF 12 FAIRY CAKE BAKING CASES7 22978 PANTRY ROLLING PIN8 23295 SET OF 12 MINI LOAF BAKING CASES9 22962 JAM JAR WITH PINK LID 从推荐结果中选取了前10个得分最高的物品，它们看起来和这个顾客购买的商品都比较相关，可解释性好，要知道推荐系统是完全不知道陶瓷罐代表什么含义的，而可解释性有时会让顾客更加信服得到的推荐结果。可以选另外的customer_id来继续验证。 总结引申阅读： 显式反馈推荐系统： Alternating Least Squares Method for Collaborative Filtering Explicit Matrix Factorization: ALS, SGD, and All That Jazz 混合推荐系统(显式/隐式) LightFM","categories":[],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://Bithub00.com/tags/推荐系统/"},{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"}]},{"title":"宝贝陈列室","slug":"宿舍一角","date":"2019-07-25T03:30:57.778Z","updated":"2019-09-04T14:36:25.228Z","comments":true,"path":"2019/07/25/宿舍一角/","link":"","permalink":"http://Bithub00.com/2019/07/25/宿舍一角/","excerpt":"展示一下自己收来的各种雕像和CD~","text":"展示一下自己收来的各种雕像和CD~","categories":[],"tags":[{"name":"宿舍","slug":"宿舍","permalink":"http://Bithub00.com/tags/宿舍/"}]},{"title":"Youtube爬虫","slug":"Youtube爬虫","date":"2019-07-25T02:31:32.337Z","updated":"2019-08-21T08:41:11.440Z","comments":true,"path":"2019/07/25/Youtube爬虫/","link":"","permalink":"http://Bithub00.com/2019/07/25/Youtube爬虫/","excerpt":"爬取相关频道Related Channels","text":"爬取相关频道Related Channels 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import scrapyimport pandas as pdimport collectionsimport json# scrapy crawl related_channel_spider 命令行启动爬虫youtube_url = 'https://www.youtube.com'kol = collections.OrderedDict()class RelatedChannelsSpider(scrapy.Spider): name = 'related_channel_spider' def close(spider, reason): # 爬虫结束后将爬取结果写入json文件 file_kol = open('RelatedChannels_new.json', 'a') json.dump(kol, file_kol, sort_keys=True, indent=2) def start_requests(self): # 爬虫数据来源:Youtube 频道链接 file_name = 'kol_utm_campaign_ad.xlsx' Channels = pd.read_excel(file_name, sheet_name='ad_channel_new', header=0, usecols=['Channel']) Titles = pd.read_excel(file_name, sheet_name='ad_channel_new', header=0, usecols=['KolName']) length = len(Channels) start_urls = [] for i in range(0, length): url = &#123;&#125; url['url'] = Channels[i:i + 1].values.item() url['title'] = Titles[i:i + 1].values.item() start_urls.append(url) for url in start_urls: request = scrapy.Request(url['url'], callback=self.parse) request.meta['title'] = url['title'] yield request def parse(self, response): related = collections.OrderedDict() meta = response.meta channel_title = meta['title'] channel_url = response.url yield &#123; 'channel_title': channel_title, 'channel_url': channel_url &#125; # xpath解析网页 channel_item_lis = response.xpath( '//li[contains(@class, \"branded-page-related-channels-item\")]' ) for channel_item_li in channel_item_lis: related_channel_title = channel_item_li.xpath( 'span/div[contains(@class, \"yt-lockup-content\")]/h3/a/text()' ).extract()[0] relative_url = channel_item_li.xpath( 'span/div[contains(@class, \"yt-lockup-content\")]/h3/a/@href' ).extract()[0] related[related_channel_title] = youtube_url + relative_url print(related_channel_title, youtube_url + relative_url) if related: kol[channel_title] = related 爬取视频评论(包含评论内容、评论日期等)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import timeimport jsonimport requestsimport lxml.htmlimport pandas as pdfrom lxml.cssselect import CSSSelectorYOUTUBE_COMMENTS_URL = 'https://www.youtube.com/all_comments?v=&#123;youtube_id&#125;'YOUTUBE_COMMENTS_AJAX_URL = 'https://www.youtube.com/comment_ajax'youtube_video_url = 'https://www.youtube.com/watch?v='USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'def find_value(html, key, num_chars=2): pos_begin = html.find(key) + len(key) + num_chars pos_end = html.find('\"', pos_begin) return html[pos_begin: pos_end]def extract_comments(html): tree = lxml.html.fromstring(html) item_sel = CSSSelector('.comment-item') text_sel = CSSSelector('.comment-text-content') time_sel = CSSSelector('.time') author_sel = CSSSelector('.user-name') # vote_sel = CSSSelector('.like-count') 是否爬取评论点赞数 for item in item_sel(tree): yield &#123; 'cid': item.get('data-cid'), 'text': text_sel(item)[0].text_content(), 'time': time_sel(item)[0].text_content().strip(), 'author': author_sel(item)[0].text_content() # 'like-count': vote_sel(item)[0].text_content() &#125;def extract_reply_cids(html): tree = lxml.html.fromstring(html) sel = CSSSelector('.comment-replies-header &gt; .load-comments') return [i.get('data-cid') for i in sel(tree)]def ajax_request(session, url, params, data, retries=10, sleep=20): for _ in range(retries): response = session.post(url, params=params, data=data) if response.status_code == 200: response_dict = json.loads(response.text) return response_dict.get('page_token', None), response_dict['html_content'] else: time.sleep(sleep)def download_comments(youtube_id, sleep=1): session = requests.Session() session.headers['User-Agent'] = USER_AGENT # 获取初始页面的评论 response = session.get(YOUTUBE_COMMENTS_URL.format(youtube_id=youtube_id)) html = response.text reply_cids = extract_reply_cids(html) ret_cids = [] for comment in extract_comments(html): ret_cids.append(comment['cid']) yield comment page_token = find_value(html, 'data-token') session_token = find_value(html, 'XSRF_TOKEN', 4) first_iteration = True # 获取剩下的评论(等同于点击'show more') while page_token: data = &#123; 'video_id': youtube_id, 'session_token': session_token &#125; params = &#123; 'action_load_comments': 1, 'order_by_time': True, 'filter': youtube_id &#125; if first_iteration: params['order_menu'] = True else: data['page_token'] = page_token response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data) if not response: break page_token, html = response reply_cids += extract_reply_cids(html) for comment in extract_comments(html): if comment['cid'] not in ret_cids: ret_cids.append(comment['cid']) yield comment first_iteration = False time.sleep(sleep) # 获取评论回复 for cid in reply_cids: data = &#123;'comment_id': cid, 'video_id': youtube_id, 'can_reply': 1, 'session_token': session_token&#125; params = &#123;'action_load_replies': 1, 'order_by_time': False, 'filter': youtube_id, 'tab': 'inbox'&#125; response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data) if not response: break _, html = response for comment in extract_comments(html): if comment['cid'] not in ret_cids: ret_cids.append(comment['cid']) yield comment time.sleep(sleep)def main(): # 使用 for comment in download_comments(videoId): ··· 爬取某个视频主所有的视频12345678910111213141516171819202122232425262728293031# 使用Seleniumfrom selenium import webdriverfrom bs4 import BeautifulSoupChannel_videos = 'https://www.youtube.com/channel/UCGK0RMoHboOVUbdxDhLD1xw/videos'Video_Lists = []option = webdriver.ChromeOptions()option.add_argument('headless')youtube_url = 'https://www.youtube.com'browser = webdriver.Chrome(chrome_options=option, executable_path='D:\\Tool\\Software\\chromedriver_win32\\\\chromedriver.exe')# 去掉option选项可以让chrome在前台显示，看看模拟的效果browser = webdriver.Chrome(executable_path='D:\\Tool\\Software\\chromedriver_win32\\\\chromedriver.exe')browser.get(Channel_videos)time.sleep(5)old_height = browser.execute_script(\"return document.documentElement.scrollHeight;\")browser.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")time.sleep(5)new_height = browser.execute_script(\"return document.documentElement.scrollHeight;\")# 模拟浏览器向下滚动页面，直到所有视频都被加载出来while new_height != old_height: old_height = new_height browser.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\") time.sleep(5) new_height = browser.execute_script(\"return document.documentElement.scrollHeight;\") html = browser.page_source soup = BeautifulSoup(html, 'lxml') videoList = soup.findAll(\"a\", &#123;\"class\": \"yt-simple-endpoint style-scope ytd-grid-video-renderer\"&#125;) for video in videoList: ··· 获取视频的播放量和发布日期1234567891011121314151617181920212223242526272829303132333435363738394041424344# 因为Youtube API每日有访问限制，超过它的配额后就无法再使用接口获取某个视频的播放量和发布日期了，使用爬虫就没有这种限制# coding=utf-8import requestsfrom lxml import etreeimport osimport jsonyoutube_video_url = 'https://www.youtube.com/watch?v='def viewCount(): videoId = 'P4ItC6jWN0s' url = \"https://www.youtube.com/watch\" querystring = &#123;\"v\": videoId&#125; payload = \"\" headers = &#123; 'Content-Type': \"application/json\", 'User-Agent': \"PostmanRuntime/7.15.0\", 'Accept': \"*/*\", 'Cache-Control': \"no-cache\", 'Postman-Token': \"296c1155-2adc-4028-95c2-26cffec91784,f5c8088f-432f-4a67-a815-13464bfca373\", 'Host': \"www.youtube.com\", 'cookie': \"YSC=nwR5fai12Kg; VISITOR_INFO1_LIVE=gAl5VFO7Gjo; PREF=f1=50000000; GPS=1\", 'accept-encoding': \"gzip, deflate\", 'Connection': \"keep-alive\", 'cache-control': \"no-cache\" &#125; response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring) html = etree.HTML(response.text) datePublished = html.xpath('//meta[@itemprop=\"datePublished\"]/@content') if datePublished: datePublished = datePublished[0] view_count = html.xpath('//meta[@itemprop=\"interactionCount\"]/@content') if view_count: view_count = int(view_count[0]) print(datePublished, view_count) else: print('Not Exist:',videoId) file = open('video_statstics.json', 'a') json.dump(video_statstics, file, indent=2)if __name__ == '__main__': viewCount() 使用Selenium模拟搜索 避免Youtube API对/search接口的使用限制，根据官方文档，一个项目每日的配额是10000，而/search接口调用一次就至少是100 1234567891011121314151617181920from selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECCampaign = campaign_phase(GA.loc[i, 'Campaign'])option = webdriver.ChromeOptions()option.add_argument('headless')youtube_url = 'https://www.youtube.com'# 添加option后程序启动时就不会弹出chrome窗口，减少资源调用，提供效率，如果需要直观地观看程序的流程就用下面一行注释的不带option调用的命令browser = webdriver.Chrome(chrome_options=option,executable_path='XXX\\\\chromedriver.exe')# browser = webdriver.Chrome(executable_path='XXX\\\\chromedriver.exe') browser.get('https://www.youtube.com/')wait = WebDriverWait(browser, 10)input = wait.until( EC.presence_of_element_located ( (By.ID, 'search') #这里一定要加一个括号，详情见另外一篇博客 )) Selenium TypeError init() takes 2 positional arguments but 3 were given_解决方案123456789101112131415161718192021222324252627submit = wait.until( EC.element_to_be_clickable ( (By.ID, 'search-icon-legacy') ))input.send_keys(Campaign)submit.click()# 到这里就完成了模拟youtube输入关键词搜索的过程，下面是获取搜索结果中与关键词最匹配的频道time.sleep(5)html = browser.page_sourcesoup = BeautifulSoup(html, 'html.parser')searchResults = soup.findAll(\"a\", &#123; \"class\": \"yt-simple-endpoint style-scope ytd-channel-renderer\"&#125;) if searchResults: for result in searchResults: channel_url = youtube_url + result['href'] title = result.find(\"span\", &#123; \"class\": \"style-scope ytd-channel-renderer\" &#125;) channel_title = title.text","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://Bithub00.com/tags/爬虫/"}]},{"title":"python常用操作","slug":"python常用操作","date":"2019-07-25T02:13:08.319Z","updated":"2020-09-28T04:38:28.170Z","comments":true,"path":"2019/07/25/python常用操作/","link":"","permalink":"http://Bithub00.com/2019/07/25/python常用操作/","excerpt":"Python 读取excel文件某一列","text":"Python 读取excel文件某一列 12345678import pandas as pdnames = pd.read_excel( 'NOT_EXIST_List.xlsx', sheet_name='NOT EXIST', header=0, usecols=['KolName'])for i in range(0, len(names)): name = names[i:i + 1].values.item() List去重 12import pandas as pdList = pd.unique(List).tolist() 将接口返回值解析成json格式 1234567import requestresponse = requests.request( \"GET\", url, data=payload, headers=headers, params=querystring)json_response = json.loads(response.text) 移除字符串中的标点符号 123def removePunctuation(text): str = ''.join(c for c in text if c not in string.punctuation) return str 读取和写入json文件 1234file = open('XXX.json', 'r')XXX = json.loads(file.read())file = open('XXX.json', 'a')json.dump(XXX, file, indent=2) 将List写入excel文件 1234df = pd.DataFrame(List, columns=['name', 'url'])writer = pd.ExcelWriter('remain.xlsx')df.to_excel(writer, 'remain')writer.save() dict根据key排序 123456def sortdict(data): result = collections.OrderedDict() dict = sorted(data.items(), key=lambda d: d[0]) for i in range(0, len(dict)): result[dict[i][0]] = dict[i][1] return result 以追加方式写入excel 1234567891011rexcel = open_workbook(\"ad_match_new.xls\")rows = rexcel.sheets()[0].nrowsexcel = copy(rexcel)table = excel.get_sheet(0)row = rowstable.write(row, 0, title)table.write(row, 1, channel)table.write(row, 2, video_url)table.write(row, 3, url)row += 1excel.save(\"ad_match_new.xls\") 好处是在爬取数据或者使用接口时可以动态保存数据，不需要全部爬取完后再一次性存储，避免中途出错导致前功尽弃 PyTorch 得到一个张量中每一行的第一个非零元素的索引1234idx = torch.arrange(data.shape[1], 0, -1)tmp2 = data * idxindices = torch.argmax(tmp2, 1, keepdim=True)","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"}]},{"title":"imputing structured missing values in spatial data with clsutered adversarial matrix factorization","slug":"imputing structured missing values in spatial data with clsutered adversarial matrix factorization","date":"2019-04-12T15:55:11.077Z","updated":"2019-08-07T07:59:01.419Z","comments":true,"path":"2019/04/12/imputing structured missing values in spatial data with clsutered adversarial matrix factorization/","link":"","permalink":"http://Bithub00.com/2019/04/12/imputing structured missing values in spatial data with clsutered adversarial matrix factorization/","excerpt":"一种基于对抗模型用于补全带有结构性缺失信息的空间数据的矩阵分解技术","text":"一种基于对抗模型用于补全带有结构性缺失信息的空间数据的矩阵分解技术 &#8194; 摘要：在数据分析时，缺失的数据总是会成为一个重大的挑战，因为它带来了不确定性。在许多领域中，矩阵补全技术有着出色的表现。然而，在特定的空间数据集如地理坐标点时，这种传统的矩阵补全技术有着两个主要的限制：第一，这些方法往往假设缺失的数据是随机产生的，而这种假设对于空间数据集可能并不总是成立；第二，它们可能无法运用这些空间数据集中的结构信息。为了解决这些局限性，本论文提出了一种利用先验结构信息和生成对抗模型的矩阵分解技术。这个模型使用一个对抗网络通过学习数据集的概率分布来改善补全的结果。 关键词：缺失数据估计；深度对抗网络；空间数据 前言&#8194; 很多现实生活中的应用容易面临数据缺失的问题。而对于空间数据集，造成这种情况的原因有很多种。例如，在森林监测中，因为收集成本的原因，数据的缺失很普遍。【3】过去十年许多针对数据补全的工作在开展，从基本的统计方法到复杂的模型使用。后者的典例低秩矩阵补全技术为许多领域如推荐系统或图像重构带来了可观的改善【9】。这些方法通过发现并利用数据矩阵的低秩属性来建立已有值与缺失值的联系。而在这些矩阵补全方法里，矩阵分解是最为常用的方法之一，它将输入的数据矩阵分解成两个低秩矩阵的乘积，称为“特征因子”，接着通过最小化这个乘积与已有值的误差来学习这两个特征因子，随后利用它们来补全缺失的信息。【18】其它方法还有如带门槛的矩阵奇异值分解，核心思想是迭代地使用截断奇异值分解来补全。 &#8194; 这些矩阵补全的方法，往往假设缺失的数据是随机产生的。【2】然而，这个假设在空间数据上可能并不成立，因为它往往带有空间结构。例如，一项针对加拿大青少年的研究指出，家庭收入这一栏数据空缺的青少年有更低的可能性居住在富人区。【15】因此，当数据并不是随机缺失时，只是单纯地最小化两个特征因子的乘积与已有值的误差并不能保证补全数据的有效性。 &#8194; 而另一个限制则是这些方法无法把数据集里的结构信息利用起来。而在补全缺失的空间数据时，这些结构信息格外重要。【12】例如，淡水湖数据就有强烈的空间结构，因为相邻的湖泊往往有相似的降水量等。【17】如果这些结构信息能够被一个矩阵补全的方法利用起来，它可以显著地提升结果，因为这些结构信息代表了一个子空间，在这个子空间里，不同湖泊之间相似的信息互相传递。 &#8194; 因为为了解决这两点局限，我们提出了一种利用先验结构信息和生成对抗模型的矩阵分解技术。这个框架找到一个低维的子空间来与数据中的结构信息相符合，因此可以利用同一类中其它数据点的信息来补全某一点的缺失值。而且，估计值的概率分布也尽可能的与已有值相似，这么做的好处是它把缺失值与已有值连接起来了。如果估计值与实际值偏差太大，那么它出现的概率应该很小。然而，实际数据的概率分布往往是未知的，因此我们借鉴了生成对抗网络的思想引入了一个判别器来区分估计值与已有值。我们在合成数据集与显示数据集上均做了实验，来说明这个框架的有效性。 相关工作&#8194; 截断奇异值算法是近年来使用频率较高的一个方法，它在数据矩阵中迭代的使用截断奇异值分解接着通过保持一个较小的奇异值重构整个矩阵。矩阵分解是另一项常用的技术，关于它的过程前面部分已有讲述。 &#8194; 生成对抗网络（GAN）被广泛地用于生成图像【5】【16】。在【8】【14】中，作者提出了一个想法，利用GAN的思想和整幅图片的结构来推测一幅图片中随机缺失的像素。虽然这个想法在这类问题上效果较好，但它补全图片时是将每张图片看成一个个独立的个体，而空间数据与此相反，它们之间有着强烈的依赖性。 方法A.矩阵分解引入&#8194; 矩阵分解在推荐系统中十分常用，例如如下的一个评分矩阵，列为用户，行为物品，矩阵中的值为用户对物品的评分，如电影和书籍。现实情况中这个评分矩阵往往很稀疏，许多物品上缺少用户的评分，而推荐系统就是要预估用户在某个物品上的评分来判断用户对它的倾向程度，从而进行推荐。 &#8194; 矩阵分解的方法是将原始评分矩阵$R^{m\\times n}$分解成两个矩阵$P^{m\\times k}$和$Q^{k\\times n}$，根据评分矩阵中已有的值来判断分解是否准确，而判别标准常用均方差。如图所示。 &#8194; 分解后的矩阵P和Q可以称为特征因子（latent factor），其中要求分解后$k&lt;&lt;min(m,n)$，即低秩要求，因为如果输入矩阵满秩，则各元素行之间线性无关，如果有线性相关关系，则某个元素行可以通过其他行的线性组合表示，相当于引入了冗余的信息，这样就可以将矩阵投影到更低维的空间，只保留非冗余信息，同时冗余信息可以用来对缺失值进行补全。 &#8194; 矩阵分解的直观意义为，找出矩阵中的潜在特征，如图2中假设特征为3，特征可以是书籍作者、类型等等，而矩阵P表示用户对某个特征的喜爱程度，而矩阵Q表示某个物品与该特征的关联程度。 B.低秩补全&#8194; 给定一个带有缺失值的矩阵，矩阵补全技术旨在通过已有值的某种潜在的结构来对缺失值进行估计补全。一个常用的潜在结构是矩阵的低秩性，因为它可以将该矩阵投影到一个去除冗余信息的子空间中，低秩意味着矩阵中的值存在线性关系，因此某些值可以通过另外的值来线性表示，如同坐标系中的基底一样。在这类方法中有凸也有非凸的技术。凸方法通过对矩阵迹的约束来保证具有良好理论性的全局最优结果，而诸如矩阵分解的非凸方法进行局部搜索过程并提供更大的灵活性和效率。给定一个矩阵$X\\in R^{d\\times n}$，n代表样本个数，d代表特征维度，矩阵分解技术通过将X分解为两个矩阵U和V，$U\\in R^{d\\times n}$，$V\\in R^{r\\times n}$要求$r &lt; min(d, n)$；U和V的求解可以通过对下列式子运用块坐标下降法求得： &#8194; $\\bigodot$代表哈德蒙德内积（即矩阵各元素相乘），M矩阵的大小同X一致，如果$X_{ij}$有值则$M_{ij}$为1，否则为0。局部解用$U^、V^$表示，因此，它们可以通过如下的式子来重构矩阵X： &#8194; 矩阵分解在推荐系统中使用较为普遍，它用来估计一个用户在某项新物品上的评分。 &#8194; 将此方法应用于空间数据集时，矩阵分解不会包含有关数据集中空间聚类结构的先验知识信息。 然而，这些先验知识通常有助于发现需要的子空间。此外，对于结构化缺失值问题，经典矩阵补全提供较差的结果，因为缺失值不是随机的。 C.聚类对抗式矩阵分解&#8194; 为了解决上一小节中提到的矩阵分解的两个局限性，我们提出了一种新的聚类对抗矩阵分解框架。 在我们的框架中，我们找到整个样本的聚类信息，并将补全值的概率分布与已有值的分布靠近，以得到可靠的补全结果。X为输入矩阵，每一列代表一个数据样本，有些数据点有完整的特征信息，而某些数据点以结构性缺失了某些特征信息。我们将输入矩阵中完整部分记为$X_n$，而缺失部分记为$X_m$。我们假设每个数据点都符合某个概率分布$p_{data(x)}$。接下来的公式中包含两个部分：矩阵重构以及概率分布近似。 矩阵重构&#8194; 为了利用数据的低秩属性和空间聚类结构，我们决定在矩阵分解中使用l2聚类项： &#8194; 式中$v_i$代表矩阵V中第i列，$r_1,r_2,r_3$均为正则化参数，第二项和第三项加的约束是为了防止过拟合，最后一项则用来引入空间数据集中的聚类结构信息。$d_{ij}$是第i个样本和第j个样本的相似度，它可以手动设置，原则为：当$v_i$和$v_j$很靠近即在同一类时，将$d_{ij}$的值设置得较大，反之较小。$r_3$用来调整结构信息在重构时所占的比重，如果$r_3$较大，则对一个样本的缺失数据进行估计补全时会更多的参考同一类其它数据点的信息。因此当$r_3$为0时，结构信息将不被使用，这也使得这个式子变成了常规的矩阵分解方法。而对于UV矩阵直观的理解为，U为特征因子，而V为样本因子，如同推荐系统里的用户因子和物品因子，两者相互独立。 生成对抗网络思想（GAN）&#8194; 在继续讲到使用概率分布近似来优化前，先引入生成对抗网络的基本思想加深理解。GAN的非常的直观，就是生成器和判别器两个极大极小的博弈。 GAN的目标函数为： &#8194; 从判别器D的角度看，它希望自己能尽可能区分真实样本和虚假样本，因此希望 D(x)尽可能大，D(G(z))尽可能小，即 V(D,G)尽可能大。从生成器G的角度看，它希望自己尽可能骗过D，也就是希望 D(G(z))尽可能大，即 V(D,G)V(D,G) 尽可能小。两个模型相对抗，最后达到全局最优。 &#8194; 图中，黑色曲线是真实样本的概率分布函数，绿色曲线是虚假样本的概率分布函数，蓝色曲线是判别器D的输出，它的值越大表示这个样本越有可能是真实样本。最下方的平行线是噪声z，它映射到了x。 &#8194; 一开始， 虽然 G(z)和x是在同一个特征空间里的，但它们分布的差异很大，这时，虽然鉴别真实样本和虚假样本的模型 D性能也不强，但它很容易就能把两者区分开来，而随着训练的推进，虚假样本的分布逐渐与真实样本重合，D虽然也在不断更新，但也已经力不从心了。 &#8194; 最后，黑线和绿线最后几乎重合，模型达到了最优状态，这时 判别器的输出对于任意样本都是 0.5。 GAN的最优化&#8194; 在建立好理论框架后，需要对所需要的生成器G和判别器D进行优化，在此之前先引入交叉熵的概念：它一般用来求目标与预测值之间的差距。 &#8194; 在信息论与编码中，熵可以用来衡量信息量的多少，而如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异，计算式如下： &#8194; 在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]，直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但并不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，此时Q就等价于P。 而对KL散度的计算式进行变形，可以得到： 等式的前一项即为P的熵，而后一项就是交叉熵的计算式： &#8194; 在机器学习中，我们需要评估labels和predictions之间的差距，可以使用KL散度，即$D_{KL}(y\\mid\\mid\\hat{y})$，由于KL散度中的前一部分−H(y)即P的熵不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做损失函数，评估模型。 &#8194; 在引入交叉熵后，就可以定义最优化表达式。首先我们需要定义一个判别器 D以判别样本是不是从$p_{data(x)}$分布中取出来的，因此有: &#8194; 其中E代表取期望。这一项是根据「正类」（即辨别出 x 属于真实数据data）的对数损失函数而构建的。最大化这一项相当于令判别器 D在 x 服从于 data 的概率密度时能准确地预测 D(x)=1，即： 另外一项是企图欺骗判别器的生成器 G。该项根据「负类」的对数损失函数而构建，即 因此目标函数为： 它的含义是，对于D而言要尽量使公式最大化（识别能力强），而对于G又想使它最小（生成的数据接近实际数据）。整个训练是一个迭代过程。极小极大化博弈可以分开理解，即在给定G的情况下先最大化$V(D,G)$而来得到D，然后固定D，并最小化$V(D,G)$而得到G。其中，给定 G，最大化$V(D,G)$评估了$P_g$和$P_{data}$之间的差异或距离。 概率分布近似&#8194; 接下来，论文中就使用生成对抗网络中的对抗策略来使得推算样本具有与完整数据类似的概率分布。为了实现这一目标，我们使用鉴别器来区分推算和完整样本之间的分布差异： &#8194; 其中$p_r(x_r)$代表估计值的概率分布，它将从补全的矩阵Xr中得到；$x_r$代表从$p_r(x_r)$中选取的一个数据点；D为一个鉴别器，我们通过一个以SOFTMAX为输出层的全连接的深度神经网络来实现。D将输出一个概率值，判断输入的数据为已有值还是估计值。我们使用了负交叉熵作为损失函数，通过最大化$l_d$得到一个鉴别器D，能够有效地区分已有值与估计值 完整公式将前节提到的两个部分进行合并，我们得到了如下的公式： &#8194; 其中λ是用来平衡矩阵重构与概率分布近似所占比例的一个参数，因此最小化该式时，不仅使得重构的矩阵与已有值所构成的矩阵的误差尽可能得小，同时通过鉴别器使得这两者的概率分布尽可能相似。这个同时最大最小化的要求就像是进行一场对抗。一方面，鉴别器尽可能地区别重构样本与已有样本的概率分布，而另一方面，重构矩阵又尽可能地逼近已有值的概率分布，以骗过鉴别器。因此当算法收敛时，重构矩阵的概率分布将会近似于已有值，训练出一个有效的鉴别器，同时重构矩阵的值也足够接近实际值以至于可以骗过这个鉴别器。在最小化部分中，我们求解出使得误差最小的矩阵U和V，接着使用它们来进行缺失值的计算。同时，这个部分也尽可能地让估计值去骗过鉴别器。而在最大化部分中，鉴别器通过区分已有值与最小化部分所得的估计值来进行更新，整个框架的流程如图所示。 最优化然而在实际情况中，输入样本的概率分布往往是未知的，因此，我们使用如下式子来进行近似：在每次的更新迭代中，我们随机从Xn与Xr选取k个样本，来计算概率分布： &#8194; 其中r1与rk分别代表从Xn中选取的k个样本中的第一个和最后一个，q1和qk从Xr中选取的k个样本中的第一个和最后一个，$X^i_n$和$X^i_r$分别代表从Xn与Xr中所选取的第i个样本。因此，上面的合成式将变成： 算法流程如下所示： 实验","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://Bithub00.com/tags/机器学习/"},{"name":"GAN","slug":"GAN","permalink":"http://Bithub00.com/tags/GAN/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"http://Bithub00.com/tags/矩阵分解/"}]},{"title":"数据库设计","slug":"database","date":"2019-04-10T08:50:19.836Z","updated":"2019-04-10T08:52:10.289Z","comments":true,"path":"2019/04/10/database/","link":"","permalink":"http://Bithub00.com/2019/04/10/database/","excerpt":"数据库设计 以下所有表默认自带一个自增 id 以下所有表默认自带 created_at 和 updated_at 两个字段 为了方便查询，以下所有下划线命名法在实际设计中可能全部转为驼峰命名法","text":"数据库设计 以下所有表默认自带一个自增 id 以下所有表默认自带 created_at 和 updated_at 两个字段 为了方便查询，以下所有下划线命名法在实际设计中可能全部转为驼峰命名法 用户信息manager 字段 类型 描述 约束 name varchar 姓名 not null card_id varchar 校园卡号 not null authorizerId int 授权人 id 外键，引用自 manager 表的 id 属性 privilege tinyint 0 为超级管理员，1为普通管理员，其余待定 not null student 字段 类型 描述 约束 card_id varchar 校园卡号 无 stu_id varchar 学号 primary key name varchar 姓名 not null college varchar 学院 无 teacher 字段 类型 描述 约束 name varchar 讲师名称 not null college varchar 所属单位 not null intro text 老师简介 无 phone varchar 手机号码 无 office varchar 办公地址 无 email varchar 邮箱 无 image_url varchar 导师照片 url 无 teach_form varchar 授课形式 无 teach_topic 字段 类型 描述 约束 teacher_id int 讲师 id 外键，引用自 teacher 表的 id 属性 topic varchar 授课专题 not null 课程信息course 字段 类型 描述 约束 course_name varchar 课程名称 not null teacher_id int 讲师 id 外键，引用自 teacher 表的 id 属性 start_time datetime 上课开始时间 not null end_time datetime 上课结束时间 not null course_id varchar 课程编号 primary key location varchar 上课地址 not null course_student 字段 类型 描述 约束 course_id varchar 课程编号 外键，引用自 course 表的 course_id 属性 stu_id varchar 学生学号 外键，引用自 student 表的 stu_id 属性 评价模板comment_template 字段 类型 描述 约束 template_name varchar 模板名称 not null template_question 字段 类型 描述 约束 template_id int 模板 id 外键，引用自 comment_template 表的 id 属性 question text 问题 not null _type tinyint 问题类型，0为打分题，1为问答题，其余待定 not null 评价信息course_comment 字段 类型 描述 约束 course_id varchar 课程编号 外键，引用自 course 表的 course_id 属性 template_id int 模板 id 外键，引用自 comment_template 表的 id 属性 comment 字段 类型 描述 约束 course_id varchar 课程编号 外键，引用自 course 表的 course_id 属性 stu_id varchar 学生学号 外键，引用自 student 表的 stu_id 属性 star tinyint 1为精选评论 无 comment_result 字段 类型 描述 约束 comment_id int 评价 id 外键，引用自 comment 表的 id 属性 question_id int 问题 id 外键，引用自 template_question 表的 id 属性 result text 问题结果，根据问题类型来决定存储值类型 not null","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://Bithub00.com/tags/数据库/"}]},{"title":"API接口设计","slug":"api","date":"2019-04-10T08:50:19.820Z","updated":"2019-04-10T08:52:14.750Z","comments":true,"path":"2019/04/10/api/","link":"","permalink":"http://Bithub00.com/2019/04/10/api/","excerpt":"API 接口设计接口地址 环境 URL 开发环境 http://like.suevily.cn/ 生产环境 待定 通用 以下接口分为开放接口和私有接口 开放接口：无需登录校验即可请求 私有接口：需校验登录 token 方可请求 以下所有接口 url 都默认自带 /api 前缀","text":"API 接口设计接口地址 环境 URL 开发环境 http://like.suevily.cn/ 生产环境 待定 通用 以下接口分为开放接口和私有接口 开放接口：无需登录校验即可请求 私有接口：需校验登录 token 方可请求 以下所有接口 url 都默认自带 /api 前缀 token 在使用统一身份验证成功登录后会附带到 redirect url 的 query 参数上，前端需自行存储，开发环境的 redirect url 为 http://localhost:8081/#/ 私有接口请求方法：将 token 附在请求 Headers 中的 Authorization 字段上，value 格式为 Bearer ${token} （PS: 建议使用 postman 进行接口测试） 前端在拿到返回数据时务必先检查 code 是否为零，如若不为零，需给用户正确的反馈，回传数据遵循以下格式： 1234&#123; code: Number, data: Object&#125; code 说明 0 成功 -1 未知错误，查看 message 1 拒绝访问 2 无效的请求参数 3 上传图片出错 4 token 校验失败 其它 待定 获取用户信息 method url type GET /user 私有接口 请求参数无 返回参数 参数 类型 说明 cardId String 校园卡号 name String 姓名 college String 学院 stuId String 学号 privilege Number 0 为超级管理员，1为普通管理员，如没有该字段则为普通师生 获取管理人员 method url type POST /getManagers 私有接口 请求参数 参数 类型 说明 必填 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 管理人员总数 managers - Array 一个包含指定页数内所有管理人员的数组 - id Number 该管理员在数据库中的唯一标识 - name String 姓名 - cardId String 校园卡号 - authorizer String 授权人姓名 - createdAt Date 添加时间戳 - privilege String 管理权限，超级管理员或普通管理员 添加管理人员 method url type POST /addManager 私有接口 请求参数 参数 类型 说明 必填 name String 姓名 是 cardId String 校园卡号 是 privilege Number 权限，0 为超级管理员，1为普通管理员 是 返回参数 参数 类型 说明 message String 添加结果 错误代码 code 说明 1001 校园卡号冲突 修改管理人员 method url type POST /updateManager 私有接口 请求参数 参数 类型 说明 必填 id Number 要修改的管理员的id 是 name String 姓名 是 cardId String 校园卡号 是 privilege Number 权限，0 为超级管理员，1为普通管理员 是 返回参数 参数 类型 说明 message String 修改结果 错误代码 code 说明 1002 当你要把最后一个超级管理员更改为普通管理员时会出错 删除管理人员 method url type POST /deleteManager 私有接口 请求参数 参数 类型 说明 必填 id String 要删除的管理员的id 是 返回参数 参数 类型 说明 message String 删除结果 错误代码 code 说明 1003 当你要把最后一个超级管理员删除时会出错 搜索管理人员 method url type POST /searchManagers 私有接口 请求参数 参数 类型 说明 必填 keyword String 搜索关键词 是 maxLength Number 搜索结果最大返回数目（默认为5） 否 返回参数 参数 字段 类型 说明 managers - Array 满足搜索关键词的管理员数组 - id Number 该管理员在数据库中的唯一标识 - name String 姓名 - cardId String 校园卡号 - authorizer String 授权人姓名 - createdAt Date 添加时间戳 - privilege String 管理权限，超级管理员或普通管理员 获取课程信息 method url type POST /getCourses 私有接口 请求参数 参数 类型 说明 必填 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 课程总数 courses - Array 一个包含指定页数所有课程信息的数组 - courseName String 课程名称 - teacher String 讲师姓名 time String 上课时间，前端直接展示即可 courseId String 课程编号 location String 上课地点 studentCount Number 选课人数 根据课程编号获取课程详细信息 method url type POST /getCourseInfo 私有接口 请求参数 参数 类型 说明 必填 courseId String 课程编号 是 返回参数 参数 字段 类型 说明 courseName - String 课程名称 teacherId - String 讲师在数据库中的id teacherName - String 讲师姓名 startTime - Date 上课开始时间戳 endTime - Date 上课结束时间戳 location - String 上课地点 students - Array 一个包含指定课程选课所有学生信息的数组 name String 学生姓名 stuId String 学生学号 错误代码 code 说明 2003 课程编号错误 查询选课名单 method url type POST /getStudentList 私有接口 请求参数 参数 类型 说明 必填 courseId String 课程编号 是 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 选课总人数 students - Array 一个包含指定课程选课所有学生信息的数组 name String 学生姓名 stuId String 学生学号 删除课程 method url type POST /deleteCourse 私有接口 请求参数 参数 类型 说明 必填 courseId String 要删除的课程的课程编号 是 返回参数 参数 类型 说明 message String 删除结果 搜索课程 method url type POST /searchCourses 私有接口 请求参数 参数 类型 说明 必填 keyword String 搜索关键词 是 maxLength Number 搜索结果最大返回数目（默认为5） 否 返回参数 参数 字段 类型 说明 courses - Array 满足搜索关键词的课程数组 - courseName String 课程名称 - teacher String 讲师姓名 time String 上课时间，前端直接展示即可 courseId String 课程编号 location String 上课地点 studentCount Number 选课人数 生成课程 method url type POST /addCourse 私有接口 请求参数 参数 字段 类型 说明 必填 courseName - String 课程名称 是 teacherId - String 讲师在数据库中的id 是 startTime - Date 上课开始时间戳 是 endTime - Date 上课结束时间戳 是 location - String 上课地点 是 students - Array 选课学生数组 是 - name String 学生姓名 是 - stuId String 学生学号 是 返回参数 参数 类型 说明 courseId String 生成的课程编号 错误代码 code 说明 2001 讲师不存在 修改课程 method url type POST /updateCourse 私有接口 请求参数 参数 字段 类型 说明 必填 courseId - String 课程编号（乱传打爆你▄︻┻═┳一） 是 courseName - String 课程名称 是 teacherId - String 讲师在数据库中的id 是 startTime - String 上课开始时间戳 是 endTime - String 上课结束时间戳 是 location - String 上课地点 是 students - Array 选课学生数组 是 - name String 学生姓名 是 - stuId String 学生学号 是 返回参数 参数 类型 说明 message String 修改结果 错误代码 code 说明 2001 讲师不存在 2002 课程编号不存在 获取讲师信息 method url type POST /getTeachers 私有接口 请求参数 参数 类型 说明 必填 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 讲师总数 teachers - Array 一个包含指定页数所有讲师信息的数组 - id Number 该讲师在数据库的唯一标识 - name String 讲师姓名 imageUrl String 讲师照片的 url 删除讲师 method url type POST /deleteTeacher 私有接口 请求参数 参数 类型 说明 必填 id Number 讲师 id ，在 /getTeachers 接口获取到的 是 返回参数 参数 类型 说明 message String 删除结果 搜索讲师 method url type POST /searchTeachers 私有接口 请求参数 参数 类型 说明 必填 keyword String 搜索关键词 是 maxLength Number 搜索结果最大返回数目（默认为5） 否 返回参数 参数 字段 类型 说明 teachers - Array 满足搜索关键字的讲师数组 id Number 该讲师在数据库的唯一标识 - name String 讲师姓名 imageUrl String 讲师照片的 url 查询一个讲师的详细信息 method url type POST /getTeacherInfo 私有接口 请求参数 参数 类型 说明 必填 id Number 讲师 id ，在 /getTeachers 接口获取到的 是 返回参数 参数 字段 类型 说明 name - String 讲师姓名 college - String 所属单位 intro - String 老师简介 phone - String 手机号码 office - String 办公地址 email - String 邮箱地址 imageUrl - String 导师照片 url teachForm - String 授课形式 teachTopic - Array 该讲师授课专题的数组 - String 授课专题 图片上传接口 method url type POST /upload 私有接口 请求参数 参数 类型 说明 必填 - FormData 图片数据 是 返回参数 参数 类型 说明 url String 图片 url 添加讲师 method url type POST /addTeacher 私有接口 请求参数 参数 字段 类型 说明 必填 name - String 讲师姓名 是 college - String 所属单位 是 intro - String 老师简介 是 phone - String 手机号码 是 office - String 办公地址 是 email - String 邮箱地址 是 imageUrl - String 导师照片 url 是 teachForm - String 授课形式 是 teachTopic - Array 该讲师授课专题的数组 是 - String 授课专题 是 返回参数 参数 类型 说明 id Number 该讲师在数据库的唯一标识 修改讲师信息 method url type POST /updateTeacher 私有接口 请求参数 参数 字段 类型 说明 必填 id - Number 要修改的讲师的 id 是 name - String 讲师姓名 是 college - String 所属单位 是 intro - String 老师简介 是 phone - String 手机号码 是 office - String 办公地址 是 email - String 邮箱地址 是 imageUrl - String 导师照片 url 是 teachForm - String 授课形式 是 teachTopic - Array 该讲师授课专题的数组 是 - String 授课专题 是 返回参数 参数 类型 说明 message String 修改结果 错误代码 code 说明 3001 指定 id 的老师不存在","categories":[],"tags":[{"name":"API","slug":"API","permalink":"http://Bithub00.com/tags/API/"},{"name":"Node.js","slug":"Node-js","permalink":"http://Bithub00.com/tags/Node-js/"}]},{"title":"AWS:S3 + Athena + Glue","slug":"AWS","date":"2019-04-03T09:52:53.423Z","updated":"2019-08-07T07:58:35.080Z","comments":true,"path":"2019/04/03/AWS/","link":"","permalink":"http://Bithub00.com/2019/04/03/AWS/","excerpt":"整理一下自己了解的S3、Athena和Glue","text":"整理一下自己了解的S3、Athena和Glue S3AWS使用S3（Simple Storage Service）进行存储，它可以存储海量的数据，存储的往往是不常使用的冷数据，而且采取特定的Parquet格式进行列式存储或分区，可以节省存储空间而且提升查询性能。 使用S3存储动态数据会使得系统依赖于S3本身的可用性，可以通过增加缓存层或CDN(Content Deliver Network)服务来减轻影响。 S3提供REST接口,两个组成层次是容器(bucket)和对象(object)。容器是S3最上面的分类，所有的对象都放在容器里，因此容器名称必须是唯一的，每个对象都有唯一可识别的URL，应该尽量避免对容器进行操作，使用对象名称就可以实现分层次和分类了. 存储在S3的对象，除了本身的值以外,还会记录标头、元数据、访问控制列表等等，标头中存储了对象类型等信息，元数据就是用户自己定义的表头，为键值对，访问控制列表就是访问权限。因为S3有所谓偷窥对象的功能，即只读取对象的标头的信息，我们可以先看标头信息，再决定要不要把对象读取下来。 S3没有目录的概念，是扁平化的存储结构，“photo/1.jpg”和“photo/2.jpg”可能存在于不同的服务器集群。 AthenaAthena是一个查询服务，可以使用标准的SQL来对S3上存储的数据进行查询。而且它是一个serverless的服务，不需要去考虑底层的硬件设施，只需要为查询服务付费。同时，Athena使用IAM来管理权限，部分操作需要对应的权限才能进行。 Athena使用SerDe来与各种数据格式进行交互，包括CSV,JSON和Parquet。在使用时进行指定。 GlueGlue是一个元数据系统，它维护了信息诸如数据具体存储的位置以及数据的结构，它本身还提供了ETL的能力。Glue里面几个关键的概念是Database, Table, Crawler, Classifier, Job: Database 跟我们普通理解的数据库的概念是类似的，是一组table的逻辑集合。 Table 是数据的元数据，它定义数据保存在哪里(比如S3的路径)，有哪些column，怎么分区的。 Crawler 是元数据的爬虫，你给它一个路径，告诉它每天去爬一次，Crawler就可以及时把更新的元数据，比如新增的分区同步到Glue里面来供计算引擎消费。 Classifier 是数据结构的解析器，你给Crawler一个S3的路径它怎么就能解析出其中的结构呢，这就是Classifier要干的事情，Glue里面已经内置了一些Classfier, 用户也可以自定义Classifier。 Job是一个ETL脚本 Glue与AthenaAWS Glue 是一项完全托管的 ETL (提取、转换和加载) 服务，能够对数据进行分类、清理和扩充，并在各种数据存储之间可靠地移动数据。AWS Glue 爬网程序自动从源数据推断数据库和表架构，从而将关联的元数据存储在 AWS Glue 数据目录中。在 Athena 中创建表时，可以选择使用 AWS Glue 爬网程序创建表。 实例 使用Glue读取csv并转换成Parquet格式随后使用Athena查询 Glue自定义分类器识别服务器日志 How to extract, transform, and load data for analytic processing using AWS Glue (Part 2) In Search of Happiness: A Quick ETL Use Case with AWS Glue + Redshift Athena与S3Athena 可帮助分析在 Amazon S3 中存储的非结构化、半结构化和结构化数据。包括 CSV、JSON 或列式数据格式，如 Apache Parquet 和 Apache ORC。可以使用 ANSI SQL 通过 Athena 运行临时查询，而无需将数据聚合或加载到 Athena 中。","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://Bithub00.com/tags/AWS/"}]},{"title":"L1与L2范数","slug":"范数","date":"2019-04-03T04:59:52.310Z","updated":"2019-10-21T12:29:36.747Z","comments":true,"path":"2019/04/03/范数/","link":"","permalink":"http://Bithub00.com/2019/04/03/范数/","excerpt":"看到的一篇很好的介绍L1与L2范数的文章，mark下来","text":"看到的一篇很好的介绍L1与L2范数的文章，mark下来","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://Bithub00.com/tags/机器学习/"}]},{"title":"安卓Activity移植","slug":"安卓Activity移植","date":"2018-09-07T04:31:55.617Z","updated":"2019-08-06T04:38:29.791Z","comments":true,"path":"2018/09/07/安卓Activity移植/","link":"","permalink":"http://Bithub00.com/2018/09/07/安卓Activity移植/","excerpt":"这篇文章记录一下之前将写的一个Activity加入现有应用时遇到的坑","text":"这篇文章记录一下之前将写的一个Activity加入现有应用时遇到的坑 找不到R&#8194;这个问题移植完后马上就会遇到，在将layout里的xml文件等都复制过来后，如果问题还存在，直接Android Studio中 Build-Clen Project 一次，问题基本上就解决了 包名&#8194;形如com.package.XXXXX等问题，复制代码文件时IDE会自动帮我们修正，但布局文件中IDE是不会帮我们纠正过来的,例如`\"tools:context=\".MainActivity\"\"`就要进行修改，或者在标签中出现com.package.XXXX没有修改的情况，只能一个个xml文件去找了 Application&#8194;如果在移植的Activity中继承了Application类创造了自己的，那移植过去后一定要在AndroidManifest.xml文件的\\里的android:name中进行修改 注册&#8194;最后就是一直过去后要在AndroidManifest.xml文件中添加移植过去的Activity，不然运行时会报错，Logcat中也会给出建议","categories":[],"tags":[{"name":"Android","slug":"Android","permalink":"http://Bithub00.com/tags/Android/"}]},{"title":"SIMLR算法论文个人翻译","slug":"SIMLR算法","date":"2018-07-25T12:00:18.373Z","updated":"2019-08-07T09:19:27.860Z","comments":true,"path":"2018/07/25/SIMLR算法/","link":"","permalink":"http://Bithub00.com/2018/07/25/SIMLR算法/","excerpt":"基于多核函数的单细胞RNA序列分析与可视化算法 &#8194; 摘要：我们将要介绍的SIMLR算法，在理论框架和软件应用上已经实现，它从输入的单细胞RNA序列中学习到一种相似度的信息，随后可以用这种信息来实现降维、聚类和可视化等后续处理。在七个公开的数据集上我们对算法进行了检验并于一些常用的算法进行了比较分析。结果证明了SIMLR算法对大规模数据的良好处理以及极大程度上提高了聚类的效果，同时提升了可视化的效果以及对细胞间相似程度的识别。","text":"基于多核函数的单细胞RNA序列分析与可视化算法 &#8194; 摘要：我们将要介绍的SIMLR算法，在理论框架和软件应用上已经实现，它从输入的单细胞RNA序列中学习到一种相似度的信息，随后可以用这种信息来实现降维、聚类和可视化等后续处理。在七个公开的数据集上我们对算法进行了检验并于一些常用的算法进行了比较分析。结果证明了SIMLR算法对大规模数据的良好处理以及极大程度上提高了聚类的效果，同时提升了可视化的效果以及对细胞间相似程度的识别。 关键词：多核函数；低秩约束；单细胞RNA序列 背景介绍及概览&#8194; 此前，对单细胞RNA序列的研究揭示了细胞种群间未被洞悉的异构性与功能分化。近期的研究阐释说明了通过对RNA序列的无偏分析，是有可能做到对细胞子群功能特异性的de novo分析的。然而，大部分用来应用的算法都是针对传统的大规模RNA序列数据，而基因的表达结果在一群细胞中被平均化了。这些算法并不能成功地处理如下问题：如噪声数据、离散点以及dropout现象（获取基因表达数据时未能成功识别基因表达结果而标记为0，而实际情况是基因进行了表达）。诸如DropSeq和GemCode的平台已经显著增加了数千个细胞的细胞信息，然而，这类平台产生的多为稀疏数据，其中95%的基因测量结果被标记为0。对于诸如降维、聚类以及数据可视化等无监督学习的方法来说，其中一个关键就是相似度矩阵的学习，而这个矩阵对于不同平台或者生物实验得到的数据并不通用。为了解决上述问题，我们提出了SIMLR算法，一个从输入的单细胞RNA序列数据中学习细胞与细胞之间相似度矩阵的框架。 &#8194; 相较于传统的算法，SIMLR算法有三个主要的优势：首先，它通过使用多个核函数的方法来学习一个最符合输入数据的结构的相似性矩阵。常规的降维或聚类算法对数据的假设有时并不适用于单细胞RNA序列数据。而多个核函数被证实在描绘数据多角度的信息下有着更好的效果，而且相对于单个核函数具有更好的灵活性。第二，对于高维度下的dropout现象，SIMLR算法通过对学习的相似度矩阵应用秩约束以及使用图扩散的方法来解决。秩约束的应用增强了相似度矩阵的分块对角结构，而图扩散方法提高了对弱相似度的识别。第三，算法习得的相似度矩阵可以被高效的用来后续的数据分析，比如通过SNE算法进行数据在低维空间下的可视化。 &#8194; 我们通过在四个公开的单细胞数据集上应用SIMLR算法来与传统的算法比较，结果是SIMLR算法习得的相似度矩阵在表现数据相似度上要表现得更好。每个数据集中细胞所属的种类是先验的并且在研究中已经被证实正确。通过输入数据集和细胞的种类数，SIMLR算法就能学习出一个细胞间的相似度矩阵，而不需要输入细胞真实所属类别的标签信息。而且相较于传统的相关系数或欧几里得距离衡量相似度，SIMLR算法的结果要更加接近真实结果。特别的是，Buettner数据集的真实标签是细胞周期的状态，我们在这个数据集上额外应用了SIMLR算法来对基因进行排序。算法输出一个基因网络，来展现不同的基因在细胞各个周期、翻译以及代谢过程中的相关性。 &#8194; 图表1：SIMLR算法的总览。给定一个基因表达数据的矩阵作为输入，算法构建并学习多个核函数之间的权重，并构建出一个相似度矩阵。给定细胞种类数C作为输入，构建出的相似度矩阵将有一个含C个分块的分块对角矩阵，每个分块中的细胞都更为相似。算法使用得到的相似度矩阵来进行降维、可视化、聚类等后续处理。实心箭头代表了需要被用来构建相似度矩阵的信息，而点线箭头则表明聚类的结果可以与可视化以及基因排序相结合。2D散点图中的每一个点代表一个细胞，而不同的颜色代表细胞所属的不同类别。 &#8194; 为了分析算法在降维上的效果，我们与8个传统的降维算法进行了比较，包括主成分分析、tSNE以及ZIFA算法。在六种不同的表现算法优劣矩阵中，SIMLE算法在四个公开的数据集上表现出色，并且远远的拉开了与第二名的差距。 &#8194; 我们还进行了低维数据的可视化实验。结果表明，在各个数据集上SIMLR算法的结果不仅能吻合数据集给出的真实标签，甚至在秩约束有关的参数没有贡献时同样能保持相似度矩阵的分块对角结构。特别地，在Kolodziejczyk这个数据集上，我们还从SIMLR算法的结果上发现，在已知的分类结果上其实还能继续往下细分，这个结果也符合与这个数据集有关的理论研究。 &#8194; SIMLR算法同样可以用来进行细胞聚类，通过降维后应用k-means算法或者直接对习得的相似度矩阵使用AP近邻算法来实现。后者的表现性要远远超过使用皮尔逊相关系数或欧几里得距离来衡量相似性的方法。而前者的表现性在四个数据集上也比现有的针对单细胞的聚类算法要更好。 &#8194; 为了检验算法的能力，我们应用了更多更有挑战性的方案。我们分析了一个GemCode平台上提供的周边血液单核球细胞的稀疏数据集，里面包含了2700个细胞且其中95%的基因表达结果被标记为0。通过降维后应用k-means算法，我们识别出八种主要的细胞类别，包括一个只含12个细胞的megakaryocyte种类。除此之外，我们还在不同的已经得到充分研究的数据集上试验了SIMLR算法的表现性。 &#8194; 为了说明SIMLR算法在大规模数据上的表现性，我们在三个公开的大规模数据集上进行了试验。我们对真实标签与算法输出的预测表情的相关性进行了计算。对于Zeisei数据集，我们应用了一个二级聚类的方法，发现SIMLR算法可以用来进行在对细胞的层次结构的分析。而且，低维可视化的结果也很好的符合了真实的数据标签。即使是大规模数据集中因为噪声和离散点所造成的相似信息被隐藏的情况，SIMLR算法也能学习一个合适的细胞之间的距离。 &#8194; 图表2：在不同数据集上的测试结果。分别给出了SIMLR算法、基于高斯核的欧几里得距离以及皮尔逊相关系数所得到的相似度矩阵。排放的次序基于相似度的高低。矩阵中的细胞都按照真实的种类来进行排列，使得同一种类的细胞排列在一起，坐标轴上的不同颜色代表不同的种类。可以看出，SIMLR算法的相似度矩阵的分块对角结构与真实标签基本符合。&#8194; &#8194; 总的来说，SIMLR算法可以基于不同的数据集通用地判断那些细胞更为相似，即判断结果不受特定数据集影响，并应用降维、聚类、数据可视化等分析方法。SIMLR算法在有着清晰分类的数据集上表现出色，而我们预测这个多核学习的框架在分类不明显的数据集上也会同样产生作用。 &#8194; 图表3： 2维可视化结果的比较。坐标轴无实际意义。每个点代表一个细胞，而靠得越近的点代表相似度越高。数据可视化时没有输入真实的标签信息，在算法的输出时才让各个数据点标上真实的颜色以检验算法的效果。 算法详述：SIMLR算法提供了Matlab和R语言两个版本的实现（https://github.com/BatzoglouLabSU/SIMLR） &#8194; k-means算法的实现我们使用了Matlab和R语言自带的模块。而SNE算法我们修改了两个语言中这一模块的源代码。四个公开的数据集随着源代码一起被提供。而三个大型的数据集可以在相应的平台上得到。输入一个N×M的的基因表达矩阵，N代表细胞个数，M代表基因数。SIMLR算法将输出一个S×S的相似性矩阵。其中Sij表示两个细胞之间的相似度。给定一个细胞种类数C，算法假定输出的相似度矩阵将有一个含C个分块的分块对角矩阵，各个分块中的细胞更为相似。我们对两个细胞之间的距离定义为： 其中wl代表核函数的权重 算法依据如下损失函数来计算细胞与细胞之间的相似性： &#8194; 其中IN和IC分别为N×N和C×C的单位矩阵，tr(.)代表矩阵的秩，β和γ均为非零值，||S||F为范数表示，L为辅助的用来对S进行低秩约束。因此这个损失函数求解三个参数：相似度矩阵S，核函数权重向量w以及一个结构为N×C的秩约束矩阵L。 &#8194; 损失函数的第一项含义为，如果两个细胞间的距离很远，则它们的相似度应该很低。第二项是一个对S的正则化，防止S矩阵过于接近一个单位矩阵。如果细胞可以被划分为C类，则每一类中的细胞更为相似，理想情况下矩阵S的秩为C。因此，损失函数的第三项以及L矩阵的引入增强了S的低秩结构，而矩阵（IN-S）即为拉普拉斯矩阵，在一个相似图中，每个节点代表一个细胞，边衡量节点间的相似性。第四项对核函数的权重进行约束，防止单核函数情况的出现。实践证明，这个正则化项提高了相似矩阵的表现。 核函数的构建我们以带有不同超参数的高斯核为基础构建不同的核函数，实践证明相对其它核函数高斯核的表现更好。 式中||ci - cj||表示细胞i和j之间的欧几里得距离。 方差ɛij的定义式如下： &#8194; 因此，每一个核函数被一对参数(σ，k).我们设定k = 10，12，14,…,30 以及σ = 1.0，1.25，1.5，1.75，2，产生了55个不同的核函数。然而，实践证明，算法对核函数的数量以及参数的选择并不敏感。 初始化 核函数的权重w被初始化为核函数数量的倒数： 相似度矩阵S被初始化为： 而矩阵L被初始化为拉普拉斯矩阵（IN - S）的前C个特征向量。 优化算法我们对S,L和w进行优化。上文中的优化式非凸，但固定某两个参数得到的目标函数为凸函数。因此我们可以有效的应用凸优化算法来进行求解。 步骤1：固定L和w对S进行更新。损失函数可以被重写为： &#8194; 目标函数中第一项求和式以及约束项均为线性，而第二项是一个二次项，它可以在多项式复杂度的时间内计算出来。 步骤2：固定S和w对L进行更新。损失函数可以被重写为： &#8194; 此时对L矩阵的求解就是拉普拉斯矩阵（S-IN）对应的C个最大特征值的特征向量。 步骤三：固定S和L对w进行更新。同样地，损失函数可以被重写为如下形式： &#8194; 对于这样一个包含凸函数和线性约束的问题，任何一个凸优化算法都可以进行求解。 步骤四：基于扩散方法的相似度矩阵优化。我们应用了一种扩散方法来减少噪声和dropout现象对S矩阵的影响。给定矩阵S，我们构建如下形式的过渡矩阵P： &#8194; Ak(i)代表一个集合，里面包含了细胞i的k个近邻细胞的索引。构建出来的过渡矩阵是稀疏的，并且保留了极大部分的相似度结构。算法的更新方法如下所示： &#8194; H(0)ij = Sij作为输入，而最终迭代出来的结果Hij作为新的相似值Sij。这个额外的扩散方法将会很大程度上避免单细胞RNA序列数据中的噪声值所带来的影响。然而，因为这个算法的高计算复杂度，在面对大规模数据集时它无法发挥有效作用。 &#8194; SIMLR算法重复步骤1-4直到算法收敛。随后使用得到的相似度矩阵S进行后续分析： 后续分析降维处理：&#8194; 算法基于SNE算法进行降维，并进行了调整。不同点在于，tSNE算法基于高斯核来计算高维度空间下数据之间的相似度，随后将其映射到低维空间并保留这个相似度信息。我们没有选择直接输入基因表达矩阵而是输入了相似度矩阵S。 可视化：&#8194; 我们使用降维算法来投影到二维或三维空间进行可视化。如k-means聚类，我们将维度降到B维，得到一个N×B相应的矩阵Z，随后应用k-means算法来对于细胞进行聚类。B的值与输入的C的值相同。C同时也是上文提到的秩约束的参数。 基于相似度矩阵的基因排序：&#8194; 我们通过计算某个基因在不同细胞中表达的值与习得的相似度的相关程度来对基因进行排序。给定相似度矩阵S和某个基因在所有细胞中的表达结果f，表达式如下： &#8194; 这是一个经典的用来衡量基因和相似度之间相关性的无监督特征排序算法。表达式的值越高，则基因在不同细胞中的表达越重要。然而，表达式对相似度矩阵中的噪声值非常敏感。为了克服这个问题，我们随机选取一定比例的细胞（如细胞总数的80%），随后根据剩余细胞的相似度矩阵来对基因进行排序。 大规模数据集上的应用：&#8194; 我们在含有数万个细胞的数据集上进行了试验，关键因素在于用KNN相似度来近似于细胞的相似度。第一步，我们采用了目前更为先进的近邻搜索算法ANNOY，ANNOY算法认为，一个近邻点的近邻也可能是一个近邻点。因此，在构建出KNN图后，算法只更新每个细胞所预先选定的前k个近邻点。因为得到的相似度矩阵是稀疏的，我们使用Spectra来对L进行求解。当我们按照这种方式而不是进行涉及到矩阵求逆运算的闭式求解，我们只需要在有限次的迭代中就可以得到一个结果。 &#8194; 在我们得到相似度矩阵后，我们就可以进行细胞可视化和细胞聚类了。聚类时，从t-SNE算法中获得嵌入的低维空间的过程的计算量很大。相反，我们采用了一种谱聚类算法，它基本上等同于我们的SIMLR算法中对矩阵L应用k均值。这种简单的算法对稀疏相似性的聚类非常有效，并可扩展到数以万计的细胞中。对于可视化，由于我们仅将细胞到细胞的相似性映射到二维或三维空间，因此应用t-SNE算法在计算上仍是可行的。我们对tSNE算法中的Barnes–Hut算法进行了调整。 数据来源&#8194; 我们在本文中使用了七个单细胞RNA-seq的数据集。前四个数据集每个包含少于1,000个细胞，而最后三个数据集包含数千到数万个细胞。下面是所有单细胞RNA-seq数据集的详细描述。 &#8194;（1）11个细胞群，包括神经细胞和血细胞（Pollen数据集）。该数据集旨在测试低覆盖率单细胞RNA-seq在鉴别不同细胞群体方面的效用，因此含有多种细胞类型的混合物：皮肤细胞，多能干细胞，血细胞和神经细胞。该数据集包括在高深度和低深度处测序的样本。我们分析了高深度样本，每个样本的平均测序数为890万。 &#8194; （2）具有感觉亚型的神经元细胞（Usoskin数据集）。该数据集包含来自小鼠背根神经节的622个细胞，每个细胞平均有114万个读段。作者将细胞分为四种神经元类型：肽能伤害性伤害感受器，非肽能伤害性伤害感受器，含神经丝，含酪氨酸羟化酶。 &#8194; （3）不同细胞周期阶段的胚胎干细胞（Buettner数据集）。该数据集来自对照研究，该对照研究量化了细胞周期对个体小鼠胚胎干细胞（mESC）中基因表达水平的影响。对于182个细胞中的每一个细胞，获得平均五十万个读数，并且至少20％的读数被定位于mm9小鼠基因组上的已知外显子。使用荧光激活细胞分选将细胞分选为细胞周期的三个阶段，并且使用金标准Hoechst染色对它们进行验证。 &#8194; （4）不同环境条件下的多能细胞（Kolodziejczyk数据集）。该数据集是从干细胞研究中获得的，研究不同培养条件如何影响mESC的多能状态。该研究从涉及三种不同培养条件的九个不同实验中量化了704个mESC中约10,000个基因的表达水平。每个细胞平均获得900万个读数，超过60％的读数映射到小家鼠基因组上的外显子。 &#8194; （5）具有39个亚型的小鼠视网膜细胞（Macoskco数据集）。通过基于液滴的高通量技术Drop-seq获得，该数据集包括44,808个单元的UMI（3端）计数（由其定制的计算管道识别）。细胞类型通过PCA和基于密度的聚类进行分类，并且通过差异基因表达进行验证。根据原始处理程序，我们过滤掉少于900个基因的细胞（涉及到11,040个细胞）用于无监督分析。 &#8194; （6）来自一个健康人类的PBMCs数据集（PBMC68k数据集）。通过GemCode平台生成scRNA-seq文库，这是一种基于液滴的高通量技术，以及具有UMI（3’端）计数的68,560个细胞通过其定制的计算流水线来识别。这种细胞群包括健康人体内的主要免疫细胞类型。 &#8194; （7）使用独特的分子识别（UMI）分析和3’端计数收集来自小鼠皮质和海马的细胞（Zeisel数据集）。收集来自小鼠脑的3,005个细胞，并且通过分级双聚类鉴定了47个亚型，并通过基因标记进行了验证。 &#8194; 对于以上涉及到的数据集，我们进行了如下的数据预处理：","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://Bithub00.com/tags/机器学习/"}]},{"title":"Jupyter Notebook 的快捷键","slug":"jupyter notebook快捷键","date":"2018-07-24T06:16:20.378Z","updated":"2018-07-24T06:22:27.766Z","comments":true,"path":"2018/07/24/jupyter notebook快捷键/","link":"","permalink":"http://Bithub00.com/2018/07/24/jupyter notebook快捷键/","excerpt":"Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。","text":"Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。 命令模式 (按键 Esc 开启) Enter : 转入编辑模式 Shift-Enter : 运行本单元，选中下个单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在其下插入新单元 Y : 单元转入代码状态 M :单元转入markdown状态 R : 单元转入raw状态 1 : 设定 1 级标题 2 : 设定 2 级标题 3 : 设定 3 级标题 4 : 设定 4 级标题 5 : 设定 5 级标题 6 : 设定 6 级标题 Up : 选中上方单元 K : 选中上方单元 Down : 选中下方单元 J : 选中下方单元 Shift-K : 扩大选中上方单元 Shift-J : 扩大选中下方单元 A : 在上方插入新单元 B : 在下方插入新单元 X : 剪切选中的单元 C : 复制选中的单元 Shift-V : 粘贴到上方单元 V : 粘贴到下方单元 Z : 恢复删除的最后一个单元 D,D : 删除选中的单元 Shift-M : 合并选中的单元 Ctrl-S : 文件存盘 S : 文件存盘 L : 转换行号 O : 转换输出 Shift-O : 转换输出滚动 Esc : 关闭页面 Q : 关闭页面 H : 显示快捷键帮助 I,I : 中断Notebook内核 0,0 : 重启Notebook内核 Shift : 忽略 Shift-Space : 向上滚动 Space : 向下滚动 编辑模式 ( Enter 键启动) Tab : 代码补全或缩进 Shift-Tab : 提示 Ctrl-] : 缩进 Ctrl-[ : 解除缩进 Ctrl-A : 全选 Ctrl-Z : 复原 Ctrl-Shift-Z : 再做 Ctrl-Y : 再做 Ctrl-Home : 跳到单元开头 Ctrl-Up : 跳到单元开头 Ctrl-End : 跳到单元末尾 Ctrl-Down : 跳到单元末尾 Ctrl-Left : 跳到左边一个字首 Ctrl-Right : 跳到右边一个字首 Ctrl-Backspace : 删除前面一个字 Ctrl-Delete : 删除后面一个字 Esc : 进入命令模式 Ctrl-M : 进入命令模式 Shift-Enter : 运行本单元，选中下一单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在下面插入一单元 Ctrl-Shift— : 分割单元 Ctrl-Shift-Subtract : 分割单元 Ctrl-S : 文件存盘 Shift : 忽略 Up : 光标上移或转入上一单元 Down :光标下移或转入下一单元","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"}]},{"title":"VPN","slug":"vpn","date":"2018-04-06T11:58:10.712Z","updated":"2019-08-07T07:59:44.374Z","comments":true,"path":"2018/04/06/vpn/","link":"","permalink":"http://Bithub00.com/2018/04/06/vpn/","excerpt":"edu.cn的学校邮箱+5美元 = 11个月的境外服务器 领取github的vps优惠码","text":"edu.cn的学校邮箱+5美元 = 11个月的境外服务器 领取github的vps优惠码 教程https://www.ichenfei.com/get-github-students-gift.html 领取优惠码页面https://education.github.com/pack/offers#digitalocean vps开通页面(需要翻墙，学校使用ipv6地址可以直接翻出去)https://www.digitalocean.com/不要使用一次性邮箱注册账号，因为以后每次登陆都要邮箱验证 登陆页面https://cloud.digitalocean.com/login (建议翻墙访问) 注册paypal账号来支付https://www.paypal.com/c2/home paypal绑定了银行卡和手机，建议用不常用的银行卡，然后充35块钱进去支付成功后，激活digitalocean账号，填入github的优惠卷，获得50美元 创建自己的服务器建议选SFO一区的服务器，不容易被墙，配置的话5美元那种就差不多了，服务器型号建议ubuntu 连接服务器通过console连上服务器后，使用秋水逸冰大大的命令一键安装:123wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.shchmod +x shadowsocks-all.sh &amp;&amp; ./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 探索新世界吧:)","categories":[],"tags":[{"name":"vpn","slug":"vpn","permalink":"http://Bithub00.com/tags/vpn/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-04-05T14:57:07.298Z","updated":"2019-08-07T08:05:31.440Z","comments":true,"path":"2018/04/05/hello-world/","link":"","permalink":"http://Bithub00.com/2018/04/05/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}