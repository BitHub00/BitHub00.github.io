{"meta":{"title":"原力小站","subtitle":"扎导的原版正联出了吗？","description":"May 4th be with you","author":"Mr.shuan","url":"http://Bithub00.com"},"pages":[{"title":"","date":"2018-04-06T12:26:47.993Z","updated":"2018-04-06T11:53:10.347Z","comments":true,"path":"404.html","permalink":"http://Bithub00.com/404.html","excerpt":"","text":"404"},{"title":"tags","date":"2019-04-03T05:42:34.000Z","updated":"2019-04-03T05:48:03.768Z","comments":false,"path":"tags/index.html","permalink":"http://Bithub00.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"推荐系统学习记录","slug":"推荐系统","date":"2019-08-05T11:40:06.163Z","updated":"2019-08-05T12:39:30.963Z","comments":true,"path":"2019/08/05/推荐系统/","link":"","permalink":"http://Bithub00.com/2019/08/05/推荐系统/","excerpt":"用本文记录学习推荐系统的过程，以及一些实用的资料与资源。","text":"用本文记录学习推荐系统的过程，以及一些实用的资料与资源。 推荐系统数据集基于隐式反馈数据的推荐系统【理论及python实践】 Yifan Hu, Yehuda Koren, Chris Volinsky.Collaborative Filtering for Implicit Feedback Datasets[J].IEEE International Conference on Data Mining,2008 前言隐式反馈(Implict)就是用户的行为数据，包括点击，浏览和停留等，它不像评分和点赞一样直观地表示了用户的喜好。实际情况中像评分这种显式数据往往很难获得，因为它输入用户额外的进行操作，而像点击这种隐式反馈的数据，是随着用户的行为自然产生的，不需要额外的获取成本,因此实际情况中隐式反馈的数据规模要远大于显式反馈，因此很有必要研究基于隐式反馈数据的推荐系统 隐式反馈数据的特征： 没有负样本。不同于评分，用户可以通过打低分来表达对某个物品的厌恶，我们只能通过点击猜测用户可能对某个物品有偏好，而不能通过没有点击来说明用户不喜欢，可能只是他还没接触过这个物品。处理显式数据时，缺失的评分项可以当作缺失值处理，而处理隐式数据时，为了避免只得到正向反馈，必须要对数据整体进行分析。 隐含很多的噪声数据。例如用户购买某个物品，不代表他一定喜欢这个物品，可能只是作为礼物或者其它原因，而给一个物品打高分可以很大程度上表示他偏好这个用品。 数值含义不同。在显式数据里,数值的含义代表偏好程度，如评分；而隐式数据里，数值代表置信度,往往表现为行为的频率，例如观看次数等等。频率越高，我们越能确定它与用户的偏好相关联，而不是一个偶然性情况。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"},{"name":"recommender systems","slug":"recommender-systems","permalink":"http://Bithub00.com/tags/recommender-systems/"}]},{"title":"宝贝陈列室","slug":"宿舍一角","date":"2019-07-25T03:30:57.778Z","updated":"2019-07-25T03:40:57.406Z","comments":true,"path":"2019/07/25/宿舍一角/","link":"","permalink":"http://Bithub00.com/2019/07/25/宿舍一角/","excerpt":"展示一下自己收来的各种雕像和CD~","text":"展示一下自己收来的各种雕像和CD~","categories":[],"tags":[{"name":"宿舍","slug":"宿舍","permalink":"http://Bithub00.com/tags/宿舍/"}]},{"title":"Youtube爬虫","slug":"Youtube爬虫","date":"2019-07-25T02:31:32.337Z","updated":"2019-07-25T02:56:22.371Z","comments":true,"path":"2019/07/25/Youtube爬虫/","link":"","permalink":"http://Bithub00.com/2019/07/25/Youtube爬虫/","excerpt":"爬取相关频道Related Channels","text":"爬取相关频道Related Channels 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import scrapyimport pandas as pdimport collectionsimport json# scrapy crawl related_channel_spider 命令行启动爬虫youtube_url = 'https://www.youtube.com'kol = collections.OrderedDict()class RelatedChannelsSpider(scrapy.Spider): name = 'related_channel_spider' def close(spider, reason): # 爬虫结束后将爬取结果写入json文件 file_kol = open('RelatedChannels_new.json', 'a') json.dump(kol, file_kol, sort_keys=True, indent=2) def start_requests(self): # 爬虫数据来源:Youtube 频道链接 file_name = 'kol_utm_campaign_ad.xlsx' Channels = pd.read_excel(file_name, sheet_name='ad_channel_new', header=0, usecols=['Channel']) Titles = pd.read_excel(file_name, sheet_name='ad_channel_new', header=0, usecols=['KolName']) length = len(Channels) start_urls = [] for i in range(0, length): url = &#123;&#125; url['url'] = Channels[i:i + 1].values.item() url['title'] = Titles[i:i + 1].values.item() start_urls.append(url) for url in start_urls: request = scrapy.Request(url['url'], callback=self.parse) request.meta['title'] = url['title'] yield request def parse(self, response): related = collections.OrderedDict() meta = response.meta channel_title = meta['title'] channel_url = response.url yield &#123; 'channel_title': channel_title, 'channel_url': channel_url &#125; # xpath解析网页 channel_item_lis = response.xpath( '//li[contains(@class, \"branded-page-related-channels-item\")]' ) for channel_item_li in channel_item_lis: related_channel_title = channel_item_li.xpath( 'span/div[contains(@class, \"yt-lockup-content\")]/h3/a/text()' ).extract()[0] relative_url = channel_item_li.xpath( 'span/div[contains(@class, \"yt-lockup-content\")]/h3/a/@href' ).extract()[0] related[related_channel_title] = youtube_url + relative_url print(related_channel_title, youtube_url + relative_url) if related: kol[channel_title] = related 爬取视频评论(包含评论内容、评论日期等)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import timeimport jsonimport requestsimport lxml.htmlimport pandas as pdfrom lxml.cssselect import CSSSelectorYOUTUBE_COMMENTS_URL = 'https://www.youtube.com/all_comments?v=&#123;youtube_id&#125;'YOUTUBE_COMMENTS_AJAX_URL = 'https://www.youtube.com/comment_ajax'youtube_video_url = 'https://www.youtube.com/watch?v='USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'def find_value(html, key, num_chars=2): pos_begin = html.find(key) + len(key) + num_chars pos_end = html.find('\"', pos_begin) return html[pos_begin: pos_end]def extract_comments(html): tree = lxml.html.fromstring(html) item_sel = CSSSelector('.comment-item') text_sel = CSSSelector('.comment-text-content') time_sel = CSSSelector('.time') author_sel = CSSSelector('.user-name') # vote_sel = CSSSelector('.like-count') 是否爬取评论点赞数 for item in item_sel(tree): yield &#123; 'cid': item.get('data-cid'), 'text': text_sel(item)[0].text_content(), 'time': time_sel(item)[0].text_content().strip(), 'author': author_sel(item)[0].text_content() # 'like-count': vote_sel(item)[0].text_content() &#125;def extract_reply_cids(html): tree = lxml.html.fromstring(html) sel = CSSSelector('.comment-replies-header &gt; .load-comments') return [i.get('data-cid') for i in sel(tree)]def ajax_request(session, url, params, data, retries=10, sleep=20): for _ in range(retries): response = session.post(url, params=params, data=data) if response.status_code == 200: response_dict = json.loads(response.text) return response_dict.get('page_token', None), response_dict['html_content'] else: time.sleep(sleep)def download_comments(youtube_id, sleep=1): session = requests.Session() session.headers['User-Agent'] = USER_AGENT # 获取初始页面的评论 response = session.get(YOUTUBE_COMMENTS_URL.format(youtube_id=youtube_id)) html = response.text reply_cids = extract_reply_cids(html) ret_cids = [] for comment in extract_comments(html): ret_cids.append(comment['cid']) yield comment page_token = find_value(html, 'data-token') session_token = find_value(html, 'XSRF_TOKEN', 4) first_iteration = True # 获取剩下的评论(等同于点击'show more') while page_token: data = &#123; 'video_id': youtube_id, 'session_token': session_token &#125; params = &#123; 'action_load_comments': 1, 'order_by_time': True, 'filter': youtube_id &#125; if first_iteration: params['order_menu'] = True else: data['page_token'] = page_token response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data) if not response: break page_token, html = response reply_cids += extract_reply_cids(html) for comment in extract_comments(html): if comment['cid'] not in ret_cids: ret_cids.append(comment['cid']) yield comment first_iteration = False time.sleep(sleep) # 获取评论回复 for cid in reply_cids: data = &#123;'comment_id': cid, 'video_id': youtube_id, 'can_reply': 1, 'session_token': session_token&#125; params = &#123;'action_load_replies': 1, 'order_by_time': False, 'filter': youtube_id, 'tab': 'inbox'&#125; response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data) if not response: break _, html = response for comment in extract_comments(html): if comment['cid'] not in ret_cids: ret_cids.append(comment['cid']) yield comment time.sleep(sleep)def main(): # 使用 for comment in download_comments(videoId): ··· 爬取某个视频主所有的视频12345678910111213141516171819202122232425262728293031# 使用Seleniumfrom selenium import webdriverfrom bs4 import BeautifulSoupChannel_videos = 'https://www.youtube.com/channel/UCGK0RMoHboOVUbdxDhLD1xw/videos'Video_Lists = []option = webdriver.ChromeOptions()option.add_argument('headless')youtube_url = 'https://www.youtube.com'browser = webdriver.Chrome(chrome_options=option, executable_path='D:\\Tool\\Software\\chromedriver_win32\\\\chromedriver.exe')# 去掉option选项可以让chrome在前台显示，看看模拟的效果browser = webdriver.Chrome(executable_path='D:\\Tool\\Software\\chromedriver_win32\\\\chromedriver.exe')browser.get(Channel_videos)time.sleep(5)old_height = browser.execute_script(\"return document.documentElement.scrollHeight;\")browser.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")time.sleep(5)new_height = browser.execute_script(\"return document.documentElement.scrollHeight;\")# 模拟浏览器向下滚动页面，直到所有视频都被加载出来while new_height != old_height: old_height = new_height browser.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\") time.sleep(5) new_height = browser.execute_script(\"return document.documentElement.scrollHeight;\") html = browser.page_source soup = BeautifulSoup(html, 'lxml') videoList = soup.findAll(\"a\", &#123;\"class\": \"yt-simple-endpoint style-scope ytd-grid-video-renderer\"&#125;) for video in videoList: ··· 获取视频的播放量和发布日期1234567891011121314151617181920212223242526272829303132333435363738394041424344# 因为Youtube API每日有访问限制，超过它的配额后就无法再使用接口获取某个视频的播放量和发布日期了，使用爬虫就没有这种限制# coding=utf-8import requestsfrom lxml import etreeimport osimport jsonyoutube_video_url = 'https://www.youtube.com/watch?v='def viewCount(): videoId = 'P4ItC6jWN0s' url = \"https://www.youtube.com/watch\" querystring = &#123;\"v\": videoId&#125; payload = \"\" headers = &#123; 'Content-Type': \"application/json\", 'User-Agent': \"PostmanRuntime/7.15.0\", 'Accept': \"*/*\", 'Cache-Control': \"no-cache\", 'Postman-Token': \"296c1155-2adc-4028-95c2-26cffec91784,f5c8088f-432f-4a67-a815-13464bfca373\", 'Host': \"www.youtube.com\", 'cookie': \"YSC=nwR5fai12Kg; VISITOR_INFO1_LIVE=gAl5VFO7Gjo; PREF=f1=50000000; GPS=1\", 'accept-encoding': \"gzip, deflate\", 'Connection': \"keep-alive\", 'cache-control': \"no-cache\" &#125; response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring) html = etree.HTML(response.text) datePublished = html.xpath('//meta[@itemprop=\"datePublished\"]/@content') if datePublished: datePublished = datePublished[0] view_count = html.xpath('//meta[@itemprop=\"interactionCount\"]/@content') if view_count: view_count = int(view_count[0]) print(datePublished, view_count) else: print('Not Exist:',videoId) file = open('video_statstics.json', 'a') json.dump(video_statstics, file, indent=2)if __name__ == '__main__': viewCount()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"},{"name":"Youtube","slug":"Youtube","permalink":"http://Bithub00.com/tags/Youtube/"},{"name":"爬虫","slug":"爬虫","permalink":"http://Bithub00.com/tags/爬虫/"}]},{"title":"python常用操作","slug":"python常用操作","date":"2019-07-25T02:13:08.319Z","updated":"2019-07-30T03:07:59.969Z","comments":true,"path":"2019/07/25/python常用操作/","link":"","permalink":"http://Bithub00.com/2019/07/25/python常用操作/","excerpt":"读取excel文件某一列","text":"读取excel文件某一列 12345678import pandas as pdnames = pd.read_excel( 'NOT_EXIST_List.xlsx', sheet_name='NOT EXIST', header=0, usecols=['KolName'])for i in range(0, len(names)): name = names[i:i + 1].values.item() List去重 12import pandas as pdList = pd.unique(List).tolist() 将接口返回值解析成json格式 1234567import requestresponse = requests.request( \"GET\", url, data=payload, headers=headers, params=querystring)json_response = json.loads(response.text) 移除字符串中的标点符号 123def removePunctuation(text): str = ''.join(c for c in text if c not in string.punctuation) return str 读取和写入json文件 1234file = open('XXX.json', 'r')XXX = json.loads(file.read())file = open('XXX.json', 'a')json.dump(XXX, file, indent=2) 将List写入excel文件 1234df = pd.DataFrame(List, columns=['name', 'url'])writer = pd.ExcelWriter('remain.xlsx')df.to_excel(writer, 'remain')writer.save() dict根据key排序 123456def sortdict(data): result = collections.OrderedDict() dict = sorted(data.items(), key=lambda d: d[0]) for i in range(0, len(dict)): result[dict[i][0]] = dict[i][1] return result","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"}]},{"title":"imputing structured missing values in spatial data with clsutered adversarial matrix factorization","slug":"imputing structured missing values in spatial data with clsutered adversarial matrix factorization","date":"2019-04-12T15:55:11.077Z","updated":"2019-04-17T11:46:55.177Z","comments":true,"path":"2019/04/12/imputing structured missing values in spatial data with clsutered adversarial matrix factorization/","link":"","permalink":"http://Bithub00.com/2019/04/12/imputing structured missing values in spatial data with clsutered adversarial matrix factorization/","excerpt":"一种基于对抗模型用于补全带有结构性缺失信息的空间数据的矩阵分解技术","text":"一种基于对抗模型用于补全带有结构性缺失信息的空间数据的矩阵分解技术 &#8194; 摘要：在数据分析时，缺失的数据总是会成为一个重大的挑战，因为它带来了不确定性。在许多领域中，矩阵补全技术有着出色的表现。然而，在特定的空间数据集如地理坐标点时，这种传统的矩阵补全技术有着两个主要的限制：第一，这些方法往往假设缺失的数据是随机产生的，而这种假设对于空间数据集可能并不总是成立；第二，它们可能无法运用这些空间数据集中的结构信息。为了解决这些局限性，本论文提出了一种利用先验结构信息和生成对抗模型的矩阵分解技术。这个模型使用一个对抗网络通过学习数据集的概率分布来改善补全的结果。 关键词：缺失数据估计；深度对抗网络；空间数据 前言&#8194; 很多现实生活中的应用容易面临数据缺失的问题。而对于空间数据集，造成这种情况的原因有很多种。例如，在森林监测中，因为收集成本的原因，数据的缺失很普遍。【3】过去十年许多针对数据补全的工作在开展，从基本的统计方法到复杂的模型使用。后者的典例低秩矩阵补全技术为许多领域如推荐系统或图像重构带来了可观的改善【9】。这些方法通过发现并利用数据矩阵的低秩属性来建立已有值与缺失值的联系。而在这些矩阵补全方法里，矩阵分解是最为常用的方法之一，它将输入的数据矩阵分解成两个低秩矩阵的乘积，称为“特征因子”，接着通过最小化这个乘积与已有值的误差来学习这两个特征因子，随后利用它们来补全缺失的信息。【18】其它方法还有如带门槛的矩阵奇异值分解，核心思想是迭代地使用截断奇异值分解来补全。 &#8194; 这些矩阵补全的方法，往往假设缺失的数据是随机产生的。【2】然而，这个假设在空间数据上可能并不成立，因为它往往带有空间结构。例如，一项针对加拿大青少年的研究指出，家庭收入这一栏数据空缺的青少年有更低的可能性居住在富人区。【15】因此，当数据并不是随机缺失时，只是单纯地最小化两个特征因子的乘积与已有值的误差并不能保证补全数据的有效性。 &#8194; 而另一个限制则是这些方法无法把数据集里的结构信息利用起来。而在补全缺失的空间数据时，这些结构信息格外重要。【12】例如，淡水湖数据就有强烈的空间结构，因为相邻的湖泊往往有相似的降水量等。【17】如果这些结构信息能够被一个矩阵补全的方法利用起来，它可以显著地提升结果，因为这些结构信息代表了一个子空间，在这个子空间里，不同湖泊之间相似的信息互相传递。 &#8194; 因为为了解决这两点局限，我们提出了一种利用先验结构信息和生成对抗模型的矩阵分解技术。这个框架找到一个低维的子空间来与数据中的结构信息相符合，因此可以利用同一类中其它数据点的信息来补全某一点的缺失值。而且，估计值的概率分布也尽可能的与已有值相似，这么做的好处是它把缺失值与已有值连接起来了。如果估计值与实际值偏差太大，那么它出现的概率应该很小。然而，实际数据的概率分布往往是未知的，因此我们借鉴了生成对抗网络的思想引入了一个判别器来区分估计值与已有值。我们在合成数据集与显示数据集上均做了实验，来说明这个框架的有效性。 相关工作&#8194; 截断奇异值算法是近年来使用频率较高的一个方法，它在数据矩阵中迭代的使用截断奇异值分解接着通过保持一个较小的奇异值重构整个矩阵。矩阵分解是另一项常用的技术，关于它的过程前面部分已有讲述。 &#8194; 生成对抗网络（GAN）被广泛地用于生成图像【5】【16】。在【8】【14】中，作者提出了一个想法，利用GAN的思想和整幅图片的结构来推测一幅图片中随机缺失的像素。虽然这个想法在这类问题上效果较好，但它补全图片时是将每张图片看成一个个独立的个体，而空间数据与此相反，它们之间有着强烈的依赖性。 方法A.矩阵分解引入&#8194; 矩阵分解在推荐系统中十分常用，例如如下的一个评分矩阵，列为用户，行为物品，矩阵中的值为用户对物品的评分，如电影和书籍。现实情况中这个评分矩阵往往很稀疏，许多物品上缺少用户的评分，而推荐系统就是要预估用户在某个物品上的评分来判断用户对它的倾向程度，从而进行推荐。 &#8194; 矩阵分解的方法是将原始评分矩阵$R^{m\\times n}$分解成两个矩阵$P^{m\\times k}$和$Q^{k\\times n}$，根据评分矩阵中已有的值来判断分解是否准确，而判别标准常用均方差。如图所示。 &#8194; 分解后的矩阵P和Q可以称为特征因子（latent factor），其中要求分解后$k&lt;&lt;min(m,n)$，即低秩要求，因为如果输入矩阵满秩，则各元素行之间线性无关，如果有线性相关关系，则某个元素行可以通过其他行的线性组合表示，相当于引入了冗余的信息，这样就可以将矩阵投影到更低维的空间，只保留非冗余信息，同时冗余信息可以用来对缺失值进行补全。 &#8194; 矩阵分解的直观意义为，找出矩阵中的潜在特征，如图2中假设特征为3，特征可以是书籍作者、类型等等，而矩阵P表示用户对某个特征的喜爱程度，而矩阵Q表示某个物品与该特征的关联程度。 B.低秩补全&#8194; 给定一个带有缺失值的矩阵，矩阵补全技术旨在通过已有值的某种潜在的结构来对缺失值进行估计补全。一个常用的潜在结构是矩阵的低秩性，因为它可以将该矩阵投影到一个去除冗余信息的子空间中，低秩意味着矩阵中的值存在线性关系，因此某些值可以通过另外的值来线性表示，如同坐标系中的基底一样。在这类方法中有凸也有非凸的技术。凸方法通过对矩阵迹的约束来保证具有良好理论性的全局最优结果，而诸如矩阵分解的非凸方法进行局部搜索过程并提供更大的灵活性和效率。给定一个矩阵$X\\in R^{d\\times n}$，n代表样本个数，d代表特征维度，矩阵分解技术通过将X分解为两个矩阵U和V，$U\\in R^{d\\times n}$，$V\\in R^{r\\times n}$要求$r &lt; min(d, n)$；U和V的求解可以通过对下列式子运用块坐标下降法求得： &#8194; $\\bigodot$代表哈德蒙德内积（即矩阵各元素相乘），M矩阵的大小同X一致，如果$X_{ij}$有值则$M_{ij}$为1，否则为0。局部解用$U^、V^$表示，因此，它们可以通过如下的式子来重构矩阵X： &#8194; 矩阵分解在推荐系统中使用较为普遍，它用来估计一个用户在某项新物品上的评分。 &#8194; 将此方法应用于空间数据集时，矩阵分解不会包含有关数据集中空间聚类结构的先验知识信息。 然而，这些先验知识通常有助于发现需要的子空间。此外，对于结构化缺失值问题，经典矩阵补全提供较差的结果，因为缺失值不是随机的。 C.聚类对抗式矩阵分解&#8194; 为了解决上一小节中提到的矩阵分解的两个局限性，我们提出了一种新的聚类对抗矩阵分解框架。 在我们的框架中，我们找到整个样本的聚类信息，并将补全值的概率分布与已有值的分布靠近，以得到可靠的补全结果。X为输入矩阵，每一列代表一个数据样本，有些数据点有完整的特征信息，而某些数据点以结构性缺失了某些特征信息。我们将输入矩阵中完整部分记为$X_n$，而缺失部分记为$X_m$。我们假设每个数据点都符合某个概率分布$p_{data(x)}$。接下来的公式中包含两个部分：矩阵重构以及概率分布近似。 矩阵重构&#8194; 为了利用数据的低秩属性和空间聚类结构，我们决定在矩阵分解中使用l2聚类项： &#8194; 式中$v_i$代表矩阵V中第i列，$r_1,r_2,r_3$均为正则化参数，第二项和第三项加的约束是为了防止过拟合，最后一项则用来引入空间数据集中的聚类结构信息。$d_{ij}$是第i个样本和第j个样本的相似度，它可以手动设置，原则为：当$v_i$和$v_j$很靠近即在同一类时，将$d_{ij}$的值设置得较大，反之较小。$r_3$用来调整结构信息在重构时所占的比重，如果$r_3$较大，则对一个样本的缺失数据进行估计补全时会更多的参考同一类其它数据点的信息。因此当$r_3$为0时，结构信息将不被使用，这也使得这个式子变成了常规的矩阵分解方法。而对于UV矩阵直观的理解为，U为特征因子，而V为样本因子，如同推荐系统里的用户因子和物品因子，两者相互独立。 生成对抗网络思想（GAN）&#8194; 在继续讲到使用概率分布近似来优化前，先引入生成对抗网络的基本思想加深理解。GAN的非常的直观，就是生成器和判别器两个极大极小的博弈。 GAN的目标函数为： &#8194; 从判别器D的角度看，它希望自己能尽可能区分真实样本和虚假样本，因此希望 D(x)尽可能大，D(G(z))尽可能小，即 V(D,G)尽可能大。从生成器G的角度看，它希望自己尽可能骗过D，也就是希望 D(G(z))尽可能大，即 V(D,G)V(D,G) 尽可能小。两个模型相对抗，最后达到全局最优。 &#8194; 图中，黑色曲线是真实样本的概率分布函数，绿色曲线是虚假样本的概率分布函数，蓝色曲线是判别器D的输出，它的值越大表示这个样本越有可能是真实样本。最下方的平行线是噪声z，它映射到了x。 &#8194; 一开始， 虽然 G(z)和x是在同一个特征空间里的，但它们分布的差异很大，这时，虽然鉴别真实样本和虚假样本的模型 D性能也不强，但它很容易就能把两者区分开来，而随着训练的推进，虚假样本的分布逐渐与真实样本重合，D虽然也在不断更新，但也已经力不从心了。 &#8194; 最后，黑线和绿线最后几乎重合，模型达到了最优状态，这时 判别器的输出对于任意样本都是 0.5。 GAN的最优化&#8194; 在建立好理论框架后，需要对所需要的生成器G和判别器D进行优化，在此之前先引入交叉熵的概念：它一般用来求目标与预测值之间的差距。 &#8194; 在信息论与编码中，熵可以用来衡量信息量的多少，而如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异，计算式如下： &#8194; 在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]，直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但并不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，此时Q就等价于P。 而对KL散度的计算式进行变形，可以得到： 等式的前一项即为P的熵，而后一项就是交叉熵的计算式： &#8194; 在机器学习中，我们需要评估labels和predictions之间的差距，可以使用KL散度，即$D_{KL}(y\\mid\\mid\\hat{y})$，由于KL散度中的前一部分−H(y)即P的熵不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做损失函数，评估模型。 &#8194; 在引入交叉熵后，就可以定义最优化表达式。首先我们需要定义一个判别器 D以判别样本是不是从$p_{data(x)}$分布中取出来的，因此有: &#8194; 其中E代表取期望。这一项是根据「正类」（即辨别出 x 属于真实数据data）的对数损失函数而构建的。最大化这一项相当于令判别器 D在 x 服从于 data 的概率密度时能准确地预测 D(x)=1，即： 另外一项是企图欺骗判别器的生成器 G。该项根据「负类」的对数损失函数而构建，即 因此目标函数为： 它的含义是，对于D而言要尽量使公式最大化（识别能力强），而对于G又想使它最小（生成的数据接近实际数据）。整个训练是一个迭代过程。极小极大化博弈可以分开理解，即在给定G的情况下先最大化$V(D,G)$而来得到D，然后固定D，并最小化$V(D,G)$而得到G。其中，给定 G，最大化$V(D,G)$评估了$P_g$和$P_{data}$之间的差异或距离。 概率分布近似&#8194; 接下来，论文中就使用生成对抗网络中的对抗策略来使得推算样本具有与完整数据类似的概率分布。为了实现这一目标，我们使用鉴别器来区分推算和完整样本之间的分布差异： &#8194; 其中$p_r(x_r)$代表估计值的概率分布，它将从补全的矩阵Xr中得到；$x_r$代表从$p_r(x_r)$中选取的一个数据点；D为一个鉴别器，我们通过一个以SOFTMAX为输出层的全连接的深度神经网络来实现。D将输出一个概率值，判断输入的数据为已有值还是估计值。我们使用了负交叉熵作为损失函数，通过最大化$l_d$得到一个鉴别器D，能够有效地区分已有值与估计值 完整公式将前节提到的两个部分进行合并，我们得到了如下的公式： &#8194; 其中λ是用来平衡矩阵重构与概率分布近似所占比例的一个参数，因此最小化该式时，不仅使得重构的矩阵与已有值所构成的矩阵的误差尽可能得小，同时通过鉴别器使得这两者的概率分布尽可能相似。这个同时最大最小化的要求就像是进行一场对抗。一方面，鉴别器尽可能地区别重构样本与已有样本的概率分布，而另一方面，重构矩阵又尽可能地逼近已有值的概率分布，以骗过鉴别器。因此当算法收敛时，重构矩阵的概率分布将会近似于已有值，训练出一个有效的鉴别器，同时重构矩阵的值也足够接近实际值以至于可以骗过这个鉴别器。在最小化部分中，我们求解出使得误差最小的矩阵U和V，接着使用它们来进行缺失值的计算。同时，这个部分也尽可能地让估计值去骗过鉴别器。而在最大化部分中，鉴别器通过区分已有值与最小化部分所得的估计值来进行更新，整个框架的流程如图所示。 最优化然而在实际情况中，输入样本的概率分布往往是未知的，因此，我们使用如下式子来进行近似：在每次的更新迭代中，我们随机从Xn与Xr选取k个样本，来计算概率分布： &#8194; 其中r1与rk分别代表从Xn中选取的k个样本中的第一个和最后一个，q1和qk从Xr中选取的k个样本中的第一个和最后一个，$X^i_n$和$X^i_r$分别代表从Xn与Xr中所选取的第i个样本。因此，上面的合成式将变成： 算法流程如下所示： 实验","categories":[],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://Bithub00.com/tags/machine-learning/"},{"name":"GAN","slug":"GAN","permalink":"http://Bithub00.com/tags/GAN/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"http://Bithub00.com/tags/矩阵分解/"}]},{"title":"数据库设计","slug":"database","date":"2019-04-10T08:50:19.836Z","updated":"2019-04-10T08:52:10.289Z","comments":true,"path":"2019/04/10/database/","link":"","permalink":"http://Bithub00.com/2019/04/10/database/","excerpt":"数据库设计 以下所有表默认自带一个自增 id 以下所有表默认自带 created_at 和 updated_at 两个字段 为了方便查询，以下所有下划线命名法在实际设计中可能全部转为驼峰命名法","text":"数据库设计 以下所有表默认自带一个自增 id 以下所有表默认自带 created_at 和 updated_at 两个字段 为了方便查询，以下所有下划线命名法在实际设计中可能全部转为驼峰命名法 用户信息manager 字段 类型 描述 约束 name varchar 姓名 not null card_id varchar 校园卡号 not null authorizerId int 授权人 id 外键，引用自 manager 表的 id 属性 privilege tinyint 0 为超级管理员，1为普通管理员，其余待定 not null student 字段 类型 描述 约束 card_id varchar 校园卡号 无 stu_id varchar 学号 primary key name varchar 姓名 not null college varchar 学院 无 teacher 字段 类型 描述 约束 name varchar 讲师名称 not null college varchar 所属单位 not null intro text 老师简介 无 phone varchar 手机号码 无 office varchar 办公地址 无 email varchar 邮箱 无 image_url varchar 导师照片 url 无 teach_form varchar 授课形式 无 teach_topic 字段 类型 描述 约束 teacher_id int 讲师 id 外键，引用自 teacher 表的 id 属性 topic varchar 授课专题 not null 课程信息course 字段 类型 描述 约束 course_name varchar 课程名称 not null teacher_id int 讲师 id 外键，引用自 teacher 表的 id 属性 start_time datetime 上课开始时间 not null end_time datetime 上课结束时间 not null course_id varchar 课程编号 primary key location varchar 上课地址 not null course_student 字段 类型 描述 约束 course_id varchar 课程编号 外键，引用自 course 表的 course_id 属性 stu_id varchar 学生学号 外键，引用自 student 表的 stu_id 属性 评价模板comment_template 字段 类型 描述 约束 template_name varchar 模板名称 not null template_question 字段 类型 描述 约束 template_id int 模板 id 外键，引用自 comment_template 表的 id 属性 question text 问题 not null _type tinyint 问题类型，0为打分题，1为问答题，其余待定 not null 评价信息course_comment 字段 类型 描述 约束 course_id varchar 课程编号 外键，引用自 course 表的 course_id 属性 template_id int 模板 id 外键，引用自 comment_template 表的 id 属性 comment 字段 类型 描述 约束 course_id varchar 课程编号 外键，引用自 course 表的 course_id 属性 stu_id varchar 学生学号 外键，引用自 student 表的 stu_id 属性 star tinyint 1为精选评论 无 comment_result 字段 类型 描述 约束 comment_id int 评价 id 外键，引用自 comment 表的 id 属性 question_id int 问题 id 外键，引用自 template_question 表的 id 属性 result text 问题结果，根据问题类型来决定存储值类型 not null","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://Bithub00.com/tags/数据库/"}]},{"title":"API接口设计","slug":"api","date":"2019-04-10T08:50:19.820Z","updated":"2019-04-10T08:52:14.750Z","comments":true,"path":"2019/04/10/api/","link":"","permalink":"http://Bithub00.com/2019/04/10/api/","excerpt":"API 接口设计接口地址 环境 URL 开发环境 http://like.suevily.cn/ 生产环境 待定 通用 以下接口分为开放接口和私有接口 开放接口：无需登录校验即可请求 私有接口：需校验登录 token 方可请求 以下所有接口 url 都默认自带 /api 前缀","text":"API 接口设计接口地址 环境 URL 开发环境 http://like.suevily.cn/ 生产环境 待定 通用 以下接口分为开放接口和私有接口 开放接口：无需登录校验即可请求 私有接口：需校验登录 token 方可请求 以下所有接口 url 都默认自带 /api 前缀 token 在使用统一身份验证成功登录后会附带到 redirect url 的 query 参数上，前端需自行存储，开发环境的 redirect url 为 http://localhost:8081/#/ 私有接口请求方法：将 token 附在请求 Headers 中的 Authorization 字段上，value 格式为 Bearer ${token} （PS: 建议使用 postman 进行接口测试） 前端在拿到返回数据时务必先检查 code 是否为零，如若不为零，需给用户正确的反馈，回传数据遵循以下格式： 1234&#123; code: Number, data: Object&#125; code 说明 0 成功 -1 未知错误，查看 message 1 拒绝访问 2 无效的请求参数 3 上传图片出错 4 token 校验失败 其它 待定 获取用户信息 method url type GET /user 私有接口 请求参数无 返回参数 参数 类型 说明 cardId String 校园卡号 name String 姓名 college String 学院 stuId String 学号 privilege Number 0 为超级管理员，1为普通管理员，如没有该字段则为普通师生 获取管理人员 method url type POST /getManagers 私有接口 请求参数 参数 类型 说明 必填 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 管理人员总数 managers - Array 一个包含指定页数内所有管理人员的数组 - id Number 该管理员在数据库中的唯一标识 - name String 姓名 - cardId String 校园卡号 - authorizer String 授权人姓名 - createdAt Date 添加时间戳 - privilege String 管理权限，超级管理员或普通管理员 添加管理人员 method url type POST /addManager 私有接口 请求参数 参数 类型 说明 必填 name String 姓名 是 cardId String 校园卡号 是 privilege Number 权限，0 为超级管理员，1为普通管理员 是 返回参数 参数 类型 说明 message String 添加结果 错误代码 code 说明 1001 校园卡号冲突 修改管理人员 method url type POST /updateManager 私有接口 请求参数 参数 类型 说明 必填 id Number 要修改的管理员的id 是 name String 姓名 是 cardId String 校园卡号 是 privilege Number 权限，0 为超级管理员，1为普通管理员 是 返回参数 参数 类型 说明 message String 修改结果 错误代码 code 说明 1002 当你要把最后一个超级管理员更改为普通管理员时会出错 删除管理人员 method url type POST /deleteManager 私有接口 请求参数 参数 类型 说明 必填 id String 要删除的管理员的id 是 返回参数 参数 类型 说明 message String 删除结果 错误代码 code 说明 1003 当你要把最后一个超级管理员删除时会出错 搜索管理人员 method url type POST /searchManagers 私有接口 请求参数 参数 类型 说明 必填 keyword String 搜索关键词 是 maxLength Number 搜索结果最大返回数目（默认为5） 否 返回参数 参数 字段 类型 说明 managers - Array 满足搜索关键词的管理员数组 - id Number 该管理员在数据库中的唯一标识 - name String 姓名 - cardId String 校园卡号 - authorizer String 授权人姓名 - createdAt Date 添加时间戳 - privilege String 管理权限，超级管理员或普通管理员 获取课程信息 method url type POST /getCourses 私有接口 请求参数 参数 类型 说明 必填 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 课程总数 courses - Array 一个包含指定页数所有课程信息的数组 - courseName String 课程名称 - teacher String 讲师姓名 time String 上课时间，前端直接展示即可 courseId String 课程编号 location String 上课地点 studentCount Number 选课人数 根据课程编号获取课程详细信息 method url type POST /getCourseInfo 私有接口 请求参数 参数 类型 说明 必填 courseId String 课程编号 是 返回参数 参数 字段 类型 说明 courseName - String 课程名称 teacherId - String 讲师在数据库中的id teacherName - String 讲师姓名 startTime - Date 上课开始时间戳 endTime - Date 上课结束时间戳 location - String 上课地点 students - Array 一个包含指定课程选课所有学生信息的数组 name String 学生姓名 stuId String 学生学号 错误代码 code 说明 2003 课程编号错误 查询选课名单 method url type POST /getStudentList 私有接口 请求参数 参数 类型 说明 必填 courseId String 课程编号 是 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 选课总人数 students - Array 一个包含指定课程选课所有学生信息的数组 name String 学生姓名 stuId String 学生学号 删除课程 method url type POST /deleteCourse 私有接口 请求参数 参数 类型 说明 必填 courseId String 要删除的课程的课程编号 是 返回参数 参数 类型 说明 message String 删除结果 搜索课程 method url type POST /searchCourses 私有接口 请求参数 参数 类型 说明 必填 keyword String 搜索关键词 是 maxLength Number 搜索结果最大返回数目（默认为5） 否 返回参数 参数 字段 类型 说明 courses - Array 满足搜索关键词的课程数组 - courseName String 课程名称 - teacher String 讲师姓名 time String 上课时间，前端直接展示即可 courseId String 课程编号 location String 上课地点 studentCount Number 选课人数 生成课程 method url type POST /addCourse 私有接口 请求参数 参数 字段 类型 说明 必填 courseName - String 课程名称 是 teacherId - String 讲师在数据库中的id 是 startTime - Date 上课开始时间戳 是 endTime - Date 上课结束时间戳 是 location - String 上课地点 是 students - Array 选课学生数组 是 - name String 学生姓名 是 - stuId String 学生学号 是 返回参数 参数 类型 说明 courseId String 生成的课程编号 错误代码 code 说明 2001 讲师不存在 修改课程 method url type POST /updateCourse 私有接口 请求参数 参数 字段 类型 说明 必填 courseId - String 课程编号（乱传打爆你▄︻┻═┳一） 是 courseName - String 课程名称 是 teacherId - String 讲师在数据库中的id 是 startTime - String 上课开始时间戳 是 endTime - String 上课结束时间戳 是 location - String 上课地点 是 students - Array 选课学生数组 是 - name String 学生姓名 是 - stuId String 学生学号 是 返回参数 参数 类型 说明 message String 修改结果 错误代码 code 说明 2001 讲师不存在 2002 课程编号不存在 获取讲师信息 method url type POST /getTeachers 私有接口 请求参数 参数 类型 说明 必填 perPage Number 每页包含元素个数 是 page Number 页码，第几页 是 返回参数 参数 字段 类型 说明 length - Number 讲师总数 teachers - Array 一个包含指定页数所有讲师信息的数组 - id Number 该讲师在数据库的唯一标识 - name String 讲师姓名 imageUrl String 讲师照片的 url 删除讲师 method url type POST /deleteTeacher 私有接口 请求参数 参数 类型 说明 必填 id Number 讲师 id ，在 /getTeachers 接口获取到的 是 返回参数 参数 类型 说明 message String 删除结果 搜索讲师 method url type POST /searchTeachers 私有接口 请求参数 参数 类型 说明 必填 keyword String 搜索关键词 是 maxLength Number 搜索结果最大返回数目（默认为5） 否 返回参数 参数 字段 类型 说明 teachers - Array 满足搜索关键字的讲师数组 id Number 该讲师在数据库的唯一标识 - name String 讲师姓名 imageUrl String 讲师照片的 url 查询一个讲师的详细信息 method url type POST /getTeacherInfo 私有接口 请求参数 参数 类型 说明 必填 id Number 讲师 id ，在 /getTeachers 接口获取到的 是 返回参数 参数 字段 类型 说明 name - String 讲师姓名 college - String 所属单位 intro - String 老师简介 phone - String 手机号码 office - String 办公地址 email - String 邮箱地址 imageUrl - String 导师照片 url teachForm - String 授课形式 teachTopic - Array 该讲师授课专题的数组 - String 授课专题 图片上传接口 method url type POST /upload 私有接口 请求参数 参数 类型 说明 必填 - FormData 图片数据 是 返回参数 参数 类型 说明 url String 图片 url 添加讲师 method url type POST /addTeacher 私有接口 请求参数 参数 字段 类型 说明 必填 name - String 讲师姓名 是 college - String 所属单位 是 intro - String 老师简介 是 phone - String 手机号码 是 office - String 办公地址 是 email - String 邮箱地址 是 imageUrl - String 导师照片 url 是 teachForm - String 授课形式 是 teachTopic - Array 该讲师授课专题的数组 是 - String 授课专题 是 返回参数 参数 类型 说明 id Number 该讲师在数据库的唯一标识 修改讲师信息 method url type POST /updateTeacher 私有接口 请求参数 参数 字段 类型 说明 必填 id - Number 要修改的讲师的 id 是 name - String 讲师姓名 是 college - String 所属单位 是 intro - String 老师简介 是 phone - String 手机号码 是 office - String 办公地址 是 email - String 邮箱地址 是 imageUrl - String 导师照片 url 是 teachForm - String 授课形式 是 teachTopic - Array 该讲师授课专题的数组 是 - String 授课专题 是 返回参数 参数 类型 说明 message String 修改结果 错误代码 code 说明 3001 指定 id 的老师不存在","categories":[],"tags":[{"name":"API","slug":"API","permalink":"http://Bithub00.com/tags/API/"},{"name":"Node.js","slug":"Node-js","permalink":"http://Bithub00.com/tags/Node-js/"}]},{"title":"AWS:S3 + Athena + Glue","slug":"AWS","date":"2019-04-03T09:52:53.423Z","updated":"2019-04-10T10:52:05.753Z","comments":true,"path":"2019/04/03/AWS/","link":"","permalink":"http://Bithub00.com/2019/04/03/AWS/","excerpt":"整理一下自己了解的S3、Athena和Glue","text":"整理一下自己了解的S3、Athena和Glue S3AWS使用S3（Simple Storage Service）进行存储，它可以存储海量的数据，存储的往往是不常使用的冷数据，而且采取特定的Parquet格式进行列式存储或分区，可以节省存储空间而且提升查询性能。 使用S3存储动态数据会使得系统依赖于S3本身的可用性，可以通过增加缓存层或CDN(Content Deliver Network)服务来减轻影响。 S3提供REST接口,两个组成层次是容器(bucket)和对象(object)。容器是S3最上面的分类，所有的对象都放在容器里，因此容器名称必须是唯一的，每个对象都有唯一可识别的URL，应该尽量避免对容器进行操作，使用对象名称就可以实现分层次和分类了. 存储在S3的对象，除了本身的值以外,还会记录标头、元数据、访问控制列表等等，标头中存储了对象类型等信息，元数据就是用户自己定义的表头，为键值对，访问控制列表就是访问权限。因为S3有所谓偷窥对象的功能，即只读取对象的标头的信息，我们可以先看标头信息，再决定要不要把对象读取下来。 S3没有目录的概念，是扁平化的存储结构，“photo/1.jpg”和“photo/2.jpg”可能存在于不同的服务器集群。 AthenaAthena是一个查询服务，可以使用标准的SQL来对S3上存储的数据进行查询。而且它是一个serverless的服务，不需要去考虑底层的硬件设施，只需要为查询服务付费。同时，Athena使用IAM来管理权限，部分操作需要对应的权限才能进行。 Athena使用SerDe来与各种数据格式进行交互，包括CSV,JSON和Parquet。在使用时进行指定。 GlueGlue是一个元数据系统，它维护了信息诸如数据具体存储的位置以及数据的结构，它本身还提供了ETL的能力。Glue里面几个关键的概念是Database, Table, Crawler, Classifier, Job: Database 跟我们普通理解的数据库的概念是类似的，是一组table的逻辑集合。 Table 是数据的元数据，它定义数据保存在哪里(比如S3的路径)，有哪些column，怎么分区的。 Crawler 是元数据的爬虫，你给它一个路径，告诉它每天去爬一次，Crawler就可以及时把更新的元数据，比如新增的分区同步到Glue里面来供计算引擎消费。 Classifier 是数据结构的解析器，你给Crawler一个S3的路径它怎么就能解析出其中的结构呢，这就是Classifier要干的事情，Glue里面已经内置了一些Classfier, 用户也可以自定义Classifier。 Job是一个ETL脚本 Glue与AthenaAWS Glue 是一项完全托管的 ETL (提取、转换和加载) 服务，能够对数据进行分类、清理和扩充，并在各种数据存储之间可靠地移动数据。AWS Glue 爬网程序自动从源数据推断数据库和表架构，从而将关联的元数据存储在 AWS Glue 数据目录中。在 Athena 中创建表时，可以选择使用 AWS Glue 爬网程序创建表。 实例 使用Glue读取csv并转换成Parquet格式随后使用Athena查询 Glue自定义分类器识别服务器日志 How to extract, transform, and load data for analytic processing using AWS Glue (Part 2) In Search of Happiness: A Quick ETL Use Case with AWS Glue + Redshift Athena与S3Athena 可帮助分析在 Amazon S3 中存储的非结构化、半结构化和结构化数据。包括 CSV、JSON 或列式数据格式，如 Apache Parquet 和 Apache ORC。可以使用 ANSI SQL 通过 Athena 运行临时查询，而无需将数据聚合或加载到 Athena 中。","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://Bithub00.com/tags/AWS/"},{"name":"S3","slug":"S3","permalink":"http://Bithub00.com/tags/S3/"},{"name":"Athena","slug":"Athena","permalink":"http://Bithub00.com/tags/Athena/"},{"name":"Glue","slug":"Glue","permalink":"http://Bithub00.com/tags/Glue/"}]},{"title":"L1与L2范数","slug":"范数","date":"2019-04-03T04:59:52.310Z","updated":"2019-04-10T10:52:27.435Z","comments":true,"path":"2019/04/03/范数/","link":"","permalink":"http://Bithub00.com/2019/04/03/范数/","excerpt":"看到的一篇很好的介绍L1与L2范数的文章，mark下来","text":"看到的一篇很好的介绍L1与L2范数的文章，mark下来","categories":[],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://Bithub00.com/tags/machine-learning/"},{"name":"data-mining","slug":"data-mining","permalink":"http://Bithub00.com/tags/data-mining/"}]},{"title":"安卓Activity移植","slug":"安卓Activity移植","date":"2018-09-07T04:31:55.617Z","updated":"2018-09-07T05:00:48.426Z","comments":true,"path":"2018/09/07/安卓Activity移植/","link":"","permalink":"http://Bithub00.com/2018/09/07/安卓Activity移植/","excerpt":"这篇文章记录一下之前将写的一个Activity加入现有应用时遇到的坑","text":"这篇文章记录一下之前将写的一个Activity加入现有应用时遇到的坑 找不到R&#8194;这个问题移植完后马上就会遇到，在将layout里的xml文件等都复制过来后，如果问题还存在，直接Android Studio中 Build-Clen Project 一次，问题基本上就解决了 包名&#8194;形如com.package.XXXXX等问题，复制代码文件时IDE会自动帮我们修正，但布局文件中IDE是不会帮我们纠正过来的,例如&quot;tools:context=&quot;.MainActivity&quot;&quot; 就要进行修改，或者在标签中出现com.package.XXXX没有修改的情况，只能一个个xml文件去找了 Application&#8194;如果在移植的Activity中继承了Application类创造了自己的，那移植过去后一定要在AndroidManifest.xml文件的\\里的android:name中进行修改 注册&#8194;最后就是一直过去后要在AndroidManifest.xml文件中添加移植过去的Activity，不然运行时会报错，Logcat中也会给出建议","categories":[],"tags":[{"name":"Android","slug":"Android","permalink":"http://Bithub00.com/tags/Android/"}]},{"title":"SIMLR算法论文个人翻译","slug":"SIMLR算法","date":"2018-07-25T12:00:18.373Z","updated":"2019-07-25T03:05:58.254Z","comments":true,"path":"2018/07/25/SIMLR算法/","link":"","permalink":"http://Bithub00.com/2018/07/25/SIMLR算法/","excerpt":"基于多核函数的单细胞RNA序列分析与可视化算法 &#8194; 摘要：我们将要介绍的SIMLR算法，在理论框架和软件应用上已经实现，它从输入的单细胞RNA序列中学习到一种相似度的信息，随后可以用这种信息来实现降维、聚类和可视化等后续处理。在七个公开的数据集上我们对算法进行了检验并于一些常用的算法进行了比较分析。结果证明了SIMLR算法对大规模数据的良好处理以及极大程度上提高了聚类的效果，同时提升了可视化的效果以及对细胞间相似程度的识别。","text":"基于多核函数的单细胞RNA序列分析与可视化算法 &#8194; 摘要：我们将要介绍的SIMLR算法，在理论框架和软件应用上已经实现，它从输入的单细胞RNA序列中学习到一种相似度的信息，随后可以用这种信息来实现降维、聚类和可视化等后续处理。在七个公开的数据集上我们对算法进行了检验并于一些常用的算法进行了比较分析。结果证明了SIMLR算法对大规模数据的良好处理以及极大程度上提高了聚类的效果，同时提升了可视化的效果以及对细胞间相似程度的识别。关键词：多核函数；低秩约束；单细胞RNA序列 背景介绍及概览&#8194; 此前，对单细胞RNA序列的研究揭示了细胞种群间未被洞悉的异构性与功能分化。近期的研究阐释说明了通过对RNA序列的无偏分析，是有可能做到对细胞子群功能特异性的de novo分析的。然而，大部分用来应用的算法都是针对传统的大规模RNA序列数据，而基因的表达结果在一群细胞中被平均化了。这些算法并不能成功地处理如下问题：如噪声数据、离散点以及dropout现象（获取基因表达数据时未能成功识别基因表达结果而标记为0，而实际情况是基因进行了表达）。诸如DropSeq和GemCode的平台已经显著增加了数千个细胞的细胞信息，然而，这类平台产生的多为稀疏数据，其中95%的基因测量结果被标记为0。对于诸如降维、聚类以及数据可视化等无监督学习的方法来说，其中一个关键就是相似度矩阵的学习，而这个矩阵对于不同平台或者生物实验得到的数据并不通用。为了解决上述问题，我们提出了SIMLR算法，一个从输入的单细胞RNA序列数据中学习细胞与细胞之间相似度矩阵的框架。 &#8194; 相较于传统的算法，SIMLR算法有三个主要的优势：首先，它通过使用多个核函数的方法来学习一个最符合输入数据的结构的相似性矩阵。常规的降维或聚类算法对数据的假设有时并不适用于单细胞RNA序列数据。而多个核函数被证实在描绘数据多角度的信息下有着更好的效果，而且相对于单个核函数具有更好的灵活性。第二，对于高维度下的dropout现象，SIMLR算法通过对学习的相似度矩阵应用秩约束以及使用图扩散的方法来解决。秩约束的应用增强了相似度矩阵的分块对角结构，而图扩散方法提高了对弱相似度的识别。第三，算法习得的相似度矩阵可以被高效的用来后续的数据分析，比如通过SNE算法进行数据在低维空间下的可视化。 &#8194; 我们通过在四个公开的单细胞数据集上应用SIMLR算法来与传统的算法比较，结果是SIMLR算法习得的相似度矩阵在表现数据相似度上要表现得更好。每个数据集中细胞所属的种类是先验的并且在研究中已经被证实正确。通过输入数据集和细胞的种类数，SIMLR算法就能学习出一个细胞间的相似度矩阵，而不需要输入细胞真实所属类别的标签信息。而且相较于传统的相关系数或欧几里得距离衡量相似度，SIMLR算法的结果要更加接近真实结果。特别的是，Buettner数据集的真实标签是细胞周期的状态，我们在这个数据集上额外应用了SIMLR算法来对基因进行排序。算法输出一个基因网络，来展现不同的基因在细胞各个周期、翻译以及代谢过程中的相关性。 &#8194; 图表1：SIMLR算法的总览。给定一个基因表达数据的矩阵作为输入，算法构建并学习多个核函数之间的权重，并构建出一个相似度矩阵。给定细胞种类数C作为输入，构建出的相似度矩阵将有一个含C个分块的分块对角矩阵，每个分块中的细胞都更为相似。算法使用得到的相似度矩阵来进行降维、可视化、聚类等后续处理。实心箭头代表了需要被用来构建相似度矩阵的信息，而点线箭头则表明聚类的结果可以与可视化以及基因排序相结合。2D散点图中的每一个点代表一个细胞，而不同的颜色代表细胞所属的不同类别。 &#8194; 为了分析算法在降维上的效果，我们与8个传统的降维算法进行了比较，包括主成分分析、tSNE以及ZIFA算法。在六种不同的表现算法优劣矩阵中，SIMLE算法在四个公开的数据集上表现出色，并且远远的拉开了与第二名的差距。 &#8194; 我们还进行了低维数据的可视化实验。结果表明，在各个数据集上SIMLR算法的结果不仅能吻合数据集给出的真实标签，甚至在秩约束有关的参数没有贡献时同样能保持相似度矩阵的分块对角结构。特别地，在Kolodziejczyk这个数据集上，我们还从SIMLR算法的结果上发现，在已知的分类结果上其实还能继续往下细分，这个结果也符合与这个数据集有关的理论研究。 &#8194; SIMLR算法同样可以用来进行细胞聚类，通过降维后应用k-means算法或者直接对习得的相似度矩阵使用AP近邻算法来实现。后者的表现性要远远超过使用皮尔逊相关系数或欧几里得距离来衡量相似性的方法。而前者的表现性在四个数据集上也比现有的针对单细胞的聚类算法要更好。 &#8194; 为了检验算法的能力，我们应用了更多更有挑战性的方案。我们分析了一个GemCode平台上提供的周边血液单核球细胞的稀疏数据集，里面包含了2700个细胞且其中95%的基因表达结果被标记为0。通过降维后应用k-means算法，我们识别出八种主要的细胞类别，包括一个只含12个细胞的megakaryocyte种类。除此之外，我们还在不同的已经得到充分研究的数据集上试验了SIMLR算法的表现性。 &#8194; 为了说明SIMLR算法在大规模数据上的表现性，我们在三个公开的大规模数据集上进行了试验。我们对真实标签与算法输出的预测表情的相关性进行了计算。对于Zeisei数据集，我们应用了一个二级聚类的方法，发现SIMLR算法可以用来进行在对细胞的层次结构的分析。而且，低维可视化的结果也很好的符合了真实的数据标签。即使是大规模数据集中因为噪声和离散点所造成的相似信息被隐藏的情况，SIMLR算法也能学习一个合适的细胞之间的距离。 &#8194; 图表2：在不同数据集上的测试结果。分别给出了SIMLR算法、基于高斯核的欧几里得距离以及皮尔逊相关系数所得到的相似度矩阵。排放的次序基于相似度的高低。矩阵中的细胞都按照真实的种类来进行排列，使得同一种类的细胞排列在一起，坐标轴上的不同颜色代表不同的种类。可以看出，SIMLR算法的相似度矩阵的分块对角结构与真实标签基本符合。&#8194; &#8194; 总的来说，SIMLR算法可以基于不同的数据集通用地判断那些细胞更为相似，即判断结果不受特定数据集影响，并应用降维、聚类、数据可视化等分析方法。SIMLR算法在有着清晰分类的数据集上表现出色，而我们预测这个多核学习的框架在分类不明显的数据集上也会同样产生作用。 &#8194; 图表3： 2维可视化结果的比较。坐标轴无实际意义。每个点代表一个细胞，而靠得越近的点代表相似度越高。数据可视化时没有输入真实的标签信息，在算法的输出时才让各个数据点标上真实的颜色以检验算法的效果。 算法详述：算法的具体实现以及数据的可靠性：SIMLR算法提供了Matlab和R语言两个版本的实现（https://github.com/BatzoglouLabSU/SIMLR） &#8194; k-means算法的实现我们使用了Matlab和R语言自带的模块。而SNE算法我们修改了两个语言中这一模块的源代码。四个公开的数据集随着源代码一起被提供。而三个大型的数据集可以在相应的平台上得到。输入一个N×M的的基因表达矩阵，N代表细胞个数，M代表基因数。SIMLR算法将输出一个S×S的相似性矩阵。其中Sij表示两个细胞之间的相似度。给定一个细胞种类数C，算法假定输出的相似度矩阵将有一个含C个分块的分块对角矩阵，各个分块中的细胞更为相似。我们对两个细胞之间的距离定义为： 其中wl代表核函数的权重 算法依据如下损失函数来计算细胞与细胞之间的相似性： &#8194; 其中IN和IC分别为N×N和C×C的单位矩阵，tr(.)代表矩阵的秩，β和γ均为非零值，||S||F为范数表示，L为辅助的用来对S进行低秩约束。因此这个损失函数求解三个参数：相似度矩阵S，核函数权重向量w以及一个结构为N×C的秩约束矩阵L。 &#8194; 损失函数的第一项含义为，如果两个细胞间的距离很远，则它们的相似度应该很低。第二项是一个对S的正则化，防止S矩阵过于接近一个单位矩阵。如果细胞可以被划分为C类，则每一类中的细胞更为相似，理想情况下矩阵S的秩为C。因此，损失函数的第三项以及L矩阵的引入增强了S的低秩结构，而矩阵（IN-S）即为拉普拉斯矩阵，在一个相似图中，每个节点代表一个细胞，边衡量节点间的相似性。第四项对核函数的权重进行约束，防止单核函数情况的出现。实践证明，这个正则化项提高了相似矩阵的表现。 核函数的构建：我们以带有不同超参数的高斯核为基础构建不同的核函数，实践证明相对其它核函数高斯核的表现更好。 式中||ci - cj||表示细胞i和j之间的欧几里得距离。 方差ɛij的定义式如下：&#8194; 因此，每一个核函数被一对参数(σ，k).我们设定k = 10，12，14,…,30 以及σ = 1.0，1.25，1.5，1.75，2，产生了55个不同的核函数。然而，实践证明，算法对核函数的数量以及参数的选择并不敏感。### 初始化：#### 核函数的权重w被初始化为核函数数量的倒数：#### 相似度矩阵S被初始化为：#### 而矩阵L被初始化为拉普拉斯矩阵（IN - S）的前C个特征向量。### 优化算法：我们对S,L和w进行优化。上文中的优化式非凸，但固定某两个参数得到的目标函数为凸函数。因此我们可以有效的应用凸优化算法来进行求解。### 步骤1：固定L和w对S进行更新。损失函数可以被重写为：&#8194; 目标函数中第一项求和式以及约束项均为线性，而第二项是一个二次项，它可以在多项式复杂度的时间内计算出来。### 步骤2：固定S和w对L进行更新。损失函数可以被重写为：&#8194; 此时对L矩阵的求解就是拉普拉斯矩阵（S-IN）对应的C个最大特征值的特征向量。### 步骤三：固定S和L对w进行更新。同样地，损失函数可以被重写为如下形式：&#8194; 对于这样一个包含凸函数和线性约束的问题，任何一个凸优化算法都可以进行求解。### 步骤四：基于扩散方法的相似度矩阵优化。我们应用了一种扩散方法来减少噪声和dropout现象对S矩阵的影响。给定矩阵S，我们构建如下形式的过渡矩阵P：&#8194; Ak(i)代表一个集合，里面包含了细胞i的k个近邻细胞的索引。构建出来的过渡矩阵是稀疏的，并且保留了极大部分的相似度结构。算法的更新方法如下所示：&#8194; H(0)ij = Sij作为输入，而最终迭代出来的结果Hij作为新的相似值Sij。这个额外的扩散方法将会很大程度上避免单细胞RNA序列数据中的噪声值所带来的影响。然而，因为这个算法的高计算复杂度，在面对大规模数据集时它无法发挥有效作用。&#8194; SIMLR算法重复步骤1-4直到算法收敛。随后使用得到的相似度矩阵S进行后续分析： 后续分析降维处理：&#8194; 算法基于SNE算法进行降维，并进行了调整。不同点在于，tSNE算法基于高斯核来计算高维度空间下数据之间的相似度，随后将其映射到低维空间并保留这个相似度信息。我们没有选择直接输入基因表达矩阵而是输入了相似度矩阵S。 可视化：&#8194; 我们使用降维算法来投影到二维或三维空间进行可视化。如k-means聚类，我们将维度降到B维，得到一个N×B相应的矩阵Z，随后应用k-means算法来对于细胞进行聚类。B的值与输入的C的值相同。C同时也是上文提到的秩约束的参数。 基于相似度矩阵的基因排序：&#8194; 我们通过计算某个基因在不同细胞中表达的值与习得的相似度的相关程度来对基因进行排序。给定相似度矩阵S和某个基因在所有细胞中的表达结果f，表达式如下： &#8194; 这是一个经典的用来衡量基因和相似度之间相关性的无监督特征排序算法。表达式的值越高，则基因在不同细胞中的表达越重要。然而，表达式对相似度矩阵中的噪声值非常敏感。为了克服这个问题，我们随机选取一定比例的细胞（如细胞总数的80%），随后根据剩余细胞的相似度矩阵来对基因进行排序。 大规模数据集上的应用：&#8194; 我们在含有数万个细胞的数据集上进行了试验，关键因素在于用KNN相似度来近似于细胞的相似度。第一步，我们采用了目前更为先进的近邻搜索算法ANNOY (https://github.com/spotify/annoy)，ANNOY算法认为，一个近邻点的近邻也可能是一个近邻点。因此，在构建出KNN图后，算法只更新每个细胞所预先选定的前k个近邻点。因为得到的相似度矩阵是稀疏的，我们使用Spectra (http://yixuan.cos.name/spectra/)来对L进行求解。当我们按照这种方式而不是进行涉及到矩阵求逆运算的闭式求解，我们只需要在有限次的迭代中就可以得到一个结果。 &#8194; 在我们得到相似度矩阵后，我们就可以进行细胞可视化和细胞聚类了。聚类时，从t-SNE算法中获得嵌入的低维空间的过程的计算量很大。相反，我们采用了一种谱聚类算法，它基本上等同于我们的SIMLR算法中对矩阵L应用k均值。这种简单的算法对稀疏相似性的聚类非常有效，并可扩展到数以万计的细胞中。对于可视化，由于我们仅将细胞到细胞的相似性映射到二维或三维空间，因此应用t-SNE算法在计算上仍是可行的。我们对tSNE算法中的Barnes–Hut算法进行了调整。 数据来源：&#8194; 我们在本文中使用了七个单细胞RNA-seq的数据集。前四个数据集每个包含少于1,000个细胞，而最后三个数据集包含数千到数万个细胞。下面是所有单细胞RNA-seq数据集的详细描述。 &#8194;（1）11个细胞群，包括神经细胞和血细胞（Pollen数据集）。该数据集旨在测试低覆盖率单细胞RNA-seq在鉴别不同细胞群体方面的效用，因此含有多种细胞类型的混合物：皮肤细胞，多能干细胞，血细胞和神经细胞。该数据集包括在高深度和低深度处测序的样本。我们分析了高深度样本，每个样本的平均测序数为890万。 &#8194; （2）具有感觉亚型的神经元细胞（Usoskin数据集）。该数据集包含来自小鼠背根神经节的622个细胞，每个细胞平均有114万个读段。作者将细胞分为四种神经元类型：肽能伤害性伤害感受器，非肽能伤害性伤害感受器，含神经丝，含酪氨酸羟化酶。 &#8194; （3）不同细胞周期阶段的胚胎干细胞（Buettner数据集）。该数据集来自对照研究，该对照研究量化了细胞周期对个体小鼠胚胎干细胞（mESC）中基因表达水平的影响。对于182个细胞中的每一个细胞，获得平均五十万个读数，并且至少20％的读数被定位于mm9小鼠基因组上的已知外显子。使用荧光激活细胞分选将细胞分选为细胞周期的三个阶段，并且使用金标准Hoechst染色对它们进行验证。 &#8194; （4）不同环境条件下的多能细胞（Kolodziejczyk数据集）。该数据集是从干细胞研究中获得的，研究不同培养条件如何影响mESC的多能状态。该研究从涉及三种不同培养条件的九个不同实验中量化了704个mESC中约10,000个基因的表达水平。每个细胞平均获得900万个读数，超过60％的读数映射到小家鼠基因组上的外显子。 &#8194; （5）具有39个亚型的小鼠视网膜细胞（Macoskco数据集）。通过基于液滴的高通量技术Drop-seq获得，该数据集包括44,808个单元的UMI（3端）计数（由其定制的计算管道识别）。细胞类型通过PCA和基于密度的聚类进行分类，并且通过差异基因表达进行验证。根据原始处理程序，我们过滤掉少于900个基因的细胞（涉及到11,040个细胞）用于无监督分析。 &#8194; （6）来自一个健康人类的PBMCs数据集（PBMC68k数据集）。通过GemCode平台生成scRNA-seq文库，这是一种基于液滴的高通量技术，以及具有UMI（3’端）计数的68,560个细胞通过其定制的计算流水线来识别。这种细胞群包括健康人体内的主要免疫细胞类型。 &#8194; （7）使用独特的分子识别（UMI）分析和3’端计数收集来自小鼠皮质和海马的细胞（Zeisel数据集）。收集来自小鼠脑的3,005个细胞，并且通过分级双聚类鉴定了47个亚型，并通过基因标记进行了验证。 &#8194; 对于以上涉及到的数据集，我们进行了如下的数据预处理：","categories":[],"tags":[{"name":"SIMLR","slug":"SIMLR","permalink":"http://Bithub00.com/tags/SIMLR/"},{"name":"machine-learning","slug":"machine-learning","permalink":"http://Bithub00.com/tags/machine-learning/"}]},{"title":"Jupyter Notebook 的快捷键","slug":"jupyter notebook快捷键","date":"2018-07-24T06:16:20.378Z","updated":"2018-07-24T06:22:27.766Z","comments":true,"path":"2018/07/24/jupyter notebook快捷键/","link":"","permalink":"http://Bithub00.com/2018/07/24/jupyter notebook快捷键/","excerpt":"Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。","text":"Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。 命令模式 (按键 Esc 开启) Enter : 转入编辑模式 Shift-Enter : 运行本单元，选中下个单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在其下插入新单元 Y : 单元转入代码状态 M :单元转入markdown状态 R : 单元转入raw状态 1 : 设定 1 级标题 2 : 设定 2 级标题 3 : 设定 3 级标题 4 : 设定 4 级标题 5 : 设定 5 级标题 6 : 设定 6 级标题 Up : 选中上方单元 K : 选中上方单元 Down : 选中下方单元 J : 选中下方单元 Shift-K : 扩大选中上方单元 Shift-J : 扩大选中下方单元 A : 在上方插入新单元 B : 在下方插入新单元 X : 剪切选中的单元 C : 复制选中的单元 Shift-V : 粘贴到上方单元 V : 粘贴到下方单元 Z : 恢复删除的最后一个单元 D,D : 删除选中的单元 Shift-M : 合并选中的单元 Ctrl-S : 文件存盘 S : 文件存盘 L : 转换行号 O : 转换输出 Shift-O : 转换输出滚动 Esc : 关闭页面 Q : 关闭页面 H : 显示快捷键帮助 I,I : 中断Notebook内核 0,0 : 重启Notebook内核 Shift : 忽略 Shift-Space : 向上滚动 Space : 向下滚动 编辑模式 ( Enter 键启动) Tab : 代码补全或缩进 Shift-Tab : 提示 Ctrl-] : 缩进 Ctrl-[ : 解除缩进 Ctrl-A : 全选 Ctrl-Z : 复原 Ctrl-Shift-Z : 再做 Ctrl-Y : 再做 Ctrl-Home : 跳到单元开头 Ctrl-Up : 跳到单元开头 Ctrl-End : 跳到单元末尾 Ctrl-Down : 跳到单元末尾 Ctrl-Left : 跳到左边一个字首 Ctrl-Right : 跳到右边一个字首 Ctrl-Backspace : 删除前面一个字 Ctrl-Delete : 删除后面一个字 Esc : 进入命令模式 Ctrl-M : 进入命令模式 Shift-Enter : 运行本单元，选中下一单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在下面插入一单元 Ctrl-Shift– : 分割单元 Ctrl-Shift-Subtract : 分割单元 Ctrl-S : 文件存盘 Shift : 忽略 Up : 光标上移或转入上一单元 Down :光标下移或转入下一单元","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://Bithub00.com/tags/python/"}]},{"title":"VPN","slug":"vpn","date":"2018-04-06T11:58:10.712Z","updated":"2019-07-25T03:23:26.986Z","comments":true,"path":"2018/04/06/vpn/","link":"","permalink":"http://Bithub00.com/2018/04/06/vpn/","excerpt":"edu.cn的学校邮箱+5美元 = 11个月的境外服务器 领取github的vps优惠码","text":"edu.cn的学校邮箱+5美元 = 11个月的境外服务器 领取github的vps优惠码 教程https://www.ichenfei.com/get-github-students-gift.html 领取优惠码页面https://education.github.com/pack/offers#digitalocean vps开通页面(需要翻墙，学校使用ipv6地址可以直接翻出去)https://www.digitalocean.com/不要使用一次性邮箱注册账号，因为以后每次登陆都要邮箱验证 登陆页面https://cloud.digitalocean.com/login (建议翻墙访问) 注册paypal账号来支付https://www.paypal.com/c2/home paypal绑定了银行卡和手机，建议用不常用的银行卡，然后充35块钱进去支付成功后，激活digitalocean账号，填入github的优惠卷，获得50美元 创建自己的服务器建议选SFO一区的服务器，不容易被墙，配置的话5美元那种就差不多了，服务器型号建议ubuntu 连接服务器通过console连上服务器后，使用秋水逸冰大大的命令一键安装:123wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.shchmod +x shadowsocks-all.sh &amp;&amp; ./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 探索新世界吧:)","categories":[],"tags":[{"name":"shadowsock","slug":"shadowsock","permalink":"http://Bithub00.com/tags/shadowsock/"},{"name":"vpn","slug":"vpn","permalink":"http://Bithub00.com/tags/vpn/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-04-05T14:57:07.298Z","updated":"2018-04-06T12:22:17.621Z","comments":true,"path":"2018/04/05/hello-world/","link":"","permalink":"http://Bithub00.com/2018/04/05/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"tutorial","slug":"tutorial","permalink":"http://Bithub00.com/tags/tutorial/"}]}]}