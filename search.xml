<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ARROW - Approximating Reachability using Random walks Over Web-scale Graphs[ICDE&#39;19]</title>
    <url>/2020/12/22/ARROW%5BICDE19%5D/</url>
    <content><![CDATA[<p>ICDE19年的一篇论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>给定一个有向图$G=(V,E)$以及一系列顶点对$(u,v)$,判断两个顶点之间是否连通，对应两种查询情况：  </p>
<ul>
<li>Chained Queries：查询路径上每条边开始于上一条边结束，并且总时间在规定的范围内</li>
<li>Snapshot Queries：对于一个动态变化的图，在给定的时间范围内至少在$c$个快闪图中存在连通</li>
</ul>
<h3 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h3><ol>
<li>以起始顶点和目的顶点各自进行多次固定长度的随机漫步构建两个集合，两个集合的交集非空说明连通</li>
<li>对于随机漫步的长度及次数的选取给出了理论证明<br>2.1 随机漫步的长度：有向图中最长的最短路径的长度，通过10次的深度优先搜索得到<br>2.2 随机漫步的次数：类比于扔球问题，给定$n$个篮子和数量相等的红球与蓝球，需要扔多少个球来保证有一个篮子中同时有红球与蓝球的概率高？红球可以看作起始顶点$u$可以到达的顶点，蓝球可以看作目的顶点$v$可以到达的顶点</li>
<li>模型的一个假设前提是构建的两个反向的随机漫步的平稳分布应尽可能接近，这对应于正向随机漫步选定一个出边的概率与反向随机漫步选定一个入边的概率相等，而这个概率恰等于顶点度的倒数。使用同配性(assortativity)作为这两个平稳分布接近程度的指标。</li>
</ol>
]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>C++刷题</title>
    <url>/2019/09/01/C++%E5%88%B7%E9%A2%98/</url>
    <content><![CDATA[<p>C++刷题记录</p>
<a id="more"></a>
<h2 id="题目集一"><a href="#题目集一" class="headerlink" title="题目集一"></a>题目集一</h2><p><a href="https://blog.csdn.net/qq_36426650/article/category/8452043/1" target="_blank" rel="noopener">C++面向对象程序设计50道编程题</a></p>
<h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem 1"></a>Problem 1</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fract</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">int</span> num, den;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   Fract(<span class="keyword">int</span> a = <span class="number">0</span>, <span class="keyword">int</span> b = <span class="number">1</span>) &#123; num = a; den = b; &#125;</span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">ged</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span>;</span><br><span class="line">   <span class="function">Fract <span class="title">add</span><span class="params">(Fract f)</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> Fract::ged(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">if</span>(m &gt;= n)</span><br><span class="line">       k = n;</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">       k = m;</span><br><span class="line">   <span class="keyword">for</span>(; k &gt;= <span class="number">1</span>; k--)</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="keyword">if</span>(m % k == <span class="number">0</span> &amp;&amp; n % k == <span class="number">0</span>)</span><br><span class="line">           <span class="keyword">break</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> k;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Fract Fract::add(Fract f)</span><br><span class="line">&#123;</span><br><span class="line">   Fract ff;</span><br><span class="line">   <span class="keyword">int</span> v;</span><br><span class="line">   v = ged(f.den, den);</span><br><span class="line">   <span class="keyword">int</span> vv = den / v * f.den;</span><br><span class="line">   <span class="keyword">int</span> uu = vv / den * num + vv / f.den * f.num;</span><br><span class="line">   <span class="keyword">int</span> cc = ged(vv, uu);</span><br><span class="line">   <span class="keyword">if</span> (cc != <span class="number">1</span>) &#123;vv = vv / cc; uu = uu / cc;&#125;</span><br><span class="line">   ff.den = vv;</span><br><span class="line">   ff.num = uu;</span><br><span class="line">   <span class="keyword">return</span> ff;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Fract::show()</span><br><span class="line">&#123;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="string">'/'</span> &lt;&lt; den &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   Fract f1(1,5), f2(7,20), f3;</span><br><span class="line">   f3 = f1.add(f2);</span><br><span class="line">   f3.show();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem 2"></a>Problem 2</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ARRAY</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">float</span> a[<span class="number">10</span>], b[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   ARRAY(<span class="keyword">float</span> t[<span class="number">10</span>])</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line">           a[i] = t[i];</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">()</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> pre = (i - <span class="number">1</span>) % <span class="number">10</span>;</span><br><span class="line">           <span class="keyword">if</span>(pre &lt; <span class="number">0</span>)</span><br><span class="line">               pre = <span class="number">10</span> + pre;</span><br><span class="line">           <span class="keyword">int</span> aft = (i + <span class="number">1</span>) % <span class="number">10</span>;</span><br><span class="line">           b[i] = (a[pre] + a[i] + a[aft]) / <span class="number">3</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line">           <span class="built_in">cout</span> &lt;&lt; a[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line">           <span class="built_in">cout</span> &lt;&lt; b[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">float</span> aa[<span class="number">10</span>] = &#123;<span class="number">0</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">15</span>,<span class="number">18</span>,<span class="number">21</span>,<span class="number">24</span>,<span class="number">27</span>&#125;;</span><br><span class="line">   <span class="function">ARRAY <span class="title">v</span><span class="params">(aa)</span></span>;</span><br><span class="line">   v.process();</span><br><span class="line">   v.print();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem 3"></a>Problem 3</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">char</span> s[<span class="number">18</span>], x[<span class="number">11</span>];</span><br><span class="line">   <span class="keyword">int</span> w[<span class="number">17</span>];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   ID(<span class="keyword">char</span> *str)&#123;</span><br><span class="line">       <span class="keyword">int</span> i;</span><br><span class="line">       <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">18</span>; i++)</span><br><span class="line">           s[i] = <span class="string">'0'</span>;</span><br><span class="line">       <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(str); i++)&#123;</span><br><span class="line">           s[i] = str[i];</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">char</span> x1[] = &#123;<span class="string">'1'</span>,<span class="string">'0'</span>,<span class="string">'X'</span>,<span class="string">'9'</span>,<span class="string">'8'</span>,<span class="string">'7'</span>,<span class="string">'6'</span>,<span class="string">'5'</span>,<span class="string">'4'</span>,<span class="string">'3'</span>,<span class="string">'2'</span>&#125;;</span><br><span class="line">       <span class="built_in">strcpy</span>(x,x1);</span><br><span class="line">       <span class="keyword">int</span> w1[] = &#123;<span class="number">7</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">2</span>&#125;;</span><br><span class="line">       <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">17</span>; i++)</span><br><span class="line">           w[i] = w1[i];</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">int</span> i;</span><br><span class="line">       <span class="keyword">for</span>(i = <span class="number">16</span>; i &gt; <span class="number">7</span>; i--)</span><br><span class="line">           s[i] = s[i - <span class="number">2</span>];</span><br><span class="line">       s[<span class="number">6</span>] = <span class="string">'1'</span>; s[<span class="number">7</span>] = <span class="string">'9'</span>;</span><br><span class="line">       <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">17</span>; i++)&#123;</span><br><span class="line">           <span class="keyword">int</span> temp = s[i] - <span class="string">'0'</span>;</span><br><span class="line">           sum += temp * w[i];</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">int</span> index = sum % <span class="number">11</span>;</span><br><span class="line">       s[<span class="number">17</span>] = x[index];</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">char</span> *str = <span class="string">"340524800101001"</span>;</span><br><span class="line">   <span class="function">ID <span class="title">id</span><span class="params">(str)</span></span>;</span><br><span class="line">   id.fun();</span><br><span class="line">   id.print();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem 4"></a>Problem 4</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">String</span>&#123;</span></span><br><span class="line">   <span class="keyword">char</span> *str1, *str2;</span><br><span class="line">   <span class="keyword">char</span> *str;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   String(<span class="keyword">char</span> *s1, <span class="keyword">char</span>* s2)&#123;</span><br><span class="line">       str1=<span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(s1)+<span class="number">1</span>];</span><br><span class="line">       <span class="built_in">strcpy</span>(str1,s1);</span><br><span class="line">       str2=<span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(s2)+<span class="number">1</span>];</span><br><span class="line">       <span class="built_in">strcpy</span>(str2,s2);</span><br><span class="line">       str=<span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(s1)+<span class="built_in">strlen</span>(s2)+<span class="number">1</span>];</span><br><span class="line">       <span class="built_in">strcpy</span>(str,s1);<span class="built_in">strcat</span>(str,s2);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">del</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">char</span> *ptr1 = str;</span><br><span class="line">       <span class="keyword">char</span> *ptr2 = str;</span><br><span class="line">       <span class="keyword">while</span>(*ptr1)&#123;</span><br><span class="line">           <span class="keyword">if</span>(*ptr1 != <span class="string">' '</span>)&#123;</span><br><span class="line">               *ptr2 = *ptr1;</span><br><span class="line">               ptr2++;</span><br><span class="line">           &#125;</span><br><span class="line">           ptr1++;</span><br><span class="line">       &#125;</span><br><span class="line">       *ptr2 = <span class="string">'\0'</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">void</span> _sort()&#123;</span><br><span class="line">       <span class="keyword">char</span> *ptr1 = str;</span><br><span class="line">       <span class="keyword">char</span> *ptr2, temp;</span><br><span class="line">       <span class="keyword">while</span>(*ptr1)&#123;</span><br><span class="line">           <span class="keyword">for</span>(ptr2 = ptr1; *ptr2; ptr2++)&#123;</span><br><span class="line">               <span class="keyword">if</span>(*ptr1 &gt; *ptr2)&#123;</span><br><span class="line">                   temp = *ptr1;</span><br><span class="line">                   *ptr1 = *ptr2;</span><br><span class="line">                   *ptr2 = temp;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           ptr1++;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">"str1: "</span> &lt;&lt; str1 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">"str2: "</span> &lt;&lt; str2 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">"str: "</span> &lt;&lt; str &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   ~String()&#123;</span><br><span class="line">       <span class="built_in">free</span>(str1);</span><br><span class="line">       <span class="built_in">free</span>(str2);</span><br><span class="line">       <span class="built_in">free</span>(str);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">char</span> *s1 = <span class="string">"db a"</span>;</span><br><span class="line">   <span class="keyword">char</span> *s2 = <span class="string">"4  1"</span>;</span><br><span class="line">   <span class="function">String <span class="title">str</span><span class="params">(s1, s2)</span></span>;</span><br><span class="line">   str.del();</span><br><span class="line">   str.show();</span><br><span class="line">   str._sort();</span><br><span class="line">   str.show();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem 5"></a>Problem 5</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Array</span>&#123;</span></span><br><span class="line">   <span class="keyword">int</span> *p,k;</span><br><span class="line">   <span class="keyword">float</span> s;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   Array(<span class="keyword">int</span> *ptr, <span class="keyword">int</span> n)&#123;</span><br><span class="line">       k = n;</span><br><span class="line">       p = <span class="keyword">new</span> <span class="keyword">int</span>[k];</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)</span><br><span class="line">           p[i] = ptr[i];</span><br><span class="line">       s = <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">fun</span><span class="params">(<span class="keyword">int</span> n)</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">sum</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line">   ~Array()&#123;</span><br><span class="line">       <span class="keyword">delete</span> []p;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> Array::fun(<span class="keyword">int</span> n)&#123;</span><br><span class="line">   <span class="keyword">if</span>(n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">int</span> i;</span><br><span class="line">   <span class="keyword">for</span>(i = <span class="number">2</span>; i &lt; n; i++)</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="keyword">if</span>(n % i == <span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Array::sum()&#123;</span><br><span class="line">   <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">int</span> length = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)&#123;</span><br><span class="line">       <span class="keyword">if</span>(fun(p[i]))&#123;</span><br><span class="line">           sum += p[i];</span><br><span class="line">           length++;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   s = sum / length;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Array::show()&#123;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; k &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="keyword">int</span> i;</span><br><span class="line">   <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; k; i++)&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; p[i] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line">       <span class="keyword">if</span>(i % <span class="number">4</span> ==<span class="number">0</span> &amp;&amp; i != <span class="number">0</span>)</span><br><span class="line">           <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> ptr[] = &#123;<span class="number">5</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">8</span>,<span class="number">23</span>,<span class="number">65</span>,<span class="number">1</span>,<span class="number">40</span>&#125;;</span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">9</span>;</span><br><span class="line">   <span class="function">Array <span class="title">arr</span><span class="params">(ptr, n)</span></span>;</span><br><span class="line">   arr.sum();</span><br><span class="line">   arr.show();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-6"><a href="#Problem-6" class="headerlink" title="Problem 6"></a>Problem 6</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">STR</span>&#123;</span></span><br><span class="line">   <span class="keyword">char</span> s1[<span class="number">80</span>], s2[<span class="number">80</span>];</span><br><span class="line">   <span class="keyword">char</span> s3[<span class="number">160</span>];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   STR(<span class="keyword">char</span> a[], <span class="keyword">char</span> b[])&#123;</span><br><span class="line">       <span class="keyword">char</span> *ptr1 = s1;</span><br><span class="line">       <span class="keyword">char</span> *ptr2 = s2;</span><br><span class="line">       <span class="keyword">while</span>(*a)&#123;</span><br><span class="line">           *ptr1 = *a;</span><br><span class="line">           ptr1++;</span><br><span class="line">           a++;</span><br><span class="line">       &#125;</span><br><span class="line">       *ptr1 = <span class="string">'\0'</span>;</span><br><span class="line">       <span class="keyword">while</span>(*b)&#123;</span><br><span class="line">           *ptr2 = *b;</span><br><span class="line">           ptr2++;</span><br><span class="line">           b++;</span><br><span class="line">       &#125;</span><br><span class="line">       *ptr2 = <span class="string">'\0'</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">consort</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> STR::consort()&#123;</span><br><span class="line">   <span class="built_in">strcpy</span>(s3, s1);</span><br><span class="line">   <span class="built_in">strcat</span>(s3, s2);</span><br><span class="line">   <span class="keyword">char</span> *ptr1 = s3, *ptr2, temp;</span><br><span class="line">   <span class="keyword">while</span>(*ptr1)&#123;</span><br><span class="line">       <span class="keyword">for</span>(ptr2 = ptr1; *ptr2; ptr2++)&#123;</span><br><span class="line">           <span class="keyword">if</span>(*ptr2 &lt; *ptr1)&#123;</span><br><span class="line">               temp = *ptr2;</span><br><span class="line">               *ptr2 = *ptr1;</span><br><span class="line">               *ptr1 = temp;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       ptr1++;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> STR::show()&#123;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; s1 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; s2 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; s3 &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">char</span> a[] = <span class="string">"pear"</span>;</span><br><span class="line">   <span class="keyword">char</span> b[] = <span class="string">"apple"</span>;</span><br><span class="line">   <span class="function">STR <span class="title">str</span><span class="params">(a, b)</span></span>;</span><br><span class="line">   str.consort();</span><br><span class="line">   str.show();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-7"><a href="#Problem-7" class="headerlink" title="Problem 7"></a>Problem 7</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RECT</span>&#123;</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">   <span class="keyword">double</span> x,y;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   RECT(<span class="keyword">double</span> x1, <span class="keyword">double</span> y1)&#123;</span><br><span class="line">       x = x1;</span><br><span class="line">       y = y1;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">virtual</span> <span class="keyword">double</span> <span class="title">area</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">double</span> are = x * y;</span><br><span class="line">       <span class="keyword">return</span> are;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">double</span> <span class="title">peri</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">double</span> per = <span class="number">2</span> * (x + y);</span><br><span class="line">       <span class="keyword">return</span> per;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">isSquare</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(x == y)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CUB</span>:</span><span class="keyword">public</span> RECT&#123;</span><br><span class="line">   <span class="keyword">double</span> height;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   CUB(<span class="keyword">double</span> x1, <span class="keyword">double</span> y1, <span class="keyword">double</span> h):RECT(x1,y1)&#123;</span><br><span class="line">       height = h;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">double</span> <span class="title">volume</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">double</span> <span class="title">area</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">isSquare</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> CUB::volume()&#123;</span><br><span class="line">   <span class="keyword">double</span> square = RECT::area();</span><br><span class="line">   <span class="keyword">double</span> v = square * height;</span><br><span class="line">   <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> CUB::area()&#123;</span><br><span class="line">   <span class="keyword">double</span> square = RECT::area();</span><br><span class="line">   <span class="keyword">double</span> result = <span class="number">2</span> * square + peri() * height;</span><br><span class="line">   <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> CUB::isSquare()&#123;</span><br><span class="line">   <span class="keyword">if</span>(RECT::isSquare())</span><br><span class="line">       <span class="keyword">return</span> x == height;</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">double</span> a,b,c;</span><br><span class="line">	<span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b&gt;&gt;c;</span><br><span class="line">	<span class="function">CUB <span class="title">cu</span><span class="params">(a,b,c)</span></span>;</span><br><span class="line">	RECT *re;</span><br><span class="line">	re=&amp;cu;</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="string">"长方体的体积为："</span>&lt;&lt;cu.volume()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="string">"长方体的表面积为："</span>&lt;&lt;re-&gt;area()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	<span class="keyword">if</span>(re-&gt;isSquare())</span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt;<span class="string">"该长方体是正方体\n"</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt;<span class="string">"该长方体不是正方体\n"</span>;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-8"><a href="#Problem-8" class="headerlink" title="Problem 8"></a>Problem 8</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ARR</span>&#123;</span></span><br><span class="line">   <span class="keyword">int</span> n;</span><br><span class="line">   <span class="keyword">int</span> a[<span class="number">100</span>];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   ARR(<span class="keyword">int</span> x[], <span class="keyword">int</span> size)&#123;</span><br><span class="line">       n = size;</span><br><span class="line">       <span class="keyword">int</span> i;</span><br><span class="line">       <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">           a[i] = x[i];</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">change</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> ARR::change()&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>, j = n - <span class="number">1</span>;</span><br><span class="line">   <span class="keyword">for</span>(; i &lt; j; i++)&#123;</span><br><span class="line">       <span class="keyword">while</span>(a[i] &lt; <span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(i == j)</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           i++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">while</span>(a[j] &gt; <span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">if</span>(i == j)</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           j--;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">int</span> temp = a[i];</span><br><span class="line">       a[i] = a[j];</span><br><span class="line">       a[j] = temp;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> ARR::show()&#123;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; a[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> b[<span class="number">10</span>] = &#123;<span class="number">1</span>,<span class="number">-3</span>,<span class="number">-1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">-4</span>,<span class="number">5</span>,<span class="number">-5</span>,<span class="number">-2</span>&#125;;</span><br><span class="line">   <span class="function">ARR <span class="title">arr</span><span class="params">(b, <span class="number">10</span>)</span></span>;</span><br><span class="line">   arr.show();</span><br><span class="line">   arr.change();</span><br><span class="line">   arr.show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-9"><a href="#Problem-9" class="headerlink" title="Problem 9"></a>Problem 9</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span>&#123;</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">   <span class="keyword">char</span> name[<span class="number">8</span>];</span><br><span class="line">   <span class="keyword">int</span> num;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   Base()&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入姓名："</span>;</span><br><span class="line">       <span class="built_in">cin</span> &gt;&gt; name;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; name &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">isGood</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>:</span><span class="keyword">public</span> Base&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   Student()&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入成绩："</span>;</span><br><span class="line">       <span class="built_in">cin</span> &gt;&gt; num;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">isGood</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(num &gt; <span class="number">90</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Teacher</span>:</span><span class="keyword">public</span> Base&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   Teacher()&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入论文数："</span>;</span><br><span class="line">       <span class="built_in">cin</span> &gt;&gt; num;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">int</span> <span class="title">isGood</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(num &gt; <span class="number">3</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"学生情况："</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   Student s[<span class="number">2</span>];</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"老师情况："</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   Teacher t[<span class="number">2</span>];</span><br><span class="line">   Base *p;</span><br><span class="line">   <span class="keyword">int</span> i;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"优秀学生："</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="keyword">for</span>(i = <span class="number">0</span>, p = s; i &lt; <span class="number">2</span>; i++)&#123;</span><br><span class="line">       <span class="keyword">if</span>(p-&gt;isGood())</span><br><span class="line">           p-&gt;print();</span><br><span class="line">       p++;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"优秀老师："</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="keyword">for</span>(i = <span class="number">0</span>, p = t; i &lt; <span class="number">2</span>; i ++)&#123;</span><br><span class="line">       <span class="keyword">if</span>(p-&gt;isGood())</span><br><span class="line">           p-&gt;print();</span><br><span class="line">       p++;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-10"><a href="#Problem-10" class="headerlink" title="Problem 10"></a>Problem 10</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> M 4</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Array</span>&#123;</span></span><br><span class="line">   <span class="keyword">int</span> b[M][M];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   Array(<span class="keyword">int</span> (*p)[M]);</span><br><span class="line">   <span class="keyword">void</span> <span class="keyword">operator</span>+();</span><br><span class="line">   <span class="keyword">friend</span> <span class="keyword">void</span> <span class="keyword">operator</span>-(Array &amp;b);</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Array::Array(<span class="keyword">int</span> (*p)[M])&#123;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">           b[i][j] = p[i][j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Array::<span class="keyword">operator</span>+()&#123;</span><br><span class="line">   <span class="keyword">int</span> t[M][M];</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">           t[i][j] = b[j][M<span class="number">-1</span>-i];</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">           b[i][j] = t[i][j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> <span class="keyword">operator</span>-(Array &amp;b)&#123;</span><br><span class="line">   <span class="keyword">int</span> t[M][M];</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">           t[i][j] = b.b[M<span class="number">-1</span>-j][i];</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">           b.b[i][j] = t[i][j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Array::print()&#123;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++)</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; M; j++)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">cout</span> &lt;&lt; b[i][j] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> a[][M]=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>&#125;;</span><br><span class="line">   <span class="function">Array <span class="title">arr</span><span class="params">(a)</span></span>;</span><br><span class="line">   arr.print();</span><br><span class="line">   +arr;</span><br><span class="line">   arr.print();</span><br><span class="line">   -arr;</span><br><span class="line">   arr.print();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-11"><a href="#Problem-11" class="headerlink" title="Problem 11"></a>Problem 11</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">String_Integer</span>&#123;</span></span><br><span class="line">   <span class="keyword">char</span> *s;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   String_Integer(<span class="keyword">char</span> *str);</span><br><span class="line">   <span class="function"><span class="keyword">operator</span> <span class="title">int</span><span class="params">()</span></span>&#123;</span><br><span class="line">       <span class="keyword">char</span> *ptr = s;</span><br><span class="line">       <span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(*ptr)&#123;</span><br><span class="line">           <span class="keyword">if</span>(*ptr &gt;= <span class="string">'0'</span> &amp;&amp; *ptr &lt;= <span class="string">'9'</span>)</span><br><span class="line">               &#123;</span><br><span class="line">                   num = num*<span class="number">10</span> + *ptr - <span class="string">'0'</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           ptr++;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> num;</span><br><span class="line">       &#125;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line">   ~String_Integer();</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">String_Integer::String_Integer(<span class="keyword">char</span> *str)&#123;</span><br><span class="line">   s = <span class="keyword">new</span> <span class="keyword">char</span>(<span class="built_in">strlen</span>(str) + <span class="number">1</span>);</span><br><span class="line">   <span class="built_in">strcpy</span>(s, str);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> String_Integer::show()&#123;</span><br><span class="line">   <span class="keyword">char</span> *ptr = s;</span><br><span class="line">   <span class="keyword">while</span>(*ptr)&#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; *ptr;</span><br><span class="line">       ptr++;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">String_Integer::~String_Integer()&#123;</span><br><span class="line">   <span class="keyword">delete</span> []s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">char</span> *s = <span class="string">"ab12  3c00d45ef"</span>;</span><br><span class="line">   <span class="function">String_Integer <span class="title">str</span><span class="params">(s)</span></span>;</span><br><span class="line">   str.show();</span><br><span class="line">   <span class="keyword">int</span> n = str;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"输出的整数为："</span> &lt;&lt; n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-12"><a href="#Problem-12" class="headerlink" title="Problem 12"></a>Problem 12</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SET</span>&#123;</span></span><br><span class="line">   <span class="keyword">int</span> *a;</span><br><span class="line">   <span class="keyword">int</span> len;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   SET(<span class="keyword">int</span> *p, <span class="keyword">int</span> n);</span><br><span class="line">   <span class="keyword">int</span> <span class="keyword">operator</span> ==(<span class="keyword">int</span> m);</span><br><span class="line">   <span class="keyword">friend</span> <span class="keyword">int</span> <span class="keyword">operator</span> ==(SET &amp;s1, SET &amp;s2);</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">   ~SET();</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">SET::SET(<span class="keyword">int</span> *p, <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">   len = n;</span><br><span class="line">   a = <span class="keyword">new</span> <span class="keyword">int</span>(len);</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">       a[i] = p[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> SET::<span class="keyword">operator</span> ==(<span class="keyword">int</span> m)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">       <span class="keyword">if</span>(m == a[i])</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> <span class="keyword">operator</span> ==(SET &amp;s1, SET &amp;s2)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">if</span>(s1.len != s2.len)</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">   &#123;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s1.len; i++)</span><br><span class="line">           <span class="keyword">if</span>(!(s2 == s1.a[i]))</span><br><span class="line">               <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> SET::print()</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">   &#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; a[i] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SET::~SET()</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">delete</span> []a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> a[]=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;,b[]=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;,c[]=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>&#125;,d[]=&#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>&#125;;</span><br><span class="line">	SET s1(a,5),s2(b,5),s3(c,6),s4(d,5);</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="string">"a:\t"</span>;s1.print();</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="string">"b:\t"</span>;s2.print();</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="string">"c:\t"</span>;s3.print();</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;<span class="string">"d:\t"</span>;s4.print();</span><br><span class="line">	<span class="keyword">if</span>(s1==s2)<span class="built_in">cout</span>&lt;&lt;<span class="string">"a==b\n"</span>;</span><br><span class="line">	<span class="keyword">else</span> <span class="built_in">cout</span>&lt;&lt;<span class="string">"a!=b\n"</span>;</span><br><span class="line">	<span class="keyword">if</span>(s1==s3)<span class="built_in">cout</span>&lt;&lt;<span class="string">"a==c\n"</span>;</span><br><span class="line">	<span class="keyword">else</span> <span class="built_in">cout</span>&lt;&lt;<span class="string">"a!=c\n"</span>;</span><br><span class="line">	<span class="keyword">if</span>(s1==s4)<span class="built_in">cout</span>&lt;&lt;<span class="string">"a==d\n"</span>;</span><br><span class="line">	<span class="keyword">else</span> <span class="built_in">cout</span>&lt;&lt;<span class="string">"a!=d\n"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-13"><a href="#Problem-13" class="headerlink" title="Problem 13"></a>Problem 13</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">STR</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">char</span> *s;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">   STR(<span class="keyword">char</span> *p = <span class="number">0</span>);</span><br><span class="line">   STR&amp; <span class="keyword">operator</span> =(STR &amp;str);</span><br><span class="line">   <span class="keyword">friend</span> STR&amp; <span class="keyword">operator</span> +=(STR &amp;str1, STR &amp;str2);</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">   ~STR();</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">STR::STR(<span class="keyword">char</span> *p)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">if</span>(p == <span class="number">0</span>)</span><br><span class="line">       s = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">   &#123;</span><br><span class="line">       s = <span class="keyword">new</span> <span class="keyword">char</span>(<span class="built_in">strlen</span>(p) + <span class="number">1</span>);</span><br><span class="line">       <span class="built_in">strcpy</span>(s, p);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">STR&amp; STR::<span class="keyword">operator</span> =(STR &amp;str)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">if</span>(s)</span><br><span class="line">       <span class="keyword">delete</span> []s;</span><br><span class="line">   s = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str.s) + <span class="number">1</span>];</span><br><span class="line">   <span class="built_in">strcpy</span>(s, str.s);</span><br><span class="line">   <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">STR&amp; <span class="keyword">operator</span> +=(STR &amp;str1, STR &amp;str2)</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">char</span> *p = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(str1.s) + <span class="built_in">strlen</span>(str2.s) + <span class="number">1</span>];</span><br><span class="line">   <span class="built_in">strcpy</span>(p, str1.s);</span><br><span class="line">   <span class="built_in">strcat</span>(p, str2.s);</span><br><span class="line">   <span class="keyword">if</span>(str1.s)</span><br><span class="line">       <span class="keyword">delete</span> []str1.s;</span><br><span class="line">   str1.s = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="built_in">strlen</span>(p) + <span class="number">1</span>];</span><br><span class="line">   <span class="built_in">strcpy</span>(str1.s, p);</span><br><span class="line">   <span class="keyword">return</span> str1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> STR::print()</span><br><span class="line">&#123;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">STR::~STR()</span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">delete</span> []s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   STR str1("Shenzhen"),str2(" University"),str3;</span><br><span class="line">	str1.print();</span><br><span class="line">	str2.print();</span><br><span class="line">	str3=str1+=str2;</span><br><span class="line">	str3.print();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Problem-14"><a href="#Problem-14" class="headerlink" title="Problem 14"></a>Problem 14</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> pi 3.14</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">container</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="keyword">int</span> radius;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    container(<span class="keyword">int</span> n) &#123;radius = n;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">double</span> <span class="title">square</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">double</span> <span class="title">volume</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cube</span>:</span><span class="keyword">public</span> container</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    cube(<span class="keyword">int</span> n):container(n)&#123; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">square</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> s = <span class="number">6</span> * radius * radius;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">volume</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> v = radius * radius * radius;</span><br><span class="line">        <span class="keyword">return</span> v;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sphere</span>:</span><span class="keyword">public</span> container</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    sphere(<span class="keyword">int</span> n):container(n)&#123; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">square</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> s = <span class="number">4</span> * radius * radius * pi;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">volume</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> v = (<span class="number">4</span> / <span class="number">3.0</span>) * radius * radius * radius;</span><br><span class="line">        <span class="keyword">return</span> v;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cylinder</span>:</span><span class="keyword">public</span> container</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> height;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    cylinder(<span class="keyword">int</span> n, <span class="keyword">int</span> h):container(n)&#123; height = h;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">square</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> s = <span class="number">2</span> * radius * radius * pi + <span class="number">2</span> * pi * radius * height;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">volume</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> v =  radius * radius * pi * height;</span><br><span class="line">        <span class="keyword">return</span> v;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> radius = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">int</span> height = <span class="number">4</span>;</span><br><span class="line">    container *base;</span><br><span class="line">    <span class="function">cube <span class="title">c</span><span class="params">(radius)</span></span>;</span><br><span class="line">    <span class="function">sphere <span class="title">s</span><span class="params">(radius)</span></span>;</span><br><span class="line">    <span class="function">cylinder <span class="title">cy</span><span class="params">(radius, height)</span></span>;</span><br><span class="line">    base = &amp;c;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"正方体表面积为："</span> &lt;&lt; base-&gt;square() &lt;&lt; <span class="built_in">endl</span>;;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"正方形体积为："</span> &lt;&lt; base-&gt;volume() &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    base = &amp;s;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"球体表面积为："</span> &lt;&lt; base-&gt;square() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"球体体积为："</span> &lt;&lt; base-&gt;volume() &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    base = &amp;cy;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"圆柱体表面积为："</span> &lt;&lt; base-&gt;square() &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"圆柱体体积为："</span> &lt;&lt; base-&gt;volume() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="题目集二"><a href="#题目集二" class="headerlink" title="题目集二"></a>题目集二</h2><p>王道论坛计算机机试指南</p>
<h3 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> M, N;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; M &gt;&gt; N;</span><br><span class="line">    <span class="keyword">while</span>(M != - <span class="number">1</span> &amp;&amp; N != <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> storage = M;</span><br><span class="line">        <span class="keyword">float</span> result = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">int</span> J[N], F[N];</span><br><span class="line">        <span class="keyword">float</span> rate[N];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; J[i] &gt;&gt; F[i];</span><br><span class="line">            rate[i] = <span class="keyword">float</span>(J[i]) / F[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(storage != <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">float</span> max = <span class="number">0.0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(rate[i] &gt; max)</span><br><span class="line">                &#123;</span><br><span class="line">                    index = i;</span><br><span class="line">                    max = rate[i];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(storage &gt; F[index] &amp;&amp; F[index] != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                result += J[index];</span><br><span class="line">                storage -= F[index];</span><br><span class="line">                rate[index] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                result += storage * rate[index];</span><br><span class="line">                storage = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; fixed &lt;&lt; setprecision(<span class="number">3</span>) &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; M &gt;&gt; N;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="括号匹配"><a href="#括号匹配" class="headerlink" title="括号匹配"></a>括号匹配</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//    char str[100];</span></span><br><span class="line"><span class="comment">//    cin &gt;&gt; str;</span></span><br><span class="line">    <span class="keyword">char</span> str[] = <span class="string">")(rttyy())sss)("</span>;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; S;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; index;</span><br><span class="line">    <span class="keyword">int</span> len = <span class="built_in">strlen</span>(str);</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">char</span> result[len];</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">        result[i] = <span class="string">' '</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(str[i] == <span class="string">'('</span>)&#123;</span><br><span class="line">           S.push(str[i]);</span><br><span class="line">           index.push(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(str[i] == <span class="string">')'</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(S.empty())&#123;</span><br><span class="line">                result[i] = <span class="string">'?'</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                S.pop();</span><br><span class="line">                index.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    result[i] = <span class="string">'\0'</span>;</span><br><span class="line">    <span class="keyword">while</span>(!S.empty())&#123;</span><br><span class="line">        <span class="keyword">int</span> top = index.top();</span><br><span class="line">        index.pop();</span><br><span class="line">        S.pop();</span><br><span class="line">        result[top] = <span class="string">'$'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="简易计数器-中缀表达式-gt-后缀表达式"><a href="#简易计数器-中缀表达式-gt-后缀表达式" class="headerlink" title="简易计数器[中缀表达式-&gt;后缀表达式]"></a>简易计数器[中缀表达式-&gt;后缀表达式]</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">priority</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> symbol)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> grade = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">switch</span>(symbol)&#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'+'</span>: grade = <span class="number">4</span>;<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'-'</span>: grade = <span class="number">4</span>;<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'*'</span>: grade = <span class="number">5</span>;<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'/'</span>: grade = <span class="number">5</span>;<span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> grade;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">calculate</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span> a, <span class="keyword">const</span> <span class="keyword">char</span> symbol, <span class="keyword">const</span> <span class="keyword">float</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> result = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">switch</span>(symbol)&#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'+'</span>: result = a + b;<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'-'</span>: result = a - b;<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'*'</span>: result = a * b;<span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">'/'</span>: result = <span class="keyword">float</span>(a) / b;<span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="string">' '</span> &lt;&lt; symbol &lt;&lt; <span class="string">' '</span> &lt;&lt; b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> str[] = <span class="string">"4+2*5-6/3"</span>;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; result;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; S;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(str); i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(str[i] &gt;= <span class="string">'0'</span> &amp;&amp; str[i] &lt;= <span class="string">'9'</span>)</span><br><span class="line">            result.push(str[i]);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(str[i] == <span class="string">'+'</span> || str[i] == <span class="string">'-'</span> || str[i] == <span class="string">'*'</span> || str[i] == <span class="string">'/'</span>)&#123;</span><br><span class="line">            <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(S.empty() || S.top() == <span class="string">'('</span>)&#123;</span><br><span class="line">                    S.push(str[i]);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(priority(str[i]) &gt; priority(S.top()))&#123;</span><br><span class="line">                    S.push(str[i]);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="keyword">char</span> top = S.top();</span><br><span class="line">                    result.push(top);</span><br><span class="line">                    S.pop();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(str[i] == <span class="string">'('</span>)</span><br><span class="line">                S.push(str[i]);</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">while</span>(S.top() != <span class="string">'('</span>)&#123;</span><br><span class="line">                    <span class="keyword">char</span> top = S.top();</span><br><span class="line">                    result.push(top);</span><br><span class="line">                    S.pop();</span><br><span class="line">                &#125;</span><br><span class="line">                S.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(!result.empty())&#123;</span><br><span class="line">        <span class="keyword">char</span> top = result.top();</span><br><span class="line">        S.push(top);</span><br><span class="line">        result.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">float</span>&gt; C;</span><br><span class="line">    <span class="keyword">while</span>(!S.empty())&#123;</span><br><span class="line">        <span class="keyword">char</span> top = S.top();</span><br><span class="line">        S.pop();</span><br><span class="line">        <span class="keyword">if</span>(top &gt;= <span class="string">'0'</span> &amp;&amp; top &lt;= <span class="string">'9'</span>)&#123;</span><br><span class="line">            C.push(top - <span class="string">'0'</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">float</span> b = C.top();</span><br><span class="line">            C.pop();</span><br><span class="line">            <span class="keyword">float</span> a = C.top();</span><br><span class="line">            C.pop();</span><br><span class="line">            <span class="keyword">float</span> result = calculate(a, top, b);</span><br><span class="line">            C.push(result);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">float</span> num = C.top();</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"计算结果："</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="哈夫曼树"><a href="#哈夫曼树" class="headerlink" title="哈夫曼树"></a>哈夫曼树</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    priority_queue&lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt; &gt; Q;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) != EOF)&#123;</span><br><span class="line">        <span class="keyword">while</span>(!Q.empty())</span><br><span class="line">            Q.pop();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> x;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; x;</span><br><span class="line">            Q.push(x);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(Q.size() &gt; <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">int</span> a = Q.top();</span><br><span class="line">            Q.pop();</span><br><span class="line">            <span class="keyword">int</span> b = Q.top();</span><br><span class="line">            Q.pop();</span><br><span class="line">            ans += a + b;</span><br><span class="line">            Q.push(a + b);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; ans &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="二叉树遍历-前序-中序-gt-后序"><a href="#二叉树遍历-前序-中序-gt-后序" class="headerlink" title="二叉树遍历[前序+中序-&gt;后序]"></a>二叉树遍历[前序+中序-&gt;后序]</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">TNode</span> &#123;</span></span><br><span class="line">    <span class="keyword">char</span> data;</span><br><span class="line">    TNode *lchild, *rchild;</span><br><span class="line">&#125;*Tree;</span><br><span class="line"><span class="keyword">char</span> preOrder[] = <span class="string">"FDXEAG"</span>, inOrder[] = <span class="string">"XDEFAG"</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Tree <span class="title">createTree</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Tree T = <span class="keyword">new</span> TNode;</span><br><span class="line">    T-&gt;lchild = T-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> T;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">postOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T-&gt;lchild != <span class="literal">NULL</span>)</span><br><span class="line">        postOrder(T-&gt;lchild);</span><br><span class="line">    <span class="keyword">if</span>(T-&gt;rchild != <span class="literal">NULL</span>)</span><br><span class="line">        postOrder(T-&gt;rchild);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; T-&gt;data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Tree <span class="title">BuildTree</span><span class="params">(<span class="keyword">int</span> s1, <span class="keyword">int</span> e1, <span class="keyword">int</span> s2, <span class="keyword">int</span> e2)</span></span>&#123;</span><br><span class="line">    Tree T = createTree();</span><br><span class="line">    T-&gt;data = preOrder[s1];</span><br><span class="line">    <span class="keyword">int</span> rootIdx;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(inOrder); i++)</span><br><span class="line">        <span class="keyword">if</span>(inOrder[i] == preOrder[s1])&#123;</span><br><span class="line">            rootIdx = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">if</span>(rootIdx != s2)&#123; <span class="comment">//左子树非空</span></span><br><span class="line">        T-&gt;lchild = BuildTree(s1 + <span class="number">1</span>, s1 + (rootIdx - s2), s2, rootIdx - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(rootIdx != e2)&#123; <span class="comment">//右子树非空</span></span><br><span class="line">        T-&gt;rchild = BuildTree(s1 + (rootIdx - s2) + <span class="number">1</span>, e1, rootIdx + <span class="number">1</span>, e2);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> T;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%s"</span>, preOrder) != EOF)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%s"</span>, inOrder);</span><br><span class="line">        <span class="keyword">int</span> len_pre = <span class="built_in">strlen</span>(preOrder), len_in = <span class="built_in">strlen</span>(inOrder);</span><br><span class="line">        Tree T = BuildTree(<span class="number">0</span>, len_pre - <span class="number">1</span>, <span class="number">0</span>, len_in - <span class="number">1</span>);</span><br><span class="line">        postOrder(T);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="二叉排序树"><a href="#二叉排序树" class="headerlink" title="二叉排序树"></a>二叉排序树</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">TNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">    TNode *lchild, *rchild;</span><br><span class="line">&#125;*Tree;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">preOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; T-&gt;data &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        preOrder(T-&gt;lchild);</span><br><span class="line">        preOrder(T-&gt;rchild);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">inOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        inOrder(T-&gt;lchild);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; T-&gt;data &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        inOrder(T-&gt;rchild);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">postOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        postOrder(T-&gt;lchild);</span><br><span class="line">        postOrder(T-&gt;rchild);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; T-&gt;data &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Tree <span class="title">Insert</span><span class="params">(Tree T, <span class="keyword">int</span> X)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!T)&#123;</span><br><span class="line">        T = <span class="keyword">new</span> TNode;</span><br><span class="line">        T-&gt;lchild = T-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">        T-&gt;data = X;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(X &gt; T-&gt;data)</span><br><span class="line">        T-&gt;rchild = Insert(T-&gt;rchild, X);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(X &lt; T-&gt;data)</span><br><span class="line">        T-&gt;lchild = Insert(T-&gt;lchild, X);</span><br><span class="line">    <span class="keyword">return</span> T;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) != EOF)&#123;</span><br><span class="line">        Tree T = <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> num;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;num);</span><br><span class="line">            T = Insert(T, num);</span><br><span class="line">        &#125;</span><br><span class="line">        preOrder(T);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        inOrder(T);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        postOrder(T);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="同一二叉搜索树序列"><a href="#同一二叉搜索树序列" class="headerlink" title="同一二叉搜索树序列"></a>同一二叉搜索树序列</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">TNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">    TNode *lchild, *rchild;</span><br><span class="line">&#125;*Tree;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> *str;</span><br><span class="line"><span class="keyword">char</span> str1[<span class="number">40</span>], str2[<span class="number">40</span>];</span><br><span class="line"><span class="keyword">int</span> *size;</span><br><span class="line"><span class="keyword">int</span> size1, size2;</span><br><span class="line"></span><br><span class="line"><span class="function">Tree <span class="title">Insert</span><span class="params">(Tree T, <span class="keyword">int</span> X)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        T = <span class="keyword">new</span> TNode;</span><br><span class="line">        T-&gt;data = X;</span><br><span class="line">        T-&gt;lchild = T-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(X &lt; T-&gt;data)</span><br><span class="line">        T-&gt;lchild = Insert(T-&gt;lchild, X);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(X &gt; T-&gt;data)</span><br><span class="line">        T-&gt;rchild = Insert(T-&gt;rchild, X);</span><br><span class="line">    <span class="keyword">return</span> T;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">preOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        str[(*size)++] = T-&gt;data + <span class="string">'0'</span>;</span><br><span class="line">        preOrder(T-&gt;lchild);</span><br><span class="line">        preOrder(T-&gt;rchild);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">inOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        inOrder(T-&gt;lchild);</span><br><span class="line">        str[(*size)++] = T-&gt;data + <span class="string">'0'</span>;</span><br><span class="line">        inOrder(T-&gt;rchild);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">postOrder</span><span class="params">(Tree T)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        postOrder(T-&gt;lchild);</span><br><span class="line">        postOrder(T-&gt;rchild);</span><br><span class="line">        str[(*size)++] = T-&gt;data + <span class="string">'0'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span>(n != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">char</span> originTree[<span class="number">20</span>];</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; originTree;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">char</span> compareTree[<span class="number">20</span>];</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; compareTree;</span><br><span class="line">            size1 = <span class="number">0</span>;</span><br><span class="line">            size = &amp;size1;</span><br><span class="line">            str = str1;</span><br><span class="line">            Tree T1 = <span class="literal">NULL</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="built_in">strlen</span>(originTree); j++)</span><br><span class="line">                T1 = Insert(T1, originTree[j] - <span class="string">'0'</span>);</span><br><span class="line">            preOrder(T1);</span><br><span class="line">            inOrder(T1);</span><br><span class="line">            str1[size1] = <span class="string">'\0'</span>;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">strlen</span>(compareTree) != <span class="built_in">strlen</span>(originTree))</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"NO"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                size2 = <span class="number">0</span>;</span><br><span class="line">                size = &amp;size2;</span><br><span class="line">                str = str2;</span><br><span class="line">                Tree T2 = <span class="literal">NULL</span>;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="built_in">strlen</span>(compareTree); j++)</span><br><span class="line">                    T2 = Insert(T2, compareTree[j] - <span class="string">'0'</span>);</span><br><span class="line">                preOrder(T2);</span><br><span class="line">                inOrder(T2);</span><br><span class="line">                str2[size2] = <span class="string">'\0'</span>;</span><br><span class="line">                <span class="built_in">puts</span>(<span class="built_in">strcmp</span>(str1, str2) == <span class="number">0</span> ? <span class="string">"YES"</span> : <span class="string">"NO"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="最大公约数"><a href="#最大公约数" class="headerlink" title="最大公约数"></a>最大公约数</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(b == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        gcd(b, a % b);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;a, &amp;b) != EOF)&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; gcd(a, b) &lt;&lt; <span class="built_in">endl</span>;;</span><br><span class="line">        <span class="keyword">while</span>(a != <span class="number">0</span> &amp;&amp; b != <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">int</span> temp = a;</span><br><span class="line">            a = b;</span><br><span class="line">            b = temp % b;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(a == <span class="number">0</span>)</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="最小公倍数"><a href="#最小公倍数" class="headerlink" title="最小公倍数"></a>最小公倍数</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(b == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        gcd(b, a % b);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;a, &amp;b) != EOF)&#123;</span><br><span class="line">        <span class="keyword">int</span> c = gcd(a, b);</span><br><span class="line">        <span class="keyword">int</span> l = a / c * b;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; l &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="列出连通集"><a href="#列出连通集" class="headerlink" title="列出连通集"></a>列出连通集</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Vertex int</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> WeightType int</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MaxVertexNum 1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bool</span> visited[MaxVertexNum];</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">GNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> Gv, Ge;</span><br><span class="line">    <span class="keyword">int</span> vexs[MaxVertexNum];</span><br><span class="line">    WeightType G[MaxVertexNum][MaxVertexNum];</span><br><span class="line">&#125;*MGraph;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ENode</span>&#123;</span></span><br><span class="line">    Vertex v1, v2;</span><br><span class="line">&#125;*Edge;</span><br><span class="line"></span><br><span class="line"><span class="function">MGraph <span class="title">BuildGraph</span><span class="params">(<span class="keyword">int</span> VertexNum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    MGraph Graph = <span class="keyword">new</span> GNode;</span><br><span class="line">    Graph-&gt;Gv = VertexNum;</span><br><span class="line">    Graph-&gt;Ge = <span class="number">0</span>;</span><br><span class="line">    Vertex v, w;</span><br><span class="line">    <span class="keyword">for</span>(v = <span class="number">0</span>; v &lt; Graph-&gt;Gv; v++)</span><br><span class="line">        <span class="keyword">for</span>(w = <span class="number">0</span>; w &lt; Graph-&gt;Gv; w++)</span><br><span class="line">            Graph-&gt;G[v][w] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Graph-&gt;Gv; i++)</span><br><span class="line">        Graph-&gt;vexs[i] = i;</span><br><span class="line">    <span class="keyword">return</span> Graph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(MGraph g, Edge e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    g-&gt;G[e-&gt;v1][e-&gt;v2] = <span class="number">1</span>;</span><br><span class="line">    g-&gt;G[e-&gt;v2][e-&gt;v1] = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(MGraph g, <span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    visited[i] = <span class="literal">true</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">' '</span> &lt;&lt; g-&gt;vexs[i];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; g-&gt;Gv; j++)</span><br><span class="line">        <span class="keyword">if</span>(g-&gt;G[i][j] == <span class="number">1</span> &amp;&amp; !visited[j])</span><br><span class="line">            DFS(g, j);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFSTraverse</span><span class="params">(MGraph g)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; g-&gt;Gv; i++)</span><br><span class="line">        visited[i] = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; g-&gt;Gv; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!visited[i])&#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"&#123;"</span>;</span><br><span class="line">            DFS(g, i);</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">" &#125;\n"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BFSTraverse</span><span class="params">(MGraph g)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; g-&gt;Gv; i++)</span><br><span class="line">        visited[i] = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; g-&gt;Gv; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!visited[i])&#123;</span><br><span class="line">            visited[i] = <span class="literal">true</span>;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"&#123;"</span>;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">' '</span> &lt;&lt; g-&gt;vexs[i];</span><br><span class="line">            q.push(i);</span><br><span class="line">            <span class="keyword">while</span>(!q.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">int</span> k = q.front();</span><br><span class="line">                q.pop();</span><br><span class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; g-&gt;Gv; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(g-&gt;G[k][j] == <span class="number">1</span> &amp;&amp; !visited[j])</span><br><span class="line">                    &#123;</span><br><span class="line">                        visited[j] = <span class="literal">true</span>;</span><br><span class="line">                        <span class="built_in">cout</span> &lt;&lt; <span class="string">' '</span> &lt;&lt; g-&gt;vexs[j];</span><br><span class="line">                        q.push(j);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">" &#125;\n"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N;</span><br><span class="line">    MGraph G = BuildGraph(N);</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; G-&gt;Ge;</span><br><span class="line">    <span class="keyword">if</span>(G-&gt;Ge != <span class="number">0</span>)&#123;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; G-&gt;Ge; i++)&#123;</span><br><span class="line">            Edge e = <span class="keyword">new</span> ENode;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; e-&gt;v1 &gt;&gt; e-&gt;v2;</span><br><span class="line">            Insert(G, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    DFSTraverse(G);</span><br><span class="line">    BFSTraverse(G);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="计算智能课程"><a href="#计算智能课程" class="headerlink" title="计算智能课程"></a>计算智能课程</h2><h3 id="分油问题"><a href="#分油问题" class="headerlink" title="分油问题"></a>分油问题</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> now[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">int</span> step;</span><br><span class="line">    <span class="keyword">int</span> pre;</span><br><span class="line">&#125;state;<span class="comment">//当前状态，包括油瓶现容量以及上一步操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> capcacity[<span class="number">3</span>] = &#123;<span class="number">10</span>, <span class="number">7</span>, <span class="number">3</span>&#125;; <span class="comment">//油瓶的容量</span></span><br><span class="line"><span class="keyword">int</span> visit[<span class="number">11</span>][<span class="number">8</span>][<span class="number">4</span>]; <span class="comment">//记录当前的操作，避免重复上一步操作</span></span><br><span class="line"><span class="built_in">vector</span>&lt;state&gt; path;<span class="comment">//记录找到答案时的操作</span></span><br><span class="line"><span class="keyword">int</span> index = <span class="number">-1</span>, n = <span class="number">1</span>;<span class="comment">//f记录当前操作的序号</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bfs</span><span class="params">(state &amp;S, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;<span class="comment">//使用广度优先搜索求解</span></span><br><span class="line">    <span class="keyword">if</span>(i == j) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(S.now[i] == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(S.now[j] == capcacity[j]) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(S.now[i] + S.now[j] &lt;= capcacity[j])&#123;<span class="comment">//倒油</span></span><br><span class="line">        S.now[j] = S.now[j] + S.now[i];</span><br><span class="line">        S.now[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        S.now[i] = S.now[i] - (capcacity[j] - S.now[j]);</span><br><span class="line">        S.now[j] = capcacity[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(visit[S.now[<span class="number">0</span>]][S.now[<span class="number">1</span>]][S.now[<span class="number">2</span>]]) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        visit[S.now[<span class="number">0</span>]][S.now[<span class="number">1</span>]][S.now[<span class="number">2</span>]] = <span class="number">1</span>;</span><br><span class="line">        S.pre = index;</span><br><span class="line">        path.push_back(S);</span><br><span class="line">        n++;</span><br><span class="line">    &#125;</span><br><span class="line">    S.step++;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> i)</span></span>&#123;</span><br><span class="line">    <span class="built_in">stack</span>&lt;state&gt; output;</span><br><span class="line">    <span class="keyword">while</span>(i != <span class="number">-1</span>)&#123;<span class="comment">//倒序输出</span></span><br><span class="line">        state S;</span><br><span class="line">        S.now[<span class="number">0</span>] = path[i].now[<span class="number">0</span>];</span><br><span class="line">        S.now[<span class="number">1</span>] = path[i].now[<span class="number">1</span>];</span><br><span class="line">        S.now[<span class="number">2</span>] = path[i].now[<span class="number">2</span>];</span><br><span class="line">        output.push(S);</span><br><span class="line">        i = path[i].pre;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"A\tB\tC\n"</span>);</span><br><span class="line">    <span class="keyword">while</span>(!output.empty())&#123;</span><br><span class="line">        state S = output.top();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\t%d\t%d\n"</span>, S.now[<span class="number">0</span>], S.now[<span class="number">1</span>], S.now[<span class="number">2</span>]);</span><br><span class="line">        output.pop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> end[<span class="number">3</span>] = &#123;<span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> i, j, flag, answer;</span><br><span class="line">    <span class="built_in">queue</span>&lt;state&gt;q;</span><br><span class="line">    flag = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">memset</span>(visit, <span class="number">0</span>, <span class="keyword">sizeof</span>(visit));</span><br><span class="line">    state start;<span class="comment">//初始状态</span></span><br><span class="line">    start.now[<span class="number">0</span>] = <span class="number">10</span>;</span><br><span class="line">    start.now[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">    start.now[<span class="number">2</span>] = <span class="number">0</span>;</span><br><span class="line">    start.step = <span class="number">0</span>;</span><br><span class="line">    q.push(start);</span><br><span class="line">    start.pre = <span class="number">-1</span>;</span><br><span class="line">    path.push_back(start);</span><br><span class="line">    <span class="keyword">while</span>(!q.empty())&#123;</span><br><span class="line">        state S = q.front();</span><br><span class="line">        index++;</span><br><span class="line">        q.pop();</span><br><span class="line">        <span class="keyword">if</span>(S.now[<span class="number">0</span>] == end[<span class="number">0</span>] &amp;&amp; S.now[<span class="number">1</span>] == end[<span class="number">1</span>])&#123;<span class="comment">//得到目标状态</span></span><br><span class="line">            flag = <span class="number">1</span>;</span><br><span class="line">            answer = index;</span><br><span class="line">            S.pre = answer;</span><br><span class="line">            path.push_back(S);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; <span class="number">3</span>; j++)&#123;</span><br><span class="line">                state _S = S;</span><br><span class="line">                <span class="keyword">if</span>(bfs(_S, i, j))</span><br><span class="line">                    q.push(_S);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    print(answer);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Leetcode"><a href="#Leetcode" class="headerlink" title="Leetcode"></a>Leetcode</h2><p><a href="https://github.com/BitHub00/Leetcode" target="_blank" rel="noopener">bithub00/Leetcode</a></p>
]]></content>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS:S3 + Athena + Glue</title>
    <url>/2019/04/03/AWS/</url>
    <content><![CDATA[<p><font size="3"><br>整理一下自己了解的S3、Athena和Glue
</font><br><a id="more"></a></p>
<h2 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h2><p>AWS使用S3（Simple Storage Service）进行存储，它可以存储海量的数据，存储的往往是不常使用的冷数据，而且采取特定的Parquet格式进行列式存储或分区，可以节省存储空间而且提升查询性能。</p>
<p>使用S3存储动态数据会使得系统依赖于S3本身的可用性，可以通过增加缓存层或CDN(Content Deliver Network)服务来减轻影响。</p>
<p>S3提供REST接口,两个组成层次是容器(bucket)和对象(object)。容器是S3最上面的分类，所有的对象都放在容器里，因此容器名称必须是唯一的，每个对象都有唯一可识别的URL，应该尽量避免对容器进行操作，使用对象名称就可以实现分层次和分类了.</p>
<p>存储在S3的对象，除了本身的值以外,还会记录标头、元数据、访问控制列表等等，标头中存储了对象类型等信息，元数据就是用户自己定义的表头，为键值对，访问控制列表就是访问权限。因为S3有所谓偷窥对象的功能，即只读取对象的标头的信息，我们可以先看标头信息，再决定要不要把对象读取下来。</p>
<p>S3没有目录的概念，是扁平化的存储结构，“photo/1.jpg”和“photo/2.jpg”可能存在于不同的服务器集群。</p>
<h2 id="Athena"><a href="#Athena" class="headerlink" title="Athena"></a>Athena</h2><p>Athena是一个查询服务，可以使用标准的SQL来对S3上存储的数据进行查询。而且它是一个serverless的服务，不需要去考虑底层的硬件设施，只需要为查询服务付费。同时，Athena使用IAM来管理权限，部分操作需要对应的权限才能进行。</p>
<p>Athena使用SerDe来与各种数据格式进行交互，包括CSV,JSON和Parquet。在使用时进行指定。</p>
<h2 id="Glue"><a href="#Glue" class="headerlink" title="Glue"></a>Glue</h2><p>Glue是一个元数据系统，它维护了信息诸如数据具体存储的位置以及数据的结构，它本身还提供了ETL的能力。<br>Glue里面几个关键的概念是Database, Table, Crawler, Classifier, Job:</p>
<ul>
<li>Database 跟我们普通理解的数据库的概念是类似的，是一组table的逻辑集合。</li>
<li>Table 是数据的元数据，它定义数据保存在哪里(比如S3的路径)，有哪些column，怎么分区的。</li>
<li>Crawler 是元数据的爬虫，你给它一个路径，告诉它每天去爬一次，Crawler就可以及时把更新的元数据，比如新增的分区同步到Glue里面来供计算引擎消费。</li>
<li>Classifier 是数据结构的解析器，你给Crawler一个S3的路径它怎么就能解析出其中的结构呢，这就是Classifier要干的事情，Glue里面已经内置了一些Classfier, 用户也可以自定义Classifier。</li>
<li>Job是一个ETL脚本</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/04/03/AgAio8.png" alt=""></p>
<h2 id="Glue与Athena"><a href="#Glue与Athena" class="headerlink" title="Glue与Athena"></a>Glue与Athena</h2><p>AWS Glue 是一项完全托管的 ETL (提取、转换和加载) 服务，能够对数据进行分类、清理和扩充，并在各种数据存储之间可靠地移动数据。AWS Glue 爬网程序自动从源数据推断数据库和表架构，从而将关联的元数据存储在 AWS Glue 数据目录中。在 Athena 中创建表时，可以选择使用 AWS Glue 爬网程序创建表。</p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><ul>
<li><p><a href="https://aws.amazon.com/cn/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/" target="_blank" rel="noopener">使用Glue读取csv并转换成Parquet格式随后使用Athena查询</a></p>
</li>
<li><p><a href="https://aws.amazon.com/cn/blogs/china/etl-pipeline-for-serverless-architecture-using-glue/" target="_blank" rel="noopener">Glue自定义分类器识别服务器日志</a></p>
</li>
<li><p><a href="https://aws.amazon.com/cn/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2/" target="_blank" rel="noopener">How to extract, transform, and load data for analytic processing using AWS Glue (Part 2)</a></p>
</li>
<li><p><a href="https://gorillalogic.com/blog/in-search-of-happiness-a-quick-etl-use-case-with-aws-glue-redshift/" target="_blank" rel="noopener">In Search of Happiness: A Quick ETL Use Case with AWS Glue + Redshift</a></p>
</li>
</ul>
<h2 id="Athena与S3"><a href="#Athena与S3" class="headerlink" title="Athena与S3"></a>Athena与S3</h2><p>Athena 可帮助分析在 Amazon S3 中存储的非结构化、半结构化和结构化数据。包括 CSV、JSON 或列式数据格式，如 Apache Parquet 和 Apache ORC。可以使用 ANSI SQL 通过 Athena 运行临时查询，而无需将数据聚合或加载到 Athena 中。</p>
]]></content>
      <tags>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepInf - Social Influence Prediction with Deep Learning[KDD&#39;18]</title>
    <url>/2020/12/22/DeepInf%5BKDD18%5D/</url>
    <content><![CDATA[<p>KDD18一篇将GNN应用于社交网络中用户影响力预测任务的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何在图结构的社交数据中预测顶点的影响力。</p>
<p>在图中，给定顶点$v$与它的邻域以及一个时间段，通过对开始时各顶点的状态进行建模，来对结束时顶点$v$的状态进行预测（是否被激活）。</p>
<h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><ul>
<li>邻域：给定图$G=(V,E)$，顶点$v$的邻域定义为$N_v^r=\{u:d(u,v)\le r\}$，是一个顶点集合，不包含顶点$v$自身</li>
<li>中心网络：由邻域中的顶点及边所组成的网络，以$G_v^r$表示</li>
<li>用户行为：以$s_v^t$表示，用户对应于图中的顶点，对于一个时刻$t$，如果顶点$v$有产生动作，例如转发、引用等，则$s_v^t=1$</li>
</ul>
<p>给定用户$v$的中心网络、邻域中用户的行为集合$S_v^t=\{s_i^t:i\in N_v^r\}$，论文想解决的问题是，在一段时间$Δt$后，对用户$v$的行为的预测：</p>
<script type="math/tex; mode=display">
P(s_v^{t+Δt}|G_v^r,S_v^t)</script><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p><a href="https://imgchr.com/i/BGDfOO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGDfOO.png" alt="BGDfOO.png" border="0"></a></p>
<p>数据预处理方面，论文通过带重启的随机漫步来为图中的每个顶点$v$获取固定大小$n$的中心网络$G_v^r$，接着使用$\text{DeepWalk}$来得到图中顶点的embedding，最后进行归一化。通过这几个步骤对图中的特征进行提取后，论文还进一步添加了几种人工提取的特征，包括用户是否活跃等等：</p>
<div align="center">
<a href="https://imgchr.com/i/BGyXX6" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGyXX6.png" alt="BGyXX6.png" border="0" width="70%"></a>
</div>


<blockquote>
<p>摘要里说传统的影响力建模方法都是人工提取图中顶点及结构的特征，论文的出发点就是自动学习这种特征表示，结果在预处理的最后还是添加了几种人工提取的特征，这不是自相矛盾吗？</p>
</blockquote>
<p>经过上面的步骤后，最后得到包含所有用户特征的一个特征矩阵$H\in \mathbb{R}^{n\times F}$，每一行$h_i^T$表示一个用户的特征，$F$等同于$\text{DeepWalk}$长度加上人工特征长度。</p>
<h4 id="影响力计算"><a href="#影响力计算" class="headerlink" title="影响力计算"></a>影响力计算</h4><p>这一步纯粹是在套GAT的框架，没什么可以说的，计算如下：</p>
<script type="math/tex; mode=display">
H'=\text{GAT}(H)=g(A_{\text{GAT}}(G)HW^T+b)\\
A_{\text{GAT}}(G)=[a_{ij}]_{n\times n}</script><p>其中$W\in \mathbb{R}^{F’\times F}, b\in \mathbb{R}^{F’}$是模型的参数，$a_{ij}$的计算在GAT论文的笔记中有记录，不再赘述。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>OAG、Digg、Twitter、Weibo</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Attention Networks[ICLR&#39;18]</title>
    <url>/2020/12/22/GAT%5BICLR18%5D/</url>
    <content><![CDATA[<p>ICLR18一篇解决GCN聚合信息时无法区分信息重要性的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将attention机制应用于图类型的数据上。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><div align="center">
<img src="https://s1.ax1x.com/2020/10/16/07O1IK.png" alt="1" border="0" width="60%">
<img src="https://s1.ax1x.com/2020/10/16/07O8PO.png" alt="2" border="0" width="60%">
</div>


<p>给定一个含$n$个顶点的图，其中顶点的特征构成的集合为$(\overrightarrow{h_1},\overrightarrow{h_2},\dots,\overrightarrow{h_n})$，$\overrightarrow{h_i}\in \mathbb{R}^F$且邻接矩阵为$A$。一个图卷积层根据已有的顶点特征和图的结构来计算一个新的特征集合$(\overrightarrow{h_1’},\overrightarrow{h_2’},\dots,\overrightarrow{h_n’})$，$\overrightarrow{h_i’}\in \mathbb{R}^{F’}$</p>
<p>每个图卷积层首先会进行特征转换，以特征矩阵$W$表示，$W\in \mathbb{R}^{F’\times F}$它将特征向量线性转换为$\overrightarrow{g_i}=W\overrightarrow{h_i}$，再将新得到的特征向量以某种方式进行结合。为了利用邻域的信息，一种典型的做法如下：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_i}'=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}\overrightarrow{g_j}\bigg)</script><p>其中$N_i$表示顶点$i$的邻域（典型的构造方式是选取直接相连的顶点，包括自身），$\alpha_{ij}$表示顶点$j$的特征对于顶点$i$的重要程度，也可以看成一种权重。</p>
<p>现有的做法都是显式地定义$\alpha_{ij}$，本文的创新之处在于使用attention机制隐式地定义$\alpha_{ij}$。所使用的attention机制定义为$a:R^{F’}\times \mathbb{R}^{F’} \rightarrow \mathbb{R}$，以一个权重向量$\overrightarrow{a}\in \mathbb{R}^{2F’}$表示，对应于论文中的self-attention。  </p>
<h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><ol>
<li>基于顶点的特征计算系数$e_{ij}$</li>
</ol>
<script type="math/tex; mode=display">
e_{ij}=a(W\overrightarrow{h_i},W\overrightarrow{h_j})</script><ol>
<li>以顶点的邻域将上一步计算得到的系数正则化，这么做能引入图的结构信息：</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_{ij}&=\text{softmax}_j(e_{ij})=\frac{\exp(e_{ij})}{\sum_{k\in N_i}\exp(e_{ik})}\\
&=\frac{\exp(\text{LeakyReLU}(\overrightarrow{a}^T[W\overrightarrow{h}_i||W\overrightarrow{h}_j]))}{\sum_{k\in N_i}\exp(\text{LeakyReLU}(\overrightarrow{a}^T[W\overrightarrow{h}_i||W\overrightarrow{h}_k]))}
\end{aligned}</script><p><img src="https://s1.ax1x.com/2020/10/16/0H5cX6.png" alt="0H5cX6.png" border="0" width="30%"></p>
<blockquote>
<p>次序不变性：给定$(i,j),(i,k),(i’,j),(i’,k)$表示两个顶点间的关系，可以为边或自环。$a$为对应的attention系数，如果$a_{ij}&gt;a_{ik}$，则有$a_{i’j}&gt;a_{i’k}$</p>
</blockquote>
<p>​    <a href="https://dl.acm.org/doi/10.1145/3219819.3220077" target="_blank" rel="noopener">DeepInf</a>中给出了证明：</p>
<p>​    将权重向量$\overrightarrow{a}\in \mathbb{R}^{2F’}$重写为$\overrightarrow{a}=[p^T，q^T]$，则有</p>
<script type="math/tex; mode=display">
e_{ij}=\text{LeakyReLU}(p^TWh_i+q^TWh_j)</script><p>​    由softmax与LeakyReLU的单调性可知，因为$a_{ij}&gt;a_{ik}$，有$q^TWh_j&gt;q^TWh_k$，类似地就可以得到$a_{i’j}&gt;a_{i’k}$。</p>
<p>​    这意味着，即使每个顶点都只关注于自己的邻域，但得到的attention系数却具有全局性。</p>
<ol>
<li>以上一步得到的系数$\alpha_{ij}$作为顶点$j$的特征对顶点$i$的重要程度，将领域中各顶点的特征做一个线性组合以作为顶点$i$最终输出的特征表示：</li>
</ol>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}W\overrightarrow{h_j}\bigg)</script><h4 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><p>为了稳定self-attention的学习过程，论文引入了multi-head attention，即由$K$个相互独立的self-attention得到各自的特征，再进行拼接：</p>
<p><img src="https://s1.ax1x.com/2020/10/16/0HbZlj.png" alt="0HbZlj.png" border="0" width="60%"></p>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\Vert_{k=1}^K\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)</script><p>其中$\alpha_{ij}^k$是第$k$个attention机制$(a^k)$计算出来的正则化系数，$W^k$是对应的将输入进行线性转化的权重矩阵。论文选取的拼接操作为求平均：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\sigma\bigg(\frac{1}{K}\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、PPI</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>CUDA学习笔记</title>
    <url>/2020/07/15/CUDA%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>CUDA编程的学习笔记以及踩过的一些坑<br><a id="more"></a></p>
<h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><h2 id="CUDA程序的执行流程："><a href="#CUDA程序的执行流程：" class="headerlink" title="CUDA程序的执行流程："></a>CUDA程序的执行流程：</h2><ol>
<li>分配host内存，并进行数据初始化；</li>
<li>分配device内存，并从host将数据拷贝到device上；</li>
<li>调用CUDA的核函数在device上完成指定的运算；</li>
<li>将device上的运算结果拷贝到host上；</li>
<li>释放device和host上分配的内存。</li>
</ol>
<p>上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，核函数用<strong>global</strong>符号声明，它在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，而网格又可以分为很多线程块（block），一个线程块里面包含很多线程。这构成了两层结构。</p>
<p><img src="https://s1.ax1x.com/2020/07/15/U0Mzd0.png" alt=""></p>
<h2 id="CUDA内存模型"><a href="#CUDA内存模型" class="headerlink" title="CUDA内存模型"></a>CUDA内存模型</h2><p>每个线程有自己的私有本地内存（Local Memory），而每个线程块（Block）有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）</p>
<h2 id="如何使用CUDA对程序进行优化"><a href="#如何使用CUDA对程序进行优化" class="headerlink" title="如何使用CUDA对程序进行优化"></a>如何使用CUDA对程序进行优化</h2><ol>
<li>使用共享内存减少全局内存读取次数；从常量内存（Constant Memory）中读取数据相较于全局内存会更节省内存带宽，因为常量内存是缓存的，从相同的地址中连续读取数据不会带来额外的内存读取开销。</li>
<li>流并行；通过多个流（stream）进行操作时，应该将所做的操作尽可能广地安排在各个流中，而不是尽可能地深，例如将内存拷贝和计算并行，其中的原理是：同一时刻cuda程序可以同时进行一个计算和数据传输。<br><img src="https://s1.ax1x.com/2020/07/15/U01S8U.png" alt="Stream1"><br><img src="https://s1.ax1x.com/2020/07/15/U01p2F.png" alt="Stream2"></li>
<li>合并访问；线程块内相邻的线程访问相邻的数据。因为半束(16)线程的访问16字节的数据可以合为一个访问指令。</li>
<li>使用零复制、锁页内存；零复制（zero copy）是一种特殊形式的内存映射，它允许你将host内存直接映射到device内存空间上。其实就是device可以通过直接内存访问（direct memory access，DMA）方式来访问host的锁页内存。 <blockquote>
<p>现代操作系统都支持虚拟内存，操作系统实现虚拟内存的主要方法就是通过分页机制。将内存中暂时不使用的内容换出到外存（硬盘等大容量存储）上，从而腾出空间存放将要调入内存的信息。这样，系统好像为用户提供了一个比实际内存大得多的存储器，称为虚拟存储器。锁页（page-locked host memory）就是将内存页面标记为不可被操作系统换出的内存。所以device可以使用页面的物理地址直接访问内存（DMA），从而避免从外存到内存的复制操作。在GPU上分配的内存默认都是锁页内存，这是因为GPU不支持将内存交换到磁盘上。在CPU上分配的内存默认都是可分页。</p>
</blockquote>
</li>
</ol>
<h2 id="重构时的问题及解决"><a href="#重构时的问题及解决" class="headerlink" title="重构时的问题及解决"></a>重构时的问题及解决</h2><ol>
<li><p>论文中涉及的数据结构较为复杂，在使用GPU进行并行计算时需要先将数据复制到GPU上，复制的步骤十分繁琐，容易出现内存访问出错的问题。</p>
<blockquote>
<p>CUDA6.0引入了统一内存的概念，大大简化了数据从CPU传到GPU这一步骤，只需要在host端分配好空间，写入数据，直接作为参数传入device端的核函数即可，CUDA会自动完成复制工作。</p>
</blockquote>
</li>
<li><p>c++中的std在CUDA中不被支持，例如std::vector、std::sort等都无法使用。</p>
<blockquote>
<p>CUDA提供了Thrust库作为替代，提供了sort、host_vector、device_vector等，然而这些都是host端的实现，在device端无法调用。</p>
</blockquote>
</li>
<li><p>Thrust作为host端的实现在device端无法调用，原数据结构中的类成员有vector<edge*>，在device端无法访问其中的数据。</edge*></p>
<blockquote>
<p>在后续CUDA版本中，thrust::sort可以在device端运行，只需加入参数thrust::sort(thrust::device, XX, XX)。vector在<strong>device</strong>函数中的访问仍不被支持，因此将原类成员vector<edge*>改成Edge **二维指针数组形式，vector读取完数据后，再复制到二维指针数组中，此时device端就可以访问其中的数据了。</edge*></p>
</blockquote>
</li>
<li><p>调试时nvcc编译带上-g参数，使用cuda-gdb运行程序，可以在cu文件中设置断点，发生段错误后可以使用where命令来查明是哪一行导致的错误。</p>
</li>
<li><p>addKernel&lt;&lt;<60,1>&gt;&gt;(devA, devB, devC);这里的&lt;&lt;<60,501>&gt;&gt;的意思是，调用函数的时候，开出60个线程格，每个线程格包含501个线程。在global函数中通过代码int i = threadIdx.x + blockIdx.x*blockDim.x;得到当前线程是第几个线程。</60,501></60,1></p>
</li>
<li><p>Host调用完kernel函数需要进行线程同步,而在kernel或global函数只需要在必要的地方__syncthreads()即可:</p>
</li>
<li><p>这时dev_b会保存一个指向GPU内存的指针,但是CPU是无法访问GPU上面的数据,所以利用这个指针做任何获取数据或者赋值都是会出错的,理解这点对传递数据很重要 </p>
 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> *dev_b = <span class="number">0</span></span><br><span class="line">cudaMalloc((<span class="keyword">void</span>**)&amp;dev_b, size * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>实用问答：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/14790999/how-to-pass-a-c-class-with-array-of-pointers-to-cuda" target="_blank" rel="noopener">https://stackoverflow.com/questions/14790999/how-to-pass-a-c-class-with-array-of-pointers-to-cuda</a></li>
<li><a href="https://stackoverflow.com/questions/9309195/copying-a-struct-containing-pointers-to-cuda-device" target="_blank" rel="noopener">https://stackoverflow.com/questions/9309195/copying-a-struct-containing-pointers-to-cuda-device</a></li>
<li><a href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/" target="_blank" rel="noopener">统一内存</a></li>
<li><a href="https://www.jianshu.com/p/7e1e0e2bde79" target="_blank" rel="noopener">教你一步步写一个cuda path tracer：cuda与类</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55855479" target="_blank" rel="noopener">CUDA值得注意的特性(坑)</a></li>
<li><a href="https://stackoverflow.com/questions/11874667/cuda-allocation-of-an-array-of-structs-inside-a-struct" target="_blank" rel="noopener">https://stackoverflow.com/questions/11874667/cuda-allocation-of-an-array-of-structs-inside-a-struct</a></li>
<li><a href="https://forums.developer.nvidia.com/t/how-to-cudamalloc-two-dimensional-array/4042/9" target="_blank" rel="noopener">https://forums.developer.nvidia.com/t/how-to-cudamalloc-two-dimensional-array/4042/9</a></li>
<li><a href="https://stackoverflow.com/questions/40682163/cuda-copy-inherited-class-object-to-device" target="_blank" rel="noopener">https://stackoverflow.com/questions/40682163/cuda-copy-inherited-class-object-to-device</a></li>
<li><a href="https://stackoverflow.com/questions/16024087/copy-an-object-to-device" target="_blank" rel="noopener">https://stackoverflow.com/questions/16024087/copy-an-object-to-device</a></li>
<li><a href="https://stackoverflow.com/questions/6929626/cuda-copy-to-array-within-array-of-objects" target="_blank" rel="noopener">https://stackoverflow.com/questions/6929626/cuda-copy-to-array-within-array-of-objects</a></li>
<li><a href="https://stackoverflow.com/questions/5510715/thrust-inside-user-written-kernels" target="_blank" rel="noopener">https://stackoverflow.com/questions/5510715/thrust-inside-user-written-kernels</a></li>
<li><a href="https://stackoverflow.com/questions/14284964/cuda-how-to-allocate-memory-for-data-member-of-a-class" target="_blank" rel="noopener">https://stackoverflow.com/questions/14284964/cuda-how-to-allocate-memory-for-data-member-of-a-class</a></li>
<li><a href="https://forums.developer.nvidia.com/t/how-to-measure-total-time-for-cpu-and-gpu/28234/2" target="_blank" rel="noopener">https://forums.developer.nvidia.com/t/how-to-measure-total-time-for-cpu-and-gpu/28234/2</a></li>
<li><a href="https://stackoverflow.com/questions/49878410/cuda-object-copy" target="_blank" rel="noopener">https://stackoverflow.com/questions/49878410/cuda-object-copy</a></li>
<li><a href="https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/" target="_blank" rel="noopener">Maximizing Unified Memory Performance in CUDA</a></li>
<li><a href="https://www.reddit.com/r/CUDA/comments/blegf0/are_thrust_vectors_useful/" target="_blank" rel="noopener">https://www.reddit.com/r/CUDA/comments/blegf0/are_thrust_vectors_useful/</a></li>
<li><a href="https://stackoverflow.com/questions/25702573/simple-cuda-test-always-fails-with-an-illegal-memory-access-was-encountered-er" target="_blank" rel="noopener">CUDA为什么需要将二维数组转换为一维数组进行访问(1)</a></li>
<li><a href="https://forums.developer.nvidia.com/t/cudamalloc-and-cudamemcpy-for-pointer-in-struct/35717/4" target="_blank" rel="noopener">CUDA为什么需要将二维数组转换为一维数组进行访问(2)</a></li>
<li><a href="https://stackoverflow.com/questions/18442018/how-to-pass-tree-struct-with-cuda" target="_blank" rel="noopener">https://stackoverflow.com/questions/18442018/how-to-pass-tree-struct-with-cuda</a></li>
<li><a href="https://stackoverflow.com/questions/15431365/cudamemcpy-segmentation-fault?rq=1" target="_blank" rel="noopener">https://stackoverflow.com/questions/15431365/cudamemcpy-segmentation-fault?rq=1</a></li>
<li><a href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/" target="_blank" rel="noopener">Grid-Stride Loops</a></li>
</ul>
]]></content>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>CrashSim An Efficient Algorithm for Computing SimRank over Static and Temporal Graphs[ICDE&#39;20]</title>
    <url>/2021/04/10/CrashSim%5BICDE20%5D/</url>
    <content><![CDATA[<p>ICDE20一篇高效计算SimRank相似度的论文</p>
<a id="more"></a>
<pre><code>这里我直接把组会分享的PPT和讲稿放上来了。
</code></pre><h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>​    大家好，今天我要讲的是ICDE2020年的一篇论文，“一种在静态和时序图上高效计算SimRank相似度的算法”。第一眼看到这个题目会想，首先什么是SimRank相似度，然后为什么它的计算是低效的，以及论文怎么进行高效计算。按照这个逻辑，首先介绍一下SimRank相似度，它是应用于图上的一种相似度计算方法，基本思想是，关联到相同顶点的两个顶点，相互之间具有相似性。以下面这个图片为例，两位教授相似度高因为他们来自同一所大学，而两位学生的相似度要相对低一些，在于他们并不直接来自于同一位教授。SimRank相似度原始的计算方式如下，它是一个迭代的过程。要计算顶点u和v的相似度，我们需要找到它们各自邻域中所有的顶点x和y，进行求和。前面的系数是为了避免热门顶点的影响而做的惩罚。因为是迭代计算，当图中顶点数量较多的时候计算就会变得低效。而VLDB17年的一篇论文证明了可以通过随机漫步近似计算SimRank相似度，这就为高效计算带来了新的思路。从随机漫步的角度来看，顶点u和v之间的SimRank相似度，可以看作从这两个顶点出发的随机漫步序列相交的概率。这个随机漫步的定义如下，以\sqrt{c}的概率漫步到下一个邻居，1-\sqrt{c}的概率终止于当前顶点。随机漫步可以看成是一种采样的方法，在图上的应用比较广泛，包括用来生成顶点的embedding如node2vec，或者在一个大图中构建子图捕获局部信息如GraphSAGE。既然是一个采样的方法，那么采样多少来保证获取到必要的信息就是一个关键的参数。同样地，在通过随机漫步近似计算SimRank相似度时，单次漫步的长度以及漫步的次数怎么选取，才能保证近似值与实际值之间的误差小于某个界限。论文的一个主要贡献就是给出了这两个参数的选取以及证明了它们的有效性。我对证明过程的每一步作了注解放在了ppt里，这里就不详细展开。主要是通过几何分布以及正态分布的3σ原则来证明。回到刚刚的式子，有了两个序列W(u)和W(v)，这时只剩下最后一步也就是怎么计算这里的概率。最直观的办法是通过蒙特卡洛模拟，给定一对查询顶点u和v，我们总共做n次随机漫步，用其中相遇的次数所占的比例作为概率的一个近似，然而这需要进行大量的尝试，直观但效率低。论文的改进做法是，与其去判断从u和v出发的随机漫步是否会相交，不如直接判断顶点v能否到达顶点u随机漫步的范围内。因为在静态图上固定了随机漫步的长度后，一个顶点能够漫步到的范围就是固定的，因此只需要在迭代之前计算一遍即可，不需要迭代时一次次地去计算。以左边的图为例，取漫步的长度$l_{max}=4$，在这个图中顶点A能够漫步到的范围就能用右边这颗树表示。因为随机漫步就是由概率决定是停止还是继续，所以只要将右边这棵树上的边用概率表示就可以进行近似计算。论文中给出的一个表示是下面这个式子，含义是这棵树每层之间边的一个关系。在实际编程实现时用的是一个二维矩阵$U\in \mathbb{R}^{l_{max}\times |\Omega|}$来表示这棵树，为什么这样的一个关系能够保证近似计算的有效性的证明我也放在了这里，同样不展开来讲了。单看表达式比较抽象，我就继续以刚才的例子说明这个计算过程，取参数$c=0.25$，初始时因为只能停留在起点A处，所以设这个概率值为1，$U(0,A)$表示以0步漫步到顶点A的概率。而一步能够漫步到的范围包括B、C两个顶点，以B为例，它的入度邻居数目为2，对应着上面的公式计算出它的概率值。所以漫步到顶点B的概率由先到顶点A的概率决定，这也符合随机漫步的定义。接下来以此类推。对于顶点A能够到达的顶点，它们在矩阵里对应位置的值都不为0。得到这么一个U矩阵后，迭代时不断地产生顶点v的随机漫步序列W(v)，根据这里的式子来近似计算SimRank相似度，直观上的理解是W(v)这个序列多大程度上走进了顶点u的这个漫步范围。还是刚才的例子，假设我们想得到顶点A和C之间的SimRank相似度，而某次迭代时顶点C产生的一条序列为$W(C)=(C,D,B,A)$，我们查阅刚刚得到的U矩阵中对应元素的值，进行累加，多次迭代后就得到了近似计算结果。回顾一下整个算法的流程，输入是单个顶点u和一系列顶点v，我们希望知道u和这些v之间的SimRank相似度大小。首先设定两个参数的取值，接下来在迭代前先预先计算顶点u的U矩阵，矩阵中的元素表示它漫步到某个顶点的概率。迭代过程中从顶点v产生一条随机漫步序列，通过U矩阵计算这条序列与u相遇的概率，多次取平均作为结果。到这里论文就完成了静态图部分的工作，接下来就是怎么将提出的方法继续用在时序图上。一个时序图通常由一系列快照图组成，每个快照图捕获了某个时间段内顶点之间的状态，图中就是一个包含三个快照图的时序图。联系现实可以拿微博举例子，假设我是顶点H，第一周关注了用户F，第二周取关了他，第三周用户G又新关注用户F。实际中因为用户的兴趣更新频率很快，所以时序图更能表达这种动态变化的兴趣。在时序图的情景下，最常见的两种SimRank查询分别为趋势查询和阈值查询。趋势查询的含义是，在给定的时间区间内，对于顶点u，我们希望找到一系列顶点v，它们与u的SimRank相似度在这个区间内是递增或递减的。而阈值查询是指在给定的时间区间内，它们与u的SimRank相似度大于某个阈值。因为时序图的每个快照图都可以看成静态图，最直观的想法就是把刚才的方法在每个快照图上算一遍，但这样只是照搬静态图的做法，没有用上时序图的特点，所以论文的后半部分针对时序图的特点提出了两个减少计算量的策略，分别是delta剪枝和差异剪枝，它们的思想也很直观，希望只对时序图中产生变化的顶点重新进行计算。回到刚才的例子图，会发现这三张快照图变的只是蓝框部分，红框部分一直维持不变。所以红框内的部分没必要每次都进行计算。具体到delta剪枝，一条新增或删除的边x-&gt;y影响的区域定义为下面两个部分，第一部分的意思是，如果顶点u能去的范围因为新增的边变大了，对应的这棵树也会改变。而第二部分的意思是，对于顶点y能到达的最远顶点$y_{lmax}$，反过来看顶点x刚好在它漫步范围之外，所以它在前后两个快照图里是不受影响的，涉及它的SimRank计算就可以省去。根据这个观察，delta剪枝的做法就是满足该前提的条件下，避免重计算未受影响区域的顶点。论证在下面主要从时间复杂度来说明，这里就省去了。而差异剪枝的想法要简单一些，因为近似计算依靠的就是随机漫步，如果前后两张快照图里漫步的范围没变，那计算结果也不会发生改变。所以差异剪枝的做法就是只要顶点u和v漫步范围不变，它们之间就不需要进行重计算。论证同样是通过复杂度说明。这两个策略具体放在流程里体现为图里的红框和蓝框，主要是判断是否达到对应的条件，然后删去不需要重计算的顶点来减少计算量。最后是实验部分，数据集和baseline的描述如下，结果主要是准确率和效率两方面，纵轴的ME表示近似值与真实值的最大误差，五个数据集上论文的方法都是又快又准，在时序图实验上也是一样。总结来说，论文的贡献是给出了一种高效计算SimRank相似度的方法并且证明了它的有效性，并且针对时序图的情景提出了两个优化策略。因为论文涉及比较多的证明和细节，看完可能不太明白这么计算的意义。图在许多领域应用广泛，而SimRank相似度既然是一种相似度，那它就可以应用在推荐系统、社交网络等一系列任务上，同时这些场景下往往有大量的用户，高效计算就有了实际的应用背景。不过这里也是我对论文有疑问的一个地方，因为实验用到的数据集都很小，最多的顶点也才三万多个，时序图也是以天为单位，而在现实生活中可能用户兴趣可能几个小时就发生改变了，例如微博热搜。而且即使在这么一个数据集上，论文的方法也要耗费75分钟的时间进行计算，感觉并不是很高效。</p>
<h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/cdSebj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSebj.png" alt="cdSebj.png"></a><br><a href="https://imgtu.com/i/cdSnVs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSnVs.png" alt="cdSnVs.png"></a><br><a href="https://imgtu.com/i/cdSuan" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSuan.png" alt="cdSuan.png"></a><br><a href="https://imgtu.com/i/cdSVKg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSVKg.png" alt="cdSVKg.png"></a><br><a href="https://imgtu.com/i/cdSZrQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSZrQ.png" alt="cdSZrQ.png"></a><br><a href="https://imgtu.com/i/cdSlGV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSlGV.png" alt="cdSlGV.png"></a><br><a href="https://imgtu.com/i/cdSK5q" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSK5q.png" alt="cdSK5q.png"></a><br><a href="https://imgtu.com/i/cdSQP0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSQP0.png" alt="cdSQP0.png"></a><br><a href="https://imgtu.com/i/cdSYqJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSYqJ.png" alt="cdSYqJ.png"></a><br><a href="https://imgtu.com/i/cdS12T" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS12T.png" alt="cdS12T.png"></a><br><a href="https://imgtu.com/i/cdS3xU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS3xU.png" alt="cdS3xU.png"></a><br><a href="https://imgtu.com/i/cdSGMF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSGMF.png" alt="cdSGMF.png"></a><br><a href="https://imgtu.com/i/cdSJr4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSJr4.png" alt="cdSJr4.png"></a><br><a href="https://imgtu.com/i/cdSwPx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSwPx.png" alt="cdSwPx.png"></a><br><a href="https://imgtu.com/i/cdSNZ9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSNZ9.png" alt="cdSNZ9.png"></a><br><a href="https://imgtu.com/i/cdSaI1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSaI1.png" alt="cdSaI1.png"></a><br><a href="https://imgtu.com/i/cdSUaR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSUaR.png" alt="cdSUaR.png"></a><br><a href="https://imgtu.com/i/cdS0G6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS0G6.png" alt="cdS0G6.png"></a><br><a href="https://imgtu.com/i/cdSBRK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSBRK.png" alt="cdSBRK.png"></a><br><a href="https://imgtu.com/i/cdS6qH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS6qH.png" alt="cdS6qH.png"></a><br><a href="https://imgtu.com/i/cdS5z8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS5z8.png" alt="cdS5z8.png"></a><br><a href="https://imgtu.com/i/cdS7LQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS7LQ.png" alt="cdS7LQ.png"></a><br><a href="https://imgtu.com/i/cdSsMD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSsMD.png" alt="cdSsMD.png"></a><br><a href="https://imgtu.com/i/cdSgZd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSgZd.png" alt="cdSgZd.png"></a><br><a href="https://imgtu.com/i/cdS2dA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS2dA.png" alt="cdS2dA.png"></a><br><a href="https://imgtu.com/i/cdSRII" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSRII.png" alt="cdSRII.png"></a><br><a href="https://imgtu.com/i/cdSTsg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSTsg.png" alt="cdSTsg.png"></a><br><a href="https://imgtu.com/i/cdShJP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdShJP.png" alt="cdShJP.png"></a><br><a href="https://imgtu.com/i/cdSfit" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSfit.png" alt="cdSfit.png"></a><br><a href="https://imgtu.com/i/cdSoQS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSoQS.png" alt="cdSoQS.png"></a><br><a href="https://imgtu.com/i/cdS4Rf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS4Rf.png" alt="cdS4Rf.png"></a><br><a href="https://imgtu.com/i/cdSbZj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSbZj.png" alt="cdSbZj.png"></a><br><a href="https://imgtu.com/i/cdSqds" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSqds.png" alt="cdSqds.png"></a><br><a href="https://imgtu.com/i/cdSLon" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSLon.png" alt="cdSLon.png"></a><br><a href="https://imgtu.com/i/cdSjJ0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSjJ0.png" alt="cdSjJ0.png"></a><br><a href="https://imgtu.com/i/cdSXiq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSXiq.png" alt="cdSXiq.png"></a><br><a href="https://imgtu.com/i/cdSxzT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSxzT.png" alt="cdSxzT.png"></a><br><a href="https://imgtu.com/i/cdpSQU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdpSQU.png" alt="cdpSQU.png"></a></p>
]]></content>
      <tags>
        <tag>高效计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-Supervised Classification with Graph Convolutional Network [ICLR&#39;17]</title>
    <url>/2020/12/22/GCN%5BICLR17%5D/</url>
    <content><![CDATA[<p>GCN的原始论文，发表于2017年的ICLR会议</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将神经网络应用在图结构数据上？</p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>给定以下输入：  </p>
<ol>
<li>图中顶点的特征矩阵$H\in \mathbb{R}^{n\times F}$，其中$n$为顶点数量，$F$为特征数量  </li>
<li>图的结构信息，如邻接矩阵$A$  </li>
</ol>
<p>输出：  </p>
<ol>
<li>图中顶点新的的特征表示$H’\in \mathbb{R}^{n\times F’}$，即</li>
</ol>
<script type="math/tex; mode=display">
H'=\text{GCN(H)}=g(AHW^T+b)</script><p>如果套用神经网络模型，每一层可以用一个非线性函数进行表示：</p>
<script type="math/tex; mode=display">
H^{(l+1)}=f(H^{(l)},A)</script><p>其中$H^{(0)}=X,H^{(L)}=Z$，问题在于如何选取函数$f(.,.)$</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>对于函数$f(.,.)$的选取，论文中提出了一种可能的函数形式：  </p>
<script type="math/tex; mode=display">
f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}) \\</script><p>其中$\hat{A}=A+I$，因为与矩阵$A$相乘表示对于每个顶点，我们对除了自身外所有邻居顶点的特征向量进行求和，因此加上单位矩阵是为了引入自环。而正则化是避免与矩阵$A$相乘改变特征向量的规模。实际在论文中只使用两层网络就达到了很好的效果，表示为：</p>
<script type="math/tex; mode=display">
Z_{\text{GCN}}=\text{softmax}\big(\hat{A'}\text{ReLU}\big(\hat{A'}XW_0\big)W_1\big)</script><p>其中$W_0、W_1$为这两层网络的参数，$\hat{A’}=\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$，$Z\in R\mathbb{R}{n\times c}$为预测的顶点标签，$c$为类别数目，毕竟论文解决的就是一个分类问题。</p>
<p>更一般地，使用邻域信息的图神经网络形式可以概括为：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\sigma\bigg(W_l·\text{AGGREGATE}\bigg(\{h_u^{(l-1)},\forall u\in N(v)\} \bigg)\bigg)</script><p>其中$W_l$是第$l$层网络的权重矩阵，$\text{AGGREGATE}$是与特定模型相关的聚合函数，$h_v^{(l)}$是顶点$v$在第$l$层的隐层特征表示。论文中只是用了一个两层网络就达到了很好的效果。</p>
<p>将论文所提出的函数改写为上述形式，即为：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\text{ReLU}\Big(W_l·\sum_{u\in N(v)}(deg(v)deg(u))^{-1/2}h_u^{(l-1)}\Big)</script><p>其中$deg(u)$为顶点$u$的度。</p>
<p><a href="https://papers.nips.cc/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf" target="_blank" rel="noopener">AS-GCN</a>中对这篇论文的模型形式描述如下：</p>
<script type="math/tex; mode=display">
h_{v_i}^{(l)}=\sigma\Big(W_l·\sum_{j=1}^Na(v_i,u_j)·h_{u_j}^{(l-1)}\Big),i=1,\dots,N</script><p>这里$A=(a(v_i,u_j))\in \mathbb{R}^{N\times N}$对应前面一种写法的正则化邻接矩阵$\hat{A’}$，表面上看对于顶点$v_i$，需要考虑将图中剩下的所有顶点的上一时刻的隐层表示做加权和，来作为它当前时刻的隐层表示，因为$j$的取值范围为$[1,N]$，$N$就是图中顶点的数量。但实际上，大多数顶点因为与$v_i$并无边相连，所以邻接矩阵中对应的值为0，意味着在加权和中的权重为0，相当于加权和时只会考虑有边相连的顶点，这同样是考虑邻域，只不过跟上面那种写法不同。</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>How Powerful are Graph Neural Networks?[ICLR&#39;19]</title>
    <url>/2021/01/31/GIN%5BICLR19%5D/</url>
    <content><![CDATA[<p>ICLR19一篇从图同构测试（Graph Isomorphism Test）角度说明GNN性能表现的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GNN性能表现好的原因是什么？</p>
<h4 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h4><ol>
<li>证明了GNN的性能上限是Weisfeiler-Lehman (WL) test，最多只和它一样有效</li>
<li>给出了GNN在什么条件下能够和WL test一样有效</li>
<li>指明了主流GNN框架如GCN、GraphSage无法区分的图结构，以及它们能够区分的图结构的特点</li>
<li>提出了一个简单有效的框架GIN，能够与WL test一样有效</li>
</ol>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>首先是介绍现有GNN框架的做法及图同构测试的定义，还有WL test的做法。</p>
<h4 id="GNN与WL-test"><a href="#GNN与WL-test" class="headerlink" title="GNN与WL test"></a>GNN与WL test</h4><p>论文认为主流的GNN框架可以分为下面这三步：</p>
<ol>
<li><p><strong>Aggregate</strong>：聚合邻域内的信息</p>
<script type="math/tex; mode=display">
a_v^{(k)}=\text{AGGREGATE}^{(k)}(\{h_u^{(k-1)}:u\in N(v) \})</script></li>
<li><p><strong>Combine</strong>：将聚合后的邻域信息与当前顶点信息结合</p>
<script type="math/tex; mode=display">
h_v^{(k)}=\text{COMBINE}^{(k)}(h_v^{(k-1)},a_v^{(k)})</script></li>
<li><p><strong>Readout</strong>：通过图中的每个顶点的表示得到图的表示</p>
<script type="math/tex; mode=display">
h_G=\text{READOUT}({h_v^{(K)}|v\in G})</script></li>
</ol>
<p>图同构测试就是判断两张图是否在拓扑结构上相同。而WL test的做法是迭代地进行以下步骤：</p>
<ul>
<li>聚合顶点及其邻域的标签信息</li>
<li>将聚合后的标签集合哈希成唯一的新标签</li>
</ul>
<p>如果经过若干次迭代后，两张图中的顶点的标签出现了不同则判断为不同构。基于WL test有一种核函数被提出以计算图之间的相似性。直观上来说，如下图所示，一个顶点在第$k$次迭代时的标签，实际表示了一颗以该顶点为根顶点高度为$k$的子树。</p>
<div align="center">
  <a href="https://imgchr.com/i/yVPBNQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/31/yVPBNQ.png" alt="yVPBNQ.png" border="0" width="80%"></a>
</div>

<p>而GNN同样是通过迭代地更新图中每个顶点的特征向量来捕捉图的结构信息以其周围顶点的特征，这里的结构特征同样可以是上图的根子树rooted subtree。如果给每个顶点的特征向量一个唯一的标签例如{a,b,c,…}，那一个顶点的邻域中所有顶点的特征向量可以构成一个Multiset，它的定义基本和C++中的Multiset一样，是一个Set的同时里面的元素还可以重复例如{a,a,b,c}。论文中对Multiset给出的数学定义是：$X=(S,m)$，其中$S$由Multiset中的非重复元素构成，$m$表示$S$中的元素在$X$中的频数。</p>
<p>直观上来说，一个有效的GNN应该只有在两个顶点对应的根子树结构相同，且其中对应顶点的特征向量也相同时，才将这两个顶点在特征空间中映射成相同的表示。也就是永远不会将不同的两个Multiset映射成同一个特征表示（因为Multiset中的顶点也是根子树中的顶点，既然它们都是通过聚合邻域得到的）。这也就意味着GNN中使用的聚合函数必须是单射的，对值域内的每一个$y$，存在最多一个定义域内的$x$使得$f(x)=y$。有下面这么一个引理：</p>
<blockquote>
<p>设$G_1$和$G_2$是两个非同构图，如果一个图神经网络$A:G\rightarrow \mathbb{R}^d$将$G_1$和$G_2$映射成不同的embedding，那么WL test同样会判断这两个图为非同构。</p>
</blockquote>
<p>引理表明在图区分任务上，一个图神经网络的表现最多和WL test一样好。而一样好的条件是，这个图神经网络的邻居聚合函数和图表示函数都是单射的。这里的一个局限是，函数考虑的定义域和值域都是离散集合。</p>
<p>图神经网络相较于WL test的另一个好处是，WL test输入的顶点特征向量都是one-hot编码，这无法捕捉到子树之间的结构相似度：</p>
<blockquote>
<p>Note that node feature vectors in the WL test are essentially one-hot encodings and thus cannot capture the similarity between subtrees. In contrast, a GNN satisfying the criteria in Theorem 3 generalizes the WL test by learning to embed the subtrees to low-dimensional space. This enables GNNs to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures.</p>
</blockquote>
<h4 id="图同构网络GIN"><a href="#图同构网络GIN" class="headerlink" title="图同构网络GIN"></a>图同构网络GIN</h4><p>基于上面介绍的引理和结论，论文提出的GIN框架如下：</p>
<script type="math/tex; mode=display">
h_v^{(k)}=\text{MLP}^{(k)}\Big((1+\epsilon^{(k)})·h_v^{(k-1)}+\sum_{u\in N(v)}h_u^{(k-1)} \Big)</script><p>对比一开始论文给出的GNN主流框架，可以看到是Aggregate函数选取了求和函数，Combine函数选取了MLP+(1+$\epsilon$)的形式。常见的Aggregate函数包括求和Sum、最大值Max和平均值Mean，论文花了一部分篇幅来说明求和相较于其他两个函数的好处：</p>
<p><div align="center">
  <a href="https://imgchr.com/i/yGIv5D" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGIv5D.png" alt="yGIv5D.png" border="0" width="80%"></a>
  <a href="https://imgchr.com/i/yGoirt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGoirt.png" alt="yGoirt.png" border="0" width="80%"></a>
</div><br>上面两幅图分别说明这几种聚合函数的特点及何种场景下会导致误差。第一幅图中即使减少了顶点的数量但对于取平均和最大值函数来说得到的信息保持不变，第二幅图也是想说明同样的问题，例如取平均，两个一样的顶点与三个一样的顶点取平均出来结构都是一样的，但它们分别对应的局部结构是不相同的。</p>
<p>对于顶点分类及边预测这类下游任务，只要得到顶点的embedding即可。而对于图分类任务，还需要根据所有顶点的embedding来得到图的一个表示，也就是前面提到的主流GNN框架做法的第三步Readout函数。论文的做法类似于<a href="http://www.bithub00.com/2020/12/22/JK-Net[ICML18]/" target="_blank" rel="noopener">JK-Net</a>，将所有层的表示都考虑进来，不过没有具体说是怎么做的。</p>
<p>最后，论文还探讨了那些不满足上面定理的GNN框架如GCN、GraphSAGE等，这些框架都采用一层感知机如ReLU来将Multiset映射成特征表示，而不像论文的做法采用多层感知机，而ReLU存在将不同的Multiset表示成同一种特征表示的情况，即$\exist X_1 \not=X_2,\ s.t. \ \sum_{x\in X_1}\text{ReLU}(Wx)=\sum_{x\in X_2}\text{ReLU}(Wx)$。论文中直接给了一个简单的例子：$X_1=\{1,1,1,1,1\},X_2=\{2,3\}$，因为有$\sum_{x\in X_1}\text{ReLU}(Wx)=\text{ReLU}(W\sum_{x\in X_1}x)$。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI1、PROTEINS、COLLAB、IMDB-BINARY、IMDB-MULTI、REDDIT-BINARY、REDDIT-MULTI5K</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>GCC--Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD&#39;20]</title>
    <url>/2021/01/28/GCC%5BKDD20%5D/</url>
    <content><![CDATA[<p>KDD20一篇将对比学习（contrastive learning）应用于图表示学习任务从而进行迁移的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将自监督学习的思想应用与图表示学习，通过预训练图神经网络从而仅需要微调就可以应用于新的数据集。</p>
<p>图表示学习目前受到了广泛关注，但目前绝大多数的图表示学习方法都是针对特定领域的图进行学习和建模，训练出的图神经网络难以迁移。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h4><p>对比学习是自监督学习思想的一种典型框架，一个典型的例子如下图所示：</p>
<div align="center">
  <a href="https://imgchr.com/i/yiECBF" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiECBF.png" alt="yiECBF.png" border="0" width="80%"></a>
</div>

<p>对比学习的思想是：尽管我们已经见过钞票很多次，能够轻易地分辨出一张钞票，我们也很少能画出一张完美无缺的钞票。<strong>表示学习算法不需要关注到样本的每一个细节，只要学到的特征能够将用来区分其它样本即可</strong>。不需要模型能够生成一匹栩栩如生的马之后它才能去分辨一张图片里的动物是不是马，这就是对比学习和生成对抗网络的一个区别。</p>
<p>既然是表示学习，核心就是通过一个函数把样本$x$转换成特征表示$f(x)$，而对比学习作为一种表示学习方法，它的思想是满足下面这个式子：</p>
<script type="math/tex; mode=display">
s(f(x),f(x^+))\gg s(f(x),f(x^-))</script><p>使得类似样本之间的相似度要远大于非类似样本之间的相似度，这样才能够进行区分。</p>
<h4 id="图表示学习"><a href="#图表示学习" class="headerlink" title="图表示学习"></a>图表示学习</h4><p>具体到论文的图表示学习任务中，论文的一个重要假设是，具有典型性的图结构在不同的网络之间是普遍存在而且可以迁移的（Representative graph structural patterns are universal and transferable across networks）。受对比学习在计算机视觉和自然语言处理领域的成功应用，论文想把对比学习（contrastive learning）的思想放在图表示学习中。通过预训练一个图神经网络，它能够很好地区分这些典型性的图结构，这样它的表现就不会仅仅局限于某个特定的数据集。</p>
<div align="center">
<a href="https://imgchr.com/i/y9x4KI" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/y9x4KI.png" alt="y9x4KI.png" border="0" width="80%"></a>
</div>

<p>论文首先将现有工作对顶点相似度的衡量分为了三类：</p>
<ol>
<li><p>邻域相似度</p>
<p>核心思想：越近的两个顶点之间相似度越高，包括有Jaccard、RWR、SimRank以及LINE、DeepWalk、node2vec。</p>
</li>
<li><p>结构相似度</p>
<p>核心思想：有相似的局部结构的两个顶点之间相似度更高。不同于邻域相似度，结构相似度不需要两个顶点之间有路径相连。常用的局部结构包括vertex degree、structural diversity、structural hole、k-core、motif等。</p>
</li>
<li><p>属性相似度</p>
<p>当数据集中顶点有许多标签信息时，可以将标签作为顶点的特征来衡量它们之间的相似度。</p>
</li>
</ol>
<p>在对比学习中，给定一个查询表示$q$以及一个包含$K+1$个键表示${k_0,\dots,k_K}$的字典，我们希望找到一个能与$q$匹配的键$k_+$。所以，论文优化的损失函数来自于InfoNCE：</p>
<script type="math/tex; mode=display">
L=-\log \frac{\exp(q^Tk_+\tau)}{\sum_{i=0}^K\exp(q^Tk_i/\tau)}</script><p>其中$f_q、f_k$是两个图神经网络，分别将样本$x^q$和$x^k$转换为低维表示$q$与$k$。</p>
<h4 id="正负样本获取"><a href="#正负样本获取" class="headerlink" title="正负样本获取"></a>正负样本获取</h4><p>因为查询和键可以是任意形式，具体到本论文里，定义每一个样本都是一个从特定顶点的$r$阶邻居网络中采样的子图，这里的子图定义和其它论文一致：$S_v=\{u:d(u,v)&lt;r \}$，距离顶点$v$最短路径距离小于$r$的顶点构成的集合。既然是最短路径，给定$r$那么这个集合也基本确定了，这种情况下得到的子图数量有限，在计算机视觉领域，当输入用于训练的图片数量有限时，往往会使用反转、旋转等方式对图片进行变换，以扩充训练图片的数量，这里论文也想采取类似的做法，对得到的子图$x$进行变换，来得到对比学习中的类似$x^+$与非类似样本$x^-$，具体做法如下：</p>
<ol>
<li><strong>带重启动的随机漫步</strong>。首先从子图的中心顶点$v$开始随机漫步，每一步时都有一定概率重新回到中心顶点，而漫步到任一邻居顶点的概率与当前顶点的出度有关。</li>
<li><strong>子图推演</strong>。随机漫步可以得到一系列顶点，它们构成的集合记为$\tilde{S_v}$，所形成的子图记作$\tilde{G_v}$，它就可以看作子图$S_v$的一个变换。</li>
<li><strong>匿名化</strong>。重新定义$\tilde{G_v}$中的顶点的标签，将$\{1,2,\dots,|\tilde{S_v} |\}$的顺序随机打乱作为重新定义后的标签。</li>
</ol>
<div align="center">
    <a href="https://imgchr.com/i/yiFHv8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiFHv8.png" alt="yiFHv8.png" border="0" width="70%"></a>
</div>

<p>论文对于每个子图都进行两次上述变换，而变换后的子图显然会与原子图相似，这样就有了一组相似的子图$(x^q,x^{k_+})$。要得到不相似的子图也很容易，不是同一个子图变换得到的子图就定义为不相似：$(x^q,x^k),k\not =k_+$。在上图的例子中，$x^q$和$x^{k_0}$是从红色的中心顶点采样得到的子图，我们认为它是一对正样本，而$x^{k_1}$和$x^{k_2}$作为从蓝色的中心顶点采样得到的子图，则被作为负样本。在变换时之所以要做最后一步，是为了防止图神经网络在判断两个子图是否相似时，仅仅是通过判断对应顶点的标签是不是一样，这样显然没有学到任何有用的结构信息。这里有一个小结论：</p>
<blockquote>
<p>绝大多数图神经网络对于输入图中顶点的顺序的随机扰动有稳定性</p>
</blockquote>
<p>现在有了正样本和负样本，下一步就是训练一个图神经网络对它们加以区分了，论文选取的是GIN。这就是自监督学习的思想，对比学习就是这种思想的一种典型框架。因为现有的图神经网络框架都需要额外的顶点特征作为输入，论文提出了一种位置embedding来作为其中特征：$I-D^{-1/2}AD^{-1/2}=U\Lambda U^T$，矩阵$U$中排序靠前的特征向量作为embedding。其它特征还包括顶点度的one-hot编码和中心顶点的指示向量。</p>
<h4 id="模型学习"><a href="#模型学习" class="headerlink" title="模型学习"></a>模型学习</h4><p>在模型学习时采用了何凯明组的MoCo框架的思想：</p>
<div align="center">
  <a href="https://imgchr.com/i/yimVaD" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yimVaD.png" alt="yimVaD.png" border="0" width="80%"></a>
</div>

<p>在对比学习中，我们需要维护一个大小为$K$的字典和编码器，要计算上面定义的损失函数，理想的情况是把所有负样本加入字典中进行计算，这会导致$K$很大字典难以维护。在MoCo的方法中，为了增大字典大小$K$，需要维护一个负样本的队列，队列中包含此前训练过的batch的样本作为负样本。在更新参数时，只有$q$的编码器图神经网络$f_q$中的参数通过反向传播进行更新，而$k$的编码器$f_k$中的值通过一种动量法进行更新：$\theta_k\leftarrow m\theta_k+(1-m)\theta_q$。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Academia、DBLP(SNAP)、DBLP(NetRep)、IMDB、Facebook、LiveJournal</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR&#39;19]</title>
    <url>/2020/12/22/GPUIR%5BSIGIR19%5D/</url>
    <content><![CDATA[<p>SIGIR19年有关GPU加速的一篇论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FEXIPRO[SIGMOD’17]中的对IPR问题的求解较慢，可以使用GPU进行并行加速。</p>
<h4 id="IPR问题"><a href="#IPR问题" class="headerlink" title="IPR问题"></a>IPR问题</h4><p>给定一个用户矩阵$Q\in \mathbb{R}^{d\times m}$以及一个物品矩阵$P\in \mathbb{R}^{d\times n}$，对于$Q$中的每一个用户$q$，返回内积$q^TP$中的前k个$q^Tp$对应的物品列表$p$</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="行文逻辑"><a href="#行文逻辑" class="headerlink" title="行文逻辑"></a>行文逻辑</h4><p>作者首先画出四个数据集上，SeqScan与FEXIPRO中两个步骤（内积计算与Top-k物品获取）的运行时间占比，发现内积计算占了总开销的90%以上，促使他提出方法加速这一步骤。接下来介绍GPU加速CPU程序的流程，提出了第一个改进方法，即分batch将矩阵送入GPU并行地计算内积。下一步同样地画出它各个步骤的运行时间占比，发现现在top-k物品的获取以及将内积结果从GPU内存复制到CPU内存这两个步骤变成了时间开销的大头。于是顺着分析结果提出了两个改进方法针对性地减小这两个步骤的时间开销。</p>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>前后提出了三个改进方法：GPU-IP、GPU-IPR、GPU-IPRO，分别为：<br>GPU-IP：1<br>GPU-IPR：1+2<br>GPU-IRPO：1+2+3</p>
<ol>
<li><p>并行计算$Q^TP$，并且提出了一种新的矩阵分割方法以充分利用GPU内存，从而加速内积的计算  </p>
<blockquote>
<p>给定GPU内存为$M$，各自选取用户矩阵与物品矩阵的子集$Q_s\in Q,P_s\in P$使得$Size(Q_s^TP_s)\le M$，论文的做法是取$Q_s=Q$，通过$Size(Q^TP_s)=M$来选取$P_s$的大小</p>
</blockquote>
</li>
<li><p>为每一个用户指定最佳的内积数量$g_s$为1024，从这1024个计算结果中返回top-k，减少了待排序的数据规模</p>
<blockquote>
<p>内积数量会严重影响下一步的Bitonic排序的性能。选取的依据是它应该满足每一个线程组的共享内存大小因为它会在GPU缓存层级关系中带来最小的缓存访问延迟(The size of $g_s$ should fit in the shared<br>memory of each threads group as it incurs minimum cache access latency in GPU cache hierarchy.)</p>
</blockquote>
</li>
<li><p>提出了一种剪枝方法来提前结束计算进程，减少了许多内积计算<br>假设用户$u$与其第$k$大的物品的内积为$S_k$，且$||q||\cdot||p||\le S_k$，则有  $q^Tp \le ||q||\cdot||p||\le S_k$，因为目的是得到top-k物品，满足上述不等式的物品已经被排除在top-k之外，不需要送入下一次迭代进行内积计算</p>
<blockquote>
<p>使用这种剪枝方法后，在四个数据集的前10次迭代中，分别减少了98.88%、76.61%、88.69%以及1.57%的用户数量。</p>
</blockquote>
</li>
</ol>
]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Inductive Representation Learning on Large Graphs[NIPS&#39;17]</title>
    <url>/2020/12/22/GraphSage%5BNIPS17%5D/</url>
    <content><![CDATA[<p>NIPS17一篇解决GCN不能泛化到未知顶点的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>对于学习图上顶点的embedding，现有的方法多为直推式学习，学习目标是直接生成当前顶点的embedding，不能泛化到未知顶点上</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出一种归纳式学习方法GrdaphSAGE，不为每个顶点学习单独的embedding，而是学习一种聚合函数$\text{AGGREGATE}$，从一个顶点的局部邻域聚合特征信息，为未知的顶点直接生成embedding，因此旧的顶点只要邻域发生变化也能得到一个新的embedding</p>
<blockquote>
<p>GCN不是归纳式，因为每次迭代会用到整个图的邻接矩阵$A$；而GraphSAGE可以对GCN做了精简，每次迭代只抽样取直接相连的邻居</p>
</blockquote>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol>
<li>给定顶点$v$及其特征$x_v$,作为它的初始表示$h_v^0=x_v$。</li>
<li>计算邻域向量$h^k_{N(v)}=\text{AGGREGATE}({h_u^{(k-1)}}, \forall u\in N(v))$，当前层顶点的邻居从上一层采样，且邻居个数固定，非所有邻居，这样每个顶点和采样后邻居的个数都相同，可以直接拼成一个batch送到GPU中进行批训练</li>
<li>将邻域向量与自身上一层的表示拼接，通过非线性激活函数$\sigma$后作为这一层的表示$h_v^k=\sigma(W^k\text{CONCAT}(h_v^{(k-1)},h^k_{N(v)})$</li>
<li>标准化 $h_v^k=h_v^k/||h_v^k||_2$</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/06/0tja9K.png" alt="0tja9K.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/06/0tvRaR.jpg" alt="0tvRaR.jpg" width="50%" border="0">
</div>


<h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><ol>
<li>MEAN</li>
</ol>
<script type="math/tex; mode=display">
h_v^k=\sigma(W·\text{MEAN}(\{h_v^{k-1}\}\cup\{h_u^{k-1},\forall u\in N(v) \})</script><ol>
<li>LSTM</li>
<li>Pooling<br>GraphSAGE采用的max-pooling策略能够隐式地选取领域中重要的顶点：</li>
</ol>
<script type="math/tex; mode=display">
\text{AGGREGATE}_k^{pool}=\text{max}(\{\sigma(W_{pool}h_u^k + b),\forall u\in N(v)\})</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>BioGRID、Reddit</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Representation Learning on Graphs with Jumping Knowledge Networks[ICML&#39;18]</title>
    <url>/2020/12/22/JK-Net%5BICML18%5D/</url>
    <content><![CDATA[<p>ICML18一篇解决GCN层数加深性能反而变差的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>当图卷积网络GCN的层数超过两层时模型的表现会变差，这使得GCN只能作为浅层模型使用，且在对邻域节点的信息进行聚合时，即使同样是采用$k$层网络来聚合$k$跳邻居的信息，有着不同局部结构的顶点获得的信息也可能完全不同，以下图为例：</p>
<div align="center">
<a href="https://imgchr.com/i/BpCLz8" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpCLz8.jpg" alt="BpCLz8.jpg" border="0" width="70%"></a>
</div>


<p>图$(a)$中的顶点位于核心区域，因此采用$4$层网络把几乎整个图的信息都进行聚合了，而不是它的邻域，这会导致过度平滑，而图$(b)$中顶点位于图边缘的一个树状结构中，采取同样的$4$层网络只囊括了一小部分顶点的信息，只有在第$5$层囊括了核心顶点之后才有效地囊括了更多顶点的信息。</p>
<p>所以，对于处于核心区域的顶点，GCN中每多一层即每多一次卷积操作，节点的表达会更倾向全局，这导致核心区域的很多顶点的表示到最后没有区分性。对于这样的顶点应该减少GCN的层数来让顶点更倾向局部从而在表示上可以区分；而处于边缘的顶点，即使更新多次，聚合的信息也寥寥无几，对于这样的顶点应该增加GCN的层数，来学习到更充分的信息。因此，对于不同的顶点应该选取不同的层数，传统做法对于所有顶点都用一个值会带来偏差。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>理论部分，论文主要讨论的问题是，在一个$k$层的GCN中，顶点$x$对顶点$y$的影响程度，即顶点$x$输入特征的改变，会对顶点$y$在最后一层得到的表示产生多大的变化，也可以说是顶点$y$对于顶点$x$有多敏感。假设输入的特征为$X\in \mathbb{R}^{n\times f}$，输出的预测标签为$Z\in \mathbb{R}^{n\times c}$，其中$n$为图中顶点数目，$c$为类别数目，$f$为特征数目，则这种影响程度可以表示为$I(x,y)=\sum_i\sum_j\frac{\partial Z_{yi}}{\partial X_{xj}}$。</p>
<p>更特别地，论文证明了这个影响程度与从顶点$x$开始的$k$步随机漫步的分布有关，如果对$k$取极限$k\rightarrow \infty$，则随机漫步的分布会收敛到$P_{lim}(\rightarrow y)$。详细论证过程可见原文。这说明，结果与随机漫步的的起始顶点$x$没有关系，通过这种方法来得到$x$的邻域信息是不适用的。</p>
<p>另一种说法是，一个$k$层的图卷积网络等同于一个$k$阶的多项式过滤器，其中的系数是预先确定的<a href="https://arxiv.org/abs/2007.02133v1" target="_blank" rel="noopener">SDC</a>。这么一个过滤器与随机漫步类似，最终会收敛到一个静态向量，从而导致过度平滑。</p>
<p>实践部分，论文提出JK-Net，通过Layer aggregation来让顶点最后的表示自适应地聚合不同层的信息，局部还是全部，让模型自己来学习：</p>
<div align="center">
<a href="https://imgchr.com/i/BpEvLT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpEvLT.jpg" alt="BpEvLT.jpg" border="0" width="50%"></a>
</div>


<p>论文的重点在于最后的Layer aggregation层，可选的三种操作为：Concat、Max-pooing以及LSTM-attn。</p>
<ol>
<li><p>Concat</p>
<p>将各层的表示直接拼接在一起，送入Linear层。对于小数据集及结构单一的图这种聚合方式会更好，因为它们不需要顶点在聚合邻域的顶点信息时具有什么自适应性。</p>
</li>
<li><p>Max-pooling</p>
<p>选取各层的表示中包含信息量最多的作为顶点的最终表示，在多层结构中，低层聚合更多局部信息，而高层会聚合更多全局信息，因此对于核心区域内的顶点可能会选取高层表示而边缘顶点选取低层表示。</p>
</li>
<li><p>LSTM-attention</p>
<p>对于各层的表示，attention机制通过计算一个系数$s_v^{(l)}$来表示各层表示的重要性，其中$\sum_ls_v^{(l)}=1$，顶点最终的表示就是各层表示的一个加权和：$\sum_ls_v^{(l)}·h_v^{(l)}$。</p>
<blockquote>
<p>$s_v^{(l)}$的计算：将$k$层网络各层的表示$h_v^{(1)},\dots,h_v^{(k)}$输入一个双向LSTM中，同时生成各层$l$的前向LSTM与反向LSTM的隐式特征，分别表示为$f_v^{(l)}、b_v^{(l)}$，拼接后将$|f_v^{(l)}||b_v^{(l)}|$送入一个Linear层，将Linear层的结果进行Softmax归一化操作就得到了系数$s_v^{l}$。</p>
</blockquote>
</li>
</ol>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Citeseer、Cora、Reddit、PPI</p>
<p>在模式识别上对这篇论文进行了简短的一个分享，这里直接把讲稿和PPT放上来。</p>
<h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/2EFaFI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFaFI.png" alt="2EFaFI.png"></a><br><a href="https://imgtu.com/i/2EFdYt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFdYt.png" alt="2EFdYt.png"></a><br><a href="https://imgtu.com/i/2EFDl8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFDl8.png" alt="2EFDl8.png"></a><br><a href="https://imgtu.com/i/2EFwfP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFwfP.png" alt="2EFwfP.png"></a><br><a href="https://imgtu.com/i/2EFNTA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFNTA.png" alt="2EFNTA.png"></a><br><a href="https://imgtu.com/i/2EFBSf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFBSf.png" alt="2EFBSf.png"></a><br><a href="https://imgtu.com/i/2EFr6S" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFr6S.png" alt="2EFr6S.png"></a><br><a href="https://imgtu.com/i/2EFRkn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFRkn.png" alt="2EFRkn.png"></a><br><a href="https://imgtu.com/i/2EFcwj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFcwj.png" alt="2EFcwj.png"></a><br><a href="https://imgtu.com/i/2EFff0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFff0.png" alt="2EFff0.png"></a><br><a href="https://imgtu.com/i/2EFLkR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFLkR.png" alt="2EFLkR.png"></a><br><a href="https://imgtu.com/i/2EFoXF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EFoXF.png" alt="2EFoXF.png"></a></p>
<h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>大家好，今天我要介绍的是ICML会议18年的一篇论文，通过跳跃知识网络进行图上的表示学习，论文的亮点在于引入ResNet的思想解决图神经网络层数无法加深的问题。图神经网络简称GNN，它受卷积神经网络CNN启发，主要应用于图这种非欧几里得结构数据。它根据”相邻的顶点具有相似性“这一假设，通过聚合邻域顶点的信息来进行图上顶点的表示学习。不同于图像这种排列整齐的欧几里得结构数据，在图上不同顶点的邻居顶点数目是不同的，这就为图上的问题带来了挑战。</p>
<p>给定一张由顶点和边组成的图作为输入，图神经网络通常分为如下两个步骤：邻域聚合和状态更新。邻域聚合对应式子的后半部分，而状态更新对应于式子的前半部分。在每一层的网络，首先对邻域内的顶点的表示进行聚合，再将得到的聚合结果和上一层得到的表示组合在一起，共同作为当前层顶点的表示。这里的聚合和组合有多种定义方式，因为时间原因不在这里进行介绍。虽然图神经网络的出现给图上问题的解决方法带来了新的思路，但它也面临着挑战。不同于CNN，GNN在层数加深时会出现性能下降的问题，典型的网络如图卷积网络GCN、图注意力网络GAT等都是在2~4层时取得最好的性能表现。本文的一个贡献就是从理论上分析了性能下降的原因。导致这个现象的一个主要原因是所谓的”过平滑“现象，即随着网络层数的加深，图中顶点经过网络学习得到的特征表示会趋向于相等的现象。这就会导致顶点间不再具有区分度，进而导致下游任务效果变差。例如图中的例子，从左到右表示层数的加深，可以看到，六层网络时不同的顶点几乎都混在了一起，比浅层网络时的效果要差很多。论文从随机游走的角度说明了原因，一个K层的GNN相当于从源顶点出发到目标顶点的一个K步的随机游走，当K趋向于无穷即层数不断加深时, 顶点的极限分布与初始的顶点表示无关，只与图的结构相关。也就是说，本来聚合邻域的信息是为了得到中心顶点的特征表示，当随着层数加深聚合的信息越多，反而得到的表示与中心顶点无关，这就导致每个顶点都会得到相等的特征表示。</p>
<p>因为ResNet等深层神经网络模型在计算机视觉领域的成功，本文受此启发引入ResNet中的残差模块，在GNN的框架中引入额外的连接。左边对应于论文提出的架构图，从架构图可以看到，不同于传统GNN每一层的表示只与上一层有关，现在每一层的表示都会额外地连接到最后一层，使得在最后一层得到最终的顶点表示时可以聚合不同层的信息，来避免前面说到的过平滑问题。聚合有三种方式：Concat、Max-pooling和LSTM-attention，每种方式有各自适合的场景，例如第三种方式通过双向LSTM来得到各层表示的重要程度，进而自适应地聚合各层的信息，得到更好的特征表示。因为时间关系不再展开介绍。实验部分，论文选取的baseline都是经典的GNN模型，包括图卷积网络GCN和图注意力网络GAT，可以看到在几个benchmark数据集上都取得了最好的结果，也使用了更深的网络层数。我今天的分享到此结束，谢谢大家。</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Convolutional Matrix Completion[KDD&#39;18]</title>
    <url>/2021/01/09/GCMC%5BKDD18%5D/</url>
    <content><![CDATA[<p>KDD18一篇将图卷积网络用于矩阵补全问题的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将图卷积网络应用于矩阵补全问题。</p>
<p>具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。如果将评分矩阵转换为一张图，转换方法在下面有进行介绍，这时矩阵补全问题也可以看成图上的边预测问题。要预测用户对一个物品的评分，就是预测图上两个对应顶点之间相连的边的权重。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文通过一个编码器-解码器的架构来实现从已有评分到特征表示再到预测评分的过程。</p>
<div align="center">
<a href="https://imgchr.com/i/sQUdAS" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUdAS.png" alt="sQUdAS.png" border="0" width="70%"></a>
</div>

<h4 id="Bipartite-Graph-Construction"><a href="#Bipartite-Graph-Construction" class="headerlink" title="Bipartite Graph Construction"></a>Bipartite Graph Construction</h4><p>首先是将推荐任务里的评分数据转化为一张图，具体做法是将用户和物品都看作图中的顶点，交互记录看作边，分数作为边的权重，如图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/su9fr4" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/08/su9fr4.png" alt="su9fr4.png" border="0" width="60%"></a>
</div>

<h4 id="Graph-Convolutional-Encoder"><a href="#Graph-Convolutional-Encoder" class="headerlink" title="Graph Convolutional Encoder"></a>Graph Convolutional Encoder</h4><p>上一步所构建的图的输入形式为邻接矩阵$A\in \mathbb{R}^{n\times n}$与图中顶点的特征矩阵$X\in \mathbb{R}^{n\times d}$。编码器在这一步的作用就是得到用户与物品的特征表示$A,X^u,X^v\rightarrow U,V$。</p>
<p>具体编码时，论文将不同的评分水平分开考虑$r\in \{1,2,3,4,5\}$，我的理解是它们类似于处理图像数据时的多个channel。以一个评分水平$r$为例，说明编码得到特征表示的过程。假设用户$u_i$对电影$v_j$评分为$r$，而这部电影的特征向量为$x_j$，那么这部电影对这个用户特征表示的贡献可以表示为下面的式子(1)，相当于对特征向量进行了一个线性变换。</p>
<div align="center">
<a href="https://imgchr.com/i/sQUHnx" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUHnx.png" alt="sQUHnx.png" border="0" width="80%"></a>
</div>
对当前评分水平下所有评过分的电影进行求和，再对所有评分水平求和拼接，经过一个非线性变换，就得到了用户$u_i$的特征表示$h_{u_i}$，物品的做法相同。

<div align="center">
<a href="https://imgchr.com/i/sQdv6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQdv6A.png" alt="sQdv6A.png" border="0" width="80%"></a>
</div>

<div align="center">
<a href="https://imgchr.com/i/sQwmmq" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQwmmq.png" alt="sQwmmq.png" border="0" width="80%"></a>
</div>

<h4 id="Bilinear-Decoder"><a href="#Bilinear-Decoder" class="headerlink" title="Bilinear Decoder"></a>Bilinear Decoder</h4><p>在分别得到用户与物品的特征表示$U$与$V$后，解码器计算出用户对物品评分为$r$的概率，再对每个评分的概率进行求和，得到最终预测的评分。</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P_r)_{ij}&=\frac{\exp(u_i^TQ_rv_j)}{\sum_{s\in R}\exp(u_i^TQ_sv_j)} \\
\hat{M}&=\sum_{r\in R}rP_r
\end{aligned}</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、MovieLens</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Neural Networks for Social Recommendation[WWW&#39;19]</title>
    <url>/2020/12/22/GraphRec%5BWWW19%5D/</url>
    <content><![CDATA[<p>WWW19将GNN应用于社会化推荐的一篇论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将GNN应用于社会化推荐任务上。</p>
<p>面临的挑战有三点：</p>
<ol>
<li>在一个社会化推荐任务中，输入的数据包括社会关系图和用户-物品交互图，将两张图的信息都聚合才能得到用户更好的一个表示，而此前的GNN只是在同一张图上对邻域内的信息聚合。</li>
<li>在用户-物品交互图中，顶点与顶点之间的边也包含更多的信息，除了表示是否交互，还能表示用户对一个物品的偏好（喜爱还是厌恶），而此前的GNN只是将边用来表示是否交互。</li>
<li>社会关系图中用户之间的纽带有强有弱，显然地，一个用户更可能与强纽带的其它用户有类似的喜好。如果将所有纽带关系都看成一样，会有偏差。</li>
</ol>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>创新：</p>
<ul>
<li>在不同图(user-user graph和user-item graph)上进行信息传递与聚合</li>
<li>除了捕获user-item间的交互关系，还利用了user对item的评分</li>
<li>用attention机制表示社交关系的重要性，用户纽带的强与弱</li>
</ul>
<div align="center">
<a href="https://imgchr.com/i/r0xT1A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/r0xT1A.png" alt="r0xT1A.png" border="0" width="90%"></a>
</div>

<p>整个GraphRec框架由三个部分组成，分别为user modeling、item modeling和rating prediction。其中user modeling用来学习用户的特征表示，学习的方式是两个聚合：item aggregation和social aggregation，类似地item modeling用来学习物品的特征表示，学习的方式是一个聚合：user aggregation。</p>
<h4 id="User-Modeling"><a href="#User-Modeling" class="headerlink" title="User Modeling"></a>User Modeling</h4><h5 id="item-aggregation"><a href="#item-aggregation" class="headerlink" title="item aggregation"></a>item aggregation</h5><div align="center">
<a href="https://imgchr.com/i/rBuFzt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBuFzt.png" alt="rBuFzt.png" border="0" width="40%"></a>
</div>


<p>item aggregation的目的是通过用户交互过的物品以及对这些物品的倾向，来学习物品侧的用户特征表示，数学表示为：</p>
<script type="math/tex; mode=display">
h_i^I=\sigma(W·Aggre_{items}(\{x_{ia},\forall a\in C(i)\})+b)</script><p>$C(i)$就表示用户交互过的物品的一个集合。这里的$x_{ia}$是一个表示向量，它应该能够同时表示交互关系和用户倾向。论文中的做法是通过一个MLP来结合物品的embedding和倾向的embedding，两者分别用$q_a$和$e_r$表示。倾向的embedding可能很难理解，以五分制评分为例，倾向的embedding表示为$e_r\in \mathbb{R}^d$，其中$r\in \{1,2,3,4,5\}$。</p>
<script type="math/tex; mode=display">
x_{ia}=g_v([q_a\oplus e_r])</script><p>定义好$x_{ia}$后，下一步就是如何选取聚合函数$Aggre$了。论文中使用的是attention机制，来源于<a href="#Graph Attention Networks[ICLR&#39;18]">GAT</a>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_i^I&=\sigma(W·\Big\{\sum_{a\in C(i)}
\alpha_{ia}x_{ia}\Big\}+b) \\
\alpha_{ia}'&=w_2^T·\sigma(W_1·[x_{ia}\oplus p_i]+b_1)+b_2 \\
\alpha_{ia}&=\frac{\exp(\alpha_{ia}')}{\sum_{a\in C(i)}\exp(\alpha_{ia}')}
\end{aligned}</script><p>这里的权重$\alpha_{ia}$考虑了$x_{ia}$和用户$u_i$的embedding $p_i$，使得权重能够与当前用户相关。</p>
<h5 id="social-aggregation"><a href="#social-aggregation" class="headerlink" title="social aggregation"></a>social aggregation</h5><div align="center">
<a href="https://imgchr.com/i/rBK7g1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBK7g1.png" alt="rBK7g1.png" border="0" width="40%"></a>
</div>

<p>social aggregation中，同样地使用了attention机制，通过attention机制来选取强纽带的其它用户（表现为聚合时权重更大）并聚合他们的信息，聚合的就是物品侧的用户特征表示。</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_i^S&=\sigma(W·\Big\{\sum_{o\in N(i)}
\beta_{io}h_o^I\Big\}+b) \\
\beta_{io}'&=w_2^T·\sigma(W_1·[h_o^I\oplus p_i]+b_1)+b_2 \\
\beta_{io}&=\frac{\exp(\beta_{io}')}{\sum_{o\in N(i)}\exp(\beta_{io}')}
\end{aligned}</script><p>这里跟item aggregation基本一模一样，就不多介绍了。</p>
<p>得到物品侧的用户特征表示$h_i^I$和社交侧的用户特征表示$h_i^S$后，用一个MLP将它们结合，得到用户最终的特征表示：</p>
<script type="math/tex; mode=display">
\begin{aligned}
c_1&=[h_i^I\oplus h_i^S] \\
c_2&=\sigma(W_2·c_1+b_2) \\
&······ \\
h_i&=\sigma(W_l·c_{l-1}+b_l)
\end{aligned}</script><h4 id="Item-Modeling"><a href="#Item-Modeling" class="headerlink" title="Item Modeling"></a>Item Modeling</h4><h5 id="user-aggregation"><a href="#user-aggregation" class="headerlink" title="user aggregation"></a>user aggregation</h5><div align="center">
<a href="https://imgchr.com/i/rBYtjH" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBYtjH.png" alt="rBYtjH.png" border="0" width="50%"></a>
</div>

<p>Item modeling与User modeling的做法基本一模一样…公式都是一一对应的：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_{jt}&=g_u([p_t\oplus e_r]) \\
z_j&=\sigma(W·\Big\{\sum_{t\in B(j)}
\mu_{jt}f_{jt}\Big\}+b) \\
\mu_{jt}'&=w_2^T·\sigma(W_1·[f_{jt}\oplus q_j]+b_1)+b_2 \\
\mu_{jt}&=\frac{\exp(\mu_{jt}')}{\sum_{a\in C(i)}\exp(\mu_{jt}')}
\end{aligned}</script><h4 id="Rating-Prediction"><a href="#Rating-Prediction" class="headerlink" title="Rating Prediction"></a>Rating Prediction</h4><p>最后来到评分预测部分，由上面两个部分我们得到了用户特征表示$h_i$与物品特征表示$z_j$，产生评分用的也是一个MLP：</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_1&=[h_i\oplus z_j] \\
g_2&=\sigma(W_2·g_1+b_2) \\
&······ \\
g_{l-1}&=\sigma(W_l·g_{l-1}+b_l) \\
r_{ij}&=w^T·g_{l-1}
\end{aligned}</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Ciao、Epinions</p>
<p>在科技论文写作课上对这篇论文进行了分享，这里直接把讲稿和PPT放上来。</p>
<h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/2EP9mj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP9mj.png" alt="2EP9mj.png"></a><br><a href="https://imgtu.com/i/2EPSXQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPSXQ.png" alt="2EPSXQ.png"></a><br><a href="https://imgtu.com/i/2ECz6g" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2ECz6g.png" alt="2ECz6g.png"></a><br><a href="https://imgtu.com/i/2EPP7n" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPP7n.png" alt="2EPP7n.png"></a><br><a href="https://imgtu.com/i/2EPC0s" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPC0s.png" alt="2EPC0s.png"></a><br><a href="https://imgtu.com/i/2EPkt0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPkt0.png" alt="2EPkt0.png"></a><br><a href="https://imgtu.com/i/2EPFkq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPFkq.png" alt="2EPFkq.png"></a><br><a href="https://imgtu.com/i/2EPVpT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPVpT.png" alt="2EPVpT.png"></a><br><a href="https://imgtu.com/i/2EPAhV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPAhV.png" alt="2EPAhV.png"></a><br><a href="https://imgtu.com/i/2EPZ1U" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPZ1U.png" alt="2EPZ1U.png"></a><br><a href="https://imgtu.com/i/2EPecF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPecF.png" alt="2EPecF.png"></a><br><a href="https://imgtu.com/i/2EPmX4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPmX4.png" alt="2EPmX4.png"></a><br><a href="https://imgtu.com/i/2EPunJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPunJ.png" alt="2EPunJ.png"></a><br><a href="https://imgtu.com/i/2EPKB9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPKB9.png" alt="2EPKB9.png"></a><br><a href="https://imgtu.com/i/2EPM7R" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPM7R.png" alt="2EPM7R.png"></a><br><a href="https://imgtu.com/i/2EPlA1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPlA1.png" alt="2EPlA1.png"></a><br><a href="https://imgtu.com/i/2EP1tx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP1tx.png" alt="2EP1tx.png"></a><br><a href="https://imgtu.com/i/2EPG9K" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPG9K.png" alt="2EPG9K.png"></a><br><a href="https://imgtu.com/i/2EP3h6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP3h6.png" alt="2EP3h6.png"></a><br><a href="https://imgtu.com/i/2EPJ1O" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPJ1O.png" alt="2EPJ1O.png"></a><br><a href="https://imgtu.com/i/2EPtje" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPtje.png" alt="2EPtje.png"></a><br><a href="https://imgtu.com/i/2EPYcD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPYcD.png" alt="2EPYcD.png"></a><br><a href="https://imgtu.com/i/2EPUnH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPUnH.png" alt="2EPUnH.png"></a><br><a href="https://imgtu.com/i/2EPaBd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPaBd.png" alt="2EPaBd.png"></a><br><a href="https://imgtu.com/i/2EPdHA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPdHA.png" alt="2EPdHA.png"></a><br><a href="https://imgtu.com/i/2EP0AI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP0AI.png" alt="2EP0AI.png"></a><br><a href="https://imgtu.com/i/2EPBNt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPBNt.png" alt="2EPBNt.png"></a><br><a href="https://imgtu.com/i/2EPD4P" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPD4P.png" alt="2EPD4P.png"></a><br><a href="https://imgtu.com/i/2EPy38" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPy38.png" alt="2EPy38.png"></a><br><a href="https://imgtu.com/i/2EPs9f" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPs9f.png" alt="2EPs9f.png"></a><br><a href="https://imgtu.com/i/2EiqRf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EiqRf.png" alt="2EiqRf.png"></a><br><a href="https://imgtu.com/i/2EiLz8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EiLz8.png" alt="2EiLz8.png"></a><br><a href="https://imgtu.com/i/2EibJP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EibJP.png" alt="2EibJP.png"></a><br><a href="https://imgtu.com/i/2EiHit" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EiHit.png" alt="2EiHit.png"></a></p>
<h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>老师和同学们大家好，今天我们要介绍的是WWW会议19年的一篇论文，基于图神经网络的社会推荐。WWW会议是数据挖掘领域的CCF-A类会议。本次介绍由四个部分组成，分别是背景介绍、论文细节、实验评估以及写作技巧。</p>
<p>首先是背景部分，进入信息时代，我们被越来越多的信息所淹没，表面上我们有了更多的选择，但反而不知道如何选择。而推荐系统就是能够有效缓解这种“信息超载”现象的一个很好的方法。它希望根据你的历史行为记录，来挖掘你的个人喜好，从而向你推荐可能喜欢的物品。它就像一位了解我们喜好的隐形的朋友，在我们浏览或购物时陪伴左右。实际上推荐系统已经和我们的生活息息相关：不管是QQ音乐的”为你推荐”、淘宝的”猜你喜欢”或者是亚马逊的”推荐购买”，都是为我们个性化推荐的内容。</p>
<p>因为用户和物品的交互记录很容易以图的形式进行表示，以电影评分为例，如果将用户和电影都看成图上的顶点，而评分记录看成对应顶点间的一条边，自然就形成了一张图。而图神经网络是针对图类型数据的一种神经网络架构，很自然地就想到用图神经网络来解决推荐系统的问题，这也是本文研究的动机。图神经网络根据”相邻的顶点具有相似性“这一假设，通过聚合邻域顶点的信息来将图中顶点映射为特征空间中的向量，使得结构上相似的顶点在特征空间中有相似的特征表示。给定一张由顶点和边组成的图作为输入，通常分为如下两个步骤：邻域聚合和状态更新。</p>
<p>而社会推荐任务的难点在于，首先，数据往往包含两种类型的图，分别是用户-物品交互记录以及用户间的社交关系，而传统的图神经网络都是在同一张图上进行信息的传递和聚合，如何才能利用社交关系图的信息来帮助推荐相关物品？其次，交互记录还包含了更丰富的信息，例如评分高表示喜爱，评分低表示厌恶，如何将这种偏好也体现在模型的构建中？最后，社交网络中不同的好友对我们的偏好影响程度是不同的，关系越好的朋友向我们推荐的物品我们越可能接受。论文的贡献就是解决了这几个问题。</p>
<p>论文的架构有两条主线，分别是用户侧和物品侧，得到各自的特征表示后，计算出用户对物品的预测评分。首先是用户侧，因为给定的数据中两张图都和用户有关，我们希望能将两部分的信息都利用起来。对于用户-物品交互图，用上面提到的图神经网络的邻域聚合步骤，用户的特征表示由它交互过的物品的特征表示进行聚合得到，同时还将评分的高低纳入考虑，以引入用户的偏好。这种聚合方式是通过物品来定义用户。论文在这里额外地考虑了一步，对用户交互过的物品，它们对用户偏好的贡献也是不同的，为了表示这种不同，论文使用了attention网络来为每个物品计算出一个权重系数，以自适应地聚合这些物品的信息。attention网络涉及的细节较多，因为时间关系不在这里详细介绍，只需要了解通过attention网络可以得到不同物品相对的重要性程度。</p>
<p>类似地，不同的好友给一个用户偏好带来的影响也是不同的，所以在社交网络图上聚合邻域好友的信息时，论文同样使用了attention网络来为每个好友计算出一个权重系数，以表示好友对用户的重要程度。到这里，我们就从物品和好友两个角度得到了用户的特征表示，将两部分结合起来就得到了用户的特征表示。</p>
<p>知道用户侧的做法之后再看物品侧的做法就容易理解多了，因为采用了类似的信息聚合过程。对于一个物品，我们可以得到与它交互过的所有用户，那么该物品的特征表示就由这些用户的特征表示进行聚合得到，不出意外地，这里同样使用了attention网络来表示各个用户的重要性来进行加权聚合。现在，我们得到了用户和物品的特征表示后，剩下最后一步就是预测用户对这个物品的评分，对应这个部分的输入输出。论文在这部分采取的架构是多层感知机，因为时间原因不在这里详细介绍，它可以通过用户和物品的特征向量来预测用户对物品的一个评分，评分高的物品我们就作为候选列表向用户推荐。到这里模型的细节就介绍完了，可以看到主要是两个模块的重复使用，邻域聚合和attention表示重要性。</p>
<p>实验部分，论文的优化目标是减少预测值与实际值之间的偏差，使用的两个推荐系统数据集描述如下，它们的共同点都是十分稀疏，评分数相对于用户数和物品数要少得多，这也符合现实中的情况，人们往往不愿意给出自己对于物品的意见。结果部分，选取的两个指标MAE和RMSE都是越小代表预测效果越好，论文提出的模型GraphRec在所有比较模型里取得了最好的性能，同样还做了参数实验，探究不同参数的取值对结果的影响。介绍完了论文的各个部分后，最后我们来分析一下这篇论文在写作上有什么值得我们借鉴的地方。</p>
<p>论文做的好的一个地方是图表部分，正如老师上课所说，不应该只用颜色来进行区分不同的结果，有些学者可能喜欢将论文打印成纸质版进行阅读，这时颜色传递的信息就会被丢失。这里论文除了颜色外，图表中还在不同的数据上使用了不同的底纹，这样当进行黑白打印时，仍然能通过底纹来获取颜色所传递的信息。同时，在正文里引用图表的时候，也会在随后通过文字对图标传达的信息进行说明，这样可以让读者即使不阅读图标也能明白图表的含义，而不是不给出任何信息只是单纯为了让读者去看图表。</p>
<p>然而，论文中也有我们认为的不足之处，碰巧也是图表部分。图表使用了不同的颜色却没有用图例来对每种颜色的含义进行说明，这让人感觉使用颜色仅仅是为了让论文更好看而已。与之相对比的是同年会议的另一篇论文，解决的也是社会推荐问题，其中的图表部分虽然涉及了更多的颜色，但有图例进行详细的说明。让读者能够清晰地明白每种颜色的含义。</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Laplace先验与L1正则</title>
    <url>/2019/11/18/Laplace%E4%B8%8EL1%E6%AD%A3%E5%88%99/</url>
    <content><![CDATA[<p>本篇博客探讨一下为什么说L1正则等同于Laplace先验<br><a id="more"></a></p>
<h3 id="Laplace分布"><a href="#Laplace分布" class="headerlink" title="Laplace分布"></a>Laplace分布</h3><p>首先介绍什么是Laplace分布，它的概率密度函数如下：</p>
<script type="math/tex; mode=display">f(x|\mu,b)=\frac{1}{2b}e^{-\frac{|x-\mu|}{b}}</script><p>分布的图像如下：</p>
<p><img src="https://s2.ax1x.com/2019/11/18/MynJXR.jpg" alt=""><br>由图可以看出，Laplace分布集中在$\mu$附件，b越小数据分布越集中</p>
<h3 id="Laplace先验导出L1正则"><a href="#Laplace先验导出L1正则" class="headerlink" title="Laplace先验导出L1正则"></a>Laplace先验导出L1正则</h3><p>导出之前，需要用到最大似然的知识：  </p>
<p>首先我们假设现在需要从一些样本点$(x_1,y_1)···(x_N,y_N)$中来估计参数 $\beta$，假设输出$y$与输入$x$之间线性相关，并且受噪声$\epsilon$影响:</p>
<script type="math/tex; mode=display">y_n=\beta x_n+\epsilon</script><p>这里的$\epsilon$服从均值为0，方差为$\sigma^2$的高斯分布，这个式子还可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f(X)&=\sum_i(x_i\beta_i)+\epsilon \\
&=X\theta^T+\epsilon
\end{aligned}</script><p>其中$X=(x_1,x_2…x_n)$，对数据集$(X_i,Y_i)$，用$X_i$得到$Y_i$的概率是$Y_i\backsim N(f(X_i),\sigma^2)$：</p>
<script type="math/tex; mode=display">P(Y_i|X_i,\theta)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\lVert f(X_i)-Y_i\rVert^2}{2\sigma^2}}</script><p>假设数据集中每对数据$(X_i,Y_i)$都是独立的，那么对于数据集来说由X得到Y的概率是：</p>
<script type="math/tex; mode=display">P(Y|X,\theta)=\prod_i\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\lVert f(X_i)-Y_i\rVert^2}{2\sigma^2}}</script><p>显然，我们的目标是使这个概率值最大，最大时的参数$\theta$就是我们需要的参数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta^* &= argmax_\theta(\prod_i\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{\lVert f(X_i)-Y_i\rVert^2}{2\sigma^2}}) \\
&= argmax_\theta(-\frac{1}{2\sigma^2}\sum_i\lVert f(X_i)-Y_i\rVert^2 + \sum_iln(\sigma \sqrt{2\pi})) \\
&= argmin_\theta(\sum_i \lVert f(X_i)-Y_i\rVert^2)
\end{aligned}</script><p>可以看到，这就是最小二乘法。在最大似然估计中，假设了$\theta$是均匀分布的，$p(\theta)=常数$，同这篇博客<a href="http://www.bithub00.com/2019/10/21/%E9%AB%98%E6%96%AF%E5%85%88%E9%AA%8C%E4%B8%8EL2%E6%AD%A3%E5%88%99/" target="_blank" rel="noopener">高斯先验分布与L2正则</a>一样，我们要最大化如下的后验估计：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta^* &= argmax_\theta(\prod_i P(Y_i|X_i,\theta)\prod_i P(\theta_i)) \\
&= argmin_\theta(\sum_i \lVert f(X_i)-Y_i\rVert^2+\sum_i ln(P(\theta_i)))
\end{aligned}</script><p>将第一节中的Laplace分布代入$P(\theta_i)$，我们得到：</p>
<script type="math/tex; mode=display">\theta^*=argmin_\theta(\sum_i \Vert f(X_i)-Y_i\rVert^2+\lambda\sum_i|\theta_i|)</script><p>这就是L1正则。</p>
]]></content>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Convolutional Neural Networks for Graphs[ICML&#39;16]</title>
    <url>/2020/12/22/Learning-Convolutional-Neural-Networks-for-Graphs%5BICML16%5D/</url>
    <content><![CDATA[<p>ICML16一篇将CNN应用到图数据上的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积神经网络都是应用在图像数据上，如何将它有效地应用于图类型的数据上。</p>
<p>对于图像数据，应用一个卷积神经网络可以看成将receptive field（图中为$3\times3$）以固定的步长将图像遍历，因为图像中像素点的排列有一定的次序，receptive field的移动顺序总是从上到下，从左到右。这也唯一地决定了receptive field对一个像素点的遍历方式以及它如何被映射到向量空间中。</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WKA5n.png" alt="0WKA5n.png" border="0" width="65%">
</div>


<p>然而对于图结构数据这种隐式的结构特征很多时候是缺失的，而且当给定不止一张图时，各个图之间的顶点没有必然的联系。因此，在将卷积神经网络应用在图数据上时，需要解决下面两个问题：</p>
<ol>
<li><p>决定邻域中顶点的产生次序</p>
</li>
<li><p>计算一个将图映射到向量空间的映射方法</p>
</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/12/0W1Zo4.png" alt="0W1Zo4.png" border="0" width="80%"></p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出方法的流程如下：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0W3X5Q.png" alt="0W3X5Q.png" border="0" width="60%"></p>
<h4 id="Node-Sequence-Selection"><a href="#Node-Sequence-Selection" class="headerlink" title="Node Sequence Selection"></a>Node Sequence Selection</h4><p>从图中选取固定数量$w$的顶点，它类比于图像的宽度，而选出的顶点就是卷积操作中小矩形的中心顶点。$w$就是在这个图上所做的卷积操作的个数。如下图所示，$w=6$，代表需要从图中选择6个顶点做卷积操作。论文中选取顶点的方式为$\text{DFS}$，关键点在于图标签函数$l$，这个函数的作用是决定选取顶点的次序，可以选区的函数为between centrality与WL算法等等</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WGInP.png" alt="0WGInP.png" border="0"></p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WyWOU.png" alt="0WyWOU.png" border="0"></p>
<h4 id="Neighborhood-Assembly"><a href="#Neighborhood-Assembly" class="headerlink" title="Neighborhood Assembly"></a>Neighborhood Assembly</h4><p>选取完顶点后，下一步是为它们构建receptive field，类似于第一张图中的$3\times3$矩阵。选取的方式为，以顶点$v$为中心，通过$\text{BFS}$添加领域顶点，直到满足receptive field长度$k$：</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WDBw9.png" alt="0WDBw9.png" border="0" width="60%">
</div>


<p><img src="https://s1.ax1x.com/2020/10/12/0W6V0g.png" alt="0W6V0g.png" border="0" width="80%"></p>
<h4 id="Graph-Normalization"><a href="#Graph-Normalization" class="headerlink" title="Graph Normalization"></a>Graph Normalization</h4><p>在选取了满足数量的邻域顶点后，下一步是通过图标签函数$l$为这些顶点赋予一个次序，目的在于将无序的领域映射为一个有序的向量：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0Wy1dH.png" alt="0Wy1dH.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0W6rnO.png" alt="0W6rnO.png" border="0" width="60%">
</div>


<h4 id="Convolutional-Architecture"><a href="#Convolutional-Architecture" class="headerlink" title="Convolutional Architecture"></a>Convolutional Architecture</h4><p>最后一步就是应用卷积层提取特征，顶点和边的属性对应于传统图像CNN中的channel：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpKeK.png" alt="0fpKeK.png" border="0" width="75%"></p>
<p>假设顶点特征的数目为$a_v$，边的特征个数为$a_e$，$w$为选取的顶点个数，$k$为receptive field中的顶点个数，则对于输入的一系列图中的每一个，可以得到两个张量维度分别为$(w,k,a_v)、(w,k,k,a_e)$，可以变换为$(wk,a_v)、(wk^2,a_e)$，其中$a_v$与$a_e$可以看成是传统图像卷积中channel的个数，对它们做一维的卷积操作，第一个的receptive field的大小为$k$，第二个的receptive field的大小为$k^2$。</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpWwT.png" alt="0fpWwT.png" border="0" width="75%"></p>
<p>整体卷积结构：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpbOx.png" alt="0fpbOx.png" border="0" width="75%"></p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI、D&amp;D</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>KGAT - Knowledge Graph Attention Network for Recommendation[KDD&#39;19]</title>
    <url>/2020/12/22/KGAT%5BKDD19%5D/</url>
    <content><![CDATA[<p>KDD19一篇将知识图谱与GNN融合的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在推荐系统中，如何将用户-物品交互信息与物品自身的属性相结合以做出更好的推荐，从另一个角度来说，即如何融合用户-物品交互图与知识图谱</p>
<div align="center">
<a href="https://imgchr.com/i/BnaHGn" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnaHGn.png" alt="BnaHGn.png" border="0" width="65%"></a>
</div>


<p>以上面的图为例，在电影推荐场景中，用户对应于观众，物品对应于电影，实体Entities可以有多种含义，例如导演、演员、电影类别等，对应的就会有多种关系，对应图中的$r_1-r_4$。对于用户$u_1$，协同过滤更关注于他的相似用户，即同样看过$i_1$的$u_4$与$u_5$；而有监督学习方法例如因子分解机等会更关注物品之间的联系，例如$i_1$与$i_2$同样有着属性$e_1$，但它无法进一步建模更高阶的关系，例如图中黄色圈内的用户$u_2$与$u_3$观看了同一个导演$e_1$的电影$i_2$，而这名导演$e_1$又作为演员参演了灰色圈内的电影$i_3$与$i_4$。图中上半部分对应于用户-物品交互图，下半部分对应于知识图谱。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BnDu0f" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnDu0f.png" alt="BnDu0f.png" border="0"></a></p>
<h4 id="CKG-Embedding-Layer"><a href="#CKG-Embedding-Layer" class="headerlink" title="CKG Embedding Layer"></a>CKG Embedding Layer</h4><p>知识图谱的一般形式可以表示为三元组的集合$\{(h,r,t)\}$，表示头实体$h$与尾实体$t$之间有关系$r$，例如$\text{(Hugh Jackman,ActorOf,Logan)}$表示狼叔是电影罗根的演员，这是一种主动的关系，自然就有逆向的被动关系。而对于用户-物品交互信息来说，通常的表示形式为一个矩阵$R$，$R_{ui}$表示用户$u$与物品$i$的关系，有交互则值为1，否则为0。因此，为了统一两种表示形式，论文中将用户-物品交互信息同样改成三元组的集合$\text$，这样一来得到的统一后的新图称之为Collaborative Knowledge Graph(CKG)。</p>
<p>第一个步骤是对CKG做embedding，得到图中顶点和边的向量表示形式。论文使用了知识图谱中常用的一个方法$\text{TransR}$，即对于一个三元组$(h,r,t)$，目标为：</p>
<script type="math/tex; mode=display">
e_h^r+e_r\approx e_t^r</script><p>其中$e_h,e_t\in \mathbb{R}d、e_r\in \mathbb{R}k$分别为$h、t、r$的embedding，而$e_h^r,e_t^r$为$e_h、e_t$在$r$所处空间中的投影，损失函数定义为：</p>
<script type="math/tex; mode=display">
g(h,r,t)=||W_re_h+e_r-W_re_t||^2_2</script><p>值越小说明该三元组在知识图谱中更可能存在，即头实体$h$与尾实体$t$之间更可能有关系$r$。经过这一步骤之后，CKG中所有的顶点及边我们都得到了它们的embedding。</p>
<h4 id="Attentive-Embedding-Propagation-Layers"><a href="#Attentive-Embedding-Propagation-Layers" class="headerlink" title="Attentive Embedding Propagation Layers"></a>Attentive Embedding Propagation Layers</h4><p>第二个步骤直接用的GCN与GAT的想法，在一层embedding propagation layer中，利用图卷积网络在邻域中进行信息传播，利用注意力机制来衡量邻域中各邻居顶点的重要程度。再通过堆叠$l$层来聚合$l$阶邻居顶点的信息。</p>
<p>在每一层中，首先将顶点$h$的邻域以向量形式表示，系数$\pi(h,r,t)$还会进行$\text{softmax}$归一化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_{N_h}&=\sum_{(h,r,t)\in N_h}\pi(h,r,t)e_t \\
\pi(h,r,t)&=(W_re_t)^T\text{tanh}\big(W_re_h+e_r\big)
\end{aligned}</script><p>通过堆叠$l$层来聚合$l$阶邻居顶点的信息：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_h^{(l)}&=f\big( e_h^{(l-1)},e_{N_h}^{(l-1)} \big) \\
&=\text{LeakyReLU}\big( W_1(e_h+e_{N_h})\big)+\text{LeakyReLU}\big( W_2(e_h\odot e_{N_h})\big)
\end{aligned}</script><p>论文中所使用的聚合函数$f$在GCN与GraphSage的基础上，还额外地引入了第二项中$e_h$与$e_{N_h}$的交互，这使得聚合的过程对于两者之间的相近程度更为敏感，会在更相似的顶点中传播更多的信息。</p>
<h4 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h4><p>在得到$L$层embedding propagation layer的表示后，使用JK-Net中的LSTM-attention进行聚合，在通过点积的形式给出预测分数：</p>
<script type="math/tex; mode=display">
e_u^*=\text{LSTM-attention}(e_u^{(0)},e_u^{(L)})\\e_i^*=\text{LSTM-attention()}e_i^{(0)}||\dots||e_i^{(L)}\\
\hat{y}(u,i)={e_u^*}^Te_i^*</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Amazon-book、Last-FM、Yelp2018</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGCN - Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR&#39;20]</title>
    <url>/2020/12/22/LightGCN%5BSIGIR20%5D/</url>
    <content><![CDATA[<p>SIGIR20一篇简化GCN架构的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在协同过滤中，图卷积网络中的特征转换与非线性激活对提升模型表现贡献很小，甚至有负面影响。</p>
<blockquote>
<p>在半监督顶点分类问题中，每个顶点有充分的语义特征作为输入，例如一篇文章的标题与摘要词，这种情况下加入多层的非线性特征转换能够有助于学习特征。而在协同过滤任务中，每个顶点（用户或商品）没有这么充分的语义特征，因此没有多大的作用。</p>
</blockquote>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0evf4P.png" alt="0evf4P.png" width="80%" border="0">
</div>


<script type="math/tex; mode=display">
\hat{y}_{ui}=e_u^Te_i \\
e_u=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_i=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_u^{(k+1)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(k)} \\
e_i^{(k+1)}=\sum_{i\in N_i}\frac{1}{\sqrt{|N_i||N_u|}}e_u^{(k)}</script><ol>
<li><p>仅考虑图卷积网络中的neighborhood aggregation，通过在用户-物品交互网络中线性传播来学习用户和物品的embedding，再通过加权和将各层学习的embedding作为最后的embedding</p>
</li>
<li><p>通过减少不必要的架构，相较于NGCF大大减少了需要训练的参数量。唯一需要训练的模型参数是第0层的embedding，即$e_u^{(0)}$与$e_i^{(0)}$，当它们两个给定后，后续层的embedding可以通过传播规则直接进行计算</p>
</li>
</ol>
<blockquote>
<p>以加权和的方式结合各层的embedding等价于带自连接的图卷积</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
E^{(K)}&=(A+I)E^{(K-1)}=(A+I)^KE^{(0)}\\
&=C_K^0E^{(0)}+C_K^1AE^{(0)}+C_K^2A^2E^{(0)}+\dots+C_K^KA^KE^{(0)}
\end{aligned}</script><ol>
<li>模型的可解释性更强，以二层网络为例:</li>
</ol>
<script type="math/tex; mode=display">
e_u^{(2)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(1)}=\sum_{i\in N_u}\frac{1}{|N_i|}\sum_{v\in N_i}\frac{1}{\sqrt{|N_u||N_v|}}e_v^{(0)}</script><p>如果另一个用户$v$与目标用户$u$有关联，则影响以下面的系数表示：</p>
<script type="math/tex; mode=display">
c_{v\rightarrow u}=\frac{1}{\sqrt{|N_u||N_v|}}\sum_{i\in N_u\cap N_v}\frac{1}{|N_i|}</script><p>可解释为:</p>
<ul>
<li>共同交互过的物品越多系数越大 $i\in N_u\cap N_v$</li>
<li>物品流行度越低系数越大$\frac{1}{|N_i|}$</li>
<li>用户$v$越不活跃系数越大$\frac{1}{|N_v|}$</li>
</ul>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-Book</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Inductive Matrix Completion Based on Graph Neural Networks[ICLR&#39;20]</title>
    <url>/2021/01/14/ICMC%5BICLR20%5D/</url>
    <content><![CDATA[<p>ICLR20一篇使用GNN来解决现有矩阵补全方法无法泛化问题的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何让矩阵补全方法中一个数据集得到的embedding，能够迁移到另一个数据集上，同时不依赖额外的信息。</p>
<p>与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>应用的问题相同，具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。传统的做法是将输入的评分矩阵分解成用户与物品的embedding，通过embedding重构评分矩阵，填补其中的缺失值，从而做出预测，如下图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/sd6O1S" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd6O1S.png" alt="sd6O1S.png" border="0" width="80%"></a>
</div>

<p>很多现有方法研究的都是如何得到更好的embedding，但它们都是直推式(transductive)而非启发式(inductive)的，意味着没法迁移，例如MovieLens数据集上得到的embedding就不能直接用于Douban数据集上，需要重新训练一个新的embedding。即使对于同一个数据集而言，如果加入新的评分记录，往往需要整个embedding重新训练。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Enclosing-Subgraph-Extraction"><a href="#Enclosing-Subgraph-Extraction" class="headerlink" title="Enclosing Subgraph Extraction"></a>Enclosing Subgraph Extraction</h4><p>论文的做法是为每一个评分记录提取一个子图，并且训练一个图神经网络来将得到的子图映射为预测评分。要想为评分记录提取子图，首先要将评分矩阵转换为图，转换的方法与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>相同，博客中有具体介绍，这里就不重复说明了。论文中对子图的定义方式为，给定一个评分记录$(u,v)$，表示用户$u$给物品$v$评过分，那么这个评分记录提取的子图由该用户$u$、物品$v$以及它们各自的$h$跳邻域内的顶点构成。为了具体说明是怎么从一个评分记录提取出子图的，我从论文作者的视频中截取了这部分内容，如下图所示：</p>
<p>假设第一张图中深绿色的方格是缺失值，这里先填入了模型的预测评分，倒退着来说明预测评分是怎么通过子图得到的。我们首先找到这个用户评过分的其它物品，对应于第五个物品的四分与第八个物品的两分，如第二张图所示。下一步是找到为这个物品评过分的其他用户，对应于第三个用户的五分与第四个用户的五分。</p>
<div align="center">
<a href="https://imgchr.com/i/sdfNf1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sdfNf1.png" alt="sdfNf1.png" border="0"></a>
</div>

<p>通过图二和图三找到的关系，就可以提取出这个评分记录的子图了，如下图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/sd4urT" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd4urT.png" alt="sd4urT.png" border="0" width="90%"></a>
</div>

<p>可以看到，这个提取出的子图能提供许多有用的信息，例如用户平均评分、物品平均评分、物品累计评价次数以及基于路径的结构信息。论文希望通过这种结构信息来找到一些特征，从而做出预测，例如，如果用户$u_0$喜欢一个物品$v_0$，那么对于另一个与他品味相同的用户$u_1$，我们可能发现他也喜欢$v_0$。品味相同可以表示为两个用户都喜欢另一个物品$v_1$，这个特征可以表示为这么一条路径：$u_0\rightarrow_{like}v_1\rightarrow_{liked\ by}u_1\rightarrow_{like}v_0$，如果$u_0$与$v_0$之间存在多条这样的路径，那么我们就可以推测$u_0$喜欢$v_0$。类似这样的结构特征数不胜数。因此，与其人工来手动定义大量这样的启发式特征(heuristics)，不如直接将子图输入一个图神经网络，来自动学习更通用的、更有表达能力的特征。</p>
<h4 id="Node-Labeling"><a href="#Node-Labeling" class="headerlink" title="Node Labeling"></a>Node Labeling</h4><p>这一步给顶点打标签是为了让子图中的顶点有着不同的角色，例如区分哪个是需要预测的目标用户与目标物品，区分用户顶点与物品顶点。而论文中打标签的方式十分简单：</p>
<ul>
<li>目标用户与目标物品分别标记为0和1</li>
<li>对于$h$跳邻域内的顶点，如果是用户顶点标记为$2h$，物品顶点则标记为$2h+1$</li>
</ul>
<p>标记之后，我们就能知道哪个是需要预测的目标用户与目标物品、哪些是用户顶点，因为用户顶点的标签均为偶数，以及邻域内顶点距离目标顶点距离的远近。这些标签将转换为one-hot编码的形式作为图神经网络输入的初始特征$x_0$。</p>
<p>这一节的最后论文作者还说到了这种标记方式与<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>做法的不同之处。GCMC中同样是将标签转换为one-hot编码的形式作为GNN的初始特征，不同的是它用顶点在整个bipartite graph中的全局id作为它的标签，这等价于将GNN第一层信息传递网络的参数，转换为与每个顶点的全局id相关联的embedding函数，可以理解为一个embedding查找表，输入一个全局id，输出它对应的embedding。这显然是直推式的，对于不在查找表中的id，就无法得到它的embedding。这种情况对应于在小数据集上训练网络得到embedding，然后换到大数据集上，因为大数据集的顶点数量肯定要多于小数据集，这就会使得顶点的全局id范围变大，超出了训练出来的这个embedding查找表的范围。</p>
<h4 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h4><p>这一步的目的就是训练一个GNN来将提取出的子图映射成预测评分。论文所使用的GNN分为两个部分：信息传递层与池化层。前者的作用是得到子图中各顶点的特征向量，后者是根据得到的特征向量形成子图的一个特征表示。</p>
<p>信息传递部分使用的是<a href="https://arxiv.org/pdf/1703.06103v4.pdf" target="_blank" rel="noopener">R-GCN</a>：</p>
<script type="math/tex; mode=display">
x_i^{l+1}=W_0^lx_i^l+\sum_{r\in R}\sum_{j\in N_r(i)}\frac{1}{|N_r(i)|}W_r^lx_j^l</script><p>其中$x_i^l$表示第$i$个顶点在第$l$层的特征向量，$N_r(i)$表示评分水平$r$下顶点$i$的邻域，顶点$i$以不同的边权重$r$所连接的顶点$j$用不同的参数矩阵$W_r^l$来进行处理。通过堆叠$L$层网络可以得到顶点$i$的$L$个特征向量，通过拼接的方式得到它最终的特征表示$h_i$：</p>
<script type="math/tex; mode=display">
h_i=\text{concat}(x_i^1,x_i^2,\dots,x_i^L)</script><p>池化部分只选取子图中目标用户与目标顶点的特征向量进行拼接，来得到该子图的特征表示，这么做的原因是这两个顶点携带了最多的信息。</p>
<script type="math/tex; mode=display">
g=\text{concat}(h_u,h_v)</script><p>在得到子图的特征表示后，最后一步是通过一个MLP将它转换为一个预测评分$\hat{r}$：</p>
<script type="math/tex; mode=display">
\hat{r}=w^T\sigma(Wg)</script><h4 id="Adjacent-Rating-Regularization"><a href="#Adjacent-Rating-Regularization" class="headerlink" title="Adjacent Rating Regularization"></a>Adjacent Rating Regularization</h4><p>论文对于信息传递部分使用的R-GCN还提出了一点改进，在原始的R-GCN中，不同的评分水平是独立看待的，彼此之间没有关联，例如对于1、4、5这三个评分，显然地4和5都表示了用户的喜爱而1表示了用户的厌恶，同时4和5的相似程度要大于4和1，但这种次序关系及大小关系在原始的R-GCN中都被丢掉了。因此本论文添加了一个约束来引入这部分丢失的信息，具体做法也很简单，就是使得相邻的评分水平使用的参数矩阵更加相似：</p>
<script type="math/tex; mode=display">
L_{ARR}=\sum_{i=1,2,\dots,|R|-1}||W_{r_i+1}-W_{r_i}||_F^2</script><p>这里假设评分$r_1,r_2,\dots,r_{|R|}$表示了用户喜爱程度的递增，通过这个约束就保留了评分的次序信息，同时可以使得出现次数较少的评分水平可以从相邻的评分水平中迁移信息，来弥补数据不足带来的问题。</p>
<h4 id="Graph-level-GNN-vs-Node-level-GNN"><a href="#Graph-level-GNN-vs-Node-level-GNN" class="headerlink" title="Graph-level GNN vs Node-level GNN"></a>Graph-level GNN vs Node-level GNN</h4><p>这一节还是在于GCMC作比较。在GCMC中，采用的是顶点层面的图神经网络，它应用于图中的顶点来得到顶点的embedding，再通过embedding得到预测评分，如下右图所示。这么做的缺陷是它独立地学习两个顶点所关联的子树，而忽略了这两棵子树之间可能存在的联系。</p>
<p><div align="center">
<a href="https://imgchr.com/i/swYFij" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/15/swYFij.png" alt="swYFij.png" border="0" width="50%"></a>
</div></p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、ML-100K、ML-1M</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>高效计算Personalized PageRank小综述</title>
    <url>/2021/07/04/PPR%E5%B0%8F%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<p>本学期最后一次组会我做了这两三年近似计算Personalized PageRank（PPR）的小综述，包含理论及图神经网络上的应用。</p>
<a id="more"></a>
<p>这里我直接把组会分享的PPT和讲稿放上来了。</p>
<h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>大家好，今天我在组会本来要分享ICDE20年一篇高效计算个性化PageRank的论文，但因为论文涉及过多方法上的细节而大家都不是PageRank近似计算的研究方向，所以我想转为介绍一下近几年顶会上对近似计算个性化 pagerank这个问题是怎么做的以及有哪些应用场景，涉及下面这些论文，其中前面四篇为理论而后面两篇是在GNN上的应用。</p>
<p>首先再简单回顾一下个性化PageRank。个性化PageRank简称PPR，它来自于Google创始人于1998年提出的PageRank。PageRank用于度量web网络中各网页的重要性，它的核心思想有两点：被很多网页所链接的网页重要性较高；被重要的网页所链接的网页重要性较高。原始的PageRank的计算方式是一个迭代的过程。$\pi$是n维的PageRank向量，每一维对应一个顶点的PageRank分数，下面的例子是说明PageRank的计算过程以及式子中两部分的由来，之前介绍过这里就不重复了。因为PageRank 相当于站在全局视角评价所有节点的重要程度，必须遍历所有网络上的节点，实际中很难做到，而且为了更个性化的评价，就有了PPR。它是PageRank的一种特殊形式，用于衡量图上顶点关于某一顶点的相对重要性，两者之间的关系为下面的式子。一个是相对重要性一个是全局重要性。</p>
<p>因为PPR的计算复杂度较高，所以有一些对它的近似估计方法，主要是下面这三种，它们作为后面要介绍的几篇论文的基本方法，这里大致地介绍一下。首先是蒙特卡洛模拟。它作为一种采样的方法，是通过以随机游走的概率来估计PPR值，至于为什么可以这样来估计，主要来自PPR的另一个定义，即两点之间的PPR值等价于从起始顶点出发的随机游走终止于目标顶点的概率。从大数定律知道，次数足够多时频率可以近似于概率，蒙特卡洛的做法就是进行若干次随机游走，将其中终止于目标顶点的比例作为PPR的近似值。做法很简单，关键在于随机游走的次数怎么选取来保证近似的精度。这里直接给出结论，证明过程要用到切尔诺夫界限，这里不展开了。</p>
<p>下一个是Forward Push，它为每个顶点维护两个变量，残差值和$\pi$值。在每轮迭代时，对于满足条件的顶点，首先将出度邻居的残差值增加一定比例，随后增加自己的$\pi$值并将残差值置零。直到所有顶点都不满足条件时迭代中止，将$\pi$值作为近似的PPR值。有前向类似地就有后向，做法基本一致，区别在于后向考虑的是入度邻居。两个方法的联系在于，前向的输入是源顶点s，考虑的是出度邻居；而后向的输入是目标顶点t，考虑的是入度邻居。</p>
<p>从刚刚的介绍可以看到，蒙特卡洛准确但效率低，因为需要大量的随机游走；Forward Push效率高但无法保证准确度。因此有学者将两者结合起来，就有了KDD17年的这篇论文FORA。Forward Push之所以无法保证准确度，是因为它返回的近似值与真实值之间本来就不保证接近，这篇论文找到了两者之间的一个联系，通过新加的尾项来建立两个值之间的关系。因为新加的尾项中$\pi(v,t)$同样是未知的，所以论文通过随机游走的办法来近似得到这个值，这就是蒙特卡洛的部分。那既然是通过随机游走来进行近似，具体要游走多少次来保证近似的精度，以及在Forward Push中的终止条件怎么确定就是两个关键的问题。这两个参数的选取都涉及证明这里就略过了。看论文的算法流程图，就是分为两个部分。Forward Push得到一个近似值，再通过蒙特卡洛提高近似值的精度。蒙特卡洛部分，蓝色下划线都是参数的取值，关键在于蓝色框内这个增量的操作。刚才说到论文通过新添加了一个尾项来建立Forward Push近似值与真实值之间的联系，而这里算法中+=这部分增量的期望是等于这个尾项的。</p>
<p>到这里对KDD17年这篇论文的介绍就结束了，接下来是本来组会要分享的ICDE20年的这篇论文，它主要是针对上一篇中的一些不足进行了改进，整体的算法流程没有改变。提出的第一个改进是所谓的“残差累积”，目的是为了减少Forward Push中每个顶点的转移次数。拿图中的这个简单图为例子，对于每个节点的残差值，Forward Push中涉及的操作只有两个，就是图中用红框圈出来的部分。将一部分转移给出度邻居之后置零。以顶点v1为源顶点，初始时只有v1的残差值为1。因为它的出度邻居是v2和v3，所以第一步的时候把自己的残差值转移了一部分给v2和v3，随后置零。所以这里矩阵的第一行只有v2和v3的值非零。下一步到v2，同样地把残差值转移给唯一的出度邻居v4并置零，所以第二行只有v3和v4的值非零，以此类推，可以得到矩阵每一行的值。在这个过程中，会发现顶点v2的残差值转移了两次，v1转移给它后它转给了v4，v3转移给它后它也转给了v4，这篇论文就认为这种单个顶点重复的转移是冗余的，实际上可以v1和v3都转移给v2后v2再一次性转移给v4，也就是右下角这个矩阵，这时候得到相同的结果只需要三步就可以了，虽然相对于原来的做法只减少了一步，但如果在大图上节省的操作次数是比较可观的。这种不断累积最后一次性转移出去的操作就是论文提出的第一个改进“残差累积”。</p>
<p>第二个改进是针对图中有自环的情况，拿图中的简单图为例，在顶点s处有一个自环，如果是原始的Forward Push，残差值会沿着s-&gt;v1-&gt;v2-&gt;s这样不断循环下去，直到某个节点的残差值不满足转移条件才会终止。论文给出的解决方法是在顶点s第一次转移后，就只接受其他顶点转移过来的残差值进行累积，不再把自己的残差值转移出去，这样就切断了自环的循环。但是对于一个大图来讲，要让除了顶点s外的其他顶点都转移完是非常耗时间的，所以论文提出在s的一个子图上完成这个残差累积的过程。这个过程结束后，子图中其他顶点的残差值和$\pi$值能够通过顶点s的残差值直接计算得到，不需要再重复进行转移的操作。之所以能这么做是因为论文证明了不管是否进行残差累积，顶点s涉及的转移操作都是一样的，区别只在于转移的残差值是多少，而这个残差值在两种操作间有种比例关系，根据这个比例关系和顶点s累积后的残差值就能直接计算出来。这部分在论文中涉及的定理引理和证明比较多，我就不在这里介绍了，论文是为了说明这种残差累积能够切断自环情况的同时还能保证结果的准确。因为按照论文对子图的定义，子图外的顶点即使满足转移条件也是不能进行残差值的转移的，所以还有额外的一步是扫描子图外的顶点进行残差值转移的操作。我们看论文的算法流程图会发现，也是可以分为两个部分，Forward Push和蒙特卡洛，只是Forward Push在上一篇的基础上进行了两点改进。<br>刚刚介绍的这两篇都是围绕Forward Push+蒙特卡洛展开，那有没有办法将另外一个基本方法也考虑进来？SIGMOD18年的这篇论文就是结合了这三者，来解决topk PPR查询问题。结合的动机是通过Backward Search进一步提升蒙特卡洛的精度。把前向和后向两个方法的表达式放在一起，把后向的表达式直接带入前向的尾项中，就是对应标红的部分，就得到了这篇论文的一个表达式。在第三项中的真实值$\pi(u,v)$同样是未知的，这时候通过蒙特卡洛来近似这个值，所以就完成了对三个基本方法的结合。前面针对的的问题都是点对点的PPR查询，给定两个顶点s和t，希望知道它们之间的PPR值。实际应用中我们可能更想知道，对于一个顶点s来说，topk PPR的顶点是哪些，典型的应用就是社交网络上的好友推荐，希望找到和用户相关的好友，而不是查询用户和某个好友之间的亲密程度。对于topk PPR查询问题，最直观的想法就是做一次全图PPR查询之后排序取topk，但这么做显然复杂度很高。类似地，求解topk查询也是希望在较快的时间内给出一个精度较高的近似解，按照定义需要满足topk中的近似值与真实值之间的误差也小于某个界限，但界限中的PPR真实值是预先不知道的，就无法确定topk PPR的一个大概范围。回到刚才KDD17年的那篇论文，它对于topk问题给出的解决方案是试错然后调整。首先假设真实值是一个较大的值进行查询，如果查询结果不如假设的大，将假设值调低进行下一次迭代，直到查询结果满足要求。在算法流程图里这个试错的过程体现在第一个红框，将$\delta$的值从1/2一直设到1/n。在候选集C有了k个顶点后，为每个顶点计算一个上限和下限来判断近似的精度是否满足要求，判断条件是图中的两个红色下划线部分，如果两个条件均满足则返回当前结果作为topk查询的结果。上下限的定义及判断条件的构造也是涉及一些定理和证明，这里同样就略去了。回到SIGMOD18年的这篇论文，它的做法也是类似的，维护一个候选集C，为候选集C中的顶点计算一个界限，如果界限满足某个条件则把候选集C中的节点移入结果集Vk中，直到结果集中的数量满足要求。这个界限和判断条件也是涉及定理和证明这里一样略去了。对于候选集来说，里面的顶点大致可以分为三类，确定为topk的顶点，位于topk边界的顶点以及不可能为topk的顶点，对于第二类边界的顶点希望进行更深的Backward Search来得到更精确的一个近似值，做进一步的判断。</p>
<p>实验部分的话因为几种方法都有理论上证明近似的精确度，所以这里只放了效率之间的比较，最快的方法是ICDE2020年的ResAcc，其次是SIGMOD18的TopPPR以及KDD17的FORA。到这里就介绍完了所有的理论部分，最后介绍两篇将PPR应用于图神经网络上的论文，ICLR19年的这篇论文是最早将PPR引入GNN来解决GNN层数无法加深的问题，从最开始的介绍我们知道了PPR的原始定义式，一个迭代求解的过程，它的解可以表示为下面第二个式子，这篇论文就是将求解得到的PPR矩阵作为GNN中信息传递的模式，同时将预测和传递分离开来，解决传统GNN层数无法加深的问题。那么导致GNN无法加深的一个原因是所谓的过平滑现象，就是随着层数的加深，不同顶点经过网络学习得到的特征表示会趋于相同，导致顶点间没有区分度。拿下面的图为例，从左到右从上到下是网络层数加深后顶点特征表示的可视化结果，可以看到在六层的时候顶点几乎都已经混在了一起，远没有二三层时表现来的好。</p>
<p>导致这种过平滑现象的原因ICML18年的一篇论文从随机游走的角度给出了解释。一个K层的GNN相当于从源顶点出发到目标顶点的一个K步的随机游走，当K趋向于无穷即层数不断加深时, 顶点的极限分布与初始的顶点表示无关，只与图的结构相关。也就是说，随着层数加深聚合的信息越多，反而得到的表示与中心顶点无关。所以ICLR19年这篇论文引入PPR就是引入其中的重启概率$\alpha$，每一步时会以一定的概率重新回到初始顶点，这样来保证层数加深后与中心顶点间依然有联系。而且与GCN的表达式比较，这篇论文的表达式与层数l无关。最后是KDD20年的PPRGo，从表达式来看做法几乎和上一篇是一样的，不同之处在于这里用到的PPR矩阵是一个近似值，而且对每一行取了topk来控制信息传递时的规模。</p>
<h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/RfWGee" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWGee.png" alt="RfWGee.png"></a><br><a href="https://imgtu.com/i/RfW1sO" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW1sO.png" alt="RfW1sO.png"></a><br><a href="https://imgtu.com/i/RfW3LD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW3LD.png" alt="RfW3LD.png"></a><br><a href="https://imgtu.com/i/RfW0Qf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW0Qf.png" alt="RfW0Qf.png"></a><br><a href="https://imgtu.com/i/RfWlQK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWlQK.png" alt="RfWlQK.png"></a><br><a href="https://imgtu.com/i/RfWNFA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWNFA.png" alt="RfWNFA.png"></a><br><a href="https://imgtu.com/i/RfWaWt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWaWt.png" alt="RfWaWt.png"></a><br><a href="https://imgtu.com/i/RfWYod" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWYod.png" alt="RfWYod.png"></a><br><a href="https://imgtu.com/i/RfWUJI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWUJI.png" alt="RfWUJI.png"></a><br><a href="https://imgtu.com/i/RfWwSP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWwSP.png" alt="RfWwSP.png"></a><br><a href="https://imgtu.com/i/RfWBy8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWBy8.png" alt="RfWBy8.png"></a><br><a href="https://imgtu.com/i/RfWDOS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWDOS.png" alt="RfWDOS.png"></a><br><a href="https://imgtu.com/i/RfWseg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWseg.png" alt="RfWseg.png"></a><br><a href="https://imgtu.com/i/RfWywQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWywQ.png" alt="RfWywQ.png"></a><br><a href="https://imgtu.com/i/RfWgFs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWgFs.png" alt="RfWgFs.png"></a><br><a href="https://imgtu.com/i/RfW6oj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW6oj.png" alt="RfW6oj.png"></a><br><a href="https://imgtu.com/i/RfW2Yn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW2Yn.png" alt="RfW2Yn.png"></a><br><a href="https://imgtu.com/i/RfWhlV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWhlV.png" alt="RfWhlV.png"></a><br><a href="https://imgtu.com/i/RfWRWq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWRWq.png" alt="RfWRWq.png"></a><br><a href="https://imgtu.com/i/RfW4yT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW4yT.png" alt="RfW4yT.png"></a><br><a href="https://imgtu.com/i/RfWfS0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWfS0.png" alt="RfWfS0.png"></a><br><a href="https://imgtu.com/i/RfW5OU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW5OU.png" alt="RfW5OU.png"></a><br><a href="https://imgtu.com/i/RfWomF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWomF.png" alt="RfWomF.png"></a><br><a href="https://imgtu.com/i/RfWTw4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWTw4.png" alt="RfWTw4.png"></a><br><a href="https://imgtu.com/i/RfW7TJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW7TJ.png" alt="RfW7TJ.png"></a><br><a href="https://imgtu.com/i/RfWbk9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWbk9.png" alt="RfWbk9.png"></a><br><a href="https://imgtu.com/i/RfWqYR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWqYR.png" alt="RfWqYR.png"></a><br><a href="https://imgtu.com/i/RfWLf1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWLf1.png" alt="RfWLf1.png"></a><br><a href="https://imgtu.com/i/RfWXSx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWXSx.png" alt="RfWXSx.png"></a><br><a href="https://imgtu.com/i/RfWjl6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWjl6.png" alt="RfWjl6.png"></a><br><a href="https://imgtu.com/i/RffSmD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffSmD.png" alt="RffSmD.png"></a><br><a href="https://imgtu.com/i/RfWv6K" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWv6K.png" alt="RfWv6K.png"></a><br><a href="https://imgtu.com/i/RfWxOO" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWxOO.png" alt="RfWxOO.png"></a><br><a href="https://imgtu.com/i/Rffp0e" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rffp0e.png" alt="Rffp0e.png"></a><br><a href="https://imgtu.com/i/Rff9TH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rff9TH.png" alt="Rff9TH.png"></a><br><a href="https://imgtu.com/i/RffPkd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffPkd.png" alt="RffPkd.png"></a><br><a href="https://imgtu.com/i/RffitA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffitA.png" alt="RffitA.png"></a><br><a href="https://imgtu.com/i/RffFfI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffFfI.png" alt="RffFfI.png"></a><br><a href="https://imgtu.com/i/RffApt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffApt.png" alt="RffApt.png"></a><br><a href="https://imgtu.com/i/RffE1P" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffE1P.png" alt="RffE1P.png"></a><br><a href="https://imgtu.com/i/RffV6f" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffV6f.png" alt="RffV6f.png"></a><br><a href="https://imgtu.com/i/RffZX8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffZX8.png" alt="RffZX8.png"></a><br><a href="https://imgtu.com/i/RffmnS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffmnS.png" alt="RffmnS.png"></a><br><a href="https://imgtu.com/i/Rffu7Q" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rffu7Q.png" alt="Rffu7Q.png"></a><br><a href="https://imgtu.com/i/Rffn0g" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rffn0g.png" alt="Rffn0g.png"></a><br><a href="https://imgtu.com/i/RffMkj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffMkj.png" alt="RffMkj.png"></a><br><a href="https://imgtu.com/i/RffQts" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffQts.png" alt="RffQts.png"></a></p>
<p><a href="https://imgtu.com/i/RfqIoV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqIoV.png" alt="RfqIoV.png"></a><br><a href="https://imgtu.com/i/Rfq7JU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfq7JU.png" alt="Rfq7JU.png"></a><br><a href="https://imgtu.com/i/RfqTiT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqTiT.png" alt="RfqTiT.png"></a><br><a href="https://imgtu.com/i/Rfqbz4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqbz4.png" alt="Rfqbz4.png"></a><br><a href="https://imgtu.com/i/RfqHWF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqHWF.png" alt="RfqHWF.png"></a><br><a href="https://imgtu.com/i/RfqLQJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqLQJ.png" alt="RfqLQJ.png"></a><br><a href="https://imgtu.com/i/RfqOy9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqOy9.png" alt="RfqOy9.png"></a><br><a href="https://imgtu.com/i/RfqXLR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqXLR.png" alt="RfqXLR.png"></a><br><a href="https://imgtu.com/i/Rfqve1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqve1.png" alt="Rfqve1.png"></a><br><a href="https://imgtu.com/i/Rfqxdx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqxdx.png" alt="Rfqxdx.png"></a><br><a href="https://imgtu.com/i/Rfqzo6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqzo6.png" alt="Rfqzo6.png"></a><br><a href="https://imgtu.com/i/RfLCWD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfLCWD.png" alt="RfLCWD.png"></a><br><a href="https://imgtu.com/i/RfLpFK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfLpFK.png" alt="RfLpFK.png"></a><br><a href="https://imgtu.com/i/RfL9JO" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfL9JO.png" alt="RfL9JO.png"></a></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="https://wyydsb.xin/other/ppr.html" target="_blank" rel="noopener">大图中如何快速计算PPR</a></li>
</ul>
]]></content>
      <tags>
        <tag>高效计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Graph Collaborative Filtering[SIGIR&#39;19]</title>
    <url>/2020/12/22/NGCF%5BSIGIR19%5D/</url>
    <content><![CDATA[<p>SIGIR19年将神经网络与协同过滤结合的一篇论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在现有的推荐模型中，用户和物品的embedding只考虑了它们自身的特征，没有考虑用户-物品的交互信息</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><script type="math/tex; mode=display">
\hat{y}_{NGCF}(u,i)={e^*_u}^Te^*_i \\
e^*_u = e_u^{(0)}||\dotsb||e_u^{(L)} \\
e^*_i = e_i^{(0)}||\dotsb||e_i^{(L)} \\
e_u^{(l)}=LeakyReLU(m^{(l)}_{u\leftarrow u}+\sum_{i\in N_u}m^{(l)}_{u\leftarrow i}) \\
\begin{cases}
m^{(l)}_{u\leftarrow i}=p_{ui}(W_1^{(l)}e_i^{(l-1)}+W_2^{(l)}(e_i^{(l-1)}\odot e_u^{(l-1)})) \\\\
m^{(l)}_{u\leftarrow u}=W_1^{(l)}e_u^{(l-1)}
\end{cases} \\
m_{u\leftarrow i}=\frac{1}{\sqrt{|N_u||N_i|}}(W_1e_i+W_2(e_i\odot e_u))</script><ol>
<li>通过堆叠$l$层embedding传播层，一个用户（物品）可以获得它的$l$跳邻居所传播的信息，如下图所示，通过这种方法来建模用户-物品交互信息中的高阶connectivity，下图展示的是一个三阶的例子:</li>
</ol>
<div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg" width="400" height="200" alt="0ZY5mF.jpg" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0ePimF.png" width="68%" alt="0ePimF.png" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0Z2M40.jpg" width="400" height="200" alt="0Z2M40.jpg" border="0">
</div>


<ol>
<li><p>传统GCN推荐方法中，message embedding只考虑物品embedding$e_i$，论文中将用户embedding与物品embedding的交互也纳入考虑，解释为“This makes the message dependent on the affinity between $e_i$ and $e_u$, e.g., passing more messages from the similar items.”</p>
</li>
<li><p>两个层面上的dropout：message &amp; node dropout。前者表示在第$l$层传播层中，只有部分信息会对最后的表示有贡献；后者表示在第$l$层传播层中，随机地丢弃一些顶点。</p>
</li>
</ol>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-book</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Personalized PageRank to a Target Node, Revisited[KDD&#39;20]</title>
    <url>/2021/04/24/RBS%5BKDD20%5D/</url>
    <content><![CDATA[<p>KDD20一篇近似计算单目标PPR相似度的论文</p>
<a id="more"></a>
<p>这里我直接把组会分享的PPT和讲稿放上来了。</p>
<h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>今天我要讲的是KDD20年的一篇论文，解决的是针对目标顶点的个性化PageRank查询。个性化PageRank简称PPR，它来自于Google创始人于1998年提出的PageRank。PageRank用于度量web网络中各网页的重要性，它的核心思想有两点：被很多网页所链接的网页重要性较高；被重要的网页所链接的网页重要性较高。如果将web网络转化为图结构G(V,E)，网页看作顶点，网页间的链接看作边，则PageRank的计算方式为下面的式子，$\pi$是PPR向量有n维，每一维对应一个顶点的PageRank分数，是一个迭代的计算过程。</p>
<p>它可以用于衡量各顶点的全局重要性，举个例子说明这个式子的含义，有A、B、C、D四个网页，假设一个上网的人在网页A，那他会以1/3的概率跳到B、C和D，访问一个网页的概率由链接到它的所有网页的概率来决定，例如A由B、C两个网页链接，就有第一个式子。各个网页间的转移可以用一个概率转移矩阵表示，初始时上网的人在每个网页停留的概率是相等的，右乘上转移矩阵后就得到了下一时刻对每个网页的访问概率。然后不断迭代直到收敛。实际上，如果存在一个网页只链接到自己，例如图里的C，那就会像一个陷阱一样，导致概率分布值全部转移到网页C上来，所以PageRank另一部分就是以一定的概率跳转到一个随机的网页，来避免这种问题。</p>
<p>而PPR是PageRank的一种特殊形式，它用于衡量图上顶点关于某一顶点的相对重要性，两者之间的关系为下面的式子。一个是相对重要性一个是全局重要性。这篇论文重点关注是单目标PPR的计算问题，即给定图上某一顶点t作为目标顶点，计算图上所有顶点关于t的PPR值，它希望找到使得目标顶点t重要性较高的一系列顶点。有别于我们传统上理解的单源PPR的计算问题，后者是希望找到相对源顶点较为重要的一系列顶点。</p>
<p>论文做的是一个近似求解，满足一定的误差要求来提升计算的效率。近似的方法是从PPR的另一个定义方式出发，即从源顶点出发的随机游走停止在目标顶点的概率。进一步地分解，就是以0步、1步到无穷步停止的概率进行求和，近似的做法就是进行一个截断，上限不需要是无穷，截断后满足误差小于给定极限即可。单目标PPR的计算问题相关研究较少，目前时间复杂度上最优的方法是Backward Search，为$O(\frac{\overline{d}}{\delta})$，</p>
<p>这篇论文的贡献就是在它的基础上引入随机性，使得复杂度降为$O(\frac{1}{\delta})$。这里先介绍Backward Search的做法，然后指出论文具体在哪几个步骤做了改动。Backward Search的做法是，给定一个目标顶点t，算法对图上的每个顶点u都维护两个变量residue和reserve，我这里简称为r值和$\pi$值，每次选取当前r值最大的顶点v，更新自己的$\pi$值，再将r值以一定比例push给所有的入度邻居并置零。最后每个顶点的$\pi$值就作为PPR值的近似计算结果。因为在push操作时需要遍历所有的入度邻居，所以复杂度上会带有一个$\overline{d}$，表示图中所有顶点的平均出度。</p>
<p>所以论文在push这一步做了改动，提出了自己的方法RBS。它不会push给所有的入度邻居，而是只push增长值超过阈值的部分。之所以这么做，是因为Backward Search里的增长值与入度邻居u的出度成反比，对于出度较大的这部分入邻居，这次push操作对它的\pi值改变不大，即使进行了本次push，它\pi值的变化幅度也小于误差要求，所以放弃对它的push操作以节省时间。</p>
<p>在RBS算法里，push操作有三种情况，当出度满足要求时才确定进行push操作，如果不满足条件，产生一个0、1之间的随机数，如果出度满足小于放宽后的要求，则push一个固定值，否则不进行本次push。这样，对于每个需要push操作的入度邻居，只确定地更新了一部分并且采样了一部分进行更新以保证结果的无偏。</p>
<p>在实际实现时不能遍历每个入度邻居u来判断它是否满足要求，这样同样会带来$\overline{d}$的复杂度，因为push操作的条件是逐渐递减的，只需要预先对所有的入度邻居按照出度进行排序，逐个扫描到临界条件后就可以放弃查看剩下的所有邻居，因为都不会满足条件。最后是实验部分，选用的几个图数据集描述如下，有无向图也有有向图，有小图也有大图，评测指标一个是最大计算误差，另外两个是推荐常用指标。</p>
<p>实验结果上，因为是基于Backward Search提出的，baseline只选了这一个。图中的横轴都是查询时间，纵轴分别是各个评测指标，结果上来说就是花费同等的时间，新的方法可以达到更好的结果，达到相同的结果新的方法只需要更少的时间。</p>
<p>论文所研究的单目标PPR计算虽然相对冷门，但实际应用可以结合许多问题，第一个相当于是该问题下的一个特例，对顶点的重要性有更高的要求，其次在图神经网络上也有利用PPR矩阵进行改进的工作，例如ICLR19年的这个方法。</p>
<p>最后想重点说一下的是与SimRank相似度的关系，因为前一篇讨论班论文我讲的就是ICDE20年一篇如何高效计算SimRank相似度的论文，里面提到SimRank相似度可以看成两条分别从源顶点和目标顶点出发的随机游走，以相同的步数相遇于同一个顶点的概率，因为定义上有相似之处，可以结合PPR来计算顶点u或v到这一系列顶点w的概率。</p>
<p>总结来说，这篇论文的贡献是在BackWard Search的基础上引进随机化来降低时间复杂度并且保证了理论上的近似精度，并且给出了数学证明说明做法的有效性。</p>
<h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/cjNkPH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNkPH.png" alt="cjNkPH.png"></a><br><a href="https://imgtu.com/i/cjNERA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNERA.png" alt="cjNERA.png"></a><br><a href="https://imgtu.com/i/cjNiIe" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNiIe.png" alt="cjNiIe.png"></a><br><a href="https://imgtu.com/i/cjNVxI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNVxI.png" alt="cjNVxI.png"></a><br><a href="https://imgtu.com/i/cjNAGd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNAGd.png" alt="cjNAGd.png"></a><br><a href="https://imgtu.com/i/cjNeMt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNeMt.png" alt="cjNeMt.png"></a><br><a href="https://imgtu.com/i/cjNmsP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNmsP.png" alt="cjNmsP.png"></a><br><a href="https://imgtu.com/i/cjNnqf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNnqf.png" alt="cjNnqf.png"></a><br><a href="https://imgtu.com/i/cjNKZ8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNKZ8.png" alt="cjNKZ8.png"></a><br><a href="https://imgtu.com/i/cjNMdS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNMdS.png" alt="cjNMdS.png"></a><br><a href="https://imgtu.com/i/cjNQIg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNQIg.png" alt="cjNQIg.png"></a><br><a href="https://imgtu.com/i/cjN3Gj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjN3Gj.png" alt="cjN3Gj.png"></a><br><a href="https://imgtu.com/i/cjN1iQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjN1iQ.png" alt="cjN1iQ.png"></a><br><a href="https://imgtu.com/i/cjNGzn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNGzn.png" alt="cjNGzn.png"></a><br><a href="https://imgtu.com/i/cjN8Rs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjN8Rs.png" alt="cjN8Rs.png"></a><br><a href="https://imgtu.com/i/cjNNLV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNNLV.png" alt="cjNNLV.png"></a><br><a href="https://imgtu.com/i/cjNYMq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNYMq.png" alt="cjNYMq.png"></a><br><a href="https://imgtu.com/i/cjNts0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNts0.png" alt="cjNts0.png"></a><br><a href="https://imgtu.com/i/cjNaZT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNaZT.png" alt="cjNaZT.png"></a><br><a href="https://imgtu.com/i/cjNwoF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNwoF.png" alt="cjNwoF.png"></a><br><a href="https://imgtu.com/i/cjNddU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNddU.png" alt="cjNddU.png"></a><br><a href="https://imgtu.com/i/cjNszR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNszR.png" alt="cjNszR.png"></a><br><a href="https://imgtu.com/i/cjNBi4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNBi4.png" alt="cjNBi4.png"></a><br><a href="https://imgtu.com/i/cjNDJJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNDJJ.png" alt="cjNDJJ.png"></a><br><a href="https://imgtu.com/i/cjNrW9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNrW9.png" alt="cjNrW9.png"></a></p>
]]></content>
      <tags>
        <tag>高效计算</tag>
      </tags>
  </entry>
  <entry>
    <title>Simplifying Graph Convolutional Networks[PMLR&#39;19]</title>
    <url>/2020/12/22/SGCN%5BPMLR19%5D/</url>
    <content><![CDATA[<p>PMLR19一篇简化GCN架构的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>图卷积网络中可能引入了一些不必要的复杂性及冗余的计算</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><img src="https://s1.ax1x.com/2020/10/04/0JtcnS.png" alt="0JtcnS.png" border="0">  </p>
<ol>
<li>移除图卷积网络各层之间的非线性关系，合并各层之间的权重矩阵</li>
</ol>
<h4 id="原始图卷积网络"><a href="#原始图卷积网络" class="headerlink" title="原始图卷积网络"></a>原始图卷积网络</h4><p>对于一个输入的图，图卷积网络利用多层网络为每个顶点的特征$x_i$学习一个新的特征表示，随即输入一个线性分类器。对第$k$层网络，输入为$H^{(k-1)}$，输出为$H^{(k)}$，其中$H^{(0)}=X$。一个$K$层的图卷积网络等价于对图中每个顶点的特征向量$x_i$应用一个$K$层感知机，不同之处在于顶点的隐层表示local averaging：</p>
<script type="math/tex; mode=display">
h_i^{(k)}\leftarrow \frac{1}{d_i+1}h_i^{(k-1)}+\sum^n_{j=1}\frac{a_{ij}}{\sqrt{(d_i+1)(d_j+1)}}h_j^{(k-1)}</script><p>矩阵形式：</p>
<script type="math/tex; mode=display">
S=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>其中$A=A+I$，则隐层表示用矩阵的形式表示为：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow SH^{(k-1)}</script><p>Local averaging：this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes  </p>
<p>$\Theta^{(k)}$为第$K$层网络的权重矩阵：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow \text{ReLU}(H^{(k)}\Theta^{(k)})</script><p>$Y\in \mathbb{R}^{n\times C}$，$y_{ic}$表示第$i$个顶点属于类别$C$的概率</p>
<script type="math/tex; mode=display">
Y_{GCN}=\text{softmax}(SH^{(K-1)}\Theta^{(K)})</script><h4 id="简化图卷积网络"><a href="#简化图卷积网络" class="headerlink" title="简化图卷积网络"></a>简化图卷积网络</h4><blockquote>
<p>在传统的多层感知机中，多层网络可以提高模型的表现力，是因为这样引入了特征之间的层级关系，例如第二层网络的特征是以第一层网络为基础构建的。而在图卷积网络中，这还有另外一层含义，在每一层中顶点的隐层表示都是以一跳的邻居进行平均，经过$K$层之后，一个顶点就能获得$K$跳邻居的特征信息。这类似于在卷积网路中网络的深度提升了特征的receptive field。</p>
</blockquote>
<p>保留local averaging，移除了非线性激活函数：</p>
<script type="math/tex; mode=display">
Y=\text{softmax}(S^KX\Theta^{(1)}\dots \Theta^{(K)})</script><p>其中$S^K$可以预先进行计算，大大减少了模型的训练时间</p>
<p>论文中证明了简化后的图卷积网络等价于谱空间的一个低通滤波器，它通过的低频信号对应于图中平滑后的特征</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、Reddit</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR&#39;19]</title>
    <url>/2020/12/22/PPNP%5BICLR19%5D/</url>
    <content><![CDATA[<p>ICLR19将PageRank与GNN结合以解决GCN层数无法加深的一篇论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GCN层数增加后性能反而变差，如何加深GCN的层数。</p>
<p>根据GCN的定义，每一层网络用来捕获一跳邻居的信息，例如一个三层的GCN网络捕获的就是一个顶点三跳邻居以内的信息，而现在如果只能用浅层模型，表示只能捕获有限跳内的邻域信息，而有时候要多几跳才能捕获到有用的信息，例如<a href="#Representation Learning on Graphs with Jumping Knowledge Networks[ICML&#39;18]">JK-Net</a>中的例子。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>这一篇论文的工作其实是接着JK-Net继续往下，在那篇论文中，作者分析了GCN中信息传递这个过程与随机漫步之间的关系，论证了当层数加深之后，GCN会收敛到这个随机漫步的极限分布，而这个极限分布只与图的全局属性有关，没有把随机漫步的起始顶点，或者说是GCN中从邻域中传递和聚合信息的根顶点考虑在内，这么一来，层数加深之后每个顶点聚合出来的样子都差不多，无法区分从而导致性能变差，另一个看待的角度是，因为原始GCN是对所有聚合的信息做平均操作，层数加深之后各个顶点的邻域都变得跟整张图差不多，既然每个顶点的邻域都变得差不多，做的又是平均操作，每个顶点聚合出来的样子就会都差不多。</p>
<p>论文提出的解决办法是引入PageRank的思想，这也是从JK-Net中的结论观察出来的。JK-Net中所说的GCN会收敛到的极限分布的计算方法如下：</p>
<script type="math/tex; mode=display">
\pi_{lim}=\hat{A}\pi_{lim}</script><p>而PageRank的计算方法如下：</p>
<script type="math/tex; mode=display">
\pi_{pr}=A_{rw}\pi_{pr}</script><p>其中$A_{rw}=AD^{-1}$，两个计算方法明显地相似，区别在于，PageRank中邻接矩阵$A$没有考虑根顶点自身，而极限分布的计算里$\hat{A}$是引入了自环的。而Personalized PageRank通过引入自环而考虑了根顶点自身，论文的想法就是将随机漫步的极限分布用Personalized PageRank来代替，它的计算方法为：</p>
<script type="math/tex; mode=display">
\pi_{ppr}(i_x)=(1-\alpha)\hat{A}\pi_{ppr}(i_x)+\alpha i_x \\
\rightarrow \pi_{ppr}(i_x)=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}i_x</script><p>其中$i_x$是一个one_hot指示向量，用来从根顶点重新启动。</p>
<blockquote>
<p>Personalized PageRank算法的目标是要计算所有节点相对于用户u的相关度。从用户u对应的节点开始游走，每到一个节点都以α的概率停止游走并从u重新开始，或者以1-α的概率继续游走，从当前节点指向的节点中按照均匀分布随机选择一个节点往下游走。这样经过很多轮游走之后，每个顶点被访问到的概率也会收敛趋于稳定，这个时候我们就可以用概率来进行排名了。</p>
</blockquote>
<p>相较于原始的GCN模型，现在根顶点$x$对顶点$y$的影响程度$I(x,y)$，变得与$\pi_{ppr}(i_x)$中的第$y$个元素相关，这个影响程度对于每个根顶点都有不同的取值：</p>
<script type="math/tex; mode=display">
\require{cancel}
I(x,y)\propto \prod_{ppr}^{(yx)},\prod_{ppr}^{(yx)}=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}\cancel{I_{n}}</script><h4 id="PPNP"><a href="#PPNP" class="headerlink" title="PPNP"></a>PPNP</h4><p>经过上面的铺垫与介绍，论文提出的模型PPNP可以表示为：</p>
<script type="math/tex; mode=display">
Z_{PPNP}=\text{softmax}\Big(\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}H\Big),H_{i,:}=f_{\theta}(X_i,:)</script><p>其中$X$为特征矩阵，$f_{\theta}$是一个参数为$\theta$的神经网络，用来产生预测类别$H\in \mathbb{R}^{n\times c}$。</p>
<div align="center">
<a href="https://imgchr.com/i/ravXN9" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/20/ravXN9.png" alt="ravXN9.png" border="0" width="90%"></a>
</div>

<p>由公式和图中都可以看到，PPNP其实是由两部分组成，左边的神经网络与右边的信息传递网络，神经网络部分就类似于在<a href="#Semi-Supervised Classification with Graph Convolutional Network [ICLR&#39;17]">GCN</a>中介绍的，输入顶点特征与图的结构信息（邻接矩阵），输出顶点新的特征表示。信息传递网络部分，在PPNP中通过它来得到预测标签，而原始GCN的做法是$Z_{GCN}=\text{softmax}(\hat{A}HW)$，其中$W$是每层网络的参数。</p>
<h4 id="APPNP"><a href="#APPNP" class="headerlink" title="APPNP"></a>APPNP</h4><p>从前面的构造方式可以看到，矩阵$\prod_{ppr}$将会有$\mathbb{R}^{n\times n}$大小，会带来时间和空间上的复杂度。因此论文提出了一种近似的计算方法APPNP，计算方式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Z^{(0)}&=H=f_{\theta}(X) \\
Z^{(k+1)}&=(1-\alpha)\hat{A}Z^{(k)}+\alpha H \\
Z^{(K)}&=\text{softmax}\Big((1-\alpha)\hat{A}Z^{(K-1)}+\alpha H\Big)
\end{aligned}</script><p>其中$K$为信息传递的跳数或者说是随机漫步的步数，$k\in[0,K-2]$，这样一来就不用构造一个$\mathbb{R}^{n\times n}$的矩阵了。（不知道为什么…）  </p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Citeseer、Cora-ML、Pubmed、MS Academic  </p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Link Prediction Based on Graph Neural Networks[NIPS&#39;18]</title>
    <url>/2021/01/26/SEAL%5BNIPS18%5D/</url>
    <content><![CDATA[<p>NIPS18一篇使用图神经网络来做图的边预测任务的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何能自动而非人工定义的方式来学习图中的结构信息，从而进行边预测。</p>
<p>边预测任务就是预测图中的两个顶点是否有可能有边相连。一种常用的方法为启发式方法(heuristic)，它根据定义的顶点相似度来判断这条边存在的概率有多大。几种定义相似度的方法可以根据需要使用的邻居顶点的跳数来分类，例如common neighbors与preferential attachment是一阶的，因为它们只需要一跳邻居的信息，而Adamic-Adar和resource allocation为二阶，Katz、rooted PageRank与SimRank是更高阶的相似度。</p>
<p>这种启发式方法的缺点在于，边存在的概率很大程度依赖于定义的顶点相似度。例如选取common neighbors这个相似度，在社交网络可能是成立的，因为如果两个人有很多共同的朋友，他们两个确实更有可能认识，但是在蛋白质交互网络截然相反，有越多相同邻居顶点的蛋白质反而越不可能建立联系。所以，与其预先定义一种相似度，不如根据网络的特点自动的学习出来。</p>
<p>另一个挑战是，高阶的相似度相较于低阶相似度往往能带来更好的表现，但是随着阶数越高，每个顶点所形成的子图会越来越逼近完整的图，这样会带来过高的时间复杂度与空间复杂度。本文的另一个贡献就在于，定义了一种逼近的方式，不需要$h$阶的子图也能近似的获取$h$阶子图中包含的信息，之间的误差有理论上限。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>同ICLR20的论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样（毕竟是同一个作者），论文对子图的定义方式为，给定一对顶点$(x,y)$，它的子图为顶点$x$与$y$不高于$h$阶的邻域的一个并集，数学描述如下，也就是与顶点$x$或$y$的距离小于等于$h$所构成的点的集合：</p>
<blockquote>
<p>给定一个图$G=(V,E)$，以及图上两个顶点$x、y$，它的$h$阶围绕子图(enclosing subgraph)$G^h_{x,y}$为图$G$的一个子图，满足$\{i|d(i,x)\le h\ or\ d(i,y)\le h\}$.</p>
</blockquote>
<p>接下来是定义一个$\gamma$-decaying heuristic函数，它用来逼近$h$阶子图的信息而不需要实际计算$h$阶子图：</p>
<script type="math/tex; mode=display">
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>其中$\gamma$是一个位于$(0,1)$的衰减因子，$\eta$是一个正的常数或一个上界为常数的函数。因为这里的求和从1到$\infin$，接下来的定理说明可以用有限项去逼近$H(x,y)$，误差随着$h$的增加而指数下降：</p>
<blockquote>
<p>定理一：</p>
<p>如果函数$f(x,y,l)$满足：</p>
<ol>
<li>$f(x,y,l)\le \lambda^l$，其中$\lambda &lt;\frac{1}{\gamma}$</li>
<li>对于$l=1,2,\dots,g(h)$，$f(x,y,l)$能够从$h$阶子图$G^h_{x,y}$中计算得到，其中$g(h)=ah+b$，$a,b\in \N,\ a&gt;0$</li>
</ol>
</blockquote>
<p>证明的方法很容易理解：</p>
<blockquote>
<p>逼近项为：</p>
<script type="math/tex; mode=display">
\tilde{H}(x,y)=\eta\sum_{l=1}^{g(h)}\gamma^lf(x,y,l)</script><p>计算差值可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
|H(x,y)-\tilde{H}(x,y)|&=\eta\sum_{l=g(h)+1}^{\infin}\gamma^lf(x,y,l)\\
&\le \eta\sum_{l=ah+b+1}^{\infin}\gamma^l\lambda^l\\
&=\eta\frac{(\gamma \lambda)^{ah+b+1}}{1-\gamma \lambda}
\end{aligned}</script></blockquote>
<p>第一个不等式是根据定理一的第一个条件，最后一个等号是根据等比数列的求和公式，当项数$n\rightarrow \infin$且$q\in(0,1)$时，结果为$\frac{a_1}{1-q}$。</p>
<p>到这里可能还是不知道这个$H(x,y)$和图中$h$阶的信息有什么关系，下面就通过Katz、rooted PageRank和SimRank三个高阶相似度来具体说明怎么使用：</p>
<p>在说明之前，先介绍一个引理，接下来会用到，证明起来也很直观：</p>
<blockquote>
<p>顶点$x$与$y$之间任意一条长度$l$满足$l\le2h+1$的路径都被包含在子图$G^h_{x,y}$中</p>
</blockquote>
<p>证明：</p>
<blockquote>
<p>即证明给定一条长度为$l$的路径$w=<x,v_1,\dots,v_{l-1},y>$中的每一个顶点都在子图中。取其中任意一个顶点$v_i$，满足$d(v_i,x)\ge h$且$d(v_i,y)\ge h$，根据子图$G^h_{x,y}$的定义它不在其中。那么有：</x,v_1,\dots,v_{l-1},y></p>
<script type="math/tex; mode=display">
2h+1\ge l=|<x,v_1,\dots,v_i>|+|<v_i,\dots,v_{l-1},y>|\ge d(v_i,x)+d(v_i,y)=2h+2</script><p>矛盾，不等号是因为$d(x,y)$就是表示两个顶点之间的最短路径，所以有$d(v_i,x)&lt;h$或$d(v_i,y)&lt;h$，则顶点$v_i$在子图$G^h_{x,y}$中。</p>
</blockquote>
<h4 id="Katz-index"><a href="#Katz-index" class="headerlink" title="Katz index"></a>Katz index</h4><p>给定一对顶点$(x,y)$，Katz index定义为：</p>
<script type="math/tex; mode=display">
\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}</script><p>其中$\text{walk}^{<l>}(x,y)$是这两个顶点之间长度为$l$的路径构成的集合，$A^l$是邻接矩阵的$l$次幂。从表达式可以看到，长度越长的路径在计算时会被$\beta^l$衰减的越多$(0&lt;\beta&lt;1)$，短路径有更大的权重。</l></p>
<p>对比两式可以发现：</p>
<script type="math/tex; mode=display">
\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}\\
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>Katz index是论文中定义的$\gamma$-decaying heuristic函数的一种特殊形式，取$\eta=1,\gamma=\beta$，$f(x,y,l)=|\text{walks}^{<l>}(x,y)|=[A^l]_{x,y}$。根据引理，只要取长度小于2h+1的路径，其中的顶点就会全部被子图给包含，这也就满足了定理一的第2个“可计算”条件。对于第一个条件，可以通过数学归纳法说明Katz index的表达式同样满足：</l></p>
<blockquote>
<p>给定任意的顶点$i、j$，$[A^l]_{i,j}$的上限为$d^l$，其中$d$是网络中的最大顶点度</p>
</blockquote>
<p>数学归纳法证明：</p>
<blockquote>
<p>当$l=1$时，$A_{i,j}$退化成了顶点的度，那显然有$A_{i,j}\le d$成立。假设$k=l$时也成立$[A^l]_{i,j}\le d^l$，当$k=l+1$时：</p>
<script type="math/tex; mode=display">
[A^{l+1}]_{i,j}=\sum_{k=1}^{|V|}[A^l]_{i,k}A_{k,j}\le d^l\sum_{k=1}^{|V|}A_{k,j}\le d^ld=d^{l+1}</script></blockquote>
<p>第一个等式就是矩阵乘法的定义，因为$[A^{l+1}]$的含义就是$l+1$个邻接矩阵$A$相乘。因此，对比定理一的第一个条件，我们只要取$\lambda=d$，$d$满足$d&lt;\frac{1}{\beta}$就能够成立，这样一来两个条件都被满足了，这说明Katz index能够很好地从$h$阶子图中近似。</p>
<h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><p>rooted PageRank来源于这篇论文<a href="https://dl.acm.org/doi/10.1145/511446.511513" target="_blank" rel="noopener">Topic-sensitive PageRank</a>，它通过迭代计算PageRank向量$\pi_x$来得到某一点相对于其它顶点的相似度。具体来说，它计算一个从顶点$x$开始的随机漫步的平稳分布，这个随机漫步以概率$\alpha$移动到任一邻居上或以概率$1-\alpha$回到顶点$x$。这个平稳分布满足：</p>
<script type="math/tex; mode=display">
\pi_x=\alpha P\pi_x+(1-\alpha)e_x</script><p>其中$[\pi_x]_i$表示在这个平稳分布下漫步到顶点$i$的概率，$P$为转移矩阵，其中$P_{i,j}=\frac{1}{|\Gamma(v_j)|}$，这里的$\Gamma(v_j)$表示顶点$v_j$的一跳邻居构成的集合。如果一个顶点与五个顶点相连，那它转移到其中任意一个顶点的概率就是$\frac{1}{5}$。</p>
<p>rooted PageRank应用于边预测任务时，用来得到一对顶点$(x,y)$的分数，以$[\pi_x]_y$或$[\pi_x]_y+[\pi_y]_x$（对称）表示，分数越高越有可能有边相连。</p>
<p>接下来就要说明rooted PageRank如何能够同样以论文中提出的$\gamma$-decaying heuristic函数进行表示。根据<a href="http://infolab.stanford.edu/~glenj/spws.pdf" target="_blank" rel="noopener">inverse P-distance理论</a>，$[\pi_x]_y$能够等价地改写为：</p>
<script type="math/tex; mode=display">
[\pi_x]_y=(1-\alpha)\sum_{w:x\leadsto y}P[w]\alpha^{len(w)}</script><p> 这里的求和范围$w:x\leadsto y$表示所有从$x$开始结束于$y$的路径，$P[w]$定义为$\prod_{i=0}^{k-1}\frac{1}{|\Gamma(v_i)|}$，$k$是路径长度，$v_i$是路径中的顶点，通过这条路径来从$x$到$y$的概率就是漫步到路径中每一个顶点的概率的连乘。</p>
<p>接下来就是证明这个形式满足定理一的两个条件：</p>
<blockquote>
<p>首先进一步改写：</p>
<script type="math/tex; mode=display">
[\pi_x]_y=(1-\alpha)\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]\alpha^l\\
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>对比：取$\gamma=\alpha,\eta=(1-\alpha),f(x,y,l)=\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]$。因为这时候$f(x,y,l)$表示一个随机漫步恰好以$l$步从顶点$x$漫步到$y$的概率，有$\sum_{z\in V}f(x,z,l)=1$，则$f(x,y,l)\le1&lt;\frac{1}{\alpha}$，这样就满足了定理一，而根据引理，只要取长度小于等于2h+1的路径，路径中的点就会被全部包含在子图中，也就满足了第二个”可计算“条件。</p>
</blockquote>
<h4 id="SimRank"><a href="#SimRank" class="headerlink" title="SimRank"></a>SimRank</h4><p>SimRank的核心思想是，如果两个顶点的邻域相似，那它们也相似：</p>
<script type="math/tex; mode=display">
s(x,y)=\gamma \frac{\sum_{a\in\Gamma(x)}\sum_{b\in \Gamma(y)}s(a,b)}{|\Gamma(x)|·|\Gamma(y)|}</script><p>它有一个<a href="https://dl.acm.org/doi/10.1145/775047.775126" target="_blank" rel="noopener">等价定义形式</a>：</p>
<script type="math/tex; mode=display">
s(x,y)=\sum_{w:(x,y)\multimap (z,z)}P[w]\gamma^{len(w)}</script><p>其中$w:(x,y)\multimap (z,z)$表示从顶点$x$开始的随机漫步与从顶点$y$开始的随机漫步第一次相遇于顶点$z$。证明与rooted PageRank基本一致，可以见原论文。</p>
<p>总结来说，$\gamma$-decaying heuristic函数的思想是，对于远离目标顶点的结构信息通过指数衰减的方式给一个更小的权重，因为它们带来的信息十分有限。</p>
<h4 id="SEAL框架"><a href="#SEAL框架" class="headerlink" title="SEAL框架"></a>SEAL框架</h4><div align="center">
<a href="https://imgchr.com/i/ypCC6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/ypCC6A.png" alt="ypCC6A.png" border="0" width="80%"></a>
</div>

<p>这一节就是根据上面的理论分析建立一个用于边预测任务的框架。一个图神经网络的典型输入形式是$(A,X)$，在本论文中，$A$自然地被定义为子图$G^h_{x,y}$的邻接矩阵，子图的获取即来自正样本（已知边）也来自负样本（未知边）。接下来的部分就是介绍论文怎么定义顶点的特征矩阵$X$，它包含三个部分：structural node labels、node embeddings和node attributes。</p>
<h5 id="Node-labeling"><a href="#Node-labeling" class="headerlink" title="Node labeling"></a>Node labeling</h5><p>跟作者的另一篇论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样，通过给顶点打标签的方式来区别顶点在子图中的不同角色，这么做的意义在另一篇博客说过了这里就不写了，具体打标签的方式为：</p>
<ul>
<li>起始顶点$x$与目标顶点$y$的标签都为”1“</li>
<li>如果两个顶点$i、j$距离起始顶点与目标顶点的距离都相同，那么它们的标签一样</li>
<li>$(d(i,x),d(i,y))=(a,b)\rightarrow label:a+b$</li>
</ul>
<p>将顶点的标签进行one-hot编码后作为结构特征。</p>
<h5 id="Node-embeddings-Node-attributes"><a href="#Node-embeddings-Node-attributes" class="headerlink" title="Node embeddings + Node attributes"></a>Node embeddings + Node attributes</h5><p>Node attributes一般数据集直接给定，而Node embeddings是通过一个GNN得到，具体做法是：给定正样本$E_p\in E$，负样本$E_n$，$E_p\and E_n=\empty$，在这么一个图$G’=(V,E\and E_n)$上生成embeddings，防止过拟合。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>USAir、NS、PB、Yeast、C.ele、Power、Router、E.coli</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>RUC与AOC</title>
    <url>/2019/08/07/ROC%E4%B8%8EAUC/</url>
    <content><![CDATA[<p>用本文记录自己对ROC曲线与AUC值的学习和理解<br><a id="more"></a></p>
<h3 id="ROC曲线定义"><a href="#ROC曲线定义" class="headerlink" title="ROC曲线定义"></a>ROC曲线定义</h3><p>ROC曲线与AUC值常被用来评价一个二分类器的好坏，以下内容出自项亮《推荐系统实践》的“分类问题”一节：</p>
<blockquote>
<p>对于二类分类问题常用的评价指标是精确率(precision)和召回率(recall)，通常以关注的类为正类，其他类为负类，分类器可将实例分成正类(positive)和负类(negative)，预测时会出现4种情况：  </p>
<ul>
<li>TP (True Positice)——将正类预测为正类数</li>
<li>FN (False Negative)——将正类预测为负类数</li>
<li>FP (False Positive)——将负类预测为正类数</li>
<li>TN (True Negative)——将负类预测为负类数  </li>
</ul>
<p>精确率定义为</p>
<script type="math/tex; mode=display">P =\frac{TP}{TP + FP}</script><p>召回率定义为</p>
<script type="math/tex; mode=display">R =\frac{TP}{TP + FN}</script></blockquote>
<p>而ROC曲线与上面的定义有关，FPR(False Positive Rate)为x轴，它以TPR(True Positive Rate)为y轴，它们的定义分别为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
TPR & = \frac{TP}{TP+FN} \\
FPR & =1-\frac{TN}{TN+FP} \\
& = \frac{FP}{TN+FP}
\end{aligned}</script><p>可以看到，TPR与召回率的定义是一样的，直观理解是：</p>
<ul>
<li>横坐标：伪正类率(False positive rate， FPR)，<strong>预测为正但实际为负</strong>的样本占所有<strong>负例样本</strong>的比例；</li>
<li>纵坐标：真正类率(True positive rate， TPR)，<strong>预测为正且实际为正</strong>的样本占所有<strong>正例样本</strong>的比例。</li>
</ul>
<p>其实ROC的曲线的横坐标和纵坐标是没有相关性的，不能把ROC曲线当成函数曲线来分析，应该把它看成无数个点，每个点都代表一个分类器，其横纵坐标代表了这个分类器的性能。为了更好的理解ROC曲线，引入ROC空间的概念：<br><img src="https://s2.ax1x.com/2019/08/07/eIKD5d.png" alt="ROC空间"></p>
<p>A,B,C,C’为四个分类器，指标如下：<br><img src="https://s2.ax1x.com/2019/08/07/eI82Mn.png" alt="分类器"></p>
<p>其中C’的性能最好，B的准确率为0.5，几乎是随即分类，图中左上角的点为完美分类，它代表所有的分类完全正确，分类为1的点完全正确，分类为0的点没有错误。<br>在一个二分类模型中，分类器给出每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0)，阈值最小时，对应坐标点(1,1)</p>
<p>典型的ROC曲线：<br><img src="https://s2.ax1x.com/2019/08/07/eI8jZ6.png" alt="eI8jZ6.png"></p>
<p>理想情况下，TPR应该接近1，FPR应该接近0。ROC曲线上的每一个点对应于一个阈值，对于一个分类器，每个阈值下会有一个TPR和FPR。比如阈值最大时，TP=FP=0，对应于原点；阈值最小时，TN=FN=1，对应于右上角的点(1,1)。</p>
<h3 id="绘制ROC曲线"><a href="#绘制ROC曲线" class="headerlink" title="绘制ROC曲线"></a>绘制ROC曲线</h3><p>对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组FPR和TPR结果，而要得到一个曲线，我们实际上需要一系列FPR和TPR的值，如何得到呢？办法就是从阈值下手。分类器的一个重要功能是“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本）。<br>假如我们已经得到了所有样本的概率输出（属于正样本的概率），现在的问题是如何改变阈值从而得到不同的PPR和TPR结果。根据每个测试样本属于正样本的概率值从大到小排序，下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。</p>
<p><img src="https://s2.ax1x.com/2019/08/07/eII1PK.png" alt="eII1PK.png"></p>
<p>接下来从高到低，依次将“Score”值作为阈值，当测试样本属于正样本的概率大于或等于这个阈值时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的阈值，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图：</p>
<p><img src="https://s2.ax1x.com/2019/08/07/eIIdat.png" alt="eIIdat.png"></p>
<p>当我们将阈值设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当阈值取值越多，ROC曲线越平滑。<br>其实并不一定要得到每个测试样本是正样本的概率值，只要得到这个分类器对该测试样本的“评分值”即可（评分值并不一定在(0,1)区间）。评分越高，表示分类器越肯定地认为这个测试样本是正样本，而且同时使用各个评分值作为阈值。</p>
<h3 id="AUC值定义"><a href="#AUC值定义" class="headerlink" title="AUC值定义"></a>AUC值定义</h3><p>AUC (Area Under Curve) 被定义为ROC曲线下的面积，这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。<br>将AUC值看成一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。<br>从AUC判断分类器（预测模型）优劣的标准：  </p>
<ul>
<li>AUC = 1，完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li>
<li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li>
<li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li>
<li>AUC &lt; 0.5，比随机猜测还差；对预测结果取反之后就优于随机猜测。</li>
</ul>
<p>三种AUC值示例：<br><img src="https://s2.ax1x.com/2019/08/07/eIILZR.png" alt="eIILZR.png"></p>
<h3 id="为什么用ROC曲线"><a href="#为什么用ROC曲线" class="headerlink" title="为什么用ROC曲线"></a>为什么用ROC曲线</h3><p>既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比：</p>
<p><img src="https://s2.ax1x.com/2019/08/07/eIokdI.png" alt="eIokdI.png"></p>
<p>在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">维基百科</a></li>
<li><a href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/" target="_blank" rel="noopener">孔明的博客</a></li>
<li><a href="https://blog.csdn.net/abcjennifer/article/details/7359370" target="_blank" rel="noopener">ROC曲线-阈值评价标准</a></li>
<li><a href="https://www.jianshu.com/p/c61ae11cc5f6" target="_blank" rel="noopener">简书</a></li>
<li><a href="http://www.cnblogs.com/dlml/p/4403482.html" target="_blank" rel="noopener">博客园</a></li>
<li><a href="https://www.zhihu.com/question/30643044" target="_blank" rel="noopener">知乎问答</a></li>
</ul>
]]></content>
      <tags>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title>Session-Based Recommendation with Graph Neural Networks[AAAI&#39;19]</title>
    <url>/2020/12/22/SRGCN%5BAAAI19%5D/</url>
    <content><![CDATA[<p>AAAI19一篇将gated GNN应用于序列推荐任务的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\rightarrow b \rightarrow  c$，只考虑$a\rightarrow b$或者$b\rightarrow c$</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BF3uuT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF3uuT.png" alt="BF3uuT.png" border="0"></a></p>
<h4 id="Session-Graph-Modeling"><a href="#Session-Graph-Modeling" class="headerlink" title="Session Graph Modeling"></a>Session Graph Modeling</h4><p>将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵：</p>
<div align="center">
<a href="https://imgchr.com/i/BF17nO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF17nO.png" alt="BF17nO.png" border="0" width="80%"></a>
</div>


<p>上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\in \mathbb{R}^{n\times 2n}$，其中的一行$A_{s,i:}\in \mathbb{R}^{1\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$：</p>
<div align="center">
<a href="https://imgchr.com/i/BFGCkQ" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFGCkQ.png" alt="BFGCkQ.png" border="0" width="80%"></a>
</div>


<h4 id="Node-Representation-Learning"><a href="#Node-Representation-Learning" class="headerlink" title="Node Representation Learning"></a>Node Representation Learning</h4><p>论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。</p>
<h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><p>一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate：</p>
<div align="center">
<a href="https://imgchr.com/i/BFaaAf" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFaaAf.png" alt="BFaaAf.png" border="0" width="60%"></a>
</div>


<p>直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\in \mathbb{R}^{n\times d}$，上一时刻隐层表示$H_{t-1}\in \mathbb{R}^{n\times h}$，重置门与更新门的输出由下式计算得到：</p>
<script type="math/tex; mode=display">
R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\
Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)</script><p>式中的$W$与$b$分别为权重与偏置参数。</p>
<h5 id="Reset-Gate"><a href="#Reset-Gate" class="headerlink" title="Reset Gate"></a>Reset Gate</h5><p>传统RNN网络的隐式状态更新公式为：</p>
<script type="math/tex; mode=display">
H_t=\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)</script><p>如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\tilde{H_t}$称为候选状态：</p>
<script type="math/tex; mode=display">
\tilde{H_t}=\tanh(X_tW_{xh}+(R_t\odot{H_{t-1}})W_{hh}+b_h)</script><h5 id="Update-Gate"><a href="#Update-Gate" class="headerlink" title="Update Gate"></a>Update Gate</h5><p>更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\tilde{H_t}决定$：</p>
<script type="math/tex; mode=display">
H_t=Z_t\odot H_{t-1}+(1-Z_t)\odot \tilde{H_t}</script><p>介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习：</p>
<p><a href="https://imgchr.com/i/BkiNUU" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BkiNUU.png" alt="BkiNUU.png" border="0"></a></p>
<p>最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。</p>
<h4 id="Session-Representation-Generation"><a href="#Session-Representation-Generation" class="headerlink" title="Session Representation Generation"></a>Session Representation Generation</h4><p>现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\text{s}=[v_{s,1},v_{s,2},\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\in \mathbb{R}^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。</p>
<p>局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣：</p>
<script type="math/tex; mode=display">
s_l=v_n</script><p>全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\text{soft-attention}$机制来计算得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_g&=\sum_{i=1}^{n}\alpha_iv_i\\
\alpha_i&=q^T\sigma(W_1v_n+W_2v_i+c)
\end{aligned}</script><p>最后使用一个$\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量：</p>
<script type="math/tex; mode=display">
s_h=W_3[s_l;s_g]</script><h4 id="Making-Recommendation"><a href="#Making-Recommendation" class="headerlink" title="Making Recommendation"></a>Making Recommendation</h4><p>对于一个待推荐物品$v_i\in V$，计算它在序列$s$中作为下一个被点击物品的概率：</p>
<script type="math/tex; mode=display">
\hat{y_i}=\text{softmax}(s_h^Tv_i)</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Yoochoose、Diginetica</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>SPAGAN Shortest Path Graph Attention Network</title>
    <url>/2021/05/18/SPAGAN%5BIJCAI&#39;19%5D/</url>
    <content><![CDATA[<p>IJCAI19一篇关注最短路径对中心顶点的attention聚合的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GAT中只是中心顶点与邻域顶点的顶点间attention聚合，本文关注的是路径对中心顶点的attention聚合。一条路径往往会包含很多个顶点，怎么能够对中心顶点做到“多对一”的聚合呢？在于一条路径$p^c_{ij}$的表示$\phi(p^c_{ij})$是通过路径中所有顶点的特征取平均得到，这样一来就变成“一对一”的聚合，和GAT中的顶点间attention聚合相同。</p>
<div align="center">
<a href="https://imgtu.com/i/2Az8VP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2Az8VP.png" alt="2Az8VP.png" border="0" width="60%"></a>
</div>

<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Shortest-Path-Generation"><a href="#Shortest-Path-Generation" class="headerlink" title="Shortest Path Generation"></a>Shortest Path Generation</h4><p>计算最短路径使用Dijkstra，而顶点间边的权重由它们之间的attention系数决定：</p>
<script type="math/tex; mode=display">
W_{ij}=\frac{1}{K}\sum_{k=1}^K\alpha_{ij}^{(k)}</script><h4 id="Path-Sampling"><a href="#Path-Sampling" class="headerlink" title="Path Sampling"></a>Path Sampling</h4><p>这一阶段的核心思想是，对于有着相同长度的几条最短路径，代价最小的与中心顶点的相关性更高，代价指的就是路径上边的权重的求和。记$p_{ij}^c$为顶点i到j长度为c的一条最短路径，$P^c$表示所有$p_{ij}^c$形成的集合，取样：</p>
<script type="math/tex; mode=display">
N_i^c=top_k(P^c),k=degree_i*r</script><h4 id="Hierarchical-Path-Aggregation"><a href="#Hierarchical-Path-Aggregation" class="headerlink" title="Hierarchical Path Aggregation"></a>Hierarchical Path Aggregation</h4><p>这一阶段的层次路径聚合分为两层，第一层聚合相同长度的路径，第二层聚合不同长度的路径。</p>
<p>加权系数$\alpha_{ij}^{(k)}$为中心顶点i的特征$h_i’$与路径的表示$\phi(p^c_{ij})$的attention系数：</p>
<script type="math/tex; mode=display">
l_i^c=\sum_{k=1}^K\{\sum_{p^c_{ij}\in N_i^c}\alpha_{ij}^{(k)}\phi(p^c_{ij}) \}</script><p>在第二层，进一步聚合不同长度的路径，第一层已经得到以顶点i为中心长度为c的所有路径形成的一个特征表示$l_i^c$：</p>
<script type="math/tex; mode=display">
h_i=\sigma\{\sum_{c=2}^C\beta_cl_i^c\}</script><p>这样一来就通过路径得到了中心顶点i的新的特征表示$h_i$。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>C++常用操作</title>
    <url>/2021/01/17/c++%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>记录备查</p>
<a id="more"></a>
<h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><ul>
<li>从字符串中删除子串</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">removeSubstrs</span><span class="params">(<span class="built_in">string</span>&amp; s, <span class="keyword">const</span> <span class="built_in">string</span>&amp; e)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = e.length();</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">string</span>::size_type i = s.find(e); i != <span class="built_in">string</span>::npos; i = s.find(e))</span><br><span class="line">            s.erase(i, n);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用STL库实现小顶堆</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">priority_queue &lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt; &gt; pq;</span><br></pre></td></tr></table></figure>
<ul>
<li>自定义排序</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> x, y, len;</span><br><span class="line">    Edge(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> len) : x(x), y(y), len(len)&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;Edge&gt; edges;</span><br><span class="line">sort(edges.begin(), edges.end(), [](Edge a, Edge b) -&gt; <span class="keyword">int</span> &#123;<span class="keyword">return</span> a.len &lt; b.len;&#125;);</span><br></pre></td></tr></table></figure>
<ul>
<li>初始化二维vector</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> row = <span class="number">3</span>, col = <span class="number">3</span>;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; matrix(row, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(col, <span class="number">0</span>));</span><br></pre></td></tr></table></figure>
<ul>
<li>位运算</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//求二进制中1的个数</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(n != <span class="number">0</span>)&#123;</span><br><span class="line">  n &amp;= n<span class="number">-1</span>;</span><br><span class="line">  count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//除2</span></span><br><span class="line">x &gt;&gt;= <span class="number">1</span>;</span><br><span class="line"><span class="comment">//乘2</span></span><br><span class="line">x &lt;&lt;= <span class="number">1</span>;</span><br><span class="line"><span class="comment">//大写转小写</span></span><br><span class="line">ch |= <span class="string">' '</span>;</span><br><span class="line"><span class="comment">//小写转大写</span></span><br><span class="line">ch &amp;= <span class="string">'_'</span>;</span><br><span class="line"><span class="comment">//第i位是否为1</span></span><br><span class="line">(val &gt;&gt; i) &amp; <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>分割字符串</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//完全分割</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">split</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; record, <span class="keyword">const</span> <span class="keyword">char</span> delim = <span class="string">'.'</span>)</span> </span>&#123;</span><br><span class="line">        record.clear();</span><br><span class="line">        <span class="function"><span class="built_in">istringstream</span> <span class="title">is</span><span class="params">(s)</span></span>;</span><br><span class="line">        <span class="built_in">string</span> temp;</span><br><span class="line">        <span class="keyword">while</span> (getline(is, temp, delim)) &#123;</span><br><span class="line">            record.push_back(move(temp));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//只分割一次</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">split</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; record, <span class="keyword">const</span> <span class="keyword">char</span> delim = <span class="string">'.'</span>)</span> </span>&#123;</span><br><span class="line">  			<span class="comment">//s="www.bithub00.com"➡"bithub00.com"、"com"</span></span><br><span class="line">        record.clear();</span><br><span class="line">        <span class="keyword">int</span> index = s.find(<span class="string">'.'</span>);</span><br><span class="line">  			<span class="keyword">while</span>(index &gt; <span class="number">0</span>)&#123;</span><br><span class="line">  					<span class="built_in">string</span> t = s.substr(index+<span class="number">1</span>);</span><br><span class="line">          	record.push_back(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>从句子中读取单词</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> A = <span class="string">"Data Mining"</span>;</span><br><span class="line"><span class="function"><span class="built_in">istringstream</span> <span class="title">in</span><span class="params">(A)</span></span>;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; v;</span><br><span class="line"><span class="built_in">string</span> t;</span><br><span class="line"><span class="keyword">while</span>(in &gt;&gt; t)&#123;</span><br><span class="line">	v.push_back(t);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>求文件行数</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">(<span class="keyword">char</span> *File)</span> </span>&#123;</span><br><span class="line">	<span class="function">ifstream <span class="title">myfile</span><span class="params">(File)</span></span>;</span><br><span class="line">	<span class="built_in">string</span> line;</span><br><span class="line">	<span class="keyword">int</span> lineNum = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span> (getline(myfile, line))</span><br><span class="line">		lineNum++;</span><br><span class="line">	myfile.clear();</span><br><span class="line">	myfile.seekg(<span class="number">0</span>, ios::beg); <span class="comment">//回到文件第一行</span></span><br><span class="line">	<span class="keyword">return</span> lineNum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>循环查找子串出现的所有位置</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> pos = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">string</span> str;</span><br><span class="line"><span class="built_in">string</span> substr;</span><br><span class="line"><span class="keyword">while</span>(pos != <span class="built_in">string</span>::npos) &#123;</span><br><span class="line">  pos = str.find(substr, pos);</span><br><span class="line">  pos++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>二分区间查找</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> left = <span class="number">0</span>, right = arr.size()<span class="number">-1</span>;</span><br><span class="line"><span class="keyword">while</span>(left &lt;= right) &#123;</span><br><span class="line">  <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">  <span class="keyword">if</span>(arr[mid] &gt; value) right = mid<span class="number">-1</span>;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span>(arr[mid] &lt; value) left = mid+<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    right = mid;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//此时arr[right]就是arr中小于等于value的最大数</span></span><br><span class="line"><span class="keyword">return</span> arr[right];</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Youtube爬虫</title>
    <url>/2019/07/25/Youtube%E7%88%AC%E8%99%AB/</url>
    <content><![CDATA[<h3 id="爬取相关频道Related-Channels"><a href="#爬取相关频道Related-Channels" class="headerlink" title="爬取相关频道Related Channels"></a>爬取相关频道Related Channels</h3><a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># scrapy crawl related_channel_spider 命令行启动爬虫</span></span><br><span class="line"></span><br><span class="line">youtube_url = <span class="string">'https://www.youtube.com'</span></span><br><span class="line">kol = collections.OrderedDict()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RelatedChannelsSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'related_channel_spider'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(spider, reason)</span>:</span></span><br><span class="line">        <span class="comment"># 爬虫结束后将爬取结果写入json文件</span></span><br><span class="line">        file_kol = open(<span class="string">'RelatedChannels_new.json'</span>, <span class="string">'a'</span>)</span><br><span class="line">        json.dump(kol, file_kol, sort_keys=<span class="keyword">True</span>, indent=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 爬虫数据来源:Youtube 频道链接</span></span><br><span class="line">        file_name = <span class="string">'kol_utm_campaign_ad.xlsx'</span></span><br><span class="line">        Channels = pd.read_excel(file_name, sheet_name=<span class="string">'ad_channel_new'</span>, header=<span class="number">0</span>, </span><br><span class="line">        usecols=[<span class="string">'Channel'</span>])</span><br><span class="line">        Titles = pd.read_excel(file_name, sheet_name=<span class="string">'ad_channel_new'</span>, header=<span class="number">0</span>, </span><br><span class="line">        usecols=[<span class="string">'KolName'</span>])</span><br><span class="line">        length = len(Channels)</span><br><span class="line">        start_urls = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, length):</span><br><span class="line">            url = &#123;&#125;</span><br><span class="line">            url[<span class="string">'url'</span>] = Channels[i:i + <span class="number">1</span>].values.item()</span><br><span class="line">            url[<span class="string">'title'</span>] = Titles[i:i + <span class="number">1</span>].values.item()</span><br><span class="line">            start_urls.append(url)</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> start_urls:</span><br><span class="line">            request = scrapy.Request(url[<span class="string">'url'</span>], callback=self.parse)</span><br><span class="line">            request.meta[<span class="string">'title'</span>] = url[<span class="string">'title'</span>]</span><br><span class="line">            <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        related = collections.OrderedDict()</span><br><span class="line">        meta = response.meta</span><br><span class="line">        channel_title = meta[<span class="string">'title'</span>]</span><br><span class="line">        channel_url = response.url</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'channel_title'</span>: channel_title,</span><br><span class="line">            <span class="string">'channel_url'</span>: channel_url</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># xpath解析网页</span></span><br><span class="line">        channel_item_lis = response.xpath(</span><br><span class="line">            <span class="string">'//li[contains(@class,</span></span><br><span class="line"><span class="string">            "branded-page-related-channels-item")]'</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">for</span> channel_item_li <span class="keyword">in</span> channel_item_lis:</span><br><span class="line">            related_channel_title = channel_item_li.xpath(</span><br><span class="line">                <span class="string">'span/div[contains(@class, </span></span><br><span class="line"><span class="string">                "yt-lockup-content")]/h3/a/text()'</span></span><br><span class="line">                ).extract()[<span class="number">0</span>]</span><br><span class="line">            relative_url = channel_item_li.xpath(</span><br><span class="line">                <span class="string">'span/div[contains(@class, </span></span><br><span class="line"><span class="string">                "yt-lockup-content")]/h3/a/@href'</span></span><br><span class="line">                ).extract()[<span class="number">0</span>]</span><br><span class="line">            related[related_channel_title] = youtube_url</span><br><span class="line">             + relative_url</span><br><span class="line">            print(related_channel_title, youtube_url</span><br><span class="line">             + relative_url)</span><br><span class="line">        <span class="keyword">if</span> related:</span><br><span class="line">            kol[channel_title] = related</span><br></pre></td></tr></table></figure>
<h3 id="爬取视频评论-包含评论内容、评论日期等"><a href="#爬取视频评论-包含评论内容、评论日期等" class="headerlink" title="爬取视频评论(包含评论内容、评论日期等)"></a>爬取视频评论(包含评论内容、评论日期等)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lxml.cssselect <span class="keyword">import</span> CSSSelector</span><br><span class="line"></span><br><span class="line">YOUTUBE_COMMENTS_URL = </span><br><span class="line"><span class="string">'https://www.youtube.com/all_comments?v=&#123;youtube_id&#125;'</span></span><br><span class="line">YOUTUBE_COMMENTS_AJAX_URL = </span><br><span class="line"><span class="string">'https://www.youtube.com/comment_ajax'</span></span><br><span class="line">youtube_video_url = </span><br><span class="line"><span class="string">'https://www.youtube.com/watch?v='</span></span><br><span class="line"></span><br><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_value</span><span class="params">(html, key, num_chars=<span class="number">2</span>)</span>:</span></span><br><span class="line">    pos_begin = html.find(key) + len(key) + num_chars</span><br><span class="line">    pos_end = html.find(<span class="string">'"'</span>, pos_begin)</span><br><span class="line">    <span class="keyword">return</span> html[pos_begin: pos_end]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_comments</span><span class="params">(html)</span>:</span></span><br><span class="line">    tree = lxml.html.fromstring(html)</span><br><span class="line">    item_sel = CSSSelector(<span class="string">'.comment-item'</span>)</span><br><span class="line">    text_sel = CSSSelector(<span class="string">'.comment-text-content'</span>)</span><br><span class="line">    time_sel = CSSSelector(<span class="string">'.time'</span>)</span><br><span class="line">    author_sel = CSSSelector(<span class="string">'.user-name'</span>)</span><br><span class="line">    <span class="comment"># vote_sel = CSSSelector('.like-count') 是否爬取评论点赞数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> item_sel(tree):</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'cid'</span>: item.get(<span class="string">'data-cid'</span>),</span><br><span class="line">            <span class="string">'text'</span>: text_sel(item)[<span class="number">0</span>].text_content(),</span><br><span class="line">            <span class="string">'time'</span>: time_sel(item)[<span class="number">0</span>].text_content().strip(),</span><br><span class="line">            <span class="string">'author'</span>: author_sel(item)[<span class="number">0</span>].text_content()</span><br><span class="line">          <span class="comment"># 'like-count': vote_sel(item)[0].text_content()</span></span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_reply_cids</span><span class="params">(html)</span>:</span></span><br><span class="line">    tree = lxml.html.fromstring(html)</span><br><span class="line">    sel = CSSSelector(<span class="string">'.comment-replies-header &gt; .load-comments'</span>)</span><br><span class="line">    <span class="keyword">return</span> [i.get(<span class="string">'data-cid'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> sel(tree)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ajax_request</span><span class="params">(session, url, params, data, retries=<span class="number">10</span>, sleep=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(retries):</span><br><span class="line">        response = session.post(url, params=params, data=data)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            response_dict = json.loads(response.text)</span><br><span class="line">            <span class="keyword">return</span> response_dict.get(<span class="string">'page_token'</span>, <span class="keyword">None</span>),</span><br><span class="line">            response_dict[<span class="string">'html_content'</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            time.sleep(sleep)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_comments</span><span class="params">(youtube_id, sleep=<span class="number">1</span>)</span>:</span></span><br><span class="line">    session = requests.Session()</span><br><span class="line">    session.headers[<span class="string">'User-Agent'</span>] = USER_AGENT</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取初始页面的评论</span></span><br><span class="line">    response = session.get(YOUTUBE_COMMENTS_URL.format(youtube_id=youtube_id))</span><br><span class="line">    html = response.text</span><br><span class="line">    reply_cids = extract_reply_cids(html)</span><br><span class="line"></span><br><span class="line">    ret_cids = []</span><br><span class="line">    <span class="keyword">for</span> comment <span class="keyword">in</span> extract_comments(html):</span><br><span class="line">        ret_cids.append(comment[<span class="string">'cid'</span>])</span><br><span class="line">        <span class="keyword">yield</span> comment</span><br><span class="line"></span><br><span class="line">    page_token = find_value(html, <span class="string">'data-token'</span>)</span><br><span class="line">    session_token = find_value(html, <span class="string">'XSRF_TOKEN'</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    first_iteration = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取剩下的评论(等同于点击'show more')</span></span><br><span class="line">    <span class="keyword">while</span> page_token:</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'video_id'</span>: youtube_id,</span><br><span class="line">            <span class="string">'session_token'</span>: session_token</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'action_load_comments'</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">'order_by_time'</span>: <span class="keyword">True</span>,</span><br><span class="line">            <span class="string">'filter'</span>: youtube_id</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> first_iteration:</span><br><span class="line">            params[<span class="string">'order_menu'</span>] = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[<span class="string">'page_token'</span>] = page_token</span><br><span class="line"></span><br><span class="line">        response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> response:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        page_token, html = response</span><br><span class="line"></span><br><span class="line">        reply_cids += extract_reply_cids(html)</span><br><span class="line">        <span class="keyword">for</span> comment <span class="keyword">in</span> extract_comments(html):</span><br><span class="line">            <span class="keyword">if</span> comment[<span class="string">'cid'</span>] <span class="keyword">not</span> <span class="keyword">in</span> ret_cids:</span><br><span class="line">                ret_cids.append(comment[<span class="string">'cid'</span>])</span><br><span class="line">                <span class="keyword">yield</span> comment</span><br><span class="line"></span><br><span class="line">        first_iteration = <span class="keyword">False</span></span><br><span class="line">        time.sleep(sleep)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取评论回复</span></span><br><span class="line">    <span class="keyword">for</span> cid <span class="keyword">in</span> reply_cids:</span><br><span class="line">        data = &#123;<span class="string">'comment_id'</span>: cid,</span><br><span class="line">                <span class="string">'video_id'</span>: youtube_id,</span><br><span class="line">                <span class="string">'can_reply'</span>: <span class="number">1</span>,</span><br><span class="line">                <span class="string">'session_token'</span>: session_token&#125;</span><br><span class="line"></span><br><span class="line">        params = &#123;<span class="string">'action_load_replies'</span>: <span class="number">1</span>,</span><br><span class="line">                  <span class="string">'order_by_time'</span>: <span class="keyword">False</span>,</span><br><span class="line">                  <span class="string">'filter'</span>: youtube_id,</span><br><span class="line">                  <span class="string">'tab'</span>: <span class="string">'inbox'</span>&#125;</span><br><span class="line"></span><br><span class="line">        response = ajax_request(session, YOUTUBE_COMMENTS_AJAX_URL, params, data)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> response:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        _, html = response</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> comment <span class="keyword">in</span> extract_comments(html):</span><br><span class="line">            <span class="keyword">if</span> comment[<span class="string">'cid'</span>] <span class="keyword">not</span> <span class="keyword">in</span> ret_cids:</span><br><span class="line">                ret_cids.append(comment[<span class="string">'cid'</span>])</span><br><span class="line">                <span class="keyword">yield</span> comment</span><br><span class="line">        time.sleep(sleep)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span> <span class="comment"># 使用</span></span><br><span class="line">    <span class="keyword">for</span> comment <span class="keyword">in</span> download_comments(videoId):</span><br><span class="line">    ···</span><br></pre></td></tr></table></figure>
<h3 id="爬取某个视频主所有的视频"><a href="#爬取某个视频主所有的视频" class="headerlink" title="爬取某个视频主所有的视频"></a>爬取某个视频主所有的视频</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用Selenium</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">Channel_videos = <span class="string">'https://www.youtube.com/channel/UCGK0RMoHboOVUbdxDhLD1xw/videos'</span></span><br><span class="line">Video_Lists = []</span><br><span class="line">option = webdriver.ChromeOptions()</span><br><span class="line">option.add_argument(<span class="string">'headless'</span>)</span><br><span class="line">youtube_url = <span class="string">'https://www.youtube.com'</span></span><br><span class="line">browser = webdriver.Chrome(chrome_options=option, executable_path=<span class="string">'D:\Tool\Software\chromedriver_win32\\chromedriver.exe'</span>)</span><br><span class="line"><span class="comment"># 去掉option选项可以让chrome在前台显示，看看模拟的效果</span></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">'D:\Tool\Software\chromedriver_win32\\chromedriver.exe'</span>)</span><br><span class="line">browser.get(Channel_videos)</span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">old_height = browser.execute_script(<span class="string">"return document.documentElement.scrollHeight;"</span>)</span><br><span class="line">browser.execute_script(<span class="string">"window.scrollTo(0, document.documentElement.scrollHeight);"</span>)</span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">new_height = browser.execute_script(<span class="string">"return document.documentElement.scrollHeight;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向下滚动页面，直到所有视频都被加载出来</span></span><br><span class="line"><span class="keyword">while</span> new_height != old_height:</span><br><span class="line">    old_height = new_height</span><br><span class="line">    browser.execute_script(<span class="string">"window.scrollTo(0, document.documentElement.scrollHeight);"</span>)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    new_height = browser.execute_script(<span class="string">"return document.documentElement.scrollHeight;"</span>)</span><br><span class="line"></span><br><span class="line">    html = browser.page_source</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    videoList = soup.findAll(<span class="string">"a"</span>, &#123;<span class="string">"class"</span>: <span class="string">"yt-simple-endpoint style-scope ytd-grid-video-renderer"</span>&#125;)</span><br><span class="line">    <span class="keyword">for</span> video <span class="keyword">in</span> videoList:</span><br><span class="line">        ···</span><br></pre></td></tr></table></figure>
<h3 id="获取视频的播放量和发布日期"><a href="#获取视频的播放量和发布日期" class="headerlink" title="获取视频的播放量和发布日期"></a>获取视频的播放量和发布日期</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 因为Youtube API每日有访问限制，超过它的配额后就无法再使用接口获取某个视频的播放量和发布日期了，使用爬虫就没有这种限制</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">youtube_video_url = <span class="string">'https://www.youtube.com/watch?v='</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viewCount</span><span class="params">()</span>:</span></span><br><span class="line">        videoId = <span class="string">'P4ItC6jWN0s'</span></span><br><span class="line">        url = <span class="string">"https://www.youtube.com/watch"</span></span><br><span class="line">        querystring = &#123;<span class="string">"v"</span>: videoId&#125;</span><br><span class="line">        payload = <span class="string">""</span></span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'Content-Type'</span>: <span class="string">"application/json"</span>,</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">"PostmanRuntime/7.15.0"</span>,</span><br><span class="line">            <span class="string">'Accept'</span>: <span class="string">"*/*"</span>,</span><br><span class="line">            <span class="string">'Cache-Control'</span>: <span class="string">"no-cache"</span>,</span><br><span class="line">            <span class="string">'Postman-Token'</span>: <span class="string">"296c1155-2adc-4028-95c2-26cffec91784,f5c8088f-432f-4a67-a815-13464bfca373"</span>,</span><br><span class="line">            <span class="string">'Host'</span>: <span class="string">"www.youtube.com"</span>,</span><br><span class="line">            <span class="string">'cookie'</span>: <span class="string">"YSC=nwR5fai12Kg; VISITOR_INFO1_LIVE=gAl5VFO7Gjo; PREF=f1=50000000; GPS=1"</span>,</span><br><span class="line">            <span class="string">'accept-encoding'</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">            <span class="string">'Connection'</span>: <span class="string">"keep-alive"</span>,</span><br><span class="line">            <span class="string">'cache-control'</span>: <span class="string">"no-cache"</span></span><br><span class="line">        &#125;</span><br><span class="line">        response = requests.request(<span class="string">"GET"</span>, url, data=payload, headers=headers, params=querystring)</span><br><span class="line">        html = etree.HTML(response.text)</span><br><span class="line">        datePublished = html.xpath(<span class="string">'//meta[@itemprop="datePublished"]/@content'</span>)</span><br><span class="line">        <span class="keyword">if</span> datePublished:</span><br><span class="line">            datePublished = datePublished[<span class="number">0</span>]</span><br><span class="line">            view_count = html.xpath(<span class="string">'//meta[@itemprop="interactionCount"]/@content'</span>)</span><br><span class="line">            <span class="keyword">if</span> view_count:</span><br><span class="line">                view_count = int(view_count[<span class="number">0</span>])</span><br><span class="line">                print(datePublished, view_count)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'Not Exist:'</span>,videoId)</span><br><span class="line"></span><br><span class="line">    file = open(<span class="string">'video_statstics.json'</span>, <span class="string">'a'</span>)</span><br><span class="line">    json.dump(video_statstics, file, indent=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    viewCount()</span><br></pre></td></tr></table></figure>
<h3 id="使用Selenium模拟搜索"><a href="#使用Selenium模拟搜索" class="headerlink" title="使用Selenium模拟搜索"></a>使用Selenium模拟搜索</h3><blockquote>
<p>避免Youtube API对/search接口的使用限制，根据官方文档，一个项目每日的配额是10000，而/search接口调用一次就至少是100</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line">Campaign = campaign_phase(GA.loc[i, <span class="string">'Campaign'</span>])</span><br><span class="line">option = webdriver.ChromeOptions()</span><br><span class="line">option.add_argument(<span class="string">'headless'</span>)</span><br><span class="line">youtube_url = <span class="string">'https://www.youtube.com'</span></span><br><span class="line"><span class="comment"># 添加option后程序启动时就不会弹出chrome窗口，减少资源调用，提供效率，如果需要直观地观看程序的流程就用下面一行注释的不带option调用的命令</span></span><br><span class="line">browser = webdriver.Chrome(chrome_options=option,</span><br><span class="line">executable_path=<span class="string">'XXX\\chromedriver.exe'</span>)</span><br><span class="line"><span class="comment"># browser = webdriver.Chrome(executable_path='XXX\\chromedriver.exe') </span></span><br><span class="line">browser.get(<span class="string">'https://www.youtube.com/'</span>)</span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">input = wait.until</span><br><span class="line">(</span><br><span class="line">    EC.presence_of_element_located</span><br><span class="line">    (</span><br><span class="line">        (By.ID, <span class="string">'search'</span>) <span class="comment">#这里一定要加一个括号，详情见另外一篇博客</span></span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a href="http://www.bithub00.com/2019/08/21/TypeError/" target="_blank" rel="noopener">Selenium TypeError  <strong>init</strong>() takes 2 positional arguments but 3 were given_解决方案</a><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">submit = wait.until</span><br><span class="line">(</span><br><span class="line">    EC.element_to_be_clickable</span><br><span class="line">    (</span><br><span class="line">        (By.ID, <span class="string">'search-icon-legacy'</span>)</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">input.send_keys(Campaign)</span><br><span class="line">submit.click()</span><br><span class="line"><span class="comment"># 到这里就完成了模拟youtube输入关键词搜索的过程，下面是获取搜索结果中与关键词最匹配的频道</span></span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">html = browser.page_source</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line"></span><br><span class="line">searchResults = soup.findAll(<span class="string">"a"</span>, &#123;</span><br><span class="line">    <span class="string">"class"</span>: <span class="string">"yt-simple-endpoint style-scope ytd-channel-renderer"</span></span><br><span class="line">&#125;)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> searchResults:</span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> searchResults:</span><br><span class="line">        channel_url = youtube_url + result[<span class="string">'href'</span>]</span><br><span class="line">        title = result.find(<span class="string">"span"</span>, &#123;</span><br><span class="line">            <span class="string">"class"</span>: <span class="string">"style-scope ytd-channel-renderer"</span></span><br><span class="line">        &#125;)</span><br><span class="line">        channel_title = title.text</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>SIMLR算法论文个人翻译</title>
    <url>/2018/07/25/SIMLR%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<font size="5">
基于多核函数的单细胞RNA序列分析与可视化算法
</font>

<p>&#8194; </p>
<font size="3">
摘要：我们将要介绍的SIMLR算法，在理论框架和软件应用上已经实现，它从输入的单细胞RNA序列中学习到一种相似度的信息，随后可以用这种信息来实现降维、聚类和可视化等后续处理。在七个公开的数据集上我们对算法进行了检验并于一些常用的算法进行了比较分析。结果证明了SIMLR算法对大规模数据的良好处理以及极大程度上提高了聚类的效果，同时提升了可视化的效果以及对细胞间相似程度的识别。
</font>
<a id="more"></a>

<font size="3">关键词：多核函数；低秩约束；单细胞RNA序列
</font>

<h2 id="背景介绍及概览"><a href="#背景介绍及概览" class="headerlink" title="背景介绍及概览"></a>背景介绍及概览</h2><p>&#8194; 此前，对单细胞RNA序列的研究揭示了细胞种群间未被洞悉的异构性与功能分化。近期的研究阐释说明了通过对RNA序列的无偏分析，是有可能做到对细胞子群功能特异性的de novo分析的。然而，大部分用来应用的算法都是针对传统的大规模RNA序列数据，而基因的表达结果在一群细胞中被平均化了。这些算法并不能成功地处理如下问题：如噪声数据、离散点以及dropout现象（获取基因表达数据时未能成功识别基因表达结果而标记为0，而实际情况是基因进行了表达）。诸如DropSeq和GemCode的平台已经显著增加了数千个细胞的细胞信息，然而，这类平台产生的多为稀疏数据，其中95%的基因测量结果被标记为0。对于诸如降维、聚类以及数据可视化等无监督学习的方法来说，其中一个关键就是相似度矩阵的学习，而这个矩阵对于不同平台或者生物实验得到的数据并不通用。为了解决上述问题，我们提出了SIMLR算法，一个从输入的单细胞RNA序列数据中学习细胞与细胞之间相似度矩阵的框架。</p>
<p>&#8194; 相较于传统的算法，SIMLR算法有三个主要的优势：首先，它通过使用多个核函数的方法来学习一个最符合输入数据的结构的相似性矩阵。常规的降维或聚类算法对数据的假设有时并不适用于单细胞RNA序列数据。而多个核函数被证实在描绘数据多角度的信息下有着更好的效果，而且相对于单个核函数具有更好的灵活性。第二，对于高维度下的dropout现象，SIMLR算法通过对学习的相似度矩阵应用秩约束以及使用图扩散的方法来解决。秩约束的应用增强了相似度矩阵的分块对角结构，而图扩散方法提高了对弱相似度的识别。第三，算法习得的相似度矩阵可以被高效的用来后续的数据分析，比如通过SNE算法进行数据在低维空间下的可视化。</p>
<p>&#8194; 我们通过在四个公开的单细胞数据集上应用SIMLR算法来与传统的算法比较，结果是SIMLR算法习得的相似度矩阵在表现数据相似度上要表现得更好。每个数据集中细胞所属的种类是先验的并且在研究中已经被证实正确。通过输入数据集和细胞的种类数，SIMLR算法就能学习出一个细胞间的相似度矩阵，而不需要输入细胞真实所属类别的标签信息。而且相较于传统的相关系数或欧几里得距离衡量相似度，SIMLR算法的结果要更加接近真实结果。特别的是，Buettner数据集的真实标签是细胞周期的状态，我们在这个数据集上额外应用了SIMLR算法来对基因进行排序。算法输出一个基因网络，来展现不同的基因在细胞各个周期、翻译以及代谢过程中的相关性。</p>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhleH.png" alt=""></p>
<p>&#8194; 图表1：SIMLR算法的总览。给定一个基因表达数据的矩阵作为输入，算法构建并学习多个核函数之间的权重，并构建出一个相似度矩阵。给定细胞种类数C作为输入，构建出的相似度矩阵将有一个含C个分块的分块对角矩阵，每个分块中的细胞都更为相似。算法使用得到的相似度矩阵来进行降维、可视化、聚类等后续处理。实心箭头代表了需要被用来构建相似度矩阵的信息，而点线箭头则表明聚类的结果可以与可视化以及基因排序相结合。2D散点图中的每一个点代表一个细胞，而不同的颜色代表细胞所属的不同类别。</p>
<p>&#8194; 为了分析算法在降维上的效果，我们与8个传统的降维算法进行了比较，包括主成分分析、tSNE以及ZIFA算法。在六种不同的表现算法优劣矩阵中，SIMLE算法在四个公开的数据集上表现出色，并且远远的拉开了与第二名的差距。</p>
<p>&#8194; 我们还进行了低维数据的可视化实验。结果表明，在各个数据集上SIMLR算法的结果不仅能吻合数据集给出的真实标签，甚至在秩约束有关的参数没有贡献时同样能保持相似度矩阵的分块对角结构。特别地，在Kolodziejczyk这个数据集上，我们还从SIMLR算法的结果上发现，在已知的分类结果上其实还能继续往下细分，这个结果也符合与这个数据集有关的理论研究。</p>
<p>&#8194; SIMLR算法同样可以用来进行细胞聚类，通过降维后应用k-means算法或者直接对习得的相似度矩阵使用AP近邻算法来实现。后者的表现性要远远超过使用皮尔逊相关系数或欧几里得距离来衡量相似性的方法。而前者的表现性在四个数据集上也比现有的针对单细胞的聚类算法要更好。</p>
<p>&#8194; 为了检验算法的能力，我们应用了更多更有挑战性的方案。我们分析了一个GemCode平台上提供的周边血液单核球细胞的稀疏数据集，里面包含了2700个细胞且其中95%的基因表达结果被标记为0。通过降维后应用k-means算法，我们识别出八种主要的细胞类别，包括一个只含12个细胞的megakaryocyte种类。除此之外，我们还在不同的已经得到充分研究的数据集上试验了SIMLR算法的表现性。</p>
<p>&#8194; 为了说明SIMLR算法在大规模数据上的表现性，我们在三个公开的大规模数据集上进行了试验。我们对真实标签与算法输出的预测表情的相关性进行了计算。对于Zeisei数据集，我们应用了一个二级聚类的方法，发现SIMLR算法可以用来进行在对细胞的层次结构的分析。而且，低维可视化的结果也很好的符合了真实的数据标签。即使是大规模数据集中因为噪声和离散点所造成的相似信息被隐藏的情况，SIMLR算法也能学习一个合适的细胞之间的距离。</p>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhGFI.png" alt=""><br>&#8194; 图表2：在不同数据集上的测试结果。分别给出了SIMLR算法、基于高斯核的欧几里得距离以及皮尔逊相关系数所得到的相似度矩阵。排放的次序基于相似度的高低。矩阵中的细胞都按照真实的种类来进行排列，使得同一种类的细胞排列在一起，坐标轴上的不同颜色代表不同的种类。可以看出，SIMLR算法的相似度矩阵的分块对角结构与真实标签基本符合。&#8194;</p>
<p>&#8194; 总的来说，SIMLR算法可以基于不同的数据集通用地判断那些细胞更为相似，即判断结果不受特定数据集影响，并应用降维、聚类、数据可视化等分析方法。SIMLR算法在有着清晰分类的数据集上表现出色，而我们预测这个多核学习的框架在分类不明显的数据集上也会同样产生作用。</p>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhMOe.png" alt=""><br>&#8194; 图表3： 2维可视化结果的比较。坐标轴无实际意义。每个点代表一个细胞，而靠得越近的点代表相似度越高。数据可视化时没有输入真实的标签信息，在算法的输出时才让各个数据点标上真实的颜色以检验算法的效果。</p>
<h2 id="算法详述："><a href="#算法详述：" class="headerlink" title="算法详述："></a>算法详述：</h2><p>SIMLR算法提供了Matlab和R语言两个版本的实现（<a href="https://github.com/BatzoglouLabSU/SIMLR）" target="_blank" rel="noopener">https://github.com/BatzoglouLabSU/SIMLR）</a></p>
<p>&#8194; k-means算法的实现我们使用了Matlab和R语言自带的模块。而SNE算法我们修改了两个语言中这一模块的源代码。四个公开的数据集随着源代码一起被提供。而三个大型的数据集可以在相应的平台上得到。输入一个N×M的的基因表达矩阵，N代表细胞个数，M代表基因数。SIMLR算法将输出一个S×S的相似性矩阵。其中Sij表示两个细胞之间的相似度。给定一个细胞种类数C，算法假定输出的相似度矩阵将有一个含C个分块的分块对角矩阵，各个分块中的细胞更为相似。我们对两个细胞之间的距离定义为：</p>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhuQO.png" alt=""><br>其中wl代表核函数的权重</p>
<font size="4"> 
算法依据如下损失函数来计算细胞与细胞之间的相似性：
</font>

<p><img src="https://s1.ax1x.com/2018/07/31/PdhKyD.png" alt=""><br>&#8194; 其中IN和IC分别为N×N和C×C的单位矩阵，tr(.)代表矩阵的秩，β和γ均为非零值，||S||F为范数表示，L为辅助的用来对S进行低秩约束。因此这个损失函数求解三个参数：相似度矩阵S，核函数权重向量w以及一个结构为N×C的秩约束矩阵L。</p>
<font size="3">  
&#8194; 损失函数的第一项含义为，如果两个细胞间的距离很远，则它们的相似度应该很低。第二项是一个对S的正则化，防止S矩阵过于接近一个单位矩阵。如果细胞可以被划分为C类，则每一类中的细胞更为相似，理想情况下矩阵S的秩为C。因此，损失函数的第三项以及L矩阵的引入增强了S的低秩结构，而矩阵（IN-S）即为拉普拉斯矩阵，在一个相似图中，每个节点代表一个细胞，边衡量节点间的相似性。第四项对核函数的权重进行约束，防止单核函数情况的出现。实践证明，这个正则化项提高了相似矩阵的表现。
</font>  

<h3 id="核函数的构建"><a href="#核函数的构建" class="headerlink" title="核函数的构建"></a>核函数的构建</h3><p>我们以带有不同超参数的高斯核为基础构建不同的核函数，实践证明相对其它核函数高斯核的表现更好。</p>
<p><img src="https://s1.ax1x.com/2018/07/31/Pdh1wd.png" alt=""></p>
<p>式中||ci - cj||表示细胞i和j之间的欧几里得距离。</p>
<font size="3"> 方差ɛij的定义式如下：</font>

<p><img src="https://s1.ax1x.com/2018/07/31/Pdh3TA.png" alt=""></p>
<p>&#8194; 因此，每一个核函数被一对参数(σ，k).我们设定k = 10，12，14,…,30 以及σ = 1.0，1.25，1.5，1.75，2，产生了55个不同的核函数。然而，实践证明，算法对核函数的数量以及参数的选择并不敏感。</p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ul>
<li>核函数的权重w被初始化为核函数数量的倒数：</li>
</ul>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhTt1.png" alt=""></p>
<ul>
<li>相似度矩阵S被初始化为：</li>
</ul>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhokR.png" alt=""></p>
<ul>
<li>而矩阵L被初始化为拉普拉斯矩阵（IN - S）的前C个特征向量。</li>
</ul>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>我们对S,L和w进行优化。上文中的优化式非凸，但固定某两个参数得到的目标函数为凸函数。因此我们可以有效的应用凸优化算法来进行求解。</p>
<ul>
<li>步骤1：固定L和w对S进行更新。损失函数可以被重写为：</li>
</ul>
<p><img src="https://s1.ax1x.com/2018/07/31/Pdhbp6.png" alt=""></p>
<p>&#8194; 目标函数中第一项求和式以及约束项均为线性，而第二项是一个二次项，它可以在多项式复杂度的时间内计算出来。</p>
<ul>
<li>步骤2：固定S和w对L进行更新。损失函数可以被重写为：</li>
</ul>
<p><img src="https://s1.ax1x.com/2018/07/31/Pdh579.png" alt=""></p>
<p>&#8194; 此时对L矩阵的求解就是拉普拉斯矩阵（S-IN）对应的C个最大特征值的特征向量。</p>
<ul>
<li>步骤三：固定S和L对w进行更新。同样地，损失函数可以被重写为如下形式：</li>
</ul>
<p><img src="https://s1.ax1x.com/2018/07/31/Pdh7fx.png" alt=""></p>
<p>&#8194; 对于这样一个包含凸函数和线性约束的问题，任何一个凸优化算法都可以进行求解。</p>
<ul>
<li>步骤四：基于扩散方法的相似度矩阵优化。我们应用了一种扩散方法来减少噪声和dropout现象对S矩阵的影响。给定矩阵S，我们构建如下形式的过渡矩阵P：</li>
</ul>
<p><img src="https://s1.ax1x.com/2018/07/31/Pdhjne.png" alt=""></p>
<p>&#8194; Ak(i)代表一个集合，里面包含了细胞i的k个近邻细胞的索引。构建出来的过渡矩阵是稀疏的，并且保留了极大部分的相似度结构。算法的更新方法如下所示：</p>
<p><img src="https://s1.ax1x.com/2018/07/31/Pdhq1K.png" alt=""></p>
<p>&#8194; H(0)ij = Sij作为输入，而最终迭代出来的结果Hij作为新的相似值Sij。这个额外的扩散方法将会很大程度上避免单细胞RNA序列数据中的噪声值所带来的影响。然而，因为这个算法的高计算复杂度，在面对大规模数据集时它无法发挥有效作用。</p>
<font size="3">  
&#8194; SIMLR算法重复步骤1-4直到算法收敛。随后使用得到的相似度矩阵S进行后续分析：</font>

<h2 id="后续分析"><a href="#后续分析" class="headerlink" title="后续分析"></a>后续分析</h2><h3 id="降维处理："><a href="#降维处理：" class="headerlink" title="降维处理："></a>降维处理：</h3><p>&#8194; 算法基于SNE算法进行降维，并进行了调整。不同点在于，tSNE算法基于高斯核来计算高维度空间下数据之间的相似度，随后将其映射到低维空间并保留这个相似度信息。我们没有选择直接输入基因表达矩阵而是输入了相似度矩阵S。</p>
<h3 id="可视化："><a href="#可视化：" class="headerlink" title="可视化："></a>可视化：</h3><p>&#8194; 我们使用降维算法来投影到二维或三维空间进行可视化。如k-means聚类，我们将维度降到B维，得到一个N×B相应的矩阵Z，随后应用k-means算法来对于细胞进行聚类。B的值与输入的C的值相同。C同时也是上文提到的秩约束的参数。</p>
<h3 id="基于相似度矩阵的基因排序："><a href="#基于相似度矩阵的基因排序：" class="headerlink" title="基于相似度矩阵的基因排序："></a>基于相似度矩阵的基因排序：</h3><p>&#8194; 我们通过计算某个基因在不同细胞中表达的值与习得的相似度的相关程度来对基因进行排序。给定相似度矩阵S和某个基因在所有细胞中的表达结果f，表达式如下：</p>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhL6O.png" alt=""></p>
<p>&#8194; 这是一个经典的用来衡量基因和相似度之间相关性的无监督特征排序算法。表达式的值越高，则基因在不同细胞中的表达越重要。然而，表达式对相似度矩阵中的噪声值非常敏感。为了克服这个问题，我们随机选取一定比例的细胞（如细胞总数的80%），随后根据剩余细胞的相似度矩阵来对基因进行排序。</p>
<h3 id="大规模数据集上的应用："><a href="#大规模数据集上的应用：" class="headerlink" title="大规模数据集上的应用："></a>大规模数据集上的应用：</h3><p>&#8194; 我们在含有数万个细胞的数据集上进行了试验，关键因素在于用KNN相似度来近似于细胞的相似度。第一步，我们采用了目前更为先进的近邻搜索算法<a href="https://github.com/spotify/annoy" target="_blank" rel="noopener">ANNOY</a>，ANNOY算法认为，一个近邻点的近邻也可能是一个近邻点。因此，在构建出KNN图后，算法只更新每个细胞所预先选定的前k个近邻点。因为得到的相似度矩阵是稀疏的，我们使用<a href="http://yixuan.cos.name/spectra/" target="_blank" rel="noopener">Spectra</a>来对L进行求解。当我们按照这种方式而不是进行涉及到矩阵求逆运算的闭式求解，我们只需要在有限次的迭代中就可以得到一个结果。</p>
<p>&#8194; 在我们得到相似度矩阵后，我们就可以进行细胞可视化和细胞聚类了。聚类时，从t-SNE算法中获得嵌入的低维空间的过程的计算量很大。相反，我们采用了一种谱聚类算法，它基本上等同于我们的SIMLR算法中对矩阵L应用k均值。这种简单的算法对稀疏相似性的聚类非常有效，并可扩展到数以万计的细胞中。对于可视化，由于我们仅将细胞到细胞的相似性映射到二维或三维空间，因此应用t-SNE算法在计算上仍是可行的。我们对tSNE算法中的Barnes–Hut算法进行了调整。</p>
<h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>&#8194; 我们在本文中使用了七个单细胞RNA-seq的数据集。前四个数据集每个包含少于1,000个细胞，而最后三个数据集包含数千到数万个细胞。下面是所有单细胞RNA-seq数据集的详细描述。</p>
<p>&#8194;（1）11个细胞群，包括神经细胞和血细胞（Pollen数据集）。该数据集旨在测试低覆盖率单细胞RNA-seq在鉴别不同细胞群体方面的效用，因此含有多种细胞类型的混合物：皮肤细胞，多能干细胞，血细胞和神经细胞。该数据集包括在高深度和低深度处测序的样本。我们分析了高深度样本，每个样本的平均测序数为890万。</p>
<p>&#8194; （2）具有感觉亚型的神经元细胞（Usoskin数据集）。该数据集包含来自小鼠背根神经节的622个细胞，每个细胞平均有114万个读段。作者将细胞分为四种神经元类型：肽能伤害性伤害感受器，非肽能伤害性伤害感受器，含神经丝，含酪氨酸羟化酶。</p>
<p>&#8194; （3）不同细胞周期阶段的胚胎干细胞（Buettner数据集）。该数据集来自对照研究，该对照研究量化了细胞周期对个体小鼠胚胎干细胞（mESC）中基因表达水平的影响。对于182个细胞中的每一个细胞，获得平均五十万个读数，并且至少20％的读数被定位于mm9小鼠基因组上的已知外显子。使用荧光激活细胞分选将细胞分选为细胞周期的三个阶段，并且使用金标准Hoechst染色对它们进行验证。</p>
<p>&#8194; （4）不同环境条件下的多能细胞（Kolodziejczyk数据集）。该数据集是从干细胞研究中获得的，研究不同培养条件如何影响mESC的多能状态。该研究从涉及三种不同培养条件的九个不同实验中量化了704个mESC中约10,000个基因的表达水平。每个细胞平均获得900万个读数，超过60％的读数映射到小家鼠基因组上的外显子。</p>
<p>&#8194; （5）具有39个亚型的小鼠视网膜细胞（Macoskco数据集）。通过基于液滴的高通量技术Drop-seq获得，该数据集包括44,808个单元的UMI（3端）计数（由其定制的计算管道识别）。细胞类型通过PCA和基于密度的聚类进行分类，并且通过差异基因表达进行验证。根据原始处理程序，我们过滤掉少于900个基因的细胞（涉及到11,040个细胞）用于无监督分析。</p>
<p>&#8194; （6）来自一个健康人类的PBMCs数据集（PBMC68k数据集）。通过GemCode平台生成scRNA-seq文库，这是一种基于液滴的高通量技术，以及具有UMI（3’端）计数的68,560个细胞通过其定制的计算流水线来识别。这种细胞群包括健康人体内的主要免疫细胞类型。</p>
<p>&#8194; （7）使用独特的分子识别（UMI）分析和3’端计数收集来自小鼠皮质和海马的细胞（Zeisel数据集）。收集来自小鼠脑的3,005个细胞，并且通过分级双聚类鉴定了47个亚型，并通过基因标记进行了验证。</p>
<p>&#8194;  对于以上涉及到的数据集，我们进行了如下的数据预处理：</p>
<p><img src="https://s1.ax1x.com/2018/07/31/PdhOXD.png" alt="PdhOXD.png"> </p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>API接口设计</title>
    <url>/2019/04/10/api/</url>
    <content><![CDATA[<h2 id="API-接口设计"><a href="#API-接口设计" class="headerlink" title="API 接口设计"></a>API 接口设计</h2><h3 id="接口地址"><a href="#接口地址" class="headerlink" title="接口地址"></a>接口地址</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">环境</th>
<th style="text-align:center">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">开发环境</td>
<td style="text-align:center"><code>http://like.suevily.cn/</code></td>
</tr>
<tr>
<td style="text-align:center">生产环境</td>
<td style="text-align:center">待定</td>
</tr>
</tbody>
</table>
</div>
<h3 id="通用"><a href="#通用" class="headerlink" title="通用"></a>通用</h3><ol>
<li>以下接口分为开放接口和私有接口<ul>
<li>开放接口：无需登录校验即可请求</li>
<li>私有接口：需校验登录 token 方可请求</li>
</ul>
</li>
<li>以下所有接口 url 都默认自带 <code>/api</code> 前缀<a id="more"></a></li>
<li>token 在使用统一身份验证成功登录后会附带到 redirect url 的 query 参数上，前端需自行存储，开发环境的 redirect url 为 <code>http://localhost:8081/#/</code></li>
<li><strong>私有接口请求方法</strong>：将 token 附在请求 Headers 中的 Authorization 字段上，value 格式为 <code>Bearer ${token}</code> （PS: 建议使用 postman 进行接口测试）</li>
<li>前端在拿到返回数据时务必先检查 code 是否为零，如若不为零，需给用户正确的反馈，回传数据遵循以下格式：</li>
</ol>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  code: <span class="built_in">Number</span>,</span><br><span class="line">  data: <span class="built_in">Object</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>成功</td>
</tr>
<tr>
<td>-1</td>
<td>未知错误，查看 message</td>
</tr>
<tr>
<td>1</td>
<td>拒绝访问</td>
</tr>
<tr>
<td>2</td>
<td>无效的请求参数</td>
</tr>
<tr>
<td>3</td>
<td>上传图片出错</td>
</tr>
<tr>
<td>4</td>
<td>token 校验失败</td>
</tr>
<tr>
<td>其它</td>
<td>待定</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="获取用户信息"><a href="#获取用户信息" class="headerlink" title="获取用户信息"></a>获取用户信息</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">method</th>
<th style="text-align:center">url</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">GET</td>
<td style="text-align:center">/user</td>
<td style="text-align:center">私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数"><a href="#请求参数" class="headerlink" title="请求参数"></a>请求参数</h4><p>无</p>
<h4 id="返回参数"><a href="#返回参数" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>cardId</td>
<td>String</td>
<td>校园卡号</td>
</tr>
<tr>
<td>name</td>
<td>String</td>
<td>姓名</td>
</tr>
<tr>
<td>college</td>
<td>String</td>
<td>学院</td>
</tr>
<tr>
<td>stuId</td>
<td>String</td>
<td>学号</td>
</tr>
<tr>
<td>privilege</td>
<td>Number</td>
<td>0 为超级管理员，1为普通管理员，如没有该字段则为普通师生</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="获取管理人员"><a href="#获取管理人员" class="headerlink" title="获取管理人员"></a>获取管理人员</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">method</th>
<th style="text-align:center">url</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">POST</td>
<td style="text-align:center">/getManagers</td>
<td style="text-align:center">私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-1"><a href="#请求参数-1" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>perPage</td>
<td>Number</td>
<td>每页包含元素个数</td>
<td>是</td>
</tr>
<tr>
<td>page</td>
<td>Number</td>
<td>页码，第几页</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-1"><a href="#返回参数-1" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>length</td>
<td>-</td>
<td>Number</td>
<td>管理人员总数</td>
</tr>
<tr>
<td>managers</td>
<td>-</td>
<td>Array</td>
<td>一个包含指定页数内所有管理人员的数组</td>
</tr>
<tr>
<td>-</td>
<td>id</td>
<td>Number</td>
<td>该管理员在数据库中的唯一标识</td>
</tr>
<tr>
<td>-</td>
<td>name</td>
<td>String</td>
<td>姓名</td>
</tr>
<tr>
<td>-</td>
<td>cardId</td>
<td>String</td>
<td>校园卡号</td>
</tr>
<tr>
<td>-</td>
<td>authorizer</td>
<td>String</td>
<td>授权人姓名</td>
</tr>
<tr>
<td>-</td>
<td>createdAt</td>
<td>Date</td>
<td>添加时间戳</td>
</tr>
<tr>
<td>-</td>
<td>privilege</td>
<td>String</td>
<td>管理权限，超级管理员或普通管理员</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="添加管理人员"><a href="#添加管理人员" class="headerlink" title="添加管理人员"></a>添加管理人员</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">method</th>
<th style="text-align:center">url</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">POST</td>
<td style="text-align:center">/addManager</td>
<td style="text-align:center">私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-2"><a href="#请求参数-2" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>姓名</td>
<td>是</td>
</tr>
<tr>
<td>cardId</td>
<td>String</td>
<td>校园卡号</td>
<td>是</td>
</tr>
<tr>
<td>privilege</td>
<td>Number</td>
<td>权限，0 为超级管理员，1为普通管理员</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-2"><a href="#返回参数-2" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>添加结果</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码"><a href="#错误代码" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1001</td>
<td>校园卡号冲突</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="修改管理人员"><a href="#修改管理人员" class="headerlink" title="修改管理人员"></a>修改管理人员</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">method</th>
<th style="text-align:center">url</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">POST</td>
<td style="text-align:center">/updateManager</td>
<td style="text-align:center">私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-3"><a href="#请求参数-3" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>Number</td>
<td>要修改的管理员的id</td>
<td>是</td>
</tr>
<tr>
<td>name</td>
<td>String</td>
<td>姓名</td>
<td>是</td>
</tr>
<tr>
<td>cardId</td>
<td>String</td>
<td>校园卡号</td>
<td>是</td>
</tr>
<tr>
<td>privilege</td>
<td>Number</td>
<td>权限，0 为超级管理员，1为普通管理员</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-3"><a href="#返回参数-3" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>修改结果</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码-1"><a href="#错误代码-1" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1002</td>
<td>当你要把最后一个超级管理员更改为普通管理员时会出错</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="删除管理人员"><a href="#删除管理人员" class="headerlink" title="删除管理人员"></a>删除管理人员</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">method</th>
<th style="text-align:center">url</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">POST</td>
<td style="text-align:center">/deleteManager</td>
<td style="text-align:center">私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-4"><a href="#请求参数-4" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>String</td>
<td>要删除的管理员的id</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-4"><a href="#返回参数-4" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>删除结果</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码-2"><a href="#错误代码-2" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1003</td>
<td>当你要把最后一个超级管理员删除时会出错</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="搜索管理人员"><a href="#搜索管理人员" class="headerlink" title="搜索管理人员"></a>搜索管理人员</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">method</th>
<th style="text-align:center">url</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">POST</td>
<td style="text-align:center">/searchManagers</td>
<td style="text-align:center">私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-5"><a href="#请求参数-5" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>keyword</td>
<td>String</td>
<td>搜索关键词</td>
<td>是</td>
</tr>
<tr>
<td>maxLength</td>
<td>Number</td>
<td>搜索结果最大返回数目（默认为5）</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-5"><a href="#返回参数-5" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>managers</td>
<td>-</td>
<td>Array</td>
<td>满足搜索关键词的管理员数组</td>
</tr>
<tr>
<td>-</td>
<td>id</td>
<td>Number</td>
<td>该管理员在数据库中的唯一标识</td>
</tr>
<tr>
<td>-</td>
<td>name</td>
<td>String</td>
<td>姓名</td>
</tr>
<tr>
<td>-</td>
<td>cardId</td>
<td>String</td>
<td>校园卡号</td>
</tr>
<tr>
<td>-</td>
<td>authorizer</td>
<td>String</td>
<td>授权人姓名</td>
</tr>
<tr>
<td>-</td>
<td>createdAt</td>
<td>Date</td>
<td>添加时间戳</td>
</tr>
<tr>
<td>-</td>
<td>privilege</td>
<td>String</td>
<td>管理权限，超级管理员或普通管理员</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="获取课程信息"><a href="#获取课程信息" class="headerlink" title="获取课程信息"></a>获取课程信息</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/getCourses</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-6"><a href="#请求参数-6" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>perPage</td>
<td>Number</td>
<td>每页包含元素个数</td>
<td>是</td>
</tr>
<tr>
<td>page</td>
<td>Number</td>
<td>页码，第几页</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-6"><a href="#返回参数-6" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>length</td>
<td>-</td>
<td>Number</td>
<td>课程总数</td>
</tr>
<tr>
<td>courses</td>
<td>-</td>
<td>Array</td>
<td>一个包含指定页数所有课程信息的数组</td>
</tr>
<tr>
<td>-</td>
<td>courseName</td>
<td>String</td>
<td>课程名称</td>
</tr>
<tr>
<td>-</td>
<td>teacher</td>
<td>String</td>
<td>讲师姓名</td>
</tr>
<tr>
<td></td>
<td>time</td>
<td>String</td>
<td>上课时间，前端直接展示即可</td>
</tr>
<tr>
<td></td>
<td>courseId</td>
<td>String</td>
<td>课程编号</td>
</tr>
<tr>
<td></td>
<td>location</td>
<td>String</td>
<td>上课地点</td>
</tr>
<tr>
<td></td>
<td>studentCount</td>
<td>Number</td>
<td>选课人数</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="根据课程编号获取课程详细信息"><a href="#根据课程编号获取课程详细信息" class="headerlink" title="根据课程编号获取课程详细信息"></a>根据课程编号获取课程详细信息</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/getCourseInfo</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-7"><a href="#请求参数-7" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseId</td>
<td>String</td>
<td>课程编号</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-7"><a href="#返回参数-7" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseName</td>
<td>-</td>
<td>String</td>
<td>课程名称</td>
</tr>
<tr>
<td>teacherId</td>
<td>-</td>
<td>String</td>
<td>讲师在数据库中的id</td>
</tr>
<tr>
<td>teacherName</td>
<td>-</td>
<td>String</td>
<td>讲师姓名</td>
</tr>
<tr>
<td>startTime</td>
<td>-</td>
<td>Date</td>
<td>上课开始时间戳</td>
</tr>
<tr>
<td>endTime</td>
<td>-</td>
<td>Date</td>
<td>上课结束时间戳</td>
</tr>
<tr>
<td>location</td>
<td>-</td>
<td>String</td>
<td>上课地点</td>
</tr>
<tr>
<td>students</td>
<td>-</td>
<td>Array</td>
<td>一个包含指定课程选课所有学生信息的数组</td>
</tr>
<tr>
<td></td>
<td>name</td>
<td>String</td>
<td>学生姓名</td>
</tr>
<tr>
<td></td>
<td>stuId</td>
<td>String</td>
<td>学生学号</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码-3"><a href="#错误代码-3" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>2003</td>
<td>课程编号错误</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="查询选课名单"><a href="#查询选课名单" class="headerlink" title="查询选课名单"></a>查询选课名单</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/getStudentList</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-8"><a href="#请求参数-8" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseId</td>
<td>String</td>
<td>课程编号</td>
<td>是</td>
</tr>
<tr>
<td>perPage</td>
<td>Number</td>
<td>每页包含元素个数</td>
<td>是</td>
</tr>
<tr>
<td>page</td>
<td>Number</td>
<td>页码，第几页</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-8"><a href="#返回参数-8" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>length</td>
<td>-</td>
<td>Number</td>
<td>选课总人数</td>
</tr>
<tr>
<td>students</td>
<td>-</td>
<td>Array</td>
<td>一个包含指定课程选课所有学生信息的数组</td>
</tr>
<tr>
<td></td>
<td>name</td>
<td>String</td>
<td>学生姓名</td>
</tr>
<tr>
<td></td>
<td>stuId</td>
<td>String</td>
<td>学生学号</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="删除课程"><a href="#删除课程" class="headerlink" title="删除课程"></a>删除课程</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/deleteCourse</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-9"><a href="#请求参数-9" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseId</td>
<td>String</td>
<td>要删除的课程的课程编号</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-9"><a href="#返回参数-9" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>删除结果</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="搜索课程"><a href="#搜索课程" class="headerlink" title="搜索课程"></a>搜索课程</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/searchCourses</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-10"><a href="#请求参数-10" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>keyword</td>
<td>String</td>
<td>搜索关键词</td>
<td>是</td>
</tr>
<tr>
<td>maxLength</td>
<td>Number</td>
<td>搜索结果最大返回数目（默认为5）</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-10"><a href="#返回参数-10" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>courses</td>
<td>-</td>
<td>Array</td>
<td>满足搜索关键词的课程数组</td>
</tr>
<tr>
<td>-</td>
<td>courseName</td>
<td>String</td>
<td>课程名称</td>
</tr>
<tr>
<td>-</td>
<td>teacher</td>
<td>String</td>
<td>讲师姓名</td>
</tr>
<tr>
<td></td>
<td>time</td>
<td>String</td>
<td>上课时间，前端直接展示即可</td>
</tr>
<tr>
<td></td>
<td>courseId</td>
<td>String</td>
<td>课程编号</td>
</tr>
<tr>
<td></td>
<td>location</td>
<td>String</td>
<td>上课地点</td>
</tr>
<tr>
<td></td>
<td>studentCount</td>
<td>Number</td>
<td>选课人数</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="生成课程"><a href="#生成课程" class="headerlink" title="生成课程"></a>生成课程</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/addCourse</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-11"><a href="#请求参数-11" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseName</td>
<td>-</td>
<td>String</td>
<td>课程名称</td>
<td>是</td>
</tr>
<tr>
<td>teacherId</td>
<td>-</td>
<td>String</td>
<td>讲师在数据库中的id</td>
<td>是</td>
</tr>
<tr>
<td>startTime</td>
<td>-</td>
<td>Date</td>
<td>上课开始时间戳</td>
<td>是</td>
</tr>
<tr>
<td>endTime</td>
<td>-</td>
<td>Date</td>
<td>上课结束时间戳</td>
<td>是</td>
</tr>
<tr>
<td>location</td>
<td>-</td>
<td>String</td>
<td>上课地点</td>
<td>是</td>
</tr>
<tr>
<td>students</td>
<td>-</td>
<td>Array</td>
<td>选课学生数组</td>
<td>是</td>
</tr>
<tr>
<td>-</td>
<td>name</td>
<td>String</td>
<td>学生姓名</td>
<td>是</td>
</tr>
<tr>
<td>-</td>
<td>stuId</td>
<td>String</td>
<td>学生学号</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-11"><a href="#返回参数-11" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseId</td>
<td>String</td>
<td>生成的课程编号</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码-4"><a href="#错误代码-4" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>2001</td>
<td>讲师不存在</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="修改课程"><a href="#修改课程" class="headerlink" title="修改课程"></a>修改课程</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/updateCourse</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-12"><a href="#请求参数-12" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>courseId</td>
<td>-</td>
<td>String</td>
<td>课程编号（乱传打爆你▄︻┻═┳一）</td>
<td>是</td>
</tr>
<tr>
<td>courseName</td>
<td>-</td>
<td>String</td>
<td>课程名称</td>
<td>是</td>
</tr>
<tr>
<td>teacherId</td>
<td>-</td>
<td>String</td>
<td>讲师在数据库中的id</td>
<td>是</td>
</tr>
<tr>
<td>startTime</td>
<td>-</td>
<td>String</td>
<td>上课开始时间戳</td>
<td>是</td>
</tr>
<tr>
<td>endTime</td>
<td>-</td>
<td>String</td>
<td>上课结束时间戳</td>
<td>是</td>
</tr>
<tr>
<td>location</td>
<td>-</td>
<td>String</td>
<td>上课地点</td>
<td>是</td>
</tr>
<tr>
<td>students</td>
<td>-</td>
<td>Array</td>
<td>选课学生数组</td>
<td>是</td>
</tr>
<tr>
<td>-</td>
<td>name</td>
<td>String</td>
<td>学生姓名</td>
<td>是</td>
</tr>
<tr>
<td>-</td>
<td>stuId</td>
<td>String</td>
<td>学生学号</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-12"><a href="#返回参数-12" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>修改结果</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码-5"><a href="#错误代码-5" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>2001</td>
<td>讲师不存在</td>
</tr>
<tr>
<td>2002</td>
<td>课程编号不存在</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="获取讲师信息"><a href="#获取讲师信息" class="headerlink" title="获取讲师信息"></a>获取讲师信息</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/getTeachers</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-13"><a href="#请求参数-13" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>perPage</td>
<td>Number</td>
<td>每页包含元素个数</td>
<td>是</td>
</tr>
<tr>
<td>page</td>
<td>Number</td>
<td>页码，第几页</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-13"><a href="#返回参数-13" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>length</td>
<td>-</td>
<td>Number</td>
<td>讲师总数</td>
</tr>
<tr>
<td>teachers</td>
<td>-</td>
<td>Array</td>
<td>一个包含指定页数所有讲师信息的数组</td>
</tr>
<tr>
<td>-</td>
<td>id</td>
<td>Number</td>
<td>该讲师在数据库的唯一标识</td>
</tr>
<tr>
<td>-</td>
<td>name</td>
<td>String</td>
<td>讲师姓名</td>
</tr>
<tr>
<td></td>
<td>imageUrl</td>
<td>String</td>
<td>讲师照片的 url</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="删除讲师"><a href="#删除讲师" class="headerlink" title="删除讲师"></a>删除讲师</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/deleteTeacher</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-14"><a href="#请求参数-14" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>Number</td>
<td>讲师 id ，在 <code>/getTeachers</code> 接口获取到的</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-14"><a href="#返回参数-14" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>删除结果</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="搜索讲师"><a href="#搜索讲师" class="headerlink" title="搜索讲师"></a>搜索讲师</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/searchTeachers</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-15"><a href="#请求参数-15" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>keyword</td>
<td>String</td>
<td>搜索关键词</td>
<td>是</td>
</tr>
<tr>
<td>maxLength</td>
<td>Number</td>
<td>搜索结果最大返回数目（默认为5）</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-15"><a href="#返回参数-15" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>teachers</td>
<td>-</td>
<td>Array</td>
<td>满足搜索关键字的讲师数组</td>
</tr>
<tr>
<td></td>
<td>id</td>
<td>Number</td>
<td>该讲师在数据库的唯一标识</td>
</tr>
<tr>
<td>-</td>
<td>name</td>
<td>String</td>
<td>讲师姓名</td>
</tr>
<tr>
<td></td>
<td>imageUrl</td>
<td>String</td>
<td>讲师照片的 url</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="查询一个讲师的详细信息"><a href="#查询一个讲师的详细信息" class="headerlink" title="查询一个讲师的详细信息"></a>查询一个讲师的详细信息</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/getTeacherInfo</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-16"><a href="#请求参数-16" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>Number</td>
<td>讲师 id ，在 <code>/getTeachers</code> 接口获取到的</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-16"><a href="#返回参数-16" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>-</td>
<td>String</td>
<td>讲师姓名</td>
</tr>
<tr>
<td>college</td>
<td>-</td>
<td>String</td>
<td>所属单位</td>
</tr>
<tr>
<td>intro</td>
<td>-</td>
<td>String</td>
<td>老师简介</td>
</tr>
<tr>
<td>phone</td>
<td>-</td>
<td>String</td>
<td>手机号码</td>
</tr>
<tr>
<td>office</td>
<td>-</td>
<td>String</td>
<td>办公地址</td>
</tr>
<tr>
<td>email</td>
<td>-</td>
<td>String</td>
<td>邮箱地址</td>
</tr>
<tr>
<td>imageUrl</td>
<td>-</td>
<td>String</td>
<td>导师照片 url</td>
</tr>
<tr>
<td>teachForm</td>
<td>-</td>
<td>String</td>
<td>授课形式</td>
</tr>
<tr>
<td>teachTopic</td>
<td>-</td>
<td>Array</td>
<td>该讲师授课专题的数组</td>
</tr>
<tr>
<td></td>
<td>-</td>
<td>String</td>
<td>授课专题</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="图片上传接口"><a href="#图片上传接口" class="headerlink" title="图片上传接口"></a>图片上传接口</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/upload</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-17"><a href="#请求参数-17" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td>FormData</td>
<td>图片数据</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-17"><a href="#返回参数-17" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>String</td>
<td>图片 url</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="添加讲师"><a href="#添加讲师" class="headerlink" title="添加讲师"></a>添加讲师</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/addTeacher</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-18"><a href="#请求参数-18" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>-</td>
<td>String</td>
<td>讲师姓名</td>
<td>是</td>
</tr>
<tr>
<td>college</td>
<td>-</td>
<td>String</td>
<td>所属单位</td>
<td>是</td>
</tr>
<tr>
<td>intro</td>
<td>-</td>
<td>String</td>
<td>老师简介</td>
<td>是</td>
</tr>
<tr>
<td>phone</td>
<td>-</td>
<td>String</td>
<td>手机号码</td>
<td>是</td>
</tr>
<tr>
<td>office</td>
<td>-</td>
<td>String</td>
<td>办公地址</td>
<td>是</td>
</tr>
<tr>
<td>email</td>
<td>-</td>
<td>String</td>
<td>邮箱地址</td>
<td>是</td>
</tr>
<tr>
<td>imageUrl</td>
<td>-</td>
<td>String</td>
<td>导师照片 url</td>
<td>是</td>
</tr>
<tr>
<td>teachForm</td>
<td>-</td>
<td>String</td>
<td>授课形式</td>
<td>是</td>
</tr>
<tr>
<td>teachTopic</td>
<td>-</td>
<td>Array</td>
<td>该讲师授课专题的数组</td>
<td>是</td>
</tr>
<tr>
<td></td>
<td>-</td>
<td>String</td>
<td>授课专题</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-18"><a href="#返回参数-18" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>Number</td>
<td>该讲师在数据库的唯一标识</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="修改讲师信息"><a href="#修改讲师信息" class="headerlink" title="修改讲师信息"></a>修改讲师信息</h3><div class="table-container">
<table>
<thead>
<tr>
<th>method</th>
<th>url</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>POST</td>
<td>/updateTeacher</td>
<td>私有接口</td>
</tr>
</tbody>
</table>
</div>
<h4 id="请求参数-19"><a href="#请求参数-19" class="headerlink" title="请求参数"></a>请求参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>字段</th>
<th>类型</th>
<th>说明</th>
<th>必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>-</td>
<td>Number</td>
<td>要修改的讲师的 id</td>
<td>是</td>
</tr>
<tr>
<td>name</td>
<td>-</td>
<td>String</td>
<td>讲师姓名</td>
<td>是</td>
</tr>
<tr>
<td>college</td>
<td>-</td>
<td>String</td>
<td>所属单位</td>
<td>是</td>
</tr>
<tr>
<td>intro</td>
<td>-</td>
<td>String</td>
<td>老师简介</td>
<td>是</td>
</tr>
<tr>
<td>phone</td>
<td>-</td>
<td>String</td>
<td>手机号码</td>
<td>是</td>
</tr>
<tr>
<td>office</td>
<td>-</td>
<td>String</td>
<td>办公地址</td>
<td>是</td>
</tr>
<tr>
<td>email</td>
<td>-</td>
<td>String</td>
<td>邮箱地址</td>
<td>是</td>
</tr>
<tr>
<td>imageUrl</td>
<td>-</td>
<td>String</td>
<td>导师照片 url</td>
<td>是</td>
</tr>
<tr>
<td>teachForm</td>
<td>-</td>
<td>String</td>
<td>授课形式</td>
<td>是</td>
</tr>
<tr>
<td>teachTopic</td>
<td>-</td>
<td>Array</td>
<td>该讲师授课专题的数组</td>
<td>是</td>
</tr>
<tr>
<td></td>
<td>-</td>
<td>String</td>
<td>授课专题</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h4 id="返回参数-19"><a href="#返回参数-19" class="headerlink" title="返回参数"></a>返回参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>message</td>
<td>String</td>
<td>修改结果</td>
</tr>
</tbody>
</table>
</div>
<h4 id="错误代码-6"><a href="#错误代码-6" class="headerlink" title="错误代码"></a>错误代码</h4><div class="table-container">
<table>
<thead>
<tr>
<th>code</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>3001</td>
<td>指定 id 的老师不存在</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>API</tag>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title>d2l学习笔记</title>
    <url>/2020/08/25/d2l/</url>
    <content><![CDATA[<p><a href="https://github.com/d2l-ai/d2l-en" target="_blank" rel="noopener">Dive into Deep Learning</a>学习笔记</p>
<a id="more"></a>
<h1 id="d2l学习记录"><a href="#d2l学习记录" class="headerlink" title="d2l学习记录"></a>d2l学习记录</h1><h2 id="第二章-Preliminaries"><a href="#第二章-Preliminaries" class="headerlink" title="第二章 Preliminaries"></a>第二章 Preliminaries</h2><ol>
<li>用with torch.no_grad()将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。</li>
<li>grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。</li>
<li>在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor。</li>
<li>view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。</li>
<li>在测试模型时，我们为了拿到更加确定性的结果，一般需要关闭dropout。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> isinstance(net, torch.nn.Module):</span><br><span class="line">            net.eval() <span class="comment"># 评估模式, 这会关闭dropout</span></span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            net.train() <span class="comment"># 改回训练模式</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="第三章-Linear-Neural-Networks"><a href="#第三章-Linear-Neural-Networks" class="headerlink" title="第三章 Linear Neural Networks"></a>第三章 Linear Neural Networks</h2><h3 id="cross-entropy-loss"><a href="#cross-entropy-loss" class="headerlink" title="cross-entropy loss"></a>cross-entropy loss</h3><p>给定含n个样本的数据集$\{X,Y\}$，我们可以通过将模型的预测标签与实际标签进行比较来查看模型的效果，即最大化$P(Y|X)=\prod_{i=1}^nP(y^{(i)}|x^{(i)})$，根据最大似然准则，这等同于最小化负的对数似然函数：</p>
<script type="math/tex; mode=display">-\log P(Y|X)=\sum_{i=1}^{n}-\log P(y^{(i)}|x^{(i)})=\sum_{i=1}^{n}l(y^{(i)}|\hat{y}^{(i)})</script><p>假设数据集中共有q个类别，则给定真实标签$y$与模型预测标签$\hat{y}$，损失函数l为所谓的交叉熵：</p>
<script type="math/tex; mode=display">l(y,\hat{y})=-\sum^q_{j=1}y_j\log\hat{y_j}</script><p>代入softmax的表达式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
l(y,\hat{y})&= 
-\sum_{j=1}^qy_j\log \frac{\exp(o_j)}{\sum^q_{k=1}\exp(o_k)} \\
&= \sum^q_{j=1}y_j\log \sum^q_{k=1}\exp (o_k)-\sum^q_{k=1}y_j\log \exp (o_j) \\
&= \log \sum^q_{k=1}\exp (o_k)-\sum^q_{j=1}y_jo_j
\end{aligned}</script><p>softmax可以看作是一个单层神经网络：<br><img src="https://s1.ax1x.com/2020/09/13/wBP7xf.png" alt="wBP7xf.png"></p>
<p>将损失函数对任意一个$o_j$求导，得到：</p>
<script type="math/tex; mode=display">\partial_{o_j}l(y,\hat{y}) = \frac{\exp(o_j)}{\sum^q_{k=1}\exp(o_k)} - y_j = softmax(o)_j-y_j</script><p>这表示模型预测标签与真实标签之间的差距。在任意指数族模型中，对数似然函数的梯度都是这样的形式。</p>
<p>对交叉熵的形式做更深入的理解，给定一个分布p，信息论指出，该分布的熵为  </p>
<script type="math/tex; mode=display">H[p]=\sum_j-p(j)\log p(j)</script><p>给定一个需要压缩的数据流，如果数据流中的数据是完全一致的，则预测下一个数据和对数据进行压缩是非常简单的。当我们不能准确预测下一个数据j，预测结果与实际结果之间的差别会带来所谓的surprisal，用数学形式表达为$\log \frac{1}{P(j)}=-\log P(j)$。<br>The cross-entropy from p to q, denoted H(p,q), is the expected surprisal of an observer with subjective probabilities p.</p>
<h2 id="第五章-Deep-Learning-Computation"><a href="#第五章-Deep-Learning-Computation" class="headerlink" title="第五章 Deep Learning Computation"></a>第五章 Deep Learning Computation</h2><ol>
<li><p>如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(MyModel, self).__init__(**kwargs)</span><br><span class="line">        self.weight1 = nn.Parameter(torch.rand(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">        self.weight2 = torch.rand(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">n = MyModel()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> n.named_parameters():</span><br><span class="line">    print(name)</span><br><span class="line"></span><br><span class="line">weight1</span><br></pre></td></tr></table></figure>
</li>
<li><p>共享参数，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FancyMLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(FancyMLP, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="keyword">False</span>) <span class="comment"># 不可训练参数（常数参数）</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="comment"># 使用创建的常数参数，以及nn.functional中的relu函数和mm函数</span></span><br><span class="line">        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 复用全连接层。等价于两个全连接层共享参数</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="comment"># 控制流，这里我们需要调用item函数来返回标量进行比较</span></span><br><span class="line">        <span class="keyword">while</span> x.norm().item() &gt; <span class="number">1</span>:</span><br><span class="line">            x /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> x.norm().item() &lt; <span class="number">0.8</span>:</span><br><span class="line">            x *= <span class="number">10</span></span><br><span class="line">        <span class="keyword">return</span> x.sum()</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。所以在自定义含模型参数的层时，我们应该将参数定义成Parameter。</p>
</li>
<li>PyTorch中保存和加载训练模型推荐保存和加载state_dict。state_dict是一个从参数名称隐射到参数Tesnor的字典对象。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line">        self.output = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        a = self.act(self.hidden(x))</span><br><span class="line">        <span class="keyword">return</span> self.output(a)</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">net.state_dict()</span><br><span class="line"></span><br><span class="line">OrderedDict([(<span class="string">'hidden.weight'</span>, tensor([[ <span class="number">0.2448</span>,  <span class="number">0.1856</span>, <span class="number">-0.5678</span>],</span><br><span class="line">                      [ <span class="number">0.2030</span>, <span class="number">-0.2073</span>, <span class="number">-0.0104</span>]])),</span><br><span class="line">             (<span class="string">'hidden.bias'</span>, tensor([<span class="number">-0.3117</span>, <span class="number">-0.4232</span>])),</span><br><span class="line">             (<span class="string">'output.weight'</span>, tensor([[<span class="number">-0.4556</span>,  <span class="number">0.4084</span>]])),</span><br><span class="line">             (<span class="string">'output.bias'</span>, tensor([<span class="number">-0.3573</span>]))])</span><br><span class="line"><span class="comment"># 只有具有可学习参数的层(卷积层、线性层等)才有state_dict中的条目。优化器(optim)也有一个state_dict，其中包含关于优化器状态以及所使用的超参数的信息。</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="第八章-Recurrent-Neural-Networks"><a href="#第八章-Recurrent-Neural-Networks" class="headerlink" title="第八章 Recurrent Neural Networks"></a>第八章 Recurrent Neural Networks</h2><ol>
<li><p>scatter() 一般可以用来对标签进行 one-hot 编码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># self[i][ index[i][j][k] ][k] = src[i][j][k]  # if dim == 1</span></span><br><span class="line"></span><br><span class="line">class_num = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">label = torch.LongTensor(batch_size, <span class="number">1</span>).random_() % class_num</span><br><span class="line"><span class="comment">#tensor([[6],</span></span><br><span class="line"><span class="comment">#        [0],</span></span><br><span class="line"><span class="comment">#        [3],</span></span><br><span class="line"><span class="comment">#        [2]])</span></span><br><span class="line">torch.zeros(batch_size, class_num).scatter_(<span class="number">1</span>, label, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。</p>
</li>
<li>另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</li>
<li>torch.view等方法操作需要连续的Tensor，transpose、permute 操作虽然没有修改底层一维数组，但是新建了一份Tensor元信息，并在新的元信息中的 重新指定 stride。torch.view 方法约定了不修改数组本身，只是使用新的形状查看数据。如果我们在 transpose、permute 操作后执行 view，Pytorch 会报错。<blockquote>
<p>Tensor多维数组底层实现是使用一块连续内存的1维数组（行优先顺序存储，下文描述），Tensor在元信息里保存了多维数组的形状，在访问元素时，通过多维度索引转化成1维数组相对于数组起始位置的偏移量即可找到对应的数据。某些Tensor操作（如transpose、permute、narrow、expand）与原Tensor是共享内存中的数据，不会改变底层数组的存储，但原来在语义上相邻、内存里也相邻的元素在执行这样的操作后，在语义上相邻，但在内存不相邻，即不连续了（is not contiguous）。</p>
</blockquote>
</li>
</ol>
<h2 id="第十一章-Optimization-Algorithms"><a href="#第十一章-Optimization-Algorithms" class="headerlink" title="第十一章 Optimization Algorithms"></a>第十一章 Optimization Algorithms</h2><h3 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h3><p>如果同一位置上，目标函数在竖直方向($x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大，则给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。  </p>
<p>动量法看作是对最近$1/(1−γ)$个时间步的$x_t$值的加权平均，而且离当前时间步$t$越近的$x_t$值获得的权重越大。动量法在每个时间步的自变量更新量近似于将最近1/(1−γ)个时间步的普通更新量（即学习率乘以梯度）做了指数加权移动平均后再除以1−γ。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。</p>
<h3 id="AdaGrad和RMSProp"><a href="#AdaGrad和RMSProp" class="headerlink" title="AdaGrad和RMSProp"></a>AdaGrad和RMSProp</h3><p>如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$s_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。  </p>
<p>不同于AdaGrad算法里状态变量$s_t$是截至时间步$t$所有小批量随机梯度$g_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。$s_t$所以可以看作是最近1/(1−γ)个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。</p>
<h2 id="第十四章-自然语言处理"><a href="#第十四章-自然语言处理" class="headerlink" title="第十四章 自然语言处理"></a>第十四章 自然语言处理</h2><ol>
<li>求类比词问题可以定义为：对于类比关系中的4个词 $a:b::c:d$，给定前3个词$a、b和c，求d$。设词$w$的词向量为$vec(w)$，求类比词的思路是，搜索与$vec(c)+vec(b)-vec(a)$的结果向量最相似的词向量。</li>
</ol>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Jupyter Notebook 的快捷键</title>
    <url>/2018/07/24/jupyter%20notebook%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
    <content><![CDATA[<p>Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。<br><a id="more"></a></p>
<h3 id="命令模式-按键-Esc-开启"><a href="#命令模式-按键-Esc-开启" class="headerlink" title="命令模式 (按键 Esc 开启)"></a>命令模式 (按键 Esc 开启)</h3><ul>
<li><strong>Enter</strong> : 转入编辑模式</li>
<li><strong>Shift-Enter</strong> : 运行本单元，选中下个单元</li>
<li><strong>Ctrl-Enter</strong> : 运行本单元</li>
<li><strong>Alt-Enter</strong> : 运行本单元，在其下插入新单元</li>
<li><strong>Y</strong> : 单元转入代码状态</li>
<li><strong>M</strong> :单元转入markdown状态</li>
<li><strong>R</strong> : 单元转入raw状态</li>
<li><strong>1</strong> : 设定 1 级标题</li>
<li><strong>2</strong> : 设定 2 级标题</li>
<li><strong>3</strong> : 设定 3 级标题</li>
<li><strong>4</strong> : 设定 4 级标题</li>
<li><strong>5</strong> : 设定 5 级标题</li>
<li><strong>6</strong> : 设定 6 级标题</li>
<li><strong>Up</strong> : 选中上方单元</li>
<li><strong>K</strong> : 选中上方单元</li>
<li><strong>Down</strong> : 选中下方单元</li>
<li><strong>J</strong> : 选中下方单元</li>
<li><strong>Shift-K</strong> : 扩大选中上方单元</li>
<li><strong>Shift-J</strong> : 扩大选中下方单元</li>
<li><strong>A</strong> : 在上方插入新单元</li>
<li><strong>B</strong> : 在下方插入新单元</li>
<li><strong>X</strong> : 剪切选中的单元</li>
<li><strong>C</strong> : 复制选中的单元</li>
<li><strong>Shift-V</strong> : 粘贴到上方单元</li>
<li><strong>V</strong> : 粘贴到下方单元</li>
<li><strong>Z</strong> : 恢复删除的最后一个单元</li>
<li><strong>D,D</strong> : 删除选中的单元</li>
<li><strong>Shift-M</strong> : 合并选中的单元</li>
<li><strong>Ctrl-S</strong> : 文件存盘</li>
<li><strong>S</strong> : 文件存盘</li>
<li><strong>L</strong> : 转换行号</li>
<li><strong>O</strong> : 转换输出</li>
<li><strong>Shift-O</strong> : 转换输出滚动</li>
<li><strong>Esc</strong> : 关闭页面</li>
<li><strong>Q</strong> : 关闭页面</li>
<li><strong>H</strong> : 显示快捷键帮助</li>
<li><strong>I,I</strong> : 中断Notebook内核</li>
<li><strong>0,0</strong> : 重启Notebook内核</li>
<li><strong>Shift</strong> : 忽略</li>
<li><strong>Shift-Space</strong> : 向上滚动</li>
<li><strong>Space</strong> : 向下滚动</li>
</ul>
<h3 id="编辑模式-Enter-键启动"><a href="#编辑模式-Enter-键启动" class="headerlink" title="编辑模式 ( Enter 键启动)"></a>编辑模式 ( Enter 键启动)</h3><ul>
<li><strong>Tab</strong> : 代码补全或缩进</li>
<li><strong>Shift-Tab</strong> : 提示</li>
<li><strong>Ctrl-]</strong> : 缩进</li>
<li><strong>Ctrl-[</strong> : 解除缩进</li>
<li><strong>Ctrl-A</strong> : 全选</li>
<li><strong>Ctrl-Z</strong> : 复原</li>
<li><strong>Ctrl-Shift-Z</strong> : 再做</li>
<li><strong>Ctrl-Y</strong> : 再做</li>
<li><strong>Ctrl-Home</strong> : 跳到单元开头</li>
<li><strong>Ctrl-Up</strong> : 跳到单元开头</li>
<li><strong>Ctrl-End</strong> : 跳到单元末尾</li>
<li><strong>Ctrl-Down</strong> : 跳到单元末尾</li>
<li><strong>Ctrl-Left</strong> : 跳到左边一个字首</li>
<li><strong>Ctrl-Right</strong> : 跳到右边一个字首</li>
<li><strong>Ctrl-Backspace</strong> : 删除前面一个字</li>
<li><strong>Ctrl-Delete</strong> : 删除后面一个字</li>
<li><strong>Esc</strong> : 进入命令模式</li>
<li><strong>Ctrl-M</strong> : 进入命令模式</li>
<li><strong>Shift-Enter</strong> : 运行本单元，选中下一单元</li>
<li><strong>Ctrl-Enter</strong> : 运行本单元</li>
<li><strong>Alt-Enter</strong> : 运行本单元，在下面插入一单元</li>
<li><strong>Ctrl-Shift—</strong> : 分割单元</li>
<li><strong>Ctrl-Shift-Subtract</strong> : 分割单元</li>
<li><strong>Ctrl-S</strong> : 文件存盘</li>
<li><strong>Shift</strong> : 忽略</li>
<li><strong>Up</strong> : 光标上移或转入上一单元</li>
<li><strong>Down</strong> :光标下移或转入下一单元</li>
</ul>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Variational Graph Auto-Encoders[NIPS&#39;16]</title>
    <url>/2021/01/09/VGAE%5BNIPS16%5D/</url>
    <content><![CDATA[<p>NIPS16一篇将变分自编码器迁移到图结构数据上的论文</p>
<a id="more"></a>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在图结构数据上如何使用变分自编码器</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>将已知的图进行编码（图卷积）得到图中顶点向量表示的一个分布，在分布中采样得到顶点的向量表示，然后进行解码重新构建图。</p>
<h4 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h4><p>因为这篇论文做的是一个迁移的工作，变分自编码器的背景对于理解这篇论文来说十分重要，首先进行介绍。</p>
<p>变分自编码器是自编码器的一种，一个自编码器由编码器和解码器构成，编码器将输入数据转换为低维向量表示，解码器通过得到的低维向量表示进行重构。</p>
<div align="center">
<a href="https://imgchr.com/i/sl9ZxP" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9ZxP.jpg" alt="sl9ZxP.jpg" border="0" width="80%"></a>
<a href="https://imgchr.com/i/sl9G2q" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9G2q.jpg" alt="sl9G2q.jpg" border="0" width="65%"></a>
</div>
这种结构的不足之处在于，只能产生与输入数据相似的样本，而无法产生新的样本，低维向量表示必须是有真实样本通过编码器得到的，随机产生的低维向量经过重构几乎不可能得到近似真实的样本。而变分自编码器可以解决这个问题。

变分自编码器将输入数据编码为一个分布，而不是一个个低维向量表示，然后从这个分布中随机采样来得到低维向量表示。一般假设这个分布为正态分布，因此编码器的任务就是从输入数据中得到均值$\mu$与方差$\sigma^2$。

<div align="center">
<a href="https://imgchr.com/i/slCW60" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slCW60.jpg" alt="slCW60.jpg" border="0" width="80%"></a>
<a href="https://imgchr.com/i/slPZB8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPZB8.jpg" alt="slPZB8.jpg" border="0" width="80%"></a>
</div>
然而，如果是将所有输入数据编码到同一个分布里，从这个分布中随机采样的样本$Z_i$无法与输入样本$X_i$一一对应，会影响模型的学习效果。所以，实际的变分自编码器结构如下图所示，为每一个输入样本学习一个正态分布：

<div align="center">
<a href="https://imgchr.com/i/slPgED" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPgED.jpg" alt="slPgED.jpg" border="0" width="80%"></a>
</div>
采样时常用"重参数"技巧(reparameterization trick)，从分布$N(\mu,\sigma^2)$中采样一个$Z$相当于从$N(0,1)$中采样一个$\epsilon$使得$Z=\mu+\sigma*\epsilon$。  

#### 图变分自编码器

介绍完传统的变分自编码器，接下来就是介绍这篇论文的工作，如何将变分自编码器的思想迁移到图上。

针对图这个数据结构，输入的数据变为图的邻接矩阵$A$与特征矩阵$X$：  
邻接矩阵$A$：

<div align="center">
<a href="https://imgchr.com/i/slFHhQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFHhQ.jpg" alt="slFHhQ.jpg" border="0" width="60%"></a>
</div>
特征矩阵$X$：
<div align="center">
<a href="https://imgchr.com/i/slFz7T" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFz7T.jpg" alt="slFz7T.jpg" border="0" width="60%"></a>
</div>

<p>接下来的工作与变分自编码器相同，通过编码器（图卷积）学习图中顶点低维向量表示分布的均值$\mu$与方差$\sigma^2$，再通过解码器生成图。</p>
<div align="center">
<a href="https://imgchr.com/i/slk1gA" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slk1gA.jpg" alt="slk1gA.jpg" border="0" width="80%"></a>
</div>

<p>编码器采用两层结构的图卷积网络，第一层产生一个低维的特征矩阵：</p>
<script type="math/tex; mode=display">
\bar{X}=\text{GCN}(X,A)=\text{ReLU}(\tilde{A}XW_0)\\
\tilde{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>第二层得到分布的均值$\mu$与方差$\sigma^2$：</p>
<script type="math/tex; mode=display">
\mu=\text{GCN}_{\mu}(X,A)=\tilde{A}\bar{X}W_1\\
\log\sigma^2=\text{GCN}_{\sigma}(X,A)=\tilde{A}\bar{X}W_1</script><p>将两层网络的表达式合并可以得到编码器的表达式：</p>
<script type="math/tex; mode=display">
\text{GCN}(X,A)=\tilde{A}\text{ReLU}(\tilde{A}XW_0)W_1</script><p>同样地使用重参数技巧来得到低维向量表示$Z=\mu+\sigma*\epsilon$。</p>
<p>编码器重构出图的邻接矩阵，从而得到一个新的图。之所以使用点积的形式来得到邻接矩阵，原因在于我们希望学习到每个顶点的低维向量表示$z$的相似程度，来更好地重构邻接矩阵。而点积可以计算两个向量之间的cosine相似度，这种距离度量方式不受量纲的影响。因此，重构的邻接矩阵可以学习到各个顶点之间的相似程度。</p>
<script type="math/tex; mode=display">
\hat{A}=\sigma(zz^T)</script><p>损失函数用于衡量生草样本与真是样本之间的差异，但如果只用距离度量作为损失函数，为了让编码器的效果最佳，模型会将方差的值学为0，这样从正态分布中采样出来的就是定值，有利于减小生成样本和真实样本之间的差异。但这样一来，就退化成了普通的自编码器，因此在构建损失函数时，往往还会加入各独立正态分布与标准正态分布的KL散度，来使得各个正态分布逼近标准正态分布：</p>
<script type="math/tex; mode=display">
L=E_{q(Z|X,A)}[\log p(A|Z)]-\text{KL}[q(Z|X,A)||p(Z)],\quad where\quad p(Z)=N(0,1)</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>
]]></content>
      <tags>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2018/04/05/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>word转pdf图片避免压缩</title>
    <url>/2019/08/20/pdf/</url>
    <content><![CDATA[<p>平时生活中经常会需要将word转换成pdf，比如个人简历，然而直接使用word的另存为pdf的话，文字部分虽然清晰，但会造成图像的失真。<br><a id="more"></a><br>如果是简历的话自己的照片就糊成一团了，这第一印象就不好了，即使是用网上所说的调整什么压缩选项也没用，该失真还是失真，今天记录一种可行的方法，需要用到Adobe pdf软件，相信很多人都有安装。</p>
<ul>
<li>首先，在编辑好的word中点击文件-打印，打印机选择Adobe pdf：</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/08/20/mGKRf0.png" alt="adobe pdf"></p>
<ul>
<li>接着，点击打印机属性，默认设置这里选择下拉菜单的第一项印刷质量，然后点击编辑：</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/08/20/mGMVc8.png" alt="印刷质量"></p>
<ul>
<li>左边列表中选择“图像”，然后将“彩色图像”这一区域的采样和压缩选项关闭：</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/08/20/mGMsgK.png" alt="关闭采样和压缩">这里的300可以自己设置，数值越大图像质量越好，当然转换的pdf文件也就越大</p>
<ul>
<li>可以选择左下角的另存为保存设置，方便下次使用，或者直接点击确定，也会提示你进行保存：</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/08/20/mG1E1x.png" alt="另存为"></p>
<ul>
<li><p>保存完后，回到刚刚的窗口，在默认设置中选择刚刚保存的设置，点击确定，然后在打印页面选择打印就可以了，这时候输出的pdf图像就不会失真</p>
</li>
<li><p>虽然图像不会失真，但是pdf文件往往也会变大了很多，不便于邮件发送和查看，这时候推荐一个pdf压缩的网站：<a href="https://www.ilovepdf.com/zh-cn" target="_blank" rel="noopener">ilovepdf</a>，对自己的pdf进行压缩就可以了</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库设计</title>
    <url>/2019/04/10/database/</url>
    <content><![CDATA[<h2 id="数据库设计"><a href="#数据库设计" class="headerlink" title="数据库设计"></a>数据库设计</h2><ul>
<li>以下所有表默认自带一个自增 <code>id</code></li>
<li>以下所有表默认自带 <code>created_at</code> 和 <code>updated_at</code> 两个字段</li>
<li>为了方便查询，以下所有下划线命名法在实际设计中可能全部转为驼峰命名法<a id="more"></a>
</li>
</ul>
<h3 id="用户信息"><a href="#用户信息" class="headerlink" title="用户信息"></a>用户信息</h3><h4 id="manager"><a href="#manager" class="headerlink" title="manager"></a>manager</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>varchar</td>
<td>姓名</td>
<td>not null</td>
</tr>
<tr>
<td>card_id</td>
<td>varchar</td>
<td>校园卡号</td>
<td>not null</td>
</tr>
<tr>
<td>authorizerId</td>
<td>int</td>
<td>授权人 id</td>
<td>外键，引用自 manager 表的 id 属性</td>
</tr>
<tr>
<td>privilege</td>
<td>tinyint</td>
<td>0 为超级管理员，1为普通管理员，其余待定</td>
<td>not null</td>
</tr>
</tbody>
</table>
</div>
<h4 id="student"><a href="#student" class="headerlink" title="student"></a>student</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>card_id</td>
<td>varchar</td>
<td>校园卡号</td>
<td>无</td>
</tr>
<tr>
<td>stu_id</td>
<td>varchar</td>
<td>学号</td>
<td>primary key</td>
</tr>
<tr>
<td>name</td>
<td>varchar</td>
<td>姓名</td>
<td>not null</td>
</tr>
<tr>
<td>college</td>
<td>varchar</td>
<td>学院</td>
<td>无</td>
</tr>
</tbody>
</table>
</div>
<h4 id="teacher"><a href="#teacher" class="headerlink" title="teacher"></a>teacher</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>varchar</td>
<td>讲师名称</td>
<td>not null</td>
</tr>
<tr>
<td>college</td>
<td>varchar</td>
<td>所属单位</td>
<td>not null</td>
</tr>
<tr>
<td>intro</td>
<td>text</td>
<td>老师简介</td>
<td>无</td>
</tr>
<tr>
<td>phone</td>
<td>varchar</td>
<td>手机号码</td>
<td>无</td>
</tr>
<tr>
<td>office</td>
<td>varchar</td>
<td>办公地址</td>
<td>无</td>
</tr>
<tr>
<td>email</td>
<td>varchar</td>
<td>邮箱</td>
<td>无</td>
</tr>
<tr>
<td>image_url</td>
<td>varchar</td>
<td>导师照片 url</td>
<td>无</td>
</tr>
<tr>
<td>teach_form</td>
<td>varchar</td>
<td>授课形式</td>
<td>无</td>
</tr>
</tbody>
</table>
</div>
<h4 id="teach-topic"><a href="#teach-topic" class="headerlink" title="teach_topic"></a>teach_topic</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>teacher_id</td>
<td>int</td>
<td>讲师 id</td>
<td>外键，引用自 teacher 表的 id 属性</td>
</tr>
<tr>
<td>topic</td>
<td>varchar</td>
<td>授课专题</td>
<td>not null</td>
</tr>
</tbody>
</table>
</div>
<h3 id="课程信息"><a href="#课程信息" class="headerlink" title="课程信息"></a>课程信息</h3><h4 id="course"><a href="#course" class="headerlink" title="course"></a>course</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>course_name</td>
<td>varchar</td>
<td>课程名称</td>
<td>not null</td>
</tr>
<tr>
<td>teacher_id</td>
<td>int</td>
<td>讲师 id</td>
<td>外键，引用自 teacher 表的 id 属性</td>
</tr>
<tr>
<td>start_time</td>
<td>datetime</td>
<td>上课开始时间</td>
<td>not null</td>
</tr>
<tr>
<td>end_time</td>
<td>datetime</td>
<td>上课结束时间</td>
<td>not null</td>
</tr>
<tr>
<td>course_id</td>
<td>varchar</td>
<td>课程编号</td>
<td>primary key</td>
</tr>
<tr>
<td>location</td>
<td>varchar</td>
<td>上课地址</td>
<td>not null</td>
</tr>
</tbody>
</table>
</div>
<h4 id="course-student"><a href="#course-student" class="headerlink" title="course_student"></a>course_student</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>course_id</td>
<td>varchar</td>
<td>课程编号</td>
<td>外键，引用自 course 表的 course_id 属性</td>
</tr>
<tr>
<td>stu_id</td>
<td>varchar</td>
<td>学生学号</td>
<td>外键，引用自 student 表的 stu_id 属性</td>
</tr>
</tbody>
</table>
</div>
<h3 id="评价模板"><a href="#评价模板" class="headerlink" title="评价模板"></a>评价模板</h3><h4 id="comment-template"><a href="#comment-template" class="headerlink" title="comment_template"></a>comment_template</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>template_name</td>
<td>varchar</td>
<td>模板名称</td>
<td>not null</td>
</tr>
</tbody>
</table>
</div>
<h4 id="template-question"><a href="#template-question" class="headerlink" title="template_question"></a>template_question</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>template_id</td>
<td>int</td>
<td>模板 id</td>
<td>外键，引用自 comment_template 表的 id 属性</td>
</tr>
<tr>
<td>question</td>
<td>text</td>
<td>问题</td>
<td>not null</td>
</tr>
<tr>
<td>_type</td>
<td>tinyint</td>
<td>问题类型，0为打分题，1为问答题，其余待定</td>
<td>not null</td>
</tr>
</tbody>
</table>
</div>
<h3 id="评价信息"><a href="#评价信息" class="headerlink" title="评价信息"></a>评价信息</h3><h4 id="course-comment"><a href="#course-comment" class="headerlink" title="course_comment"></a>course_comment</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>course_id</td>
<td>varchar</td>
<td>课程编号</td>
<td>外键，引用自 course 表的 course_id 属性</td>
</tr>
<tr>
<td>template_id</td>
<td>int</td>
<td>模板 id</td>
<td>外键，引用自 comment_template 表的 id 属性</td>
</tr>
</tbody>
</table>
</div>
<h4 id="comment"><a href="#comment" class="headerlink" title="comment"></a>comment</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>course_id</td>
<td>varchar</td>
<td>课程编号</td>
<td>外键，引用自 course 表的 course_id 属性</td>
</tr>
<tr>
<td>stu_id</td>
<td>varchar</td>
<td>学生学号</td>
<td>外键，引用自 student 表的 stu_id 属性</td>
</tr>
<tr>
<td>star</td>
<td>tinyint</td>
<td>1为精选评论</td>
<td>无</td>
</tr>
</tbody>
</table>
</div>
<h4 id="comment-result"><a href="#comment-result" class="headerlink" title="comment_result"></a>comment_result</h4><div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>描述</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>comment_id</td>
<td>int</td>
<td>评价 id</td>
<td>外键，引用自 comment 表的 id 属性</td>
</tr>
<tr>
<td>question_id</td>
<td>int</td>
<td>问题 id</td>
<td>外键，引用自 template_question 表的 id 属性</td>
</tr>
<tr>
<td>result</td>
<td>text</td>
<td>问题结果，根据问题类型来决定存储值类型</td>
<td>not null</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>imputing structured missing values in spatial data with clsutered adversarial matrix factorization</title>
    <url>/2019/04/12/imputing%20structured%20missing%20values%20in%20spatial%20data%20with%20clsutered%20adversarial%20matrix%20factorization/</url>
    <content><![CDATA[<font size="5">
一种基于对抗模型用于补全带有结构性缺失信息的空间数据的矩阵分解技术
</font>

<a id="more"></a>
<p>&#8194; </p>
<font size="3">
摘要：在数据分析时，缺失的数据总是会成为一个重大的挑战，因为它带来了不确定性。在许多领域中，矩阵补全技术有着出色的表现。然而，在特定的空间数据集如地理坐标点时，这种传统的矩阵补全技术有着两个主要的限制：第一，这些方法往往假设缺失的数据是随机产生的，而这种假设对于空间数据集可能并不总是成立；第二，它们可能无法运用这些空间数据集中的结构信息。为了解决这些局限性，本论文提出了一种利用先验结构信息和生成对抗模型的矩阵分解技术。这个模型使用一个对抗网络通过学习数据集的概率分布来改善补全的结果。
</font>

<font size="3">关键词：缺失数据估计；深度对抗网络；空间数据</font>

<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&#8194; 很多现实生活中的应用容易面临数据缺失的问题。而对于空间数据集，造成这种情况的原因有很多种。例如，在森林监测中，因为收集成本的原因，数据的缺失很普遍。【3】过去十年许多针对数据补全的工作在开展，从基本的统计方法到复杂的模型使用。后者的典例低秩矩阵补全技术为许多领域如推荐系统或图像重构带来了可观的改善【9】。这些方法通过发现并利用数据矩阵的低秩属性来建立已有值与缺失值的联系。而在这些矩阵补全方法里，矩阵分解是最为常用的方法之一，它将输入的数据矩阵分解成两个低秩矩阵的乘积，称为“特征因子”，接着通过最小化这个乘积与已有值的误差来学习这两个特征因子，随后利用它们来补全缺失的信息。【18】其它方法还有如带门槛的矩阵奇异值分解，核心思想是迭代地使用截断奇异值分解来补全。</p>
<p>&#8194; 这些矩阵补全的方法，往往假设缺失的数据是随机产生的。【2】然而，这个假设在空间数据上可能并不成立，因为它往往带有空间结构。例如，一项针对加拿大青少年的研究指出，家庭收入这一栏数据空缺的青少年有更低的可能性居住在富人区。【15】因此，当数据并不是随机缺失时，只是单纯地最小化两个特征因子的乘积与已有值的误差并不能保证补全数据的有效性。</p>
<p>&#8194; 而另一个限制则是这些方法无法把数据集里的结构信息利用起来。而在补全缺失的空间数据时，这些结构信息格外重要。【12】例如，淡水湖数据就有强烈的空间结构，因为相邻的湖泊往往有相似的降水量等。【17】如果这些结构信息能够被一个矩阵补全的方法利用起来，它可以显著地提升结果，因为这些结构信息代表了一个子空间，在这个子空间里，不同湖泊之间相似的信息互相传递。</p>
<p>&#8194; 因为为了解决这两点局限，我们提出了一种利用先验结构信息和生成对抗模型的矩阵分解技术。这个框架找到一个低维的子空间来与数据中的结构信息相符合，因此可以利用同一类中其它数据点的信息来补全某一点的缺失值。而且，估计值的概率分布也尽可能的与已有值相似，这么做的好处是它把缺失值与已有值连接起来了。如果估计值与实际值偏差太大，那么它出现的概率应该很小。然而，实际数据的概率分布往往是未知的，因此我们借鉴了生成对抗网络的思想引入了一个判别器来区分估计值与已有值。我们在合成数据集与显示数据集上均做了实验，来说明这个框架的有效性。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>&#8194; 截断奇异值算法是近年来使用频率较高的一个方法，它在数据矩阵中迭代的使用截断奇异值分解接着通过保持一个较小的奇异值重构整个矩阵。矩阵分解是另一项常用的技术，关于它的过程前面部分已有讲述。</p>
<p>&#8194; 生成对抗网络（GAN）被广泛地用于生成图像【5】【16】。在【8】【14】中，作者提出了一个想法，利用GAN的思想和整幅图片的结构来推测一幅图片中随机缺失的像素。虽然这个想法在这类问题上效果较好，但它补全图片时是将每张图片看成一个个独立的个体，而空间数据与此相反，它们之间有着强烈的依赖性。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="A-矩阵分解引入"><a href="#A-矩阵分解引入" class="headerlink" title="A.矩阵分解引入"></a>A.矩阵分解引入</h3><p>&#8194; 矩阵分解在推荐系统中十分常用，例如如下的一个评分矩阵，列为用户，行为物品，矩阵中的值为用户对物品的评分，如电影和书籍。现实情况中这个评分矩阵往往很稀疏，许多物品上缺少用户的评分，而推荐系统就是要预估用户在某个物品上的评分来判断用户对它的倾向程度，从而进行推荐。</p>
<p><img src="https://s2.ax1x.com/2019/04/13/AqRtk4.png" alt=""></p>
<p>&#8194; 矩阵分解的方法是将原始评分矩阵$R^{m\times n}$分解成两个矩阵$P^{m\times k}$和$Q^{k\times n}$，根据评分矩阵中已有的值来判断分解是否准确，而判别标准常用均方差。如图所示。</p>
<p><img src="https://s2.ax1x.com/2019/04/13/AqRJ7F.png" alt=""></p>
<p>&#8194; 分解后的矩阵P和Q可以称为特征因子（latent factor），其中要求分解后$k&lt;&lt;min(m,n)$，即低秩要求，因为如果输入矩阵满秩，则各元素行之间线性无关，如果有线性相关关系，则某个元素行可以通过其他行的线性组合表示，相当于引入了冗余的信息，这样就可以将矩阵投影到更低维的空间，只保留非冗余信息，同时冗余信息可以用来对缺失值进行补全。</p>
<p>&#8194; 矩阵分解的直观意义为，找出矩阵中的潜在特征，如图2中假设特征为3，特征可以是书籍作者、类型等等，而矩阵P表示用户对某个特征的喜爱程度，而矩阵Q表示某个物品与该特征的关联程度。</p>
<h3 id="B-低秩补全"><a href="#B-低秩补全" class="headerlink" title="B.低秩补全"></a>B.低秩补全</h3><p>&#8194; 给定一个带有缺失值的矩阵，矩阵补全技术旨在通过已有值的某种潜在的结构来对缺失值进行估计补全。一个常用的潜在结构是矩阵的低秩性，因为它可以将该矩阵投影到一个去除冗余信息的子空间中，低秩意味着矩阵中的值存在线性关系，因此某些值可以通过另外的值来线性表示，如同坐标系中的基底一样。在这类方法中有凸也有非凸的技术。凸方法通过对矩阵迹的约束来保证具有良好理论性的全局最优结果，而诸如矩阵分解的非凸方法进行局部搜索过程并提供更大的灵活性和效率。给定一个矩阵$X\in R^{d\times n}$，n代表样本个数，d代表特征维度，矩阵分解技术通过将X分解为两个矩阵U和V，$U\in R^{d\times n}$，$V\in R^{r\times n}$要求$r &lt; min(d, n)$；U和V的求解可以通过对下列式子运用块坐标下降法求得：</p>
<p><img src="https://s2.ax1x.com/2019/04/13/AqRG0U.png" alt=""></p>
<p>&#8194; $\bigodot$代表哈德蒙德内积（即矩阵各元素相乘），M矩阵的大小同X一致，如果$X_{ij}$有值则$M_{ij}$为1，否则为0。局部解用$U^<em>、V^</em>$表示，因此，它们可以通过如下的式子来重构矩阵X：</p>
<p><img src="https://s2.ax1x.com/2019/04/13/AqR8mT.png" alt=""></p>
<p>&#8194; 矩阵分解在推荐系统中使用较为普遍，它用来估计一个用户在某项新物品上的评分。</p>
<p>&#8194; 将此方法应用于空间数据集时，矩阵分解不会包含有关数据集中空间聚类结构的先验知识信息。 然而，这些先验知识通常有助于发现需要的子空间。此外，对于结构化缺失值问题，经典矩阵补全提供较差的结果，因为缺失值不是随机的。</p>
<h3 id="C-聚类对抗式矩阵分解"><a href="#C-聚类对抗式矩阵分解" class="headerlink" title="C.聚类对抗式矩阵分解"></a>C.聚类对抗式矩阵分解</h3><p>&#8194; 为了解决上一小节中提到的矩阵分解的两个局限性，我们提出了一种新的聚类对抗矩阵分解框架。 在我们的框架中，我们找到整个样本的聚类信息，并将补全值的概率分布与已有值的分布靠近，以得到可靠的补全结果。X为输入矩阵，每一列代表一个数据样本，有些数据点有完整的特征信息，而某些数据点以结构性缺失了某些特征信息。我们将输入矩阵中完整部分记为$X_n$，而缺失部分记为$X_m$。我们假设每个数据点都符合某个概率分布$p_{data(x)}$。接下来的公式中包含两个部分：矩阵重构以及概率分布近似。</p>
<h4 id="矩阵重构"><a href="#矩阵重构" class="headerlink" title="矩阵重构"></a>矩阵重构</h4><p>&#8194; 为了利用数据的低秩属性和空间聚类结构，我们决定在矩阵分解中使用l2聚类项：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzcSM9.png" alt=""></p>
<p>&#8194; 式中$v_i$代表矩阵V中第i列，$r_1,r_2,r_3$均为正则化参数，第二项和第三项加的约束是为了防止过拟合，最后一项则用来引入空间数据集中的聚类结构信息。$d_{ij}$是第i个样本和第j个样本的相似度，它可以手动设置，原则为：当$v_i$和$v_j$很靠近即在同一类时，将$d_{ij}$的值设置得较大，反之较小。$r_3$用来调整结构信息在重构时所占的比重，如果$r_3$较大，则对一个样本的缺失数据进行估计补全时会更多的参考同一类其它数据点的信息。因此当$r_3$为0时，结构信息将不被使用，这也使得这个式子变成了常规的矩阵分解方法。而对于UV矩阵直观的理解为，U为特征因子，而V为样本因子，如同推荐系统里的用户因子和物品因子，两者相互独立。</p>
<h4 id="生成对抗网络思想（GAN）"><a href="#生成对抗网络思想（GAN）" class="headerlink" title="生成对抗网络思想（GAN）"></a>生成对抗网络思想（GAN）</h4><p>&#8194; 在继续讲到使用概率分布近似来优化前，先引入生成对抗网络的基本思想加深理解。GAN的非常的直观，就是生成器和判别器两个极大极小的博弈。</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzcULn.png" alt=""></p>
<p>GAN的目标函数为：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzctMj.png" alt=""></p>
<p>&#8194; 从判别器D的角度看，它希望自己能尽可能区分真实样本和虚假样本，因此希望 D(x)尽可能大，D(G(z))尽可能小，即 V(D,G)尽可能大。从生成器G的角度看，它希望自己尽可能骗过D，也就是希望 D(G(z))尽可能大，即 V(D,G)V(D,G) 尽可能小。两个模型相对抗，最后达到全局最优。</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzcNss.png" alt=""></p>
<p>&#8194; 图中，黑色曲线是真实样本的概率分布函数，绿色曲线是虚假样本的概率分布函数，蓝色曲线是判别器D的输出，它的值越大表示这个样本越有可能是真实样本。最下方的平行线是噪声z，它映射到了x。</p>
<p>&#8194; 一开始， 虽然 G(z)和x是在同一个特征空间里的，但它们分布的差异很大，这时，虽然鉴别真实样本和虚假样本的模型 D性能也不强，但它很容易就能把两者区分开来，而随着训练的推进，虚假样本的分布逐渐与真实样本重合，D虽然也在不断更新，但也已经力不从心了。</p>
<p>&#8194; 最后，黑线和绿线最后几乎重合，模型达到了最优状态，这时 判别器的输出对于任意样本都是 0.5。</p>
<h4 id="GAN的最优化"><a href="#GAN的最优化" class="headerlink" title="GAN的最优化"></a>GAN的最优化</h4><p>&#8194; 在建立好理论框架后，需要对所需要的生成器G和判别器D进行优化，在此之前先引入交叉熵的概念：它一般用来求目标与预测值之间的差距。</p>
<p>&#8194; 在信息论与编码中，熵可以用来衡量信息量的多少，而如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异，计算式如下：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Azcho6.png" alt=""></p>
<p>&#8194; 在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]，直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但并不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，此时Q就等价于P。</p>
<p>而对KL散度的计算式进行变形，可以得到：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzcIJO.png" alt=""></p>
<p>等式的前一项即为P的熵，而后一项就是交叉熵的计算式：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Azc5FK.png" alt=""></p>
<p>&#8194; 在机器学习中，我们需要评估labels和predictions之间的差距，可以使用KL散度，即$D_{KL}(y\mid\mid\hat{y})$，由于KL散度中的前一部分−H(y)即P的熵不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做损失函数，评估模型。</p>
<p>&#8194; 在引入交叉熵后，就可以定义最优化表达式。首先我们需要定义一个判别器 D以判别样本是不是从$p_{data(x)}$分布中取出来的，因此有:</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Azg6tf.png" alt=""></p>
<p>&#8194; 其中E代表取期望。这一项是根据「正类」（即辨别出 x 属于真实数据data）的对数损失函数而构建的。最大化这一项相当于令判别器 D在 x 服从于 data 的概率密度时能准确地预测 D(x)=1，即：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Azgch8.png" alt=""></p>
<p>另外一项是企图欺骗判别器的生成器 G。该项根据「负类」的对数损失函数而构建，即</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzgyAP.png" alt=""></p>
<p>因此目标函数为：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Azg29S.png" alt=""></p>
<p>它的含义是，对于D而言要尽量使公式最大化（识别能力强），而对于G又想使它最小（生成的数据接近实际数据）。整个训练是一个迭代过程。极小极大化博弈可以分开理解，即在给定G的情况下先最大化$V(D,G)$而来得到D，然后固定D，并最小化$V(D,G)$而得到G。其中，给定 G，最大化$V(D,G)$评估了$P_g$和$P_{data}$之间的差异或距离。</p>
<h4 id="概率分布近似"><a href="#概率分布近似" class="headerlink" title="概率分布近似"></a>概率分布近似</h4><p>&#8194; 接下来，论文中就使用生成对抗网络中的对抗策略来使得推算样本具有与完整数据类似的概率分布。为了实现这一目标，我们使用鉴别器来区分推算和完整样本之间的分布差异：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Az29N6.png" alt=""></p>
<p>&#8194; 其中$p_r(x_r)$代表估计值的概率分布，它将从补全的矩阵Xr中得到；$x_r$代表从$p_r(x_r)$中选取的一个数据点；D为一个鉴别器，我们通过一个以SOFTMAX为输出层的全连接的深度神经网络来实现。D将输出一个概率值，判断输入的数据为已有值还是估计值。我们使用了负交叉熵作为损失函数，通过最大化$l_d$得到一个鉴别器D，能够有效地区分已有值与估计值</p>
<h4 id="完整公式"><a href="#完整公式" class="headerlink" title="完整公式"></a>完整公式</h4><p>将前节提到的两个部分进行合并，我们得到了如下的公式：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Azgvu9.png" alt=""></p>
<p>&#8194; 其中λ是用来平衡矩阵重构与概率分布近似所占比例的一个参数，因此最小化该式时，不仅使得重构的矩阵与已有值所构成的矩阵的误差尽可能得小，同时通过鉴别器使得这两者的概率分布尽可能相似。这个同时最大最小化的要求就像是进行一场对抗。一方面，鉴别器尽可能地区别重构样本与已有样本的概率分布，而另一方面，重构矩阵又尽可能地逼近已有值的概率分布，以骗过鉴别器。因此当算法收敛时，重构矩阵的概率分布将会近似于已有值，训练出一个有效的鉴别器，同时重构矩阵的值也足够接近实际值以至于可以骗过这个鉴别器。在最小化部分中，我们求解出使得误差最小的矩阵U和V，接着使用它们来进行缺失值的计算。同时，这个部分也尽可能地让估计值去骗过鉴别器。而在最大化部分中，鉴别器通过区分已有值与最小化部分所得的估计值来进行更新，整个框架的流程如图所示。</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Az2pAx.png" alt=""></p>
<h4 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h4><p>然而在实际情况中，输入样本的概率分布往往是未知的，因此，我们使用如下式子来进行近似：在每次的更新迭代中，我们随机从Xn与Xr选取k个样本，来计算概率分布：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/AzgzH1.png" alt=""><br><img src="https://s2.ax1x.com/2019/04/17/AzgxBR.png" alt=""></p>
<p>&#8194; 其中r1与rk分别代表从Xn中选取的k个样本中的第一个和最后一个，q1和qk从Xr中选取的k个样本中的第一个和最后一个，$X^i_n$和$X^i_r$分别代表从Xn与Xr中所选取的第i个样本。因此，上面的合成式将变成：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Az2l8S.png" alt=""></p>
<p>算法流程如下所示：</p>
<p><img src="https://s2.ax1x.com/2019/04/17/Az2QC8.png" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2>]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>GAN</tag>
        <tag>矩阵分解</tag>
      </tags>
  </entry>
  <entry>
    <title>python常用操作</title>
    <url>/2019/07/25/python%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul>
<li><p>读取excel文件某一列</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">names = pd.read_excel(</span><br><span class="line">    <span class="string">'NOT_EXIST_List.xlsx'</span>,</span><br><span class="line">    sheet_name=<span class="string">'NOT EXIST'</span>, </span><br><span class="line">    header=<span class="number">0</span>, </span><br><span class="line">    usecols=[<span class="string">'KolName'</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(names)):</span><br><span class="line">    name = names[i:i + <span class="number">1</span>].values.item()</span><br></pre></td></tr></table></figure>
</li>
<li><p>List去重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">List = pd.unique(List).tolist()</span><br></pre></td></tr></table></figure>
</li>
<li><p>将接口返回值解析成json格式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> request</span><br><span class="line">response = requests.request(</span><br><span class="line">    <span class="string">"GET"</span>, url,</span><br><span class="line">    data=payload, </span><br><span class="line">    headers=headers,</span><br><span class="line">    params=querystring)</span><br><span class="line">json_response = json.loads(response.text)</span><br></pre></td></tr></table></figure>
</li>
<li><p>移除字符串中的标点符号</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removePunctuation</span><span class="params">(text)</span>:</span></span><br><span class="line">    str = <span class="string">''</span>.join(c <span class="keyword">for</span> c <span class="keyword">in</span> text <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation)</span><br><span class="line">    <span class="keyword">return</span> str</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取和写入json文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file = open(<span class="string">'XXX.json'</span>, <span class="string">'r'</span>)</span><br><span class="line">XXX = json.loads(file.read())</span><br><span class="line">file = open(<span class="string">'XXX.json'</span>, <span class="string">'a'</span>)</span><br><span class="line">json.dump(XXX, file, indent=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>将List写入excel文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(List, columns=[<span class="string">'name'</span>, <span class="string">'url'</span>])</span><br><span class="line">writer = pd.ExcelWriter(<span class="string">'remain.xlsx'</span>)</span><br><span class="line">df.to_excel(writer, <span class="string">'remain'</span>)</span><br><span class="line">writer.save()</span><br></pre></td></tr></table></figure>
</li>
<li><p>dict根据key排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortdict</span><span class="params">(data)</span>:</span></span><br><span class="line">    result = collections.OrderedDict()</span><br><span class="line">    dict = sorted(data.items(), key=<span class="keyword">lambda</span> d: d[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(dict)):</span><br><span class="line">        result[dict[i][<span class="number">0</span>]] = dict[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
</li>
<li><p>以追加方式写入excel</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rexcel = open_workbook(<span class="string">"ad_match_new.xls"</span>)</span><br><span class="line">rows = rexcel.sheets()[<span class="number">0</span>].nrows</span><br><span class="line">excel = copy(rexcel)</span><br><span class="line">table = excel.get_sheet(<span class="number">0</span>)</span><br><span class="line">row = rows</span><br><span class="line">table.write(row, <span class="number">0</span>, title)</span><br><span class="line">table.write(row, <span class="number">1</span>, channel)</span><br><span class="line">table.write(row, <span class="number">2</span>, video_url)</span><br><span class="line">table.write(row, <span class="number">3</span>, url)</span><br><span class="line">row += <span class="number">1</span></span><br><span class="line">excel.save(<span class="string">"ad_match_new.xls"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p>好处是在爬取数据或者使用接口时可以动态保存数据，不需要全部爬取完后再一次性存储，避免中途出错导致前功尽弃</p>
</blockquote>
<h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><ul>
<li>得到一个张量中每一行的第一个非零元素的索引<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">idx = torch.arrange(data.shape[<span class="number">1</span>], <span class="number">0</span>, <span class="number">-1</span>)</span><br><span class="line">tmp2 = data * idx</span><br><span class="line">indices = torch.argmax(tmp2, <span class="number">1</span>, keepdim=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>K-means总结</title>
    <url>/2019/09/23/kmeans%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h1><h2 id="1-kmeans简介"><a href="#1-kmeans简介" class="headerlink" title="1.kmeans简介"></a>1.kmeans简介</h2><h3 id="1-1聚类算法（clustering-Algorithms）介绍"><a href="#1-1聚类算法（clustering-Algorithms）介绍" class="headerlink" title="1.1聚类算法（clustering Algorithms）介绍"></a>1.1聚类算法（clustering Algorithms）介绍</h3><p>聚类是一种无监督学习—对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。<br><a id="more"></a></p>
<p>聚类算法可以分为原型聚类（k均值算法（k-means）、学习向量量化、（Learning Vector Quantization -LVQ）、高斯混合聚类（Mixture-of-Gaussian），密度聚类（DBSCAN），层次聚类（AGNES）等。</p>
<h2 id="2-kmeans原理详解"><a href="#2-kmeans原理详解" class="headerlink" title="2.kmeans原理详解"></a>2.kmeans原理详解</h2><p>k-means是一种常见的聚类算法，也叫k均值或k平均。通过迭代的方式，每次迭代都将数据集中的各个点划分到距离它最近的簇内，这里的距离即数据点到簇中心的距离。</p>
<p>kmean步骤：</p>
<ol>
<li>随机初始化k个簇中心坐标</li>
<li>计算数据集内所有点到k个簇中心的距离，并将数据点划分近最近的簇</li>
<li>更新簇中心坐标为当前簇内节点的坐标平均值</li>
<li>重复2、3步骤直到簇中心坐标不再改变（收敛了）</li>
</ol>
<p><strong>优缺点及改进算法</strong></p>
<p>优点：效率高、适用于大规模数据集</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>缺点</th>
<th>改进</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>k值的确定</td>
<td>ISODATA</td>
<td>当属于某个簇的样本数过少时把这个簇去除，<br>当属于某个簇的样本数过多、分散程度较大时把这个簇分为两个子簇</td>
</tr>
<tr>
<td>对奇异点敏感</td>
<td>k-median</td>
<td>中位数代替平均值作为簇中心</td>
</tr>
<tr>
<td>只能找到球状群</td>
<td>GMM</td>
<td>以高斯分布考虑簇内数据点的分布</td>
</tr>
<tr>
<td>分群结果不稳定</td>
<td>k-means++</td>
<td>初始的聚类中心之间的相互距离要尽可能的远</td>
</tr>
</tbody>
</table>
</div>
<h2 id="算法十问"><a href="#算法十问" class="headerlink" title="算法十问"></a>算法十问</h2><p><strong>k值的选取</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k-means算法要求事先知道数据集能分为几群，主要有两种方法定义k。</span><br><span class="line"></span><br><span class="line">elbow method通过绘制k和损失函数的关系图，选拐点处的k值。</span><br><span class="line"></span><br><span class="line">经验选取人工据经验先定几个k，多次随机初始化中心选经验上最适合的。</span><br><span class="line"></span><br><span class="line">通常都是以经验选取，因为实际操作中拐点不明显，且elbow method效率不高。</span><br></pre></td></tr></table></figure>
<p><strong>K-means算法中初始点的选择对最终结果的影响</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k-means选择的初始点不同获得的最终分类结果也可能不同，随机选择的中心会导致K-means陷入局部最优解。</span><br></pre></td></tr></table></figure>
<p><strong>为什么在计算k-means之前要将数据点在各维度上归一化</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">因为数据点各维度的量级不同。</span><br><span class="line">举个栗子，最近正好做完基于RFM模型的会员分群，每个会员分别有R（最近一次购买距今的时长）、F（来店消费的频率）和M（购买金额）。如果这是一家奢侈品商店，你会发现M的量级（可能几万元）远大于F（可能平均10次以下），如果不归一化就算k-means，相当于F这个特征完全无效。如果我希望能把常客与其他顾客区别开来，不归一化就做不到。</span><br></pre></td></tr></table></figure>
<p><strong>k-means不适用哪些数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 数据特征极强相关的数据集，因为会很难收敛（损失函数是非凸函数），一般要用kernal k-means，将数据点映射到更高维度再分群。</span><br><span class="line">2. 数据集可分出来的簇密度不一，或有很多离群值（outliers），这时候考虑使用密度聚类。</span><br></pre></td></tr></table></figure>
<p><strong>k-means 中常用的距离度量</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">K-means中比较常用的距离度量是欧几里得距离和余弦相似度。</span><br></pre></td></tr></table></figure>
<p><strong>K-means是否会一直陷入选择质心的循环停不下来（为什么迭代次数后会收敛）？</strong></p>
<p>从k-means的第三步我们可以看出，每回迭代都会用簇内点的<strong>平均值</strong>去更新簇中心，所以最终簇内的平方误差和（SSE, sum of squared error）一定最小。 平方误差和的公式如下：</p>
<script type="math/tex; mode=display">
L(X) = \sum_{i=1}^{k}{\sum_{j\in C_i}{(x_{ij}-\bar{x_i})^2}}</script><p><strong>聚类和分类区别</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 产生的结果相同（将数据进行分类）</span><br><span class="line">2. 聚类事先没有给出标签（无监督学习）</span><br></pre></td></tr></table></figure>
<p><strong>如何对k-means聚类效果进行评估</strong></p>
<p>回到聚类的定义，我们希望得到簇内数据相似度尽可能地大，而簇间相似度尽可能地小。常见的评估方式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">名称</th>
<th style="text-align:center">公式</th>
<th style="text-align:left">含义</th>
<th style="text-align:left">如何比较</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sum of squares within clusters(SSW)</td>
<td style="text-align:center">$\sum_{i=1}^{K}{ \parallel x_i-c_{l_i} \parallel ^2}$</td>
<td style="text-align:left">所有簇内差异之和</td>
<td style="text-align:left">越小越好</td>
</tr>
<tr>
<td style="text-align:left">sum of squares between clusters(SSB)</td>
<td style="text-align:center">$\sum_{i=1}^{K}{n_i \parallel c_i-\bar{x} \parallel ^2}$</td>
<td style="text-align:left">簇心与簇内均值差异的加权和</td>
<td style="text-align:left">越大越好</td>
</tr>
<tr>
<td style="text-align:left">Calinski-Harabasz</td>
<td style="text-align:center">$\frac{\frac{SSB}{K-1}}{\frac{SSW}{N-K}}$</td>
<td style="text-align:left">簇间距离和簇内距离之比（除数是惩罚项，因为SSW下降地比较快）</td>
<td style="text-align:left">越大越好</td>
</tr>
<tr>
<td style="text-align:left">Ball&amp;Hall</td>
<td style="text-align:center">$\frac{SSW}{K}$</td>
<td style="text-align:left">几乎同SSW</td>
<td style="text-align:left">越小越好</td>
</tr>
<tr>
<td style="text-align:left">Dunn’s index</td>
<td style="text-align:center">$\frac{\min_{i=1}^M{\min_{j=i+1}^M{d(c_i, c_j)}}}{\max_{k=1}^M{diam(c_k)}}$ <br>$where d(c_i, c_j)=$ $\min_{x \in c_i, x’ \in c_j}{\parallel x-x’ \parallel}^2$ <br> $diam(c_k)=$ $\max_{x, x’ \in c_k}{\parallel x-x’ \parallel}^2$</td>
<td style="text-align:left">本质上也是簇间距离和簇内距离之比</td>
<td style="text-align:left">越大越好</td>
</tr>
</tbody>
</table>
</div>
<p>另一个常见的方法是画图，将不同簇的数据点用不同颜色表示。这么做的好处是最直观，缺点是无法处理高维的数据，它最多能展示三维的数据集。<br>如果维数不多也可以做一定的降维处理（PCA）后再画图，但会损失一定的信息量。 </p>
<p>聚类算法几乎没有统一的评估指标，可能还需要根据聚类目标想评估方式，如对会员作分群以后，我想检查分得的群体之间是否确实有差异，这时候可以用MANOVA计算，当p值小于0.01说明分群合理。</p>
<p><strong>K-means中空聚类的处理</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果所有的点在指派步骤都未分配到某个簇，就会得到空簇。如果这种情况发生，则需要某种策略来选择一个替补质心，否则的话，平方误差将会偏大。一种方法是选择一个距离当前任何质心最远的点。这将消除当前对总平方误差影响最大的点。另一种方法是从具有最大SEE的簇中选择一个替补的质心。这将分裂簇并降低聚类的总SEE。如果有多个空簇，则该过程重复多次。另外编程实现时，要注意空簇可能导致的程序bug。</span><br></pre></td></tr></table></figure>
<h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ol>
<li>Mann A K, Kaur N. Review paper on clustering techniques[J]. Global Journal of Computer Science and Technology, 2013.</li>
<li><a href="https://blog.csdn.net/hua111hua/article/details/86556322" target="_blank" rel="noopener">https://blog.csdn.net/hua111hua/article/details/86556322</a></li>
<li>REZAEI M. Clustering validation[J].</li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>VPN</title>
    <url>/2018/04/06/vpn/</url>
    <content><![CDATA[<font size="5">edu.cn的学校邮箱+5美元 = 11个月的境外服务器</font>

<h3 id="领取github的vps优惠码"><a href="#领取github的vps优惠码" class="headerlink" title="领取github的vps优惠码"></a>领取github的vps优惠码</h3><a id="more"></a>
<ul>
<li>教程<br><a href="https://www.ichenfei.com/get-github-students-gift.html" target="_blank" rel="noopener">https://www.ichenfei.com/get-github-students-gift.html</a></li>
<li>领取优惠码页面<br><a href="https://education.github.com/pack/offers#digitalocean" target="_blank" rel="noopener">https://education.github.com/pack/offers#digitalocean</a></li>
<li><p>vps开通页面(需要翻墙，学校使用ipv6地址可以直接翻出去)<br><a href="https://www.digitalocean.com/" target="_blank" rel="noopener">https://www.digitalocean.com/</a><br>不要使用一次性邮箱注册账号，因为以后每次登陆都要邮箱验证</p>
</li>
<li><p>登陆页面<br><a href="https://cloud.digitalocean.com/login" target="_blank" rel="noopener">https://cloud.digitalocean.com/login</a> (建议翻墙访问)</p>
</li>
<li><p>注册paypal账号来支付<br><a href="https://www.paypal.com/c2/home" target="_blank" rel="noopener">https://www.paypal.com/c2/home</a></p>
<!--more-->
<p>paypal绑定了银行卡和手机，建议用不常用的银行卡，然后充35块钱进去<br>支付成功后，激活digitalocean账号，填入github的优惠卷，获得50美元</p>
</li>
</ul>
<h3 id="创建自己的服务器"><a href="#创建自己的服务器" class="headerlink" title="创建自己的服务器"></a>创建自己的服务器</h3><p>建议选SFO一区的服务器，不容易被墙，配置的话5美元那种就差不多了，服务器型号建议ubuntu</p>
<h3 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h3><p>通过console连上服务器后，使用秋水逸冰大大的命令一键安装:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">wget --no-check-certificate -O shadowsocks-all.sh https:<span class="comment">//raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh</span></span><br><span class="line"></span><br><span class="line">chmod +x shadowsocks-all.sh &amp;&amp; ./shadowsocks-all.sh <span class="number">2</span>&gt;&amp;<span class="number">1</span> | tee shadowsocks-all.<span class="built_in">log</span></span><br></pre></td></tr></table></figure></p>
<font size="4">探索新世界吧:)</font>




]]></content>
      <tags>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title>协同过滤</title>
    <url>/2019/09/23/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="协同过滤-collaborative-filtering"><a href="#协同过滤-collaborative-filtering" class="headerlink" title="协同过滤(collaborative filtering)"></a>协同过滤(collaborative filtering)</h1><h2 id="直观解释"><a href="#直观解释" class="headerlink" title="直观解释"></a>直观解释</h2><p>协同过滤是推荐算法中最常用的算法之一，它根据user与item的交互，发现item之间的相关性，或者发现user之间的相关性，进行推荐。比如你有位朋友看电影的爱好跟你类似，然后最近新上了《调音师》，他觉得不错，就会推荐给你，这是最简单的基于user的协同过滤算法（user-based collaboratIve filtering），还有一种是基于item的协同过滤算法（item-based collaborative filtering），比如你非常喜欢电影《当幸福来敲门的时候》，那么观影系统可能会推荐一些类似的励志片给你，比如《风雨哈佛路》等。如下主要分析user-based，item-based同理。<br><a id="more"></a></p>
<h2 id="导图"><a href="#导图" class="headerlink" title="导图"></a>导图</h2><p><img src="https://s2.ax1x.com/2019/09/23/uPnzM4.png" alt="uPnzM4.png"></p>
<h2 id="核心公式"><a href="#核心公式" class="headerlink" title="核心公式"></a>核心公式</h2><ul>
<li><p>符号定义<br>$r_{u,i}$：user $u$ 对 item $i$ 的评分<br>$\bar{r}_{u}$：user $u$ 的平均评分<br>$P_{a,b}$：用户$a,b$都有评价的items集合</p>
</li>
<li><p>核心公式</p>
</li>
</ul>
<ol>
<li>item-based CF 邻域方法预测公式</li>
</ol>
<script type="math/tex; mode=display">\operatorname{Pred}(u, i)=\overline{r}_{u}+\frac{\sum_{j \in S_{i}}\left(\operatorname{sim}(i, j) \times r_{u, j}\right)}{\sum_{j \in S_{i}} \operatorname{sim}(i, j)}</script><ol>
<li>偏差优化目标</li>
</ol>
<script type="math/tex; mode=display">\min _{b} \sum_{(u, i) \in K}\left(r_{(u, i)}-\mu-b_{u}-b_{i}\right)^{2}</script><p>其中$(u，i) \in K$表示所有的评分，$\mu$总评分均值，$b_u$为user $u$的偏差，$b_i$为item $i$ 的偏差。</p>
<ol>
<li><ul>
<li>加入正则项后的Funk SVD 优化公式</li>
</ul>
</li>
</ol>
<script type="math/tex; mode=display">\min _{u v} \sum_{(u, i) \in k n o w n}\left(r_{u,i}-u_{u} v_{i}\right)+\lambda\left(|u|^{2}+|v|^{2}\right)</script><p>其中$u_u$为user $u$的偏好，即为user特征矩阵$U$的第$u$行，$v_i$为item $i$的特征，即为特征矩阵$V$的第$i$列</p>
<h2 id="注意要点"><a href="#注意要点" class="headerlink" title="注意要点"></a>注意要点</h2><ul>
<li><p>相似度与距离之间的关系<br>  距离越大，相似度越小；距离越小，相似度越高。即在求解最大相似度的时候可以转为求解最小距离。</p>
</li>
<li><p>在协同过滤中，常用的相似度函数有哪些，简要说明</p>
<ul>
<li>杰卡德相似度（Jaccard similarity）<br>公式：</li>
</ul>
<script type="math/tex; mode=display">sim_{jaccard}(u_{1}, u_{2})=\frac{ \text {items} \text { bought by } u_{1}\  and\  u_{2}}{ \text { items  bought by  } u_{1}\  or\  u_{2}}</script><p>适用于二元情况，即定性情况，比如买或者没买，喜欢或者不喜欢，在数据稀疏的情况，可以转为二元应用。</p>
<ul>
<li><p>余弦相似度<br>公式：</p>
<script type="math/tex; mode=display">\operatorname{sim}(u_{1}, u_{2})=\frac{r_{u_{1}} \cdot r_{u_{2}}}{\left|r_{u_{1}}\right|_{2}|r_{u_{2}}|_{2}}=\frac{\sum_{i \in P_{u_1,u_2}} r_{u_{1}, i} r_{u_{2}, i}}{\sqrt{\sum_{i \in P_{u_1}} r_{u_{1},i}^{2}} \sqrt{\sum_{i \in P_{u_2}}r_{u_{2},i}^{2}}}</script><p>考虑不同用户的评价范围不一样，比如乐天派一般评分范围一般会高于悲观的人，会将评分进行去中心化再进行计算，即</p>
</li>
<li>修正余弦相似度，公式变为</li>
</ul>
<script type="math/tex; mode=display">\operatorname{sim}(u_{1}, u_{2})=\frac{r_{u_{1}} \cdot r_{u_{2}}}{\left|r_{u_{1}}\right|_{2}|r_{u_{2}}|_{2}}=\frac{\sum_{i \in P_{u_1,u_2}} (r_{u_{1}, i}-{\bar{r}_{u_{1}}}) (r_{u_{2}, i}-\bar{r}_{u_2})}{\sqrt{\sum_{i \in P_{u_1}} (r_{u_{1},i}-\bar{r}_{u_{1}})^{2}} \sqrt{\sum_{i \in P_{u_2}}(r_{u_{2},i}-\bar{r}_{u_{2}})^{2}}}</script><p>适用于定量情况，比如评分场景，要求数据具有一定的稠密度。注意如果计算一个评价很少电影的用户与一个评价很多电影的用户会导致相似度为0.</p>
<ul>
<li><p>皮尔森相关系数<br>公式：</p>
<script type="math/tex; mode=display">\operatorname{sim}(u_1, u_2)=\frac{\sum_{i \in P_{u_1.u_2}}\left(r_{u_1, i}-\overline{r}_{u_1}\right)\left(r_{u_2, i}-\overline{r}_{u_2}\right)}{\sqrt{\sum_{i \in P_{u_1.u_2}}\left(r_{u_1, i}-\overline{r}_{u_1}\right)^{2}} \sqrt{\sum_{i \in P_{u_1.u_2}}\left(r_{u_2, i}-\overline{r}_{u_2}\right)^{2}}}</script><p>皮尔森系数跟修正的余弦相似度几乎一致，两者的区别在于分母上，皮尔逊系数的分母采用的评分集是两个用户的共同评分集（就是两个用户都对这个物品有评价），而修正的余弦系数则采用两个用户各自的评分集。</p>
</li>
<li><p>$L_{p}-norms$<br>公式：</p>
<script type="math/tex; mode=display">sim(u_1,u_2) =\frac{1}{ \sqrt[p]{| r_{u_1}-r_{u_2} |^p}+1}</script><p>$p$取不同的值对应不同的距离公式，空间距离公式存在的不足这边也存在。对数值比较敏感。</p>
</li>
</ul>
</li>
<li><p>有了相似度测量后，那么基于邻域的推荐思路是怎样的呢？<br>过滤掉被评论较少的items以及较少评价的users，然后计算完users之间的相似度后，寻找跟目标user偏好既有大量相同的items，又存在不同的items的近邻几个users(可采用K-top、阈值法、聚类等方式)，然后进行推荐。步骤如下：<br>(1) 选择：选出最相似几个用户，将这些用户所喜欢的物品提取出来并过滤掉目标用户已经喜欢的物品<br>(2) 评估：对余下的物品进行评分与相似度加权<br>(3) 排序：根据加权之后的值进行排序<br>(4) 推荐：由排序结果对目标用户进行推荐</p>
</li>
<li><p>协同过滤算法具有特征学习的特点，试解释原理以及如何学习</p>
</li>
</ul>
<ol>
<li>特征学习：把users做为行，items作为列，即得评分矩阵$R_{m,n}=[r_{i,j}]$，通过矩阵分解的方式进行特征学习，即将评分矩阵分解为$R=U_{m,d}V_{d,n}$，其中$U_{m,d}$为用户特征矩阵，$V_{d,n}$表示items特征矩阵，其中$d$表示对items进行$d$个主题划分。举个简单例子，比如看电影的评分矩阵划分后，$U$中每一列表示电影的一种主题成分，比如搞笑、动作等，$V$中每一行表示一个用户的偏好，比如喜欢搞笑的程度，喜欢动作的程度，值越大说明越喜欢。这样，相当于，把电影进行了主题划分，把人物偏好也进行主题划分，主题是评分矩阵潜在特征。</li>
<li>学习方式:SVD<br>分解式为</li>
</ol>
<script type="math/tex; mode=display">R_{m,n}=U_{m,m}\Sigma_{m,n}V_{n,n}^T</script><p>  其中$U$为user特征矩阵，$\Sigma$为权重矩阵体现对应特征提供的信息量，$V$为item特征矩阵。同时可通过SVD进行降维处理，如下  </p>
<p><img src="https://s2.ax1x.com/2019/09/23/uPuki6.png" alt="uPuki6.png"><br>奇异值分解的方式，便于处理要目标user（直接添加到用户特征矩阵的尾部即可），然而要求评分矩阵元素不能为空，因此需要事先进行填充处理，同时由于user和item的数量都比较多，矩阵分解的方式计算量大，且矩阵为静态的需要随时更新，因此实际中比较少用。   </p>
<ol>
<li>Funk SVD， Funk SVD 是去掉SVD的$\Sigma$成分，优化如下目标函数，可通过梯度下降法，得到的$U,V$矩阵</li>
</ol>
<script type="math/tex; mode=display">J=\min _{u v} \sum_{(u, i) \in k n o w n}\left(r_{u,i}-u_{u} v_{i}\right)+\lambda\left(|u|^{2}+|v|^{2}\right)</script><p>Funk SVD 只要利用全部有评价的信息，不需要就空置进行处理，同时可以采用梯度下降法，优化较为方便，较为常用。</p>
<p>有了user特征信息和item特征信息，就可用$u_{u} v_{i}$对目标用户进行评分预测，如果目标用户包含在所计算的特征矩阵里面的话。针对于新user、新item，协同过滤失效。</p>
<ul>
<li><p>如何简单计算user偏差以及item偏差？</p>
<script type="math/tex; mode=display">b_u=\frac{1}{|I_u|}\sum_{i \in I_u}(r_{u,i}-\mu) \\
b_i=\frac{1}{|U_i|}\sum_{u \in U_i}(r_{u,i}-b_u-\mu)</script></li>
<li><p>如何选择协同过滤算法是基于user还是基于item<br>一般，谁的量多就不选谁。然而基于user的会给推荐目标带来惊喜，选择的范围更为宽阔，而不是基于推荐目标当前的相似item。因此如果要给推荐目标意想不到的推荐，就选择基于user的方式。可以两者结合。</p>
</li>
</ul>
<ul>
<li>协同过滤的优缺点</li>
<li><p>缺点：<br> (1)稀疏性—— 这是协同过滤中最大的问题，大部分数据不足只能推荐比较流行的items，因为很多人只有对少量items进行评价，而且一般items的量非常多，很难找到近邻。导致大量的user木有数据可推荐（一般推荐比较流行的items），大量的item不会被推荐<br> (2)孤独用户——孤独user具有非一般的品味，难以找到近邻，所以推荐不准确<br> (3)冷启动——只有大量的评分之后，才能协同出较多信息，所以前期数据比较少，推荐相对不准确；而如果没有人进行评分，将无法被推荐<br> (4)相似性——协同过滤是与内容无关的推荐，只根据用户行为，所以倾向于推荐较为流行的items。</p>
</li>
<li><p>优点：  </p>
<pre><code>(1)不需要领域知识，存在users和items的互动，便能进行推荐  
 (2)简单易于理解  
(3)相似度计算可预计算，预测效率高
</code></pre></li>
<li><p>协同过滤与关联规则的异同<br>关联规则是不考虑tems或者使用它们的users情况下分析内容之间的关系，而协同过滤是不考虑内容直接分析items之间的关系或者users之间的关系。两者殊途同归均能用于推荐系统，但是计算方式不同。</p>
</li>
<li><p>实践中的一些注意点<br>(1) 过滤掉被评价较少的items<br>(2) 过滤掉评价较少的users<br>(3) 可用聚类方式缩小搜索空间，但是面临找不到相同偏好的用户（如果用户在分界点，被误分的情况），这种方式通过缩小搜索空间的方式优化协同过滤算法<br>(4) 使用的时候，可以考虑时间范围，偏好随着时间的改变会改变</p>
</li>
</ul>
<h2 id="面试真题"><a href="#面试真题" class="headerlink" title="面试真题"></a>面试真题</h2><p>使用协同过滤算法之前，数据应该如何进行预处理？<br>协同过滤的方式有哪些？<br>如何通过相似度计算设计协同过滤推荐系统？<br>请谈谈你对协同过滤中特征学习的理解？<br>如何将协同过滤用于推荐系统？<br>FUNK SVD相对于SVD有哪些优势？<br>如何求解FUNK SVD？<br>请描述下协同过滤的优缺点？  </p>
]]></content>
      <tags>
        <tag>推荐系统</tag>
        <tag>机器学习</tag>
        <tag>协同过滤</tag>
      </tags>
  </entry>
  <entry>
    <title>工业界推荐系统小综述</title>
    <url>/2021/09/20/%E5%B7%A5%E4%B8%9A%E7%95%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>借这篇博客记录看工业界推荐系统论文的心得。</p>
<a id="more"></a>
<h2 id="Logistics-Regression"><a href="#Logistics-Regression" class="headerlink" title="Logistics Regression"></a>Logistics Regression</h2><p>本节主要参考<a href="https://zhuanlan.zhihu.com/p/151036015" target="_blank" rel="noopener">刘启林的推荐系统</a></p>
<p>逻辑回归是推荐领域的经典模型之一，回归是指将值回归到[0,1]区间。</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>LR将线性回归模型与Sigmoid函数相结合，线性回归模型的常见形式为$y=w^Tx+b$，为了表达形式上的统一，常将$w_0=b,x_0=1$，则有下图的$y=\sum_{i=0}^nw_ix_i=w^Tx$：</p>
<p><a href="https://imgtu.com/i/4BEwJs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BEwJs.jpg" alt="4BEwJs.jpg"></a></p>
<p>LR按照输出y的取值可以分为$y\in\{0,1\}、y\in\{-1,1\}$两种形式：</p>
<ul>
<li><p>CTR预估（0：曝光后未被点击，1：曝光后被点击）</p>
<blockquote>
<p>伯努利分布：随机变量X只能取0和1两个值：</p>
<script type="math/tex; mode=display">
P(X=k)=p^k(1-p)^{1-k},~k={0,1}</script></blockquote>
<p><a href="https://imgtu.com/i/4BVanK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BVanK.jpg" alt="4BVanK.jpg"></a></p>
</li>
<li><p>分类预估（-1：负类，1：正类）</p>
<p><a href="https://imgtu.com/i/4BVNX6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BVNX6.jpg" alt="4BVNX6.jpg"></a></p>
</li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><a href="https://imgtu.com/i/4BZZUe" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZZUe.jpg" alt="4BZZUe.jpg"></a></p>
<p><a href="https://imgtu.com/i/4BZnCd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZnCd.jpg" alt="4BZnCd.jpg"></a><br><a href="https://imgtu.com/i/4BZVED" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZVED.jpg" alt="4BZVED.jpg"></a></p>
<p><a href="https://imgtu.com/i/4BZgPJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZgPJ.jpg" alt="4BZgPJ.jpg"></a><br><a href="https://imgtu.com/i/4BZ654" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZ654.jpg" alt="4BZ654.jpg"></a></p>
<p><a href="https://imgtu.com/i/4Besyt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4Besyt.jpg" alt="4Besyt.jpg"></a><br><a href="https://imgtu.com/i/4BerQI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BerQI.jpg" alt="4BerQI.jpg"></a><br><a href="https://imgtu.com/i/4BeDSA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BeDSA.jpg" alt="4BeDSA.jpg"></a></p>
<h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><ul>
<li>适合离散特征；增加、减少特征容易，易于拟合、快速迭代</li>
<li>特征空间大，容易过拟合；</li>
<li>去掉高度相关特征；</li>
</ul>
<h2 id="Wide-amp-Deep-Learning-for-Recommender-Systems-DLRS’16"><a href="#Wide-amp-Deep-Learning-for-Recommender-Systems-DLRS’16" class="headerlink" title="Wide &amp; Deep Learning for Recommender Systems[DLRS’16]"></a>Wide &amp; Deep Learning for Recommender Systems[DLRS’16]</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>解决推荐时Memorization和Generalization无法兼顾的问题。</p>
<h4 id="Memorization"><a href="#Memorization" class="headerlink" title="Memorization"></a>Memorization</h4><p>面对拥有大规模离散sparse特征的CTR预估问题时，可以通过将特征之间进行叉乘来捕捉特征之间的相关性，典型代表如LR模型，使用原始sparse特征和叉乘特征作为输入。但缺点是特征的叉乘需要人工进行设计，而且对于训练数据中未曾出现过的特征对，模型中对应项的权重也会为0.</p>
<h4 id="Generalization"><a href="#Generalization" class="headerlink" title="Generalization"></a>Generalization</h4><p>Generalization为sparse特征学习低维的dense embeddings来捕获相关性，它相较于Memorization涉及更少的人工设计以及更好的泛化能力，即使训练数据中未曾出现的特征对，对应的权重也会因为各自的dense embeddings而非零。但缺点也是会带来过度泛化，当user-item矩阵非常稀疏时，例如小众爱好的user和冷门商品，这时大部分user-item应该是没有关联的，但dense embedding还是能得到非零预测，导致推荐不怎么相关的商品，这时Memorization更好，因为它可以记忆这些特殊的特征组合。</p>
<p>Memorization根据历史行为数据，产生的推荐通常和用户已有行为的物品直接相关的物品。而Generalization会学习新的特征组合，提高推荐物品的多样性。 论文作者结合两者的优点，提出了一个新的学习算法——Wide &amp; Deep Learning，其中Wide &amp; Deep分别对应Memorization &amp; Generalization。</p>
<p><a href="https://imgtu.com/i/4KtfW8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/17/4KtfW8.png" alt="4KtfW8.png"></a></p>
<h3 id="做法及创新-1"><a href="#做法及创新-1" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><strong>Wide</strong>部分是一个广义线性模型，其中$x$和$\phi(x)$表示原始特征和叉乘特征：</p>
<script type="math/tex; mode=display">
y=w^T[x,\phi(x)]+b</script><p>原始特征$x=[x_1,x_2,\cdots,x_d]$有$d$维，叉乘特征的构造方式为：$\phi_k(x)=\Pi_{i=1}^dx_i^{c_{ki}},~c_{ki}\in\{0,1\}$</p>
<p>其实就是用一个布尔变量来标示哪些特征进行了叉乘。</p>
<p><strong>Deep</strong>部分是前馈神经网络，它会对一些sparse特征（如ID类特征）学习一个dense embeddings，维度在O(10)到O(100)之间。</p>
<script type="math/tex; mode=display">
a^{l+1}=f(W^la^l+b^l)</script><p><strong>损失函数</strong>选取的是logistic损失函数，模型最后的预测输出为，其中$a^{l_f}$是神经网络最后一层的激活值：</p>
<script type="math/tex; mode=display">
p(y=1|x)=\sigma(w^T_{wide}[x,\phi(x)]+w^T_{deep}a^{l_f}+b)</script><p><strong>联合训练</strong>时Wide部分只需要做一小部分的特征叉乘来弥补Deep部分的不足，不需要一个完整的Wide模型。优化方法使用的是mini-batch随机梯度下降，Wide部分是带L1正则的FTRL算法，Deep部分是AdaGrad算法。</p>
<p><a href="https://imgtu.com/i/4KB1ts" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/17/4KB1ts.png" alt="4KB1ts.png"></a></p>
<p>实验部分采取的模型设置如上图所示，其中的细节为：</p>
<ul>
<li>连续型特征会被归一化到[0,1]之间</li>
<li>离散型特征映射到32维embeddings，与原始连续特征共1200维作为网络输入</li>
<li>Wide部分只有一组特征叉乘，被推荐的App×用户下载的App，希望Wide部分能发现这样的规则：用户安装了应用A，此时曝光应用B，用户安装的B概率大。</li>
<li>线上模型更新时，用上次的embeddings和模型参数进行”热启动“</li>
</ul>
<h4 id="实践细节"><a href="#实践细节" class="headerlink" title="实践细节"></a>实践细节</h4><ol>
<li><p>为什么Wide部分要用L1 FTRL训练？</p>
<p>FTRL的介绍可见<a href="https://github.com/wzhe06/Ad-papers/blob/master/Optimization%20Method/%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3(Online%20Optimization" target="_blank" rel="noopener">文章</a>-%E5%86%AF%E6%89%AC.pdf)。这种方式注重模型的稀疏性，能让Wide部分变得更加稀疏，大部分权重都为0。</p>
</li>
<li><p>为什么Deep部分不考虑稀疏性的问题？</p>
<p>Deep部分的输入，要么是Age，#App Installs这些数值类特征，要么是已经降维并稠密化的Embeddings向量。所以Deep部分不存在严重的特征稀疏问题，自然可以使用精度更好，更适用于深度学习训练的AdaGrad去训练。</p>
</li>
</ol>
<h2 id="Factorization-Machines"><a href="#Factorization-Machines" class="headerlink" title="Factorization Machines"></a>Factorization Machines</h2><p>本节内容主要参考<a href="https://zhuanlan.zhihu.com/p/145436595" target="_blank" rel="noopener">刘启林的推荐系统</a></p>
<h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>因子分解机在线性回归模型中加入了特征的交互，来建模特征的相关性，并且解决数据的稀疏性以及特征空间维度过高的问题。</p>
<p>对于常见的categorical特征，经过one-hot编码以后，样本的数据就会变得很稀疏。举例来说，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间也会一下子暴增一百万。</p>
<h3 id="做法及创新-2"><a href="#做法及创新-2" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h4><p>线性回归模型假设特征之间相互独立：</p>
<script type="math/tex; mode=display">
y=w_0+\sum_{i=1}^nw_ix_i</script><p>而现实场景中，特征之间是有相关性的，例如&lt;程序员&gt;与&lt;计算机类书籍&gt;，因此需要在线性回归模型中加入特征组合项。最简单的组合方式是两两组合，变为二阶多项式回归模型，多出$\frac{n(n-1)}{2}$项：</p>
<script type="math/tex; mode=display">
y=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j\ge i}^nw_{ij}x_ix_j</script><p>这样做的局限是对于样本中没出现过交互的特征组合，就无法对相应的参数进行估计，而且时间复杂度上升到了$O(n^2)$。</p>
<p>上式中的二项式参数$w_{ij}$可以组成一个对称矩阵$W$，根据Cholesky分解可以分解成：</p>
<blockquote>
<p>Cholesky分解：将一个对称正定矩阵化为一个下三角矩阵与其共轭转置矩阵的积</p>
</blockquote>
<script type="math/tex; mode=display">
W=VV^T</script><p>这时V的第j列就是第j维特征的隐向量，二次项参数转化为$w_{ij}=<v_i,v_j>=\sum_{f=1}^kv_{i,f}v_{j,f}$，此时隐向量的特征维度$k$一般远小于原始特征维度$n$。FM的假设是，特征两两相关。</v_i,v_j></p>
<h4 id="计算化简"><a href="#计算化简" class="headerlink" title="计算化简"></a>计算化简</h4><p>FM的计算复杂度可以化简为线性复杂度：</p>
<p><a href="https://imgtu.com/i/4ahAk6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4ahAk6.jpg" alt="4ahAk6.jpg"></a></p>
<p><a href="https://imgtu.com/i/4ahMnA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4ahMnA.jpg" alt="4ahMnA.jpg"></a></p>
<p><a href="https://imgtu.com/i/4ahhH1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4ahhH1.jpg" alt="4ahhH1.jpg"></a></p>
<h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>FM模型可用于回归（Regression）、二分类（Binary classification）、排名（Ranking）任务，其对应的损失函数如下：</p>
<p><a href="https://imgtu.com/i/4a4d2D" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4a4d2D.jpg" alt="4a4d2D.jpg"></a></p>
<p><a href="https://imgtu.com/i/4a5Uwn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4a5Uwn.jpg" alt="4a5Uwn.jpg"></a></p>
<p>以随机梯度下降训练为例：</p>
<p><a href="https://imgtu.com/i/4aIt1O" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aIt1O.jpg" alt="4aIt1O.jpg"></a></p>
<p><a href="https://imgtu.com/i/4aovQS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aovQS.jpg" alt="4aovQS.jpg"></a></p>
<h4 id="特征工程-1"><a href="#特征工程-1" class="headerlink" title="特征工程"></a>特征工程</h4><p>FM模型对特征两两自动组合，不需要人工参与，类别特征One-Hot化，以一个电影数据集为例：</p>
<p><a href="https://imgtu.com/i/4aT2wj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aT2wj.jpg" alt="4aT2wj.jpg"></a></p>
<p><a href="https://imgtu.com/i/4aTRTs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aTRTs.jpg" alt="4aTRTs.jpg"></a></p>
<p><a href="https://imgtu.com/i/4aTIpV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aTIpV.jpg" alt="4aTIpV.jpg"></a></p>
<h2 id="Field-aware-Factorization-Machines-for-CTR-Prediction-RecSys’16"><a href="#Field-aware-Factorization-Machines-for-CTR-Prediction-RecSys’16" class="headerlink" title="Field-aware Factorization Machines for CTR Prediction[RecSys’16]"></a>Field-aware Factorization Machines for CTR Prediction[RecSys’16]</h2><p>本节主要参考<a href="https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">深入FFM原理与实践</a>、<a href="https://www.jianshu.com/p/781cde3d5f3d" target="_blank" rel="noopener">FFM模型理论和实践</a></p>
<h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FM在遇到one-hot类型的特征时遇到的数据稀疏性问题。</p>
<h3 id="做法及创新-3"><a href="#做法及创新-3" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="核心思想-2"><a href="#核心思想-2" class="headerlink" title="核心思想"></a>核心思想</h4><p>FFM模型中引入了域（field）的概念，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个域，包括用户国籍，广告类型，日期等等，以一条CTR点击数据为例，说明FFM与FM的区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Clicked</th>
<th style="text-align:center">Publisher(P)</th>
<th style="text-align:center">Advertiser(A)</th>
<th style="text-align:center">Gender(G)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Yes</td>
<td style="text-align:center">ESPN</td>
<td style="text-align:center">Nike</td>
<td style="text-align:center">Male</td>
</tr>
</tbody>
</table>
</div>
<p>对于FM，只会考虑二次交叉项：</p>
<script type="math/tex; mode=display">
\phi_{FM}=V_{ESPN}\cdot V_{Nike}+V_{ESPN}\cdot V_{Male}+V_{Nike}\cdot V_{Male}</script><p>因为Nike与Male显然属于不同的field，所以（ESPN，Nike）和（ESPN，Male）的隐含含义也可能是不同的，而FM只用一个隐向量$V_{ESPN}$来统一表示ESPN与Nike和Maled的交互作用，不够准确。而在FFM中，域之间的交互作用是不同的，每个特征有k个隐向量个数，k为其余特征field的个数：</p>
<script type="math/tex; mode=display">
\phi_{FFM}=V_{ESPN,A}\cdot V_{Nike,P}+V_{ESPN,G}\cdot V_{Male,P}+V_{Nike,G}\cdot V_{Male,A}</script><p>所以FFM的数学表达式为：</p>
<script type="math/tex; mode=display">
\phi_{FFM}(w,x)=\sum_{i=1}^n\sum_{j\ge i}^nw_{i,f_j}\cdot w_{j,f_i}x_ix_j</script><p>FFM的参数个数为kfn，FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。值得注意的是，由于隐向量与field相关，所以FFM中的二次项不能够化简，它的时间复杂度为$O(kn^2)$。</p>
<h4 id="实践细节-1"><a href="#实践细节-1" class="headerlink" title="实践细节"></a>实践细节</h4><ol>
<li>样本归一化。FFM默认是进行样本数据的归一化，否则容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。</li>
<li>特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到[0,1]是非常必要的。</li>
<li>省略零值特征。零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</li>
</ol>
<h2 id="DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction-IJCAI’17"><a href="#DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction-IJCAI’17" class="headerlink" title="DeepFM: A Factorization-Machine based Neural Network for CTR Prediction[IJCAI’17]"></a>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction[IJCAI’17]</h2><p>本节主要参考<a href="https://static001.geekbang.org/con/33/pdf/1511951900/file/%E6%9C%80%E7%BB%88%E7%89%88-%E5%BC%A0%E4%BF%8A%E6%9E%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E5%8F%8A%E5%BE%AE%E5%8D%9A%E7%9A%84%E5%BA%94%E7%94%A8.pdf" target="_blank" rel="noopener">深度学习在推荐的技术进展及微博的应用</a>、<a href="https://www.jianshu.com/p/6f1c2643d31b" target="_blank" rel="noopener">DeepFM模型理论与实践</a></p>
<h3 id="解决的问题-3"><a href="#解决的问题-3" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>对于一个基于CTR预估的推荐系统，最重要的是学习到用户点击行为背后隐含的特征组合。举例来说，在主流的app市场上，我们发现，用户喜欢在用餐时间下载送餐app， 说明二阶交叉特征“app类别-时间戳” 可以作为CTR预估的重要特征。另一个发现， 男青年喜欢射击游戏和RPG游戏，因此，三阶交叉特征“app类别-用户性别-用户年龄”也可以作为CTR预估的一个特征。但是这种交叉特征往往需要专家知识，类似“尿布-啤酒”这种经典的例子。</p>
<p>前面提到的FM虽然理论上来讲可以对高阶特征组合进行建模，但实际上因为计算复杂度的原因一般都只用到了二阶特征组合。而多层神经网络能够学习复杂的交叉特征。</p>
<h3 id="做法及创新-4"><a href="#做法及创新-4" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="DNN与高维特征"><a href="#DNN与高维特征" class="headerlink" title="DNN与高维特征"></a>DNN与高维特征</h4><p>虽然DNN能够学习复杂的特征组合，但直接应用于CTR预告等问题上时会在离散型特征上遇到阻碍，对于离散型特征典型的做法是进行one-hot编码，这会导致输入的数据维度非常高：</p>
<p><a href="https://imgtu.com/i/4wrxln" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wrxln.png" alt="4wrxln.png"></a></p>
<p><a href="https://imgtu.com/i/4wsVp9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wsVp9.png" alt="4wsVp9.png"></a></p>
<p>类似于FFM中将特征按域来进行分类，可以将输入的one-hot数据按照域形成对应的dense vector，来避免数据稀疏性的问题：</p>
<p><a href="https://imgtu.com/i/4wsmOx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wsmOx.png" alt="4wsmOx.png"></a></p>
<p><a href="https://imgtu.com/i/4wsGpd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wsGpd.png" alt="4wsGpd.png"></a></p>
<p><a href="https://imgtu.com/i/4wscXq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wscXq.png" alt="4wscXq.png"></a></p>
<p><a href="https://imgtu.com/i/4wyp3d" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wyp3d.png" alt="4wyp3d.png"></a></p>
<p>也就是希望能将DNN与FM进行一个融合，而融合的形式总的来说分为串行与并行，本节介绍的DeepFM以及前面介绍过的Wide&amp;Deep都为典型的并行结构。</p>
<p><a href="https://imgtu.com/i/40FPt1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/40FPt1.png" alt="40FPt1.png"></a></p>
<h4 id="核心思想-3"><a href="#核心思想-3" class="headerlink" title="核心思想"></a>核心思想</h4><p><a href="https://imgtu.com/i/40AJoj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/40AJoj.png" alt="40AJoj.png"></a></p>
<p>首先来看DeepFM的结构，FM与DNN分别负责提取低阶与高阶特征，这两部分<strong>共享输入</strong>：</p>
<p><a href="https://imgtu.com/i/40Eihn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/40Eihn.png" alt="40Eihn.png"></a></p>
<p>FM部分与标准的FM并无不同，而DNN部分，在进入第一层隐藏层之前，首先通过一个嵌入层来将输入的高维特征压缩到低维稠密向量。这一部分的两个特点为：</p>
<ol>
<li>尽管不同域的特征维度不同，在经过压缩后的维度均为k。</li>
<li>FM部分的隐向量V现在作为DNN的嵌入层权重。这样一来就不需要通过FM对隐向量V进行预训练之后对DNN的嵌入层进行初始化，而是在训练DNN时对V一起进行学习，做到端到端。</li>
</ol>
<h2 id="Deep-Neural-Networks-for-YouTube-Recommendations-RecSys’16"><a href="#Deep-Neural-Networks-for-YouTube-Recommendations-RecSys’16" class="headerlink" title="Deep Neural Networks for YouTube Recommendations[RecSys’16]"></a>Deep Neural Networks for YouTube Recommendations[RecSys’16]</h2><p>本节要介绍的是Youtube出品的经典工业界论文，主要内容参考<a href="https://zhuanlan.zhihu.com/p/52169807" target="_blank" rel="noopener">王喆的机器学习笔记1</a>、<a href="https://zhuanlan.zhihu.com/p/61827629" target="_blank" rel="noopener">王喆的机器学习笔记2</a>、<a href="https://www.jianshu.com/p/8fa4dcbd5588" target="_blank" rel="noopener">深度学习遇上推荐系统</a>。</p>
<h3 id="解决的问题-4"><a href="#解决的问题-4" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>作为全球最大的UGC的视频网站，Youtube需要在百万量级的视频规模下进行个性化推荐。由于候选视频集合过大，考虑online系统延迟问题，不宜用复杂网络直接进行推荐，所以Youtube采取了两层深度网络完成整个推荐过程：</p>
<ol>
<li>第一层是<strong>Candidate Generation Model</strong>完成候选视频的快速筛选，这一步候选视频集合由百万降低到了百的量级（粗排）。</li>
<li>第二层是用<strong>Ranking Model</strong>完成几百个候选视频的排序（精排）。</li>
</ol>
<div align="center">
<a href="https://imgtu.com/i/4BqpB6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BqpB6.png" alt="4BqpB6.png" border="0" width="75%"></a>
</div>

<h4 id="做法及创新-5"><a href="#做法及创新-5" class="headerlink" title="做法及创新"></a>做法及创新</h4><h4 id="粗排模型"><a href="#粗排模型" class="headerlink" title="粗排模型"></a>粗排模型</h4><div align="center">
<a href="https://imgtu.com/i/4BOiSH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BOiSH.png" alt="4BOiSH.png" border="0" width="65%/"></a>
</div>

<p>自底向上看粗排模型的结构，最底层的输入是用户观看过的video的embedding向量，以及搜索词的embedding向量，由word2vec得到。特别地，历史搜索的query分词后的token的embedding向量进行平均，能够反映用户的整体搜索历史状态。其它的特征向量还包括了用户的地理位置的embedding，年龄，性别等。然后把所有这些特征concatenate起来，输入上层的ReLU神经网络。</p>
<p><strong>引入fresh content的bias的作用？</strong></p>
<p>这里比较特别的一个特征是”example age“。每一秒中，YouTube都有大量视频被上传，推荐这些最新视频对于YouTube来说是极其重要的。同时，通过观察历史数据发现，用户更倾向于推荐相关度不高但最新（fresh）的视频。视频的点击率实际上都会受fresh的影响，训练的时候加入example age ，为的就是“显式”的告诉模型example age对点击的影响。在预测的时候，example age置0，就排除了这个特征对模型的影响。类似于广告，广告在展示列表中的位置，对广告的点击概率有非常大影响，排名越靠前的广告，越容易被点击，在产生训练样本的时候，把展示位置作为特征放在样本里面，并且在使用模型的时候，把展示位置特征统一置为0。</p>
<p>假设一个视频是十天前发布的，许多用户在当前观看了该视频，那么在当天会产生许多Sample Log，而在后面的九天里，观看记录不多，Sample Log也很少。如果我们没有加入Example Age这个特征的话，无论何时训练模型，这个视频对应的分类概率都是差不多的，但是如果我们加入这个特征，模型就会知道，如果这条记录是十天前产生的话，该视频会有很高的分类概率，如果是最近几天产生的话，分类概率应该低一些，这样可以更加逼近实际的数据。实验结果也证明了这一点：</p>
<div align="center">
<a href="https://imgtu.com/i/4BzKQe" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BzKQe.png" alt="4BzKQe.png" border="0" width="65%/"></a>
</div>

<p>训练样本的产生方面，正样本是用户所有完整观看过的视频，其余可以视作负样本。同时，针对每一个用户的观看记录，都生成了固定数量的训练样本，这样，每个用户在损失函数中的地位都是相等的，防止一小部分超级活跃用户主导损失函数。</p>
<p>在对待用户的搜索和观看历史时，Youtube并没有选择时序模型，而是完全摒弃了序列关系，采用求平均的方式对历史记录进行了处理。这是因为考虑时序关系，用户的推荐结果将过多受最近观看或搜索的一个视频的影响。文章中给出一个例子，如果用户刚搜索过“taylor swift”，你就把用户主页的推荐结果大部分变成taylor swift有关的视频，这其实是非常差的体验。为了综合考虑之前多次搜索和观看的信息，YouTube丢掉了时序信息，将用户近期的历史纪录等同看待。</p>
<p><a href="https://imgtu.com/i/4DSqNq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4DSqNq.png" alt="4DSqNq.png" border="0"></a></p>
<p>在处理测试集时，Youtube采用的是图(b)的方式。图(a)是held-out方式，利用上下文信息预估中间的一个视频；图(b)是predicting next watch的方式，则是利用上文信息，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。而且只留最后一次观看行为做测试集主要是为了避免引入future information，产生与事实不符的数据穿越。</p>
<p>输出方面，因为Youtube将推荐问题建模成一个“超大规模多分类”问题。即在时刻t，用户U（上下文信息C）会观看视频i的概率（每个具体的视频视为一个类别，i即为一个类别），所以输出应该是一个在所有candidate video上的概率分布，自然是一个多分类问题。</p>
<p>同时，输出分为线上和离线训练两个部分。离线训练阶段输出层为softmax层，输出3.1中公式表达的概率。对于在线服务来说，有严格的性能要求，Youtube没有重新跑一遍模型，而是通过保存用户的embedding和视频的embedding，通过最近邻搜索的方法得到top N（approx topN，使用hash的方法来得到近似的topN）的结果。</p>
<h4 id="精排模型"><a href="#精排模型" class="headerlink" title="精排模型"></a>精排模型</h4><p><div align="center">
<a href="https://imgtu.com/i/4D9nzT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4D9nzT.png" alt="4D9nzT.png" border="0" width="75%/"></a>
</div><br>排序过程是对生成的候选集做进一步细粒度的排序，模型架构与粗排模型基本一致，区别在于特征工程部分，图中从左至右的特征依次是：</p>
<ol>
<li><strong>impression video ID embedding</strong>: 当前要计算的video的embedding</li>
<li><strong>watched video IDs average embedding</strong>: 用户观看过的最后N个视频embedding的average pooling</li>
<li><strong>language embedding</strong>: 用户语言的embedding和当前视频语言的embedding</li>
<li><strong>time since last watch</strong>: 自上次观看同channel视频的时间</li>
<li><strong>previous impressions</strong>: 该视频已经被曝光给该用户的次数</li>
</ol>
<p>后面两个特征很好地引入了对用户行为的观察，第4个特征是用户上次观看同频道时间距现在的时间间隔,从用户的角度想一想，假如我们刚看过“DOTA经典回顾”这个channel的视频，我们很大概率是会继续看这个channel的视频的，那么该特征就很好的捕捉到了这一用户行为。第5个特征previous impressions则一定程度上引入了exploration的思想，避免同一个视频持续对同一用户进行无效曝光。尽量增加用户没看过的新视频的曝光可能性。</p>
<p>在<strong>特征处理</strong>部分分为离散与连续变量：</p>
<p><strong>离散变量</strong></p>
<ul>
<li>在进行video embedding的时候，只保留用户最常点击的N个视频的embedding，剩余的长尾视频的embedding直接用0向量代替。把大量长尾的video截断掉，主要还是为了节省online serving中宝贵的内存资源。当然从模型角度讲，低频video的embedding的准确性不佳是另一个“截断掉也不那么可惜”的理由。</li>
<li>对于相同域的特征可以共享embedding，比如用户点击过的视频ID，用户观看过的视频ID，用户收藏过的视频ID等等，这些公用一套embedding可以使其更充分的学习，同时减少模型的大小，加速模型的训练。</li>
</ul>
<p><strong>连续变量</strong></p>
<ul>
<li>主要是归一化处理，同时还把归一化后的的根号和平方作为网络输入，以期能使网络能够更容易得到特征的次线性（sub-linear）和（super-linear）超线性函数。（引入了特征的非线性）。</li>
</ul>
<p>在精排模型的<strong>训练</strong>阶段，模型采用了用户的期望观看时间作为优化目标，所以如果简单使用LR就无法引入正样本的观看时间信息。因此采用weighted LR，将观看时间$T_i$作为正样本的权重，对于负样本，权重是单位权重(可以认为是1)。在线上serving中使用$e^{w^Tx+b}$做预测可以直接得到expected watch time的近似。这里引出一个问题：</p>
<ol>
<li><p>在模型serving过程中又为何没有采用sigmoid函数预测正样本的probability，而是使用$e^{w^Tx+b}$这一指数形式预测用户观看时长？</p>
<blockquote>
<p>回到LR的定义：</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-w^Tx}}</script><p>对于二分类问题：</p>
<script type="math/tex; mode=display">
P(y=1|x)=\sigma(x) \\
P(y=0|x)=1-\sigma(x)</script><p>一件事情的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是p，那么该事件的odds是$\frac{p}{1-p}$，对于LR而言：</p>
<script type="math/tex; mode=display">
\frac{\frac{1}{1+e^{-w^Tx}}}{1-\frac{1}{1+e^{-w^Tx}}}=e^{w^Tx}</script><p>所以$e^{w^Tx+b}$求的就是LR形式下的odds。</p>
<p>Weighted LR中的单个样本的weight，并不是让这个样本发生的概率变成了weight倍，而是让这个样本，对预估的影响(也就是loss)提升了weight倍。因为观看时长的几率=$\frac{\sum T_i}{N-k}$，其中k为正样本的个数，非wieght的odds可以直接看成N+/N-，因为wieghted的lr中，N+变成了weight倍，N-没变，还是1倍，所以直接可得后来的odds是之前odds的weight倍。</p>
<p>也就是说样本i的odds变成了下面的式子，由于在视频推荐场景中，用户打开一个视频的概率p往往是一个很小的值，且YouTube采用了用户观看时长$T_i$作为权重，$w_i=T_i$，所以有：</p>
<script type="math/tex; mode=display">
odds(i)=\frac{w_ip}{1-w_ip}\approx w_ip=T_ip</script><p>这就是用户观看某视频的期望时长的计算式。</p>
</blockquote>
</li>
</ol>
<p>所以模型serving部分使用的是这个形式，经历了$e^{w^Tx+b}\rightarrow odds\rightarrow 用户期望观看时长$的过程。</p>
]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>实用技巧</title>
    <url>/2020/06/08/%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<p>用本文记录平时遇到的一些问题和解决方法，自己试过成功的<br><a id="more"></a></p>
<h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><ul>
<li>修改Ubuntu的apt-get源为国内镜像源 <a href="https://blog.csdn.net/zgljl2012/article/details/79065174" target="_blank" rel="noopener">CSDN</a></li>
<li>Ubuntu16.04安装python3.5同时保留python2.7 <a href="https://askubuntu.com/questions/1142982/install-python-3-alongside-python-2-7-on-ubuntu-18-04" target="_blank" rel="noopener">askubuntu</a></li>
<li>修改Ubuntu的pip源为阿里源 <a href="https://blog.csdn.net/w5688414/article/details/104439181" target="_blank" rel="noopener">CSDN</a></li>
<li>win10安装ubuntu第一次开启时黑屏：netsh winsock reset<a href="https://zhuanlan.zhihu.com/p/93980116" target="_blank" rel="noopener">知乎</a></li>
<li>动态查看GPU资源使用情况：watch -d -n 0.5 nvidia-smi</li>
</ul>
<h4 id="Linux通过ssh传输文件"><a href="#Linux通过ssh传输文件" class="headerlink" title="Linux通过ssh传输文件"></a>Linux通过ssh传输文件</h4><p>当我们需要下载或上传远程服务器上的某个文件，而服务器没有配置ftp服务器等途径时，使用scp命令可以轻松达到目的。</p>
<p><strong>下载远程服务器上的文件</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -P 8888 root@xx.xx.xx.xx:/root/usr/abc.tar.gz /home/usr/abc.tar.gz</span><br></pre></td></tr></table></figure>
<p>-P为端口参数，8888表示更改SSH端口后的端口，如果没有更改可以不用添加这个参数。root@xx.xx.xx.xx表示以root用户登陆服务器xx.xx.xx.xx，:/root/usr/abc.tar.gz表示服务器上的文件，/home/usr/abc.tar.gz表示保存在本地的路径和文件名。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -P 8888 -r root@xx.xx.xx.xx:/root/usr/ /home/usr/</span><br></pre></td></tr></table></figure>
<p>-r参数表示递归复制，即用来复制该目录下面的文件和子目录。</p>
<p><strong>上传本地文件到远程服务器</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -P 8888 /home/usr/abc.tar.gz root@xx.xx.xx.xx:/root/usr/abc.tar.gz</span><br></pre></td></tr></table></figure>
<p>同理，将本地的/home/usr/abc.tar.gz上传到服务器上的/root/usr/文件夹中。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp -P 8888 -r /home/usr/ root@xx.xx.xx.xx:/root/usr/</span><br></pre></td></tr></table></figure>
<p>类似地，上传整个目录。</p>
<p>另一个参数-v可以用来显示进度 . 查看连接 , 认证 , 或是配置错误。</p>
<h4 id="使用CMakeLists-txt文件"><a href="#使用CMakeLists-txt文件" class="headerlink" title="使用CMakeLists.txt文件"></a>使用CMakeLists.txt文件</h4><p>CMakeLists.txt需要通过cmake命令来执行，步骤为：</p>
<ul>
<li>cmake XXX</li>
<li>make</li>
</ul>
<p>其中XXX为CMakeLists.txt所在目录。如果提示当前cmake命令版本太低需要升级，可以通过下面的方法来临时安装一个新版本，不需要卸载旧版本，避免不必要的麻烦：</p>
<ol>
<li><p>从<a href="https://cmake.org/files/LatestRelease/" target="_blank" rel="noopener">官网</a>下载sh文件执行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://cmake.org/files/LatestRelease/cmake-3.19.4-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行sh文件，安装过程中选项都输入y</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh cmake-3.19.4-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查是否为新版本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ./cmake-3.19.4-Linux-x86_64/bin</span><br><span class="line">./cmake --version</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>显示新版本说明安装正确，对于需要新版本才能执行的CMakeLists.txt，可以调用这个目录下的cmake来执行</p>
<h4 id="编译时提示找不到头文件"><a href="#编译时提示找不到头文件" class="headerlink" title="编译时提示找不到头文件"></a>编译时提示找不到头文件</h4><p>如果编译时提示找不到自己安装的库的头文件，可以通过-I来指定头文件的位置，需要注意层级关系，例如</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdgpu/vector.cuh&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>而vector.cuh位于/stdgpu/src/stdgpu，则命令为-I stdgpu/src/</p>
<h4 id="安装和使用boost库"><a href="#安装和使用boost库" class="headerlink" title="安装和使用boost库"></a>安装和使用boost库</h4><p><a href="https://www.boost.org/doc/libs/1_75_0/more/getting_started/unix-variants.html" target="_blank" rel="noopener">官网</a><br>boost库分为静态头文件和需要Build的动态链接库部分，按照官网的教程下载并解压之后，</p>
<ul>
<li><p>静态头文件就在解压后的子目录boost下，如果不是安装在默认头文件目录如/usr/include下，而代码里是#include <boost xxx.hpp="">又不想全部改成#include “boost/XXX.hpp”时，可以用下面的命令来告诉编译器可以去哪里找这些头文件：</boost></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#如果没效果可以改成绝对路径</span></span><br><span class="line">CPLUS_INCLUDE_PATH=./boost</span><br><span class="line"><span class="built_in">export</span> CPLUS_INCLUDE_PATH</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于需要Build的动态库，按照官网的教程，首先来到解压后的文件夹boost_版本号下，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bootstrap.sh</span><br><span class="line">./b2 install</span><br></pre></td></tr></table></figure>
<p>然而默认情况下是安装在/usr/local路径下，如果没有管理员的写入权限，最好还是指定Build的路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bootstrap.sh --prefix=XXX/boost</span><br></pre></td></tr></table></figure>
<p>Build好的动态库就在子目录/lib下，但这个时候因为不是安装在默认路径（/usr/lib /lib /usr/local/lib），需要指明动态链接库的路径，在我的情况里是根据Makefile里的指令找到link.txt，在-lboost_datetime等前面加上 -L/XXX/boost/lib，也就是刚才指定Build的路径。不指明一般出现的报错是/usr/lib/ld: cannot find -lboost_datetime。如果使用gcc进行编译直接加上-L/XXX/boost/lib参数即可</p>
<p>如果出现下面的错误,参考该<a href="https://stackoverflow.com/questions/50339684/error-while-loading-shared-libraries-libboost-program-options-so-1-65-1/50345977" target="_blank" rel="noopener">回答</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">error <span class="keyword">while</span> loading shared libraries: libboost_filesystem.so.1.65.1: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">unset</span> LD_LIBRARY_PATH</span><br><span class="line"><span class="comment">## direct your library path to path of your running program. </span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/XXX/boost/lib:/XXX</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Linux安装和使用GSL库"><a href="#Linux安装和使用GSL库" class="headerlink" title="Linux安装和使用GSL库"></a>Linux安装和使用GSL库</h4><p><a href="https://ftp.gnu.org/gnu/gsl/" target="_blank" rel="noopener">官网</a></p>
<p>开始之前，不要在window系统上下载好压缩文件解压之后再上传到Linux系统上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget ftp://ftp.gnu.org/gnu/gsl/gsl-*.*.tar.gz<span class="comment">#*.*是版本号</span></span><br><span class="line">tar -zxvf gsl-*.*.tar.gz</span><br><span class="line"><span class="built_in">cd</span> gsl-*.*</span><br><span class="line">./configure --prefix=/XXX/gsl<span class="comment">#安装到指定目录</span></span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">make install</span><br><span class="line"><span class="comment">#完成后需要添加-L/XXX/GSL/lib到编译选项里，因为不是默认路径，否则会出现下面错误</span></span><br><span class="line">/usr/bin/ld: cannot find -lgsl</span><br><span class="line">/usr/bin/ld: cannot find -lgslcblas</span><br></pre></td></tr></table></figure>
<h4 id="c-11支持"><a href="#c-11支持" class="headerlink" title="c++11支持"></a>c++11支持</h4><p>编译错误：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">error: namespace <span class="string">"std"</span> has no member <span class="string">"uniform_real_distribution"</span></span><br><span class="line">error: namespace <span class="string">"std"</span> has no member <span class="string">"random_device"</span></span><br><span class="line">error: namespace <span class="string">"std"</span> has no member <span class="string">"mt19937"</span></span><br></pre></td></tr></table></figure>
<p>可能是因为ubuntu使用的是c++14的标准，很多c++11的用法不支持</p>
<p><strong>解决方法</strong></p>
<p>所有用到这些库的源文件中都包含如下头文件</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<h4 id="删除大文件前n行"><a href="#删除大文件前n行" class="headerlink" title="删除大文件前n行"></a>删除大文件前n行</h4><p>在Linux下删除大文件前n行有两种办法，推荐第二种：</p>
<ol>
<li><p>使用sed命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除前4行</span></span><br><span class="line"><span class="comment">#-i 直接修改读取的文件内容，而不是输出到终端</span></span><br><span class="line">sed -i <span class="string">'1,4d'</span> XXX</span><br><span class="line"><span class="comment">#删除第3行到最后一行</span></span><br><span class="line">sed -i <span class="string">'3,$d'</span> XXX</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用tail命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除前4行</span></span><br><span class="line"><span class="comment">#将要保留的数据重定向到新的文件中</span></span><br><span class="line">tail -n +3 old_file &gt; new_file</span><br><span class="line"><span class="comment">#保留后4行</span></span><br><span class="line">tail -n 4 old_file &gt; new_file</span><br><span class="line">rm old_file</span><br><span class="line">mv new_file old_file</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="使用nvprof工具"><a href="#使用nvprof工具" class="headerlink" title="使用nvprof工具"></a>使用nvprof工具</h4><p>nvprof是用来测试与优化CUDA程序性能的工具。它能从命令行收集、查看和分析数据。</p>
<p>使用时如果遇到错误提示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">The user does not have permission to profile on the target device</span><br></pre></td></tr></table></figure>
<p>如果加上sudo权限仍然报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo: nvprof: <span class="built_in">command</span> not found</span><br></pre></td></tr></table></figure>
<p>可以采取的一个解决办法是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo visudo</span><br><span class="line"><span class="comment">#注释掉以下语句并保存，在前面分别添加#号后Ctrl+X并输入Y回车保存退出</span></span><br><span class="line">Defaults env_reset</span><br><span class="line">Defaults mail_badpass</span><br><span class="line">Defaults secure_path=<span class="string">"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin"</span></span><br></pre></td></tr></table></figure>
<p>除了在命令行查看分析结果，还可以利用nvidia visual profiler对nvprof生成的分析报告进行可视化。通过-o参数生成nvvp文件进行进一步的分析：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvprof -o test_profile ./<span class="built_in">test</span></span><br></pre></td></tr></table></figure>
<p>如果遇到报错可以试试加上下面的参数：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvprof --unified-memory-profiling off -o test_profile ./<span class="built_in">test</span></span><br></pre></td></tr></table></figure>
<p>将生成的test_profile从服务器上下载到本地win10环境下。为了使用nvidia visual profiler，首先需要下载CUDA Toolkit：<a href="https://developer.nvidia.com/nvidia-visual-profiler，我可行的版本是CUDA10.2。" target="_blank" rel="noopener">https://developer.nvidia.com/nvidia-visual-profiler，我可行的版本是CUDA10.2。</a></p>
<p>在cmd命令行窗口输入nvvp测试安装是否成功，如果遇到下面的jdk报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a java runtime environment or java development kit must be available <span class="keyword">in</span> order to run nvvp</span><br></pre></td></tr></table></figure>
<p>原因是环境里缺少jdk，我可行的版本是jdk8：<a href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html。" target="_blank" rel="noopener">https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html。</a></p>
<p>安装完jdk后，将默认安装目录C:\Program Files\Java\jdk1.8\bin添加到系统环境变量中。</p>
<p>还需要设置CUDA库路径，否则nvvp打开后导入时会报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">unable to locate CUDA libraries</span><br></pre></td></tr></table></figure>
<p>方法是将该路径C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\extras\CUPTI\lib64添加到系统环境变量中。</p>
<p>这时重新打开nvvp把刚才的test_profile拖进来就有分析结果了。</p>
<h4 id="Visual-Code-Remote-SSH远程开发"><a href="#Visual-Code-Remote-SSH远程开发" class="headerlink" title="Visual Code Remote SSH远程开发"></a>Visual Code Remote SSH远程开发</h4><p>在Visual Code种Ctrl + Shift + X打开插件界面，安装Remote - SSH插件，接下来通过ssh key的方式免密登录：</p>
<ol>
<li>打开cmd窗口，输入命令<code>ssh-keygen -t rsa -b 4096</code>，一路回车，默认密钥生成在<code>C:\Users\XXX\.ssh</code>目录下。</li>
<li>打开目录下的id_rsa.pub文件，将其中的内容复制到远程主机的<code>~/.ssh/authorized_keys</code>文件中，没有该目录则<code>mkdir ~/.ssh</code>手动生成，没有该文件则<code>touch ~/.ssh/authorized_keys</code>手动生成。</li>
<li>在Visual Code配置远程主机的用户名和ip，以后就可以免密登陆了。</li>
<li>进一步安装c++扩展插件来使用go to definition等方便的功能：<ul>
<li>下载最新的离线安装包<a href="https://github.com/microsoft/vscode-cpptools/releases" target="_blank" rel="noopener">https://github.com/microsoft/vscode-cpptools/releases</a></li>
<li>在Visual Code中Extension窗口右上角···-Install from VSIX</li>
</ul>
</li>
</ol>
<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><h4 id="将conda环境加入jupyterlab"><a href="#将conda环境加入jupyterlab" class="headerlink" title="将conda环境加入jupyterlab"></a>将conda环境加入jupyterlab</h4><p>假设conda中现有一个虚拟环境star，现在想把它作为kernel加入到jupyterlab中，注意虚拟环境与kernel并不是同步的，即使激活star这个虚拟环境之后打开jupyterlab，使用的kernel仍然是默认的python环境，如果要使用star里面安装的各种版本的库，需要进行如下步骤：</p>
<ol>
<li>conda activate start</li>
<li>conda install ipykernel #安装 ipykernel库</li>
<li>ipython kernel install —user —name=star # —user表示当前用户</li>
<li>jupyter-lab #启动jupyterlab</li>
</ol>
<h4 id="远程使用jupyterlab"><a href="#远程使用jupyterlab" class="headerlink" title="远程使用jupyterlab"></a>远程使用jupyterlab</h4><p>有时如果需要利用服务器上的GPU资源来跑一些模型，就需要在服务器上运行jupyterlab，具体步骤如下：</p>
<p>首先建立ssh连接，将usr、remote_server、port_num替换</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -f usr@remote_server -N -L port_num:localhost:port_num</span><br></pre></td></tr></table></figure>
<p>如果需要，输入登录密码。之后再进行一次ssh连接登陆远程服务器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -X usr@remote_server</span><br></pre></td></tr></table></figure>
<p>运行jupyterlab，port_num与刚才选择的要保持一致，不与常见端口冲突即可，例如选择当天的日期1116等</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jupyter-lab --no-browser --port=port_num</span><br><span class="line"></span><br><span class="line">  To access the notebook, open this file <span class="keyword">in</span> a browser:</span><br><span class="line">      file:///xxxxxxxxxxxxxxxxxxxxx.html</span><br><span class="line">  Or copy and paste one of these URLs:</span><br><span class="line">         http://localhost:port_num/?token=jupyterlab_token</span><br></pre></td></tr></table></figure>
<p>复制cmd窗口最后一行的地址，在本地浏览器中打开，就可以使用服务器的资源来运行jupyterlab了。</p>
<h4 id="json-decoder-JSONDecodeError-Expecting-value-line-1column-1-char-0-报错解决"><a href="#json-decoder-JSONDecodeError-Expecting-value-line-1column-1-char-0-报错解决" class="headerlink" title="json.decoder.JSONDecodeError Expecting value line 1column 1 (char 0)报错解决"></a>json.decoder.JSONDecodeError Expecting value line 1column 1 (char 0)报错解决</h4><p><strong>解决方法</strong><br>将判断条件由</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br></pre></td></tr></table></figure>
<p>改为<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> response.content:</span><br></pre></td></tr></table></figure></p>
<p>  <img src="https://s1.ax1x.com/2020/06/10/toiGm4.png" alt=""></p>
<h4 id="Pycharm-from-xx-import出错"><a href="#Pycharm-from-xx-import出错" class="headerlink" title="Pycharm from xx import出错"></a>Pycharm from xx import出错</h4><p><strong>错误描述</strong><br>使用Pycharm的时候，使用from引入自己模块报错：<br><img src="https://s2.ax1x.com/2020/03/10/8CvYlj.png" alt=""></p>
<p><strong>原因</strong><br>pycharm不会将当前文件目录自动加入sourse_path，右键源代码目录make_directory as–&gt;sources root将当前工作的文件夹加入就行了</p>
<p><strong>解决方案</strong><br><img src="https://s2.ax1x.com/2020/03/10/8Cvt6s.png" alt=""></p>
<h4 id="Selenium-TypeError-init-takes-2-positional-arguments-but-3-were-given-解决方案"><a href="#Selenium-TypeError-init-takes-2-positional-arguments-but-3-were-given-解决方案" class="headerlink" title="Selenium TypeError  init() takes 2 positional arguments but 3 were given_解决方案"></a>Selenium TypeError  <strong>init</strong>() takes 2 positional arguments but 3 were given_解决方案</h4><p><strong>错误描述</strong><br>执行以下代码时会报错：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">input = wait.until</span><br><span class="line">(</span><br><span class="line">    EC.presence_of_element_located</span><br><span class="line">    (</span><br><span class="line">        By.ID, <span class="string">'search'</span> </span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>PyCharm会提示：TypeError: <strong>init</strong>() takes 2 positional arguments but 3 were given。</p>
<p><strong>问题来源</strong><br>在<a href="https://github.com/SeleniumHQ/selenium/blob/80adb709873815b88bb57494a6908f8b86285766/py/selenium/webdriver/support/expected_conditions.py#L53" target="_blank" rel="noopener">expected conditions.py</a>中找到<a href="https://github.com/SeleniumHQ/selenium/blob/80adb709873815b88bb57494a6908f8b86285766/py/selenium/webdriver/support/expected_conditions.py#L53" target="_blank" rel="noopener">presence_of_element_located</a>类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">presence_of_element_located</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" An expectation for checking that an element is present on the DOM</span></span><br><span class="line"><span class="string">    of a page. This does not necessarily mean that the element is visible.</span></span><br><span class="line"><span class="string">    locator - used to find the element</span></span><br><span class="line"><span class="string">    returns the WebElement once it is located</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, locator)</span>:</span></span><br><span class="line">        self.locator = locator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, driver)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> _find_element(driver, self.locator)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">EC.presence_of_element_located</span><br><span class="line">    (</span><br><span class="line">        By.ID, <span class="string">'search'</span> </span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>这里如果不加小括号，相当于输入了三个参数:self、By.ID、’search’，而presence_of_element_located类的_init_方法取的是两个参数self、locator，其中locator调用的是一个tuple(元组)，所以添加小括号后将(By.ID, ‘search’)作为一个整体对应于一个参数</p>
<p><strong>解决方案</strong><br>添加小括号</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">EC.presence_of_element_located</span><br><span class="line">    (</span><br><span class="line">        (By.ID, <span class="string">'search'</span> )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>参考资料</p>
<ol>
<li><a href="https://stackoverflow.com/questions/23661734/selenium-visibility-of-element-located-init-takes-exactly-2-arguments" target="_blank" rel="noopener">stackoverflow</a></li>
</ol>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>一亩三分地题库+新手上路答案</title>
    <url>/2019/09/11/%E4%B8%80%E4%BA%A9%E4%B8%89%E5%88%86%E5%9C%B0/</url>
    <content><![CDATA[<p>1.一亩三分地 新手上路 网站规则 - 满分5大米(适用于所有用户)<br>2.一亩三分地 考试中心 满分95大米(只适用于2019年之后注册的新人)<br>3.一亩三分地 每日答题 题库</p>
<a id="more"></a>
<h3 id="网站规则-满分5大米"><a href="#网站规则-满分5大米" class="headerlink" title="网站规则 - 满分5大米"></a>网站规则 - 满分5大米</h3><ol>
<li><p>一亩三分地发帖可以选择内容用hide进行隐藏。隐藏方式：[hide=200]隐藏的内容[/hide] 下面哪个选项hi‍‌‌‌‌‌‌‌‍‍‍‌‌‌‍‌‌‌‌‌de正确。 (1.0 分)<br><strong>A. 两人第一次华山论剑，争的是荣名与[hide=188]《九阴真经》[/hide]；</strong><br>B. 第二次在桃花岛过招，是为了郭靖与[hide=188]欧阳克[\hide]争婚。<br>C. 第三次海上相斗，生死只隔一线，但(hide=188)洪七公(\ hide)手下尚自容让；<br>D. 现下第四次恶战，[hide188]才是各出全力[/hide]，再无半点留情。<br>E. 洪七公伸指疾点欧阳锋背心[hide=188]“凤尾穴”(/hide)，要迫他松手。  </p>
</li>
<li><p>为什么一亩三分地除了租房广告找室友、学有飞友工友、本地版聚等少数版块之外，绝大多数板块都不允许拉群？ (1.0 分)<br>A. 地里信息是公开的，全部回复每个人都可以看到。而群里的信息，作为新入群的同学是无法看到历史消息的，并且无法查找<br>B. 公开自己的微信，将来可能会被人人肉或者骚扰<br>C. 地里的信息永远存在，有些群很可能不活跃甚至不存在了，里面的讨论也就消失了<br><strong>D. 以上全都理解并接受</strong></p>
</li>
<li><p>在下面哪些板块里留微信号等各种联系方式，事后可以要求版主删除？ (1.0 分)<br>A. 如果发在求职类板块里，可以删除，我的隐私你得尊重啊<br>B. 如果发在留学类板块里，可以删除，毕竟我年少不懂事嘛<br>C. 如果发在非正事、无聊帖子里，可以删除，毕竟没啥营养啊<br><strong>D. 绝大多数板块禁止留微信、拉群，如果你非要发出来，那就永远不删。另外，私下联系建议发站内信。</strong></p>
</li>
<li><p>为什么我们不鼓励用谐音或者各种拐弯抹角的说法来指代公司或者学校名称？比如：“湾区某元音开头和结尾公司” (1.0 分)<br>A. 也许有什么顾虑吧<br>B. 这样不好玩<br><strong>C. 写成这样子，别人看不懂，也搜不到。如果别人也这样写，你也看不懂、搜不到。信息没法分享和交流。</strong><br>D. 不知道</p>
</li>
<li><p>下面哪个说法是错误的？ (1.0 分)<br>A. 连续超过14天不登录，每天扣一个大米，直到大米数=100<br><strong>B. 抖包袱版看帖，不会消耗积分</strong><br>C. 可以消耗积分更改用户名（网站右上角设置-&gt;个人资料-&gt;更改用户名）<br>D. 看到干货帖子和回复，给作者加分，不会消耗我的积分<br>E. 每日签到、每日答题都可以拿到积分奖励<br>F. 手机app里也可以每日签到，好方便！<br>G. 给地里官方app五星好评，可以拿加分奖励: www.1point3acres.com/bbs/thread-446981-1-1.html<br>H. 在手工加分的帖子里，多次回复骗取积分，会被系统检测到，积分扣除+额外扣分！<br>I. 网站上方导航栏 -&gt; 道具中心，可以兑换匿名卡，把自己的帖子匿名。</p>
</li>
</ol>
<h3 id="网站规则-满分95大米"><a href="#网站规则-满分95大米" class="headerlink" title="网站规则 - 满分95大米"></a>网站规则 - 满分95大米</h3><ol>
<li><p>一亩三分地里有哪些方面的信息？ (5.0 分)<br><strong>A. 美国大学各专业录取信息<br>B. 美国和中国公司面试题目<br>C. 买房买车租‍‌‌‌‌‌‌‌‍‍‍‌‌‌‍‌‌‌‌‌房等生活信息<br>D. 移民签证H1B等信息<br>E. 公开课、刷题、做项目<br>F. 八卦我司版：美国公司评价（各公司员工发帖）<br>G. 抖包袱版：各公司工资offer的详细数目<br>H. 缘来如此版可以发帖征友<br>I. 职场达人：中美职场和职业发展话题<br>J. TOEFL/GRE考试信息</strong>  </p>
</li>
<li><p>一亩三分地有积分限制，请戳这里阅读《攒积分和消耗积分完全指南》然后回答：下面哪些说法正确？ (10.0 分)<br><strong>A. 认真做考卷，很容易拿到满分<br>B. 每日签到（网站右上方），奖励1颗大米<br>C. 每日答题（网站右上方），答对了奖励1颗大米，答错了消耗1颗<br>D. 设置头像，奖励一颗大米（只能获得一次）<br>E. 验证邮箱，奖励一颗大米（只能获得一次）<br>F. 管理员每日选择2-3个干货帖子全站置顶，大家会奖励给你大米<br>G. 分享面经、录取信息、工资数目等干货，或者积极参与讨论<br>H. 积极参与论坛各种活动，比如给地里官方app五星好评<br>I. 《攒积分和消耗积分完全指南》里有更多获取积分方法的说明<br>J. 积分变更记录在网站右上角设置-&gt;积分里能找到（2019年3月2日之前的部分记录不显示，之后的全显示）</strong>  </p>
</li>
<li><p>一亩三分地发帖可以选择内容进行隐藏。举个例子：<br>黄蓉哭了一会，抽抽噎噎的道：“我听爹爹说过，洪老前辈有一套武功，当真是天下无双、古今独步，甚至全真教的王重阳也忌惮三分，叫做……叫做……咦，我怎么想不起来啦，明明刚才我还记得的，我想求他教你，这套拳法叫做……叫做……”其实她哪里知道，全是信口胡吹。洪七公在树顶上听她苦苦思索，实在忍不住了，喝道：“叫做‘[hide=200]降龙十八掌[/hide]’！”说着一跃而下。上述段落里，“降龙十八掌”五个字被隐藏，只有积分&gt;=200分的用户才能看到。<br>隐藏方式：[hide=200]隐藏的内容[/hide] 注意不要添加多余的空格！第二个hide前面的斜线别弄反方向！下面隐藏的内容哪个被188分正确隐藏？真相只有一个！ (10.0 分)<br>A. 柯南的名字是[hide=188]工藤新一[\hide]<br>B. 柯南的名字是[hide]工藤新一[/hide=188]<br><strong>C. 柯南的名字是[hide=188]工藤新一[/hide]</strong><br>D. 柯南的名字是[hide=188]工藤新一[\ hide]<br>E. 柯南的名字是[hide=188]工藤新一{\hide}<br>F. 柯南的名字是(hide=188)工藤新一(\hide)  </p>
</li>
<li><p>在论坛发slack群、qq群、微信群等任何站外讨论方式，会如何？ (10.0 分)<br>A. 如果发在本地版聚、租房找室友、飞友学友工友版块，是允许的<br>B. 如果发在求职、面经、申请类板块里，都会被删帖扣分<br>C. 举报这些群，可能得到加分奖励<br><strong>D. 以上都正确</strong>  </p>
</li>
<li><p>下面哪种情况，管理员会按照你的要求，进行删帖？ (10.0 分)<br>A. 问了问题，得到了答案，然后我过河拆桥，删帖让其他人看不到<br>B. 尽管地里不允许，但是我到处留微信号，然后说隐私暴露要求删帖<br>C. 发帖赚到了积分，看到了有权限设置的内容，然后反悔<br><strong>D. 这些情况全都不删帖！</strong>  </p>
</li>
<li><p>你的一亩三分地账号很宝贵，在别的网站，考完托福你就离开了，申请完了你也不会再去。但是在一亩三分地，拿到录取以后，你找实习要回来，找全职工作要回来，申请OPT要回来，等H1b签证和绿卡要回来，甚至工作几年后要跳槽你也要回来看求职信息 - 随着地里的壮大，你会经常回来。<br>下面哪些方法可以保护你的账号？ (10.0 分)<br><strong>A. 给账号设置复杂密码，并且只在一亩三分地使用该密码。<br>B. 把地里的邮件放入白名单以免被判定为垃圾邮件而导致收不到密码重设等重要信息（地里从来不发广告）。<br>C. 绑定微信。如果账号有异常，会被系统要求扫码登录，而且扫码登录比输入密码方便。<br>D. 管理好用来注册账号的电子邮箱，如果连邮箱都没了，那就惨了。。。</strong><br>E. 别那么多有的没的，我爱咋地就咋地。  </p>
</li>
<li><p>下面哪些发帖行为，值得鼓励和倡导？ (10.0 分)<br>A. 随便发就是了，反正有版主和管理员给调整<br><strong>B. 读一下各个版块的置顶信息，了解网站规则。<br>C. 发帖之前，先浏览有哪些版块，帖子要发在对应的板块里。</strong><br>D. 帖子设置回复可见，大家必须回复才能看到，然后一堆人发“顶”<br><strong>E. 帖子标题最好能用一句话概括主要内容，方便大家阅读<br>F. 地里很多帖子有分类设置，方便大家查找，发帖确保分类信息设置合理</strong><br>G. 很多帖子有权限设置，我积分不够，到处发“看不到啊”、“积分不够啊”，水掉论坛  </p>
</li>
<li><p>新人积分不够。下面哪些做法正确？ (5.0 分)<br><strong>A. 分享干货、签到答题、参加活动等，争取早日攒够积分。</strong><br>B. 到处发帖抱怨积分不够、看不到，没准会有帮助呢。<br>C. 想各种办法绕开积分限制，比如要求楼主私下把信息发给我<br>D. 注册个小号，相互加分（嘿嘿，地里有后台系统检测哦）  </p>
</li>
<li><p>下面哪些方式，可以获得积分奖励？ (5.0 分)<br><strong>A. 分享干货，无论是录取信息、面经题目、工资数据，还是各种生活经验。</strong><br>B. 在帖子里给别人加分，告诉对方给我加回来。<br><strong>C. 认真参与网站里的讨论，贡献我的看法和信息</strong><br>D. 没有分享干货，但是可以缠着别人要积分<br><strong>E. 干货帖子被全站置顶或者公众号推出，至少拿100分，很多帖子拿到200+</strong><br>F. 我有信息可以分享，但是大家来给我加分吧，谁加了分我私下里发给你  </p>
</li>
<li><p>一亩三分地里可以购买VIP通行证，位于网站上方导航栏 -&gt; VIP。关于VIP，下面哪些说法正确？ (10.0 分)<br><strong>A. VIP可以瞬间解锁地里有权限的帖子，获得阅读权限<br>B. VIP下载附件不扣积分，不受权限限制<br>C. VIP用户可以自由搜索，不扣积分</strong><br>D. VIP是“免责金牌”，在地里不受规则约束，可以为所欲为<br><strong>E. VIP获得的只是阅读帖子权限，网站有的功能仍需积分（可以攒大米或者购买蓝莓）<br>F. VIP收入用来支付一亩三分地运营和发展的必要花费；我的打赏能支持一亩三分地做的更好</strong>  </p>
</li>
<li><p>下面哪种行为，在地里会被扣光积分，甚至封号？ (10.0 分)<br><strong>A. 戾气十足、人身攻击<br>B. 种种手段恶意刷分<br>C. 软硬广告或导流吸粉<br>D. 造谣或者发钓鱼贴<br>E. 多人共享账号<br>F. 违反版规，胡乱跨版发帖，到处留邮箱或微信号<br>G. 购买了VIP通行证，作为付费用户，违反网站规则。</strong>  </p>
</li>
</ol>
<h3 id="每日答题题库"><a href="#每日答题题库" class="headerlink" title="每日答题题库"></a>每日答题题库</h3><p>【题目】 公司之间级别对应，如下哪个是错误的？<br>Google T5 = Facebook E5/6<br>Uber Sr II = Lyft T6<br>Amazon L7 = Facebook E6<br>✓ Facebook L6 = Facebook E6</p>
<p>【题目】一亩三分地发帖可以选择内容用 hide 进行隐藏。隐藏方式：[hide=200]隐藏的内容[/hide] 下面哪个选项 hide 正确。<br>✓ A. 两人第一次华山论剑，争的是荣名与[hide=188]《九阴真经》[/hide]；<br>B. 第二次在桃花岛过招，是为了郭靖与[hide=188]欧阳克[\hide]争婚。<br>C. 第三次海上相斗，生死只隔一线，但(hide=188)洪七公(\ hide)手下尚自容让；<br>D. 现下第四次恶战，[hide188]才是各出全力[/hide]，再无半点留情。<br>E. 洪七公伸指疾点欧阳锋背心[hide=188]“凤尾穴”(/hide)，要迫他松手。</p>
<p>【题目】为什么一亩三分地除了租房广告找室友、学有飞友工友、本地版聚等少数版块之外，绝大多数板块都不允许拉群？<br>A. 地里信息是公开的，全部回复每个人都可以看到。而群里的信息，作为新入群的同学是无法看到历史消息的，并且无法查找<br>B. 公开自己的微信，将来可能会被人人肉或者骚扰<br>C. 地里的信息永远存在，有些群很可能不活跃甚至不存在了，里面的讨论也就消失了<br>✓ D. 以上全都理解并接受</p>
<p>【题目】在下面哪些板块里留微信号等各种联系方式，事后可以要求版主删除？<br>A. 如果发在求职类板块里，可以删除，我的隐私你得尊重啊<br>B. 如果发在留学类板块里，可以删除，毕竟我年少不懂事嘛<br>C. 如果发在非正事、无聊帖子里，可以删除，毕竟没啥营养啊<br>✓ D. 绝大多数板块禁止留微信、拉群，如果你非要发出来，那就永远不删。另外，私下联系建议发站内信。</p>
<p>【题目】为什么我们不鼓励用谐音或者各种拐弯抹角的说法来指代公司或者学校名称？ 比如：“湾区某元音开头和结尾公司”，你能猜到是哪家吗？<br>A. 也许有什么顾虑吧<br>B. 这样不好玩<br>✓ C. 写成这样子，别人看不懂，也搜不到。如果别人也这样写，你也看不懂、搜不到。信息没法分享和交流。<br>D. 不知道</p>
<p>【题目】下面哪个说法是错误的？<br>A. 连续超过 14 天不登录，每天扣一个大米，直到大米数=100<br>✓ B. 抖包袱版看帖，不会消耗积分<br>C. 可以消耗积分更改用户名（网站右上角设置-&gt;个人资料-&gt;更改用户名）<br>D. 看到干货帖子和回复，给作者加分，不会消耗我的积分 E. 每日签到、每日答题都可以拿到积分奖励<br>F. 手机 app 里也可以每日签到，好方便！<br>G. 给地里官方 app 五星好评，可以拿加分奖励: www.1point3acres.com/bbs/thread-446981-1-1.html H. 在手工加分的帖子里，多次回复骗取积分，会被系统检测到，积分扣除+额外扣分！<br>I. 网站上方导航栏 -&gt; 道具中心，可以兑换匿名卡，把自己的帖子匿名。</p>
<p>【题目】 下面哪个情况，不会消耗你的积分？<br>超过 14 天不登录<br>使用论坛搜索<br>下载附件<br>✓ 看到干货帖子和精华回复，给作者加分！  </p>
<p>【题目】 下面哪个州，没有 income tax?<br>✓ Nevada<br>New York<br>Nebraska<br>Massachusetts  </p>
<p>【题目】 下面哪个州，有 state income tax<br>Tennessee<br>Alaska<br>Washington<br>✓ Mississippi</p>
<p>【题目】 求内推如何作死？<br>一下子叫好多人给内推同一家公司<br>别人回复慢了就抱怨<br>简历上撒谎<br>✓ 这些都会作死  </p>
<p>【题目】 下面哪种方法，可以妥妥拿到积分？<br>上传头像<br>每日签到（需绑定微信）<br>分享干货<br>✓ 这些全都可以  </p>
<p>【题目】 回答别人的私信提问还需要消耗我 5 大米怎么办？<br>✓ 直接在版面回答，这样大家都能看见  </p>
<p>【题目】 下面哪种行为，在地里会被扣光积分，甚至封号？<br>✓ 这些全都会</p>
<p>【题目】一亩三分地发帖可以用 hide 语法隐藏内容。下面哪个写法正确？<br>✓ 柯南的名字是[hide=200]工藤新一[/hide]<br>柯南的名字是[hide=200]工藤新一[\hide]<br>柯南的名字是[hide=200]工藤新一[hide]<br>柯南的名字是[hide=200]工藤新一[/hide=200]  </p>
<p>【题目】 在 Linkedin 上求内推如何作死<br>看也不看对方情况，直接扔简历要求内推，国人必须帮助国人啊<br>写模板内容要求内推，不论男女都叫学姐<br>也不自我介绍，就要求对方介绍公司情况<br>✓ 这些都会作死</p>
<p>【题目】一亩三分地鼓励如何发面经？<br>遇到有人留邮箱，私下发面经的，点举报<br>积分隐藏[hide==188]内容[/hide]<br>✓ 以上都正确</p>
<p>【题目】 下面哪个大学在华盛顿州？<br>Washington University<br>✓ University of Washington<br>George Washington University<br>Washington College</p>
<p>【题目】下面哪个大学不在 Virginia/DC 附近<br>✓ Washington and Jefferson College<br>Trinity Washington University<br>George Washington University<br>Washington and Lee University</p>
<p>【题目】 下面哪个州，对公司友好，所以吸引了美国很多公司注册？<br>加利福尼亚<br>✓ 特拉华<br>佛罗里达<br>纽约</p>
<p>【题目】 下面哪个州，有 state income tax<br>South Dakota<br>Wyoming<br>✓ North Dakota<br>Tennessee</p>
<p>【题目】 下面哪个州，没有 state income tax<br>New York<br>New Jersey<br>✓ New Hampshire<br>New Mexico</p>
<p>【题目】 下面哪个州，没有 state income tax?<br>✓ Florida<br>Georgia<br>Hawaii<br>Idahoda  </p>
<p>【题目】 下面哪个州，没有 state income tax?<br>Alabama<br>✓ Alaska<br>Arizona<br>Arkansas  </p>
<p>【题目】下面哪个州冬天最暖和？<br>Minnesota<br>✓ Oklahoma<br>Michigan<br>Massachusetts</p>
<p>【题目】下面哪个大学实际上不存在？<br>University of California, San Francisco<br>University of Massachusetts, Dartmouth<br>✓ University of Michigan, Twin City<br>University of Nevada, Las Vegas</p>
<p>【题目】下面哪所大学所在城市不是波士顿？<br>✓ Boston College<br>Berklee College Of Music<br>Northeastern University<br>Boston University</p>
<p>【题目】下面哪个说法错误？<br>伊利诺伊大学在芝加哥有校区<br>✓ 芝加哥是美国著名的雨城<br>美国西北大学在芝加哥有校区<br>芝加哥 skydeck 上可以看到四个州</p>
<p>【题目】 Which company is the largest single✓site employer in the US?<br>Walmart<br>Ford<br>Costco<br>✓ Disney World</p>
<p>【题目】 下面哪种方法，可以妥妥拿到积分？<br>分享干货<br>上传头像<br>每日签到（需绑定微信）<br>✓ 这些全都可以</p>
<p>【题目】 下面哪家公司的总部不在西雅图<br>亚马逊<br>阿拉斯加航空公司<br>星巴克<br>✓ 波音</p>
<p>【题目】 给论坛 ios 或者安卓手机应用留评价如何获取 50 大米？<br>留 5 星评价<br>截屏作为证据<br>上传到第一个大区的”官方开发版“<br>✓ 以上步骤都需要</p>
<p>【题目】 地里发帖可以隐藏内容。假如要设置 200 积分以上才可以看到，下面哪个语法正确？<br>[hide]想要隐藏的内容[/hide]<br>[hide=200 ]想要隐藏的内容[/hide]<br>✓ [hide=200]想要隐藏的内容[/hide]<br>[hide=200]想要隐藏的内容[hide]  </p>
<p>【题目】 地里面经数目最多的是哪家公司？<br>Facebook<br>Google<br>✓ Amazon<br>Uber</p>
<p>【题目】 Negotiate 工资的时候，哪种做法有利于得到更大的包裹？<br>拿地里抖包袱版的工资数字要对方 match<br>直接告诉对方自己目前薪酬，让对方看着良心办<br>开一个天价，谈不拢就散伙<br>✓ 精读地里谈工资宝典，知己知彼，百战不殆</p>
<p>【题目】 which state is University of Miami located?<br>California<br>Nevada<br>✓ Florida<br>Ohio</p>
<p>【题目】 下面哪个城市没有 SUNY（纽约州立大学）校区？<br>Albany<br>Buffalo<br>✓ Fulton<br>Stony Brook</p>
<p>【题目】 下面哪个州里有 Disney World？<br>✓ Florida<br>New York<br>North Carolina<br>Texas</p>
<p>【题目】 下面哪所大学所在城市不是波士顿？<br>✓ MIT<br>Boston University<br>Northeastern University<br>Emerson College</p>
<p>【题目】 关于旧金山市中心描述，下面哪个不正确？<br>走路得看着路，很多流浪汉，地上屎尿一不小心会踩上<br>车里一定不要放东西，但即使不放，也可能被砸车玻璃<br>Uber/Airbnb/Pinterest/Twitter 等著名科技公司都在 SOMA 区<br>✓ 旧金山创业公司很多，被称为“硅谷”</p>
<p>【题目】 一亩三分地是哪年创立的？<br>✓ 2009<br>2011<br>2013<br>2015</p>
<p>【题目】 下面哪个州在美国西海岸<br>VirginiaNorth<br>DakotaMaine<br>✓ Washington</p>
<p>【题目】 which state is University of Miami located?<br>Ohio<br>✓ Florida<br>Nevada<br>California</p>
<p>【题目】 加州大学有多个分校，下面哪个成立时间最短？<br>UC Davis<br>✓ UC Merced<br>UC Riverside<br>UC Santa Cruz</p>
<p>【题目】 下面哪个专业，不是 STEM，OPT 没法延期？<br>会计学以前不是，现在很多学校 stem 获批<br>数据科学<br>EECS<br>✓ 教育学</p>
<p>【题目】 哪种选校策略最合理？<br>按照排名高低选，谁高谁就好<br>交给中介选，反正不想操心<br>所有学校都申，蒙中哪个算哪个<br>✓ 根据自己下一步职业和学业目标，参考地里数据和成功率，认真斟酌</p>
<p>【题目】 一亩三分地是谁创立的？<br>✓ Warald<br>俞敏洪<br>李大辉<br>徐小平</p>
<p>【题目】 下面几个州，哪个离美国首都最远？<br>Maryland<br>Delaware<br>✓ North Carolina<br>Virginia</p>
<p>【题目】 地里数据科学类职位面经放在在什么版最合理？<br>数据科学版<br>美国面经版数据科学分类<br>✓ 数科面经版<br>找工求职版</p>
<p>【题目】 下面哪个公司总部在圣地亚哥？<br>✓ Qualcomm<br>AMD<br>Nvidia<br>Netflix</p>
<p>【题目】 下面哪种情况，管理员会按照你的要求，进行删帖？<br>问了问题，得到了答案，然后我过河拆桥，删帖让其他人看不到<br>发帖赚到了积分，看到了有权限设置的内容，然后反悔<br>尽管地里不允许，但是我到处留微信号，然后说隐私暴露要求删帖<br>✓ 这些情况全都不删帖！</p>
<p>【题目】 Miami University 在哪个城市<br>Miami, Florida<br>Las Vegas, Nevada<br>✓ Oxford, Ohio<br>Los Angeles, California</p>
<p>【题目】 想找室友或者当房东，帖子发在哪里？<br>✓ 租房广告|找室友版<br>房地产版<br>生活版<br>面经版</p>
<p>【题目】 在论坛发 slack 群，qq 群，微信群，任何站外讨论方式，会如何？<br>如果发在求职面经大区，申请大区，都会被删帖扣分<br>举报这些群，可能得到加分奖励<br>✓ 以上都正确<br>如果发在版聚，或者本地版块，是允许的</p>
<p>【题目】 下面哪类版块，可以拉群，而且不会被警告扣分？<br>录取结果汇报<br>求职、面经<br>内推<br>✓ 学友工友、找室友或者版聚本地</p>
<p>【题目】下面哪个说法错误？<br>雪城大学尽管在纽约州，但是离纽约城很远！<br>✓ 中国驻纽约领事馆位于法拉盛中国城，周围全是好吃的！<br>哥伦比亚大学离纽约中央公园很近<br>纽约州立大学石溪分校学费很便宜  </p>
<p>【题目】 下面哪个学术会议不是机器学习领域的？<br>CVPR<br>ICML<br>SIGKDD<br>✓ ICSE</p>
<p>【题目】 下面哪个童话故事不是安徒生写的<br>✓ 尼尔斯骑鹅旅行记<br>冰雪女王<br>卖火柴的小女孩<br>国王的新装</p>
<p>【题目】 下面哪个作家是英国人？<br>✓Charles Dickens<br>Ernest Hemingway<br>Victor Hugo<br>Alexander Pushkin</p>
<p>【题目】 income tax on wages<br>✓North Dakota<br>South Dakota<br>Wyoming<br>Teness…  </p>
<p>【题目】 下面哪个machine learning 的模型不是supervised<br>Logistic regression<br>✓Clustering<br>SVM<br>Decision Tree</p>
<p>【题目】 Apollo 11是哪一年登月的？<br>1969  </p>
<p>【题目】 下面哪个公司的streaming service不是会员subscription付费模式运营的？<br>✓tubi</p>
<p>【题目】 著名篮球运动员姚明效力的NBA球队是休斯敦火箭队。取名“ 火箭队”是因为休斯敦是美国著名的?<br>钢城<br>汽车城<br>✓ 宇航工业城<br>电影城</p>
<p>【题目】 音乐家贝多芬出生于哪国？<br>✓ 德国<br>法国<br>意大利<br>英国  </p>
<p>【题目】 下面哪个Ivy League，离东海岸最远？<br>Brown<br>Dartmouth<br>Princeton<br>✓ Cornell</p>
<p>【题目】 美国哪个州没有夏令时？<br>南达科他州<br>爱荷华州<br>✓亚利桑那州<br>阿肯色州</p>
<p>【题目】 下面哪部作品是喜剧？<br>麦克白<br>李尔王<br>✓仲夏夜之梦<br>哈姆雷特</p>
<p>【题目】 下面哪个公司总部不在湾区？<br>google<br>✓snapchat<br>facebook<br>Apple</p>
<p>【题目】 下面哪所纽约高校坐落于中央公园附近？<br>Fordham University<br>New York University<br>New York Institute of Technology<br>✓Columbia University</p>
]]></content>
      <tags>
        <tag>一亩三分地</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库的几个证明题</title>
    <url>/2021/05/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%81%E6%98%8E/</url>
    <content><![CDATA[<p>数据库的几个证明题</p>
<a id="more"></a>
<p>一：证明R∈ 3NF，则R 也∈ 2NF。</p>
<p>证明：采用反证法。设R不是2NF，则有非主属性（Z）对码（X）存在部分函数依赖，即存在Y包含于X，X→Y，Y→Z，也就是Z传递依赖于X。这与3NF范式的定义相矛盾，所以如果R∈ 3NF，则R 也∈ 2NF。</p>
<p>二：证明R ∈ BCNF ，则 R 也∈ 3NF。</p>
<p>证明：采用反证法。设 R 不是 3NF ，则必然存在这样的码 X ，属性组 Y 和非主属性 Z （ Z 不∈ Y ），使得 X→Y （ Y 不 -&gt;X ）， Y→Z ，这样 Y→Z 函数依赖的决定因素 Y 不包含码，这与 BCNF 范式的定义相矛盾，所以如果 R ∈ BCNF ，则 R 也是 3NF 。</p>
<p>三：证明R ∈ BCNF，则 R 也∈ 2NF。</p>
<p>证明：采用反证法。假设R不属于2NF，存在X对Y的部分函数依赖，存在X的真子集X’有X’ -&gt; Y，又因为X’是码X的真子集，X’不能包含码，则X’-&gt;Y与R属于BCNF矛盾。</p>
<p>四：试由Armostrong公理系统推导出下面三条规则：</p>
<p>（1）合并规则：若X→Z，X→Y，有X→YZ</p>
<p>（2）伪传递规则：由X→Y，WY→Z，有XW→Z</p>
<p>（3）分解规则：X→Y，Z包含于Y，有X→Z</p>
<p>证明：</p>
<p>（1）已知X→Z，由增广律知XY→ZY，又因为X→Y，可得XX→XY→YZ，最后根据传递律得X→YZ。（因为XX等于X）</p>
<p>（2）已知X→Y，由增广律得XW→WY，又因为WY→Z，所以XW→WY→Z，通过传递律可知XW→Z。</p>
<p>（3）已知Z包含于Y，根据自反律得Y→Z，又因为X→Y，所以由传递律可知得X→Z。</p>
]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>安卓Activity移植</title>
    <url>/2018/09/07/%E5%AE%89%E5%8D%93Activity%E7%A7%BB%E6%A4%8D/</url>
    <content><![CDATA[<p>这篇文章记录一下之前将写的一个Activity加入现有应用时遇到的坑<br><a id="more"></a></p>
<h2 id="找不到R"><a href="#找不到R" class="headerlink" title="找不到R"></a>找不到R</h2><p>&#8194;这个问题移植完后马上就会遇到，在将layout里的xml文件等都复制过来后，如果问题还存在，直接Android Studio中 Build-Clen Project 一次，问题基本上就解决了</p>
<h2 id="包名"><a href="#包名" class="headerlink" title="包名"></a>包名</h2><p>&#8194;形如com.package.XXXXX等问题，复制代码文件时IDE会自动帮我们修正，但布局文件中IDE是不会帮我们纠正过来的,例如<center>`"tools:context=".MainActivity""`</center>就要进行修改，或者在标签中出现com.package.XXXX没有修改的情况，只能一个个xml文件去找了</p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>&#8194;如果在移植的Activity中继承了Application类创造了自己的，那移植过去后一定要在AndroidManifest.xml文件的\<application>里的android:name中进行修改</application></p>
<h2 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h2><p>&#8194;最后就是一直过去后要在AndroidManifest.xml文件中添加移植过去的Activity，不然运行时会报错，Logcat中也会给出建议</p>
]]></content>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title>托福备考</title>
    <url>/2019/09/09/%E6%89%98%E7%A6%8F%E5%A4%87%E8%80%83/</url>
    <content><![CDATA[<p>8月25号香港荃湾考试局托福首考，9月4号出分102成功分手，R29L28S22W23，没有报过补习班，分享自己复习的经验，希望能帮助大家<br><a id="more"></a></p>
<center class="half">
<img src="https://s2.ax1x.com/2019/09/08/n3jKfg.png" width="200px" alt="托福">  
</center>

<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我也会把当初有帮助的知乎回答贴在这里，我自认为自己的写作准备方法是有效的，只是可能准备时间不是很充分没发挥出来，话不多说进入正题:<br>我把自己积累的素材和格言等都放在github上，有需要的话可以自己获取：<br><a href="https://github.com/BitHub00/TOEFL" target="_blank" rel="noopener">托福个人积累素材及格言等</a></p>
<h3 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h3><p>阅读向来是中国考生的强项，我这次最好的单项也是阅读。我觉得最主要是平时要多读点英文材料，例如时事评论或者论文，然后考试时注意时间的把控，就没什么大问题了，当然也可以按照传统的复习方法使用托福模考软件多做tpo</p>
<h3 id="听力"><a href="#听力" class="headerlink" title="听力"></a>听力</h3><p>八月份的托福改革，减少了一篇听力lecture，好处是考试时不会这么累，因为托福的听力长度一般比较长，而且偏学术，需要全程保持专注和紧张，同时因为开考的时间不一致，可能你在听力时别人在考口语，会造成影响。坏处是题目量减少之后容错率降低，拿高分会变难一点，不过也不用顾虑太多，多练习才是王道。还有一点就是，网上说新的改革里加入了英国口音，我没怎么留意，然后说听力变成了1.2倍速度，我觉得这个不实，感觉和平时练的tpo语速是一样的，不要听信网上的说法给自己制造焦虑。<br>我复习的方法来自知乎，觉得还是挺有效的，如果只是单纯做tpo然后对答案其实并不会提升自己的听力水平，这里贴出来给大家参考：</p>
<p><img src="https://s2.ax1x.com/2019/09/09/nJWQKS.png" alt="托福听力"><br><a href="https://www.zhihu.com/question/20407472/answer/83390431" target="_blank" rel="noopener">如何快速提高托福听力水平？ - 梁跃的回答 - 知乎</a></p>
<h3 id="口语"><a href="#口语" class="headerlink" title="口语"></a>口语</h3><p>口语部分在八月改革之后改动很大，这次考试我也确认了网上的说法，即第一部分和第五部分是直接删掉了，因为是第一次考托福，考口语的时候我比较紧张，主要是考试界面的那个倒计时增添了我的紧张，总怕说不完，教训是平时复习的时候应该多掐表按照时间来练习，复习方法来自豆瓣：<br><a href="https://www.douban.com/group/topic/43613327/" target="_blank" rel="noopener">托福口语备考</a></p>
<h3 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h3><p>写作的确需要长期积累才能出效果，我个人觉得自己的复习方法是正确的，这次的分数23也还行，可能需要积累到一定程度才会出效果。那就是对某些话题，直接上google看本土人士是怎么写的，不要去背很多范文，因为托福写作毕竟也会跟以前的考试进行查重比对，而且很多范文说实在也是我们国人写的，怎么都没有外国人地道，我拿一篇举例，旅游的好处：</p>
<center>
<font size="5">11 Reasons Why Travel Makes You a Happier Person</font>  
</center>

<p>&#8195;I feel happy when I’m gaining new experiences and insights, and challenging my boundaries. Travel is the perfect catalyst for happiness, as it has allowed me to experience the natural, cultural and man-made wonders of the world. Being in foreign lands, it also continuously forces me to step out of my comfort zone - a great confidence-builder. Travel expands our capacity for wonder, joy and appreciation of the amazing diversity on our lovely planet.  </p>
<ol>
<li>FIND YOUR SELF-CONFIDENCE BY DEALING WITH UNEXPECTED SITUATIONS<br>&#8195;There comes a time when everyone must deal with an unexpected situation when they’re on the road. Even if you plan your trip to the letter, things can take a surprise turn. Whatever happens, there is a way around the problem and knowing that you can deal with these situations is a big boost to self-confidence and therefore your happiness.</li>
<li>HAPPINESS IS INFECTIOUS<br>&#8195;When locals are happy, smiling and friendly, <font color="red">it has an immediate knock-on effect. </font>I found the people of Thailand and Laos to be notably friendly and cheerful, despite the relative poorness of these countries and the former in particular having a very recent traumatic history. When faced with those <font color="red">big beaming smiles</font>, it’s hard to be annoyed at the hassling you might experience at busy sites like Angkor Wat; <font color="red">putting that knee-jerk irritation to one side instantly lifts your mood</font> and is a good habit to take home.</li>
<li>BEING AWAY MAKES YOU APPRECIATE FAMILY AND HOME<br>&#8195;Being away from things we often take for granted — family, close friends, home — makes us appreciate them more. Calling home isn’t a chore, but something to look forward to: <font color="red">no one enjoys listening to your envy-inducing travel stories more than your parents, so it’s the perfect excuse to wax lyrical about whatever place with which you’ve just fallen in love.</font></li>
<li>YOU MAKE NEW FRIENDS<br>&#8195;It’s much easier to make new friends on the road than it is at home, <font color="red">where people are less inclined to chat to strangers on a bus or strike up conversation in a bar. When people are away from home, there seem to be less boundaries to cross</font> and making friends becomes much easier, whether it’s a local curious to know where you’ve come from or a fellow traveler keen to have someone with whom to enjoy a beer or share a taxi. Social interactions make us happier and <font color="red">increasing our social circle</font> means that we’re talking more and meeting different, interesting people, which hopefully means we’re learning more, too.</li>
<li><font color="red">DETOX FROM SOCIAL MEDIA</font>  
&#8195;Social media can be used for both good and bad, but it's healthy for everyone to have a break from the internet every once in a while. Wi-fi is so prevalent that it's hard to turn off and you can quite often find yourself tuning out whatever amazing place you're in with your face in your phone, checking Twitter, scrolling through your Facebook feed, checking your emails... stop. Turn it off. Better yet, find somewhere with no reception and no wi-fi so that you don't have a choice. <font color="red">It's liberating and allows you to better enjoy the 'here and now', which nicely ties into the following point.</font></li>
<li>GETTING SOME ‘YOU’ TIME<br>&#8195;<font color="red">Traveling gives us breathing space that is often lost in our usual day-to-day existence.</font> Having a moment to take advantage of peace and quiet and to simply ‘be’ allows us to let go of stress and tension and just enjoy being in the moment — a key focus of meditation and a practice you can take home with you. If you’re traveling with a partner, it’s a chance to spend time with only each other for company, which is a thought that probably shouldn’t fill you with dread.</li>
<li>EDUCATION, EDUCATION, EDUCATION<br>&#8195;Whether it’s learning a new skill such as cooking Thai food or learning a new language, travel presents ways in which we can further our knowledge and education. Learning makes our brains more active, which psychologists have found increases our level of happiness - particularly when learning something we find enjoyable.</li>
<li>GET A VITAMIN D BOOST<br>&#8195;Whilst it’s a bit of myth that you need to be on a sun-lounger for twelve hours to feel the full effects of vitamin D (20 minutes of exposure to sunlight is enough), there’s no doubt that in the same way that the cold and dark of winter makes us unhappy (feeling the effects of seasonal affective disorder or SAD), sunshine and warmth generally put us in a much better mood. A beach break is a great way to relax and enjoy the health benefits of a warm climate. Admittedly, this is more of a short-term boost, but a healthy glow makes everyone feels better and lasts for a few weeks after your trip is over.</li>
<li>YOU’RE MORE INTERESTING<br>&#8195;You don’t need to be a ‘travel bore’ to have a few interesting stories to tell. <font color="red">Traveling throws up a lot of bizarre, funny and sometimes serious situations that relating back to people will make you — at least — feel interesting. Making someone laugh is an easy way to instantly bump up your self-esteem</font>, so hold on to those embarrassing memories — no matter how much they might make you cringe.</li>
<li>NEW EXPERIENCES GIVE US MOMENTS TO REMEMBER<br>&#8195;For most people, travelling is about the new experiences. <font color="red">I will never forget that moment of awe when I stood watching the sunlight leak out around the ancient temple of Chiang Mai in Thailand at sunrise, the sky turned a striking shade of violet: it was one of the most extraordinary sights I’ve ever seen. Recalling memories of happiness can sustain a feeling of contentment long after the moment has passed, and new experiences are memories that can stick with you forever</font>.</li>
<li>THE EFFECTS OF TRAVELING AREN’T JUST SHORT-TERM<br>&#8195;Aside from making you happier in the short-term, traveling can make you a much more contented, happy and relaxed person in the long run, too. Of course, most travel enthusiasts are constantly planning their next trip, but when we’re at home or past a point of being able to jet off whenever we like, past travels leave us with the memories and personal skills - such as confidence, broad-mindedness, friends and a more worldly perspective — that make people happy. And that’s why travel makes you a happier person.</li>
</ol>
<p>标红的那些部分是我个人认为很地道的表达，而且关于旅行如何让我们更幸福文章里就列举了11个观点，很多观点都很新颖，如果我们自己来写旅行，能写到多少个观点，而且有多少个大家想的都是一样的？所以我觉得这样子积累对写作会很有帮助。<br>至于素材积累，我自己使用的网站有：</p>
<ul>
<li><a href="https://techcrunch.com/" target="_blank" rel="noopener">techcrunch</a></li>
<li><a href="http://debate.org/" target="_blank" rel="noopener">Debate</a>  </li>
</ul>
<p>更多的时候是在google上搜索特定话题的时候，直接看靠前面的搜索结果，因为好文章不一定都能集中在同一个网站</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>不管什么复习方法，其实多练习才是王道，认定一个复习方法持之以恒就可以了，不存在什么最好最高效的方法，同时要有自己的主见，不要知乎上说什么就信什么。祝愿大家早日和托福分手</p>
]]></content>
      <tags>
        <tag>托福</tag>
      </tags>
  </entry>
  <entry>
    <title>宝贝陈列室</title>
    <url>/2019/07/25/%E5%AE%BF%E8%88%8D%E4%B8%80%E8%A7%92/</url>
    <content><![CDATA[<p><font size="3">展示一下自己收来的各种雕像和CD~</font><br><a id="more"></a></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDA6x.jpg" alt="全览"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDk11.jpg" alt="致远星贵族小队"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDFpR.jpg" alt="不义联盟"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDEX6.jpg" alt="阿卡姆起源限定"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDPh9.jpg" alt="电视墙背景点亮"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDcHU.jpg" alt="全览"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDyuV.jpg" alt="自制老爷场景"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZD2EF.jpg" alt="杰洛特"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZD6BT.jpg" alt="老爷与丑爷"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDW4J.jpg" alt="雕像与CD"></p>
<p><img src="https://s2.ax1x.com/2019/07/25/eZDRN4.jpg" alt="正联星战与魔戒"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZzHf0.jpg" alt="蝙蝠车"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZzqpV.jpg" alt="全览"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZzTkn.jpg" alt="猫女与蝙蝠车"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZzfOg.jpg" alt="阿卡姆骑士"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZzITs.jpg" alt="阿卡姆骑士"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZz4mQ.jpg" alt="False God"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZz5wj.jpg" alt="蝙蝠战衣"></p>
<p><img src="https://s2.ax1x.com/2019/09/04/nZz7Yq.jpg" alt="蝙蝠战衣"></p>
]]></content>
      <tags>
        <tag>宿舍</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵求导和迹</title>
    <url>/2019/10/21/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E5%92%8C%E8%BF%B9/</url>
    <content><![CDATA[<p>看到的一篇很好的介绍矩阵求导和迹的<a href="https://www.zybuluo.com/wjcper2008/note/662700" target="_blank" rel="noopener">文章</a>，mark下来</p>
<a id="more"></a>
<p><img src="https://s2.ax1x.com/2019/10/21/K112z8.png" alt=""></p>
]]></content>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>毕业设计</title>
    <url>/2019/11/04/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p>以本文记录自己毕业设计中的学习过程。<br><a id="more"></a></p>
<h2 id="NRLMF与GRNMF"><a href="#NRLMF与GRNMF" class="headerlink" title="NRLMF与GRNMF"></a>NRLMF与GRNMF</h2><p>首先是老师让我看的两篇论文</p>
<ul>
<li>GRNMF：<a href="https://academic.oup.com/bioinformatics/article/34/2/239/4101940" target="_blank" rel="noopener">a graph regularized nonnegative matrix factorization method for identifying microRNA-disease associations</a></li>
<li>NRLMF：<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004760" target="_blank" rel="noopener">Neighborhood Regularized Logistic Matrix Factorization for Drug-Target Interaction Prediction</a>  </li>
</ul>
<p>以下是自己的一点笔记：</p>
<script type="math/tex; mode=display">
\min_{W,H}\lVert Y-WH^T \rVert^2_F</script><p>两篇论文共同点：都利用了样本点的邻域信息，GRNMF为miRNA-disease，NRLMF为drug-target，而两篇论文都选取了前k个相似的样本而不是全部相似样本，目的是为了避免引入噪声。使用[0,1]区间连续取值的相似度而不是0/1的二值变量，相似度越大的两个样本它们各自的特征向量应该越相似。</p>
<h3 id="GRNMF"><a href="#GRNMF" class="headerlink" title="GRNMF"></a>GRNMF</h3><p>模型所使用的数据集没有提供现成的相似度，因此论文的一个创新点在于计算miRNA和disease的相似度矩阵，其中disease使用的是根据现有的有向无环图结构类型的数据进行计算，而miRNA是通过引入gene作为中介来计算，使用的是基因网络数据HumanNet。</p>
<p>在得到目标式进行优化之前，GRNMF的另一个创新之处在于对输入的数据矩阵Y先利用刚刚得到的相似度来进行更新，想法同NRLMF一样，也是认为没有观测值的miRNA或disease会造成精度的损失。这里同样是根据相似性去取一个k近邻，但与NRLMF不同之处在于不是更新特征向量，而是对输入矩阵逐行（miRNA）、逐列（disease）进行更新分别得到矩阵$Y_m$和$Y_d$。</p>
<script type="math/tex; mode=display">
Y_m(m_q) = \frac{1}{Q_m}\textstyle\sum_{i=1}^Kw_iY(m_i)</script><p>对不存在观测值即$Y_{ij}$=0的数据用$\frac{Y_m + Y_d}{2}$来进行更新。接下来就是构建目标式，同时引入邻域信息。</p>
<h4 id="GRNMF中的邻域"><a href="#GRNMF中的邻域" class="headerlink" title="GRNMF中的邻域"></a>GRNMF中的邻域</h4><script type="math/tex; mode=display">
\lambda_m\sum_{i,p=1}^n\lVert w_i-w_p \rVert^2S_{ip}^{m*}</script><script type="math/tex; mode=display">
S_{ij}^{m*}=X_{ij}^mS_{ij}^m</script><script type="math/tex; mode=display">
X_{ij} =
\begin{cases}
1, i\in N(m_j) \And j\in N(M_i) \And m_i,m_j\in C \\\\
0, i\notin N(m_j) \And j\notin N(M_i) \And m_i,m_j\notin C \\\\
\frac 1 2, otherwise
\end{cases}</script><p>GRNMF中利用了两种领域信息，第一种由ClusterOne算法产生，算法的结果有分成多少簇，哪些miRNA在同一簇内；第二种由k近邻产生，衡量是否近邻的相似度由论文提出的方法进行计算，miRNA和target分别用不同的方法计算。</p>
<h3 id="NRLMF"><a href="#NRLMF" class="headerlink" title="NRLMF"></a>NRLMF</h3><p>论文的目标是建模计算某个药物会对某个目标产生作用的概率值，概率的计算使用的是矩阵分解后得到的特征向量，药品和目标分别对应u和v</p>
<script type="math/tex; mode=display">
p_{ij} = \frac{exp(u_iv_j^T)}{1 + exp(u_iv_j^T)}</script><p>论文的一个创新之处在于，对于已经得到实验验证的样本，模型中会赋予更高的权重。对于样本出现的概率使用的是似然函数进行估计，同时对特征矩阵U和V假设高斯先验分布，均值为0，方差为$\sigma^2​$。可以证明这等同于引入L2正则。优化的目标式就等同于最大化取对数后的似然函数。</p>
<p>(6)式推导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(U,V|Y,\sigma^2_d,\sigma^2_t) &\propto  p(Y|U,V)p(U|\sigma^2_d)p(V|\sigma^2_t) \\
logp(U,V|Y,\sigma^2_d,\sigma^2_t) &= log(\prod^m_{i=1}\prod^n_{j=1}p_{ij}^{cy_{ij}}(1-p_{ij})^{(1-y_{ij})}\prod_{i=1}^m\frac{1} {\sqrt{2\pi} \sigma_d}e^{-\frac{u_i^2}{2\sigma_d^2}}\prod_{j=1}^n\frac{1} {\sqrt{2\pi} \sigma_t}e^{-\frac{v_j^2}{2\sigma_t^2}} \\
&=\sum_{i=1}^m\sum_{j=1}^n[cy_{ij}logp_{ij}+(1-y_{ij})log(1-p_{ij})]+\sum_{i=1}^m[log(\frac{1}{\sqrt{2\pi}\sigma_d})-\frac{u_i^2}{2\sigma_d^2}]\\+\sum_{j=1}^n[log(\frac{1}{\sqrt{2\pi}\sigma_t})&-\frac{v_j^2}{2\sigma_t^2}] \\
&=\sum_{i=1}^m\sum_{j=1}^ncy_{ij}[u_iv_j^T-log(1+e^{u_iv_j^T})]+(y_{ij}-1)log(1+e^{u_iv_j^T})-\frac{1}{2\sigma_d^2}\sum_{i=1}^m\lVert u_i \rVert^2 \\-\frac{1}{2\sigma_t^2}\sum_{j=1}^n\lVert v_j \rVert^2&+\sum_{i=1}^mlog(\frac{1}{\sqrt{2\pi}\sigma_d}+\sum_{j=1}^nlog(\frac{1}{\sqrt{2\pi}\sigma_t}) \\
&=\sum_{i=1}^m\sum_{j=1}^n[cy_{ij}u_iv_j^T-(cy_{ij}-y_{ij}+1)log(1+e^{u_iv_j^T})]-\frac{1}{2\sigma_d^2}\sum_{i=1}^m\lVert u_i \rVert^2 \\-\frac{1}{2\sigma_t^2}\sum_{j=1}^n\lVert v_j \rVert^2 &+ C
\end{aligned}</script><h4 id="NRLMF中的邻域"><a href="#NRLMF中的邻域" class="headerlink" title="NRLMF中的邻域"></a>NRLMF中的邻域</h4><script type="math/tex; mode=display">
\frac \alpha 2\sum_{i=1}^m\sum_{\mu=1}^na_{i\mu}\lVert u_i-u_\mu\rVert^2_F</script><script type="math/tex; mode=display">
a_{i\mu} = 
\begin{cases}
s_{i\mu}^d, if d_{\mu} \in N(d_i)\\\\
0, otherwise
\end{cases} \\</script><p>目标式同GRNMF基本一致，同样是通过引入类似$a_{i\mu}$来引入邻域信息，不同在于NRLMF没有利用其他算法产生的簇作为额外的邻域信息，只是利用了数据集中提供的drug和target的相似度，构建k近邻来作为邻域信息。</p>
<p>然而NRLMF在另外一个地方又使用了邻域的信息，因为在drug-target的数据集中存在某个药物没有任何治疗目标的情况，也就是说某一行全为0，论文中认为这种情况下进行矩阵分解所得到的特征向量会有很大误差，因此通过药物和药物之间的相似度，来利用其它存在观测值的药物的特征向量取一个k近邻来代替，以提高精度。</p>
<script type="math/tex; mode=display">
\tilde{u} = 
\begin{cases}
u_i, &\text{if } d_i \in D^+ \\\\
\frac{1}{\textstyle\sum_{\mu \in N^+(d_i)S_{i\mu}^d}}\textstyle\sum_{\mu \in N^+(d_i)}s_{i\mu}^du_{\mu}, &\text{if } d_i \in D^-
\end{cases}</script><h4 id="证明高斯先验分布与L2正则的等价性"><a href="#证明高斯先验分布与L2正则的等价性" class="headerlink" title="证明高斯先验分布与L2正则的等价性"></a>证明高斯先验分布与L2正则的等价性</h4><p>首先我们假设现在需要从一些样本点$(x_1,y_1)···(x_N,y_N)$中来估计参数 $\beta$，假设输出$y$与输入$x$之间线性相关，并且受噪声$\epsilon$影响:</p>
<script type="math/tex; mode=display">
y_n=\beta x_n+\epsilon</script><p>这里的$\epsilon$服从均值为0，方差为$\sigma^2$的高斯分布，问题转换为如下的似然估计式子：</p>
<script type="math/tex; mode=display">
\prod_{n=1}^N N(y_n|\beta x_n, \sigma^2)</script><p>接下来我们引入高斯先验分布$N(\beta|0, \frac{1}{\lambda})$，这里的$\lambda​$是一个正数值，把这个先验分布与上面的似然估计相结合，我们得到：</p>
<script type="math/tex; mode=display">
\prod_{n=1}^N N(y_n|\beta x_n, \sigma^2)N(\beta|0, \frac{1}{\lambda})</script><p>对这个式子取对数，并且对常数进行化简，因为它们不影响优化的结果，我们得到：</p>
<script type="math/tex; mode=display">
\sum_{n=1}^N-\frac{1}{\sigma^2}(y_n-\beta x_n)^2-\lambda \beta^2+const</script><p>根据最大似然的原则，我们需要最大化上面这个式子，这时就可以看出来为什么说高斯先验分布与L2正则等价了。</p>
<p>接下来是引入拉普拉斯约束来对特征向量进行优化，论文的另一个创新之处在于在约束中加入了邻域的信息，如上一节所讲。之后便是对目标函数进行优化得到迭代式以进行特征向量的更新。</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul>
<li>NRLMF(13)式的求导结果</li>
</ul>
<h3 id="可优化的地方"><a href="#可优化的地方" class="headerlink" title="可优化的地方"></a>可优化的地方</h3><h4 id="GRNMF-1"><a href="#GRNMF-1" class="headerlink" title="GRNMF"></a>GRNMF</h4><ul>
<li>利用邻域信息时，GRNMF只是用了一种聚类方法ClusterOne进行聚类，如果用多种聚类方法，多个结果中都被聚在同一类中认为可信度更高会不会更好？</li>
</ul>
<h4 id="NRLMF-1"><a href="#NRLMF-1" class="headerlink" title="NRLMF"></a>NRLMF</h4><ul>
<li>如果像GRNMF一样不仅利用相似度得到邻域信息，还用其它聚类算法的聚类结果作为邻域信息会不会更好？</li>
</ul>
]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统学习记录</title>
    <url>/2019/08/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>用本文记录学习推荐系统的过程，以及一些实用的资料与资源。<br><a id="more"></a></p>
<h2 id="推荐系统数据集"><a href="#推荐系统数据集" class="headerlink" title="推荐系统数据集"></a>推荐系统数据集</h2><ol>
<li><a href="http://cseweb.ucsd.edu/~jmcauley/datasets.html" target="_blank" rel="noopener">UCSD</a></li>
<li><a href="https://gist.github.com/entaroadun/1653794" target="_blank" rel="noopener">github</a></li>
<li><a href="https://www.datalearner.com/data" target="_blank" rel="noopener">科研数据共享列表</a></li>
</ol>
<h2 id="基于隐式反馈数据的推荐系统【理论及python实践】"><a href="#基于隐式反馈数据的推荐系统【理论及python实践】" class="headerlink" title="基于隐式反馈数据的推荐系统【理论及python实践】"></a>基于隐式反馈数据的推荐系统【理论及python实践】</h2><blockquote>
<p>Yifan Hu,Yehuda Koren,Chris Volinsky.<a href="http://yifanhu.net/PUB/cf.pdf" target="_blank" rel="noopener">Collaborative Filtering for Implicit Feedback Datasets</a>[J].IEEE International Conference on Data Mining，2008</p>
</blockquote>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>隐式反馈(Implict)数据就是用户的行为数据，包括点击，浏览和停留等，它不像评分和点赞一样直观地表示了用户的喜好。实际情况中像评分这种显式数据往往很难获得，因为它输入用户额外的进行操作，而像点击这种隐式反馈的数据，是随着用户的行为自然产生的，不需要额外的获取成本，因此实际情况中隐式反馈的数据规模要远大于显式反馈，因此很有必要研究基于隐式反馈数据的推荐系统</p>
<p>隐式反馈数据的特征：</p>
<ol>
<li>没有负样本。不同于评分，用户可以通过打低分来表达对某个物品的厌恶，我们只能通过点击猜测用户可能对某个物品有偏好，而不能通过没有点击来说明用户不喜欢，可能只是他还没接触过这个物品。处理显式数据时，缺失的评分项可以当作缺失值处理，而处理隐式数据时，为了避免只得到正向反馈，必须要对数据整体进行分析。</li>
<li>隐含很多的噪声数据。例如用户购买某个物品，不代表他一定喜欢这个物品，可能只是作为礼物或者其它原因，而给一个物品打高分可以很大程度上表示他偏好这个用品。</li>
<li>数值含义不同。在显式数据里，数值的含义代表偏好程度，如评分；而隐式数据里，数值代表置信度，往往表现为行为的频率，例如观看次数等等。频率越高，我们越能确定它与用户的偏好相关联，而不是一个偶然性情况。</li>
<li>评价指标不同。传统的推荐系统可以使用如均方根误差(Mean Square Error)来判断预测的好坏，而隐式数据上会有不同。</li>
</ol>
<h3 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h3><p>$r_{ui}$定义为用户$u$对物品$i$的一次观测值，例如购买物品的次数或浏览网页的次数，在电视节目推荐中，它代表完整观看某个节目的次数，0.7表示观看了节目的70%。区别于显式数据将空缺数据当作缺失值处理，我们将缺失值置为0，表示没有此观测记录。</p>
<h3 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h3><p>隐语义模型通过用户特征向量$x_u\in R^f$和物品特征向量$y_i\in R^f$来进行预测，预测评分用上面两个向量的内积形式表示：$\hat{r}_{ui}=x_u^Ty_i$，具体细节可见另一篇博客的<a href="http://www.bithub00.com/2019/04/12/imputing%20structured%20missing%20values%20in%20spatial%20data%20with%20clsutered%20adversarial%20matrix%20factorization/" target="_blank" rel="noopener">矩阵分解引入</a>部分。目标函数表示为：</p>
<script type="math/tex; mode=display">min_{x_*,y_*}\sum_{r_{ui}\ is\ known}(r_{ui}-x_u^Ty_i)^2+\lambda(\mid\mid x_u \mid\mid^2+\mid\mid y_i \mid\mid^2)</script><p>公式中的$\lambda$作为正则化参数，用于<a href="http://www.bithub00.com/2019/04/03/%E8%8C%83%E6%95%B0/" target="_blank" rel="noopener">约束</a>模型，参数的求解常使用随机梯度下降。然而这种方法在应用于隐式反馈数据时需要做调整</p>
<h3 id="论文提出的模型"><a href="#论文提出的模型" class="headerlink" title="论文提出的模型"></a>论文提出的模型</h3><p>首先引入一个二值变量$p_{ui}$来标识用户对物品是否产生过行为如购买、观看等等：</p>
<script type="math/tex; mode=display">
p_{ui} =
\begin{cases}
1, &r_{ui} > 0\\
0, &r_{ui} = 0
\end{cases}</script><p>注意，$p_{ui}=1$只是暗示了用户喜欢某个物品的可能，而且$r_{ui}$的数值越大应该表示喜欢的可能性越大，因此再引入变量表示这种可能性：</p>
<script type="math/tex; mode=display">c_{ui}=1+\alpha r_{ui}</script><p>引入$c_{ui}$后，对于每一个观测值，我们有一个最小的可能性1，随着观测值的增大，可能性也在逐渐增大，这里的可能性与概率的含义不同，论文中将常数$\alpha$设为40<br>我们的目标是为每一个用户和物品分别得到一个用户特征向量$x_u\in R^f$和物品特征向量$y_i\in R^f$，则预测值可以通过两个向量的内积表示：$p_{ui}=x^T_uy_i$，即预测用户是否会对某个物品产生行为，则目标函数可表示为：</p>
<script type="math/tex; mode=display">min_{x_*,y_*}L(X,Y)=min_{x_*,y_*}\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda(\sum_u\mid\mid x_u \mid\mid^2+\sum_i\mid\mid y_i \mid\mid^2)</script><p>同理，第二项为约束项，防止过拟合于训练数据。下一步就是根据目标函数进行求解，因为原始表达式计算复杂度太高，输入的数据集为$m\ast n$的矩阵，m为用户数，n为物品数，规模很容易达到百万级，需要进行相应的优化。<br>我们引入交替最小二乘法(Alternate Least Sqaures)， 目标函数需要优化用户和物品两个维度，可以先固定物品这个维度，对用户唯独进行优化，我的推导过程（论文只给了结果表达式）：<br>对$x_u$求导，可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L}{\partial x_u} & = -2\sum_i(p_{ui}-x^T_uy_i)y_ic_{ui}+2\lambda x_u \\
& = -2\sum_i(p_{ui}-y_i^Tx_u)y_ic_{ui} + 2\lambda x_u \\
& = -2Y^TC_up(u)+2Y^TC_uYx_u+2\lambda x_u
\end{aligned}</script><p>其中$Y_{n\ast f}$为含有所有物品特征向量的矩阵，f类似隐语义模型中的隐含特征，可见<a href="http://www.bithub00.com/2019/04/12/imputing%20structured%20missing%20values%20in%20spatial%20data%20with%20clsutered%20adversarial%20matrix%20factorization/" target="_blank" rel="noopener">矩阵分解引入</a>中的解释。令求导结果为0，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Y^TC_uYx_u+\lambda Ix_u & = Y^TC_uP_u \\
x_u & = (Y^TC_uY+\lambda I)^{-1}Y^TC_up(u)
\end{aligned}</script><p>观察表达式，计算复杂度的瓶颈在于计算$Y^TC^uY$这一部分，计算每一个用户的时间复杂度为$O(f^2n)$，我们使用线性代数的知识进行简单的变形：$Y^TC^uY=Y^TY+Y^T(C^u-I)Y$，其中$Y^TY$与用户u无关，可以在迭代开始时预先计算，而$Y^T(C^u-I)Y$中的$C^u-I$只有$n_u$个非零值，其中$n_u$是用户u$r_{ui}&gt;0$的个数，很明显$n_u\ll n$。同样地，$C^up(u)$也只含有$n_u$个非零值，则此时对于单个用户$x_u$的计算复杂度为$O(f^2n_u+f^3)$，其中$O(f^3)$求逆的时间复杂度，总共有m个用户，所以总的时间复杂度为$O(f^2N+f^3m)$，N为总的非零值的个数，隐含特征f的个数通常设置为$20-200$。则$y_i$的表达式同理：</p>
<script type="math/tex; mode=display">y_i=(X^TC^iX+\lambda I)^{-1}X^TC^ip(i)</script><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>理解完论文框架后，使用一个<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00352" target="_blank" rel="noopener">数据集</a>进行实践，自己构建一个基于隐式反馈数据的推荐系统。数据集来源于UCI大学的机器学习资源库，它包含了一个位于英国的网上零售商时间跨度为八个月的所以购买记录，数据集中包含了InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomID Country这些字段。</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><ul>
<li>下载数据并读取<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">retail_data = pd.read_excel(<span class="string">'Online Retail.xlsx'</span>)</span><br><span class="line">print(retail_data.info())</span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">541909</span> entries, <span class="number">0</span> to <span class="number">541908</span></span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line">InvoiceNo      <span class="number">541909</span> non-null object</span><br><span class="line">StockCode      <span class="number">541909</span> non-null object</span><br><span class="line">Description    <span class="number">540455</span> non-null object</span><br><span class="line">Quantity       <span class="number">541909</span> non-null int64</span><br><span class="line">InvoiceDate    <span class="number">541909</span> non-null datetime64[ns]</span><br><span class="line">UnitPrice      <span class="number">541909</span> non-null float64</span><br><span class="line">CustomerID     <span class="number">406829</span> non-null float64</span><br><span class="line">Country        <span class="number">541909</span> non-null object</span><br><span class="line">dtypes: datetime64[ns](<span class="number">1</span>), float64(<span class="number">2</span>), int64(<span class="number">1</span>), object(<span class="number">4</span>)</span><br><span class="line">memory usage: <span class="number">33.1</span>+ MB</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看到，除了CustomerID这一字段外，其它字段几乎没有缺失值，但如果CustomerID未知，我们就无法知道购买记录中是谁购买了某个商品，因此需要将CustomerID未知的记录去除：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cleaned_retail = retail_data.loc[pd.isnull(retail_data.CustomerID) == <span class="keyword">False</span>]</span><br><span class="line">print(cleaned_retail.info())</span><br><span class="line"></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">Int64Index</span>:</span> <span class="number">406829</span> entries, <span class="number">0</span> to <span class="number">541908</span></span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line">InvoiceNo      <span class="number">406829</span> non-null object</span><br><span class="line">StockCode      <span class="number">406829</span> non-null object</span><br><span class="line">Description    <span class="number">406829</span> non-null object</span><br><span class="line">Quantity       <span class="number">406829</span> non-null int64</span><br><span class="line">InvoiceDate    <span class="number">406829</span> non-null datetime64[ns]</span><br><span class="line">UnitPrice      <span class="number">406829</span> non-null float64</span><br><span class="line">CustomerID     <span class="number">406829</span> non-null float64</span><br><span class="line">Country        <span class="number">406829</span> non-null object</span><br><span class="line">dtypes: datetime64[ns](<span class="number">1</span>), float64(<span class="number">2</span>), int64(<span class="number">1</span>), object(<span class="number">4</span>)</span><br><span class="line">memory usage: <span class="number">27.9</span>+ MB</span><br></pre></td></tr></table></figure></p>
<p>这样所有的购买记录都可以对应到唯一的顾客了。接下来我们生成一个商品的描述表，方便查看某个商品编号对应的商品是什么：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_lookup = cleaned_retail[[<span class="string">'StockCode'</span>, <span class="string">'Description'</span>]].drop_duplicates()</span><br><span class="line">item_lookup[<span class="string">'StockCode'</span>] = item_lookup.StockCode.astype(str)</span><br><span class="line">print(item_lookup.info()) </span><br><span class="line"></span><br><span class="line">        StockCode	Description</span><br><span class="line"><span class="number">0</span>	<span class="number">85123</span>A	WHITE HANGING HEART T-LIGHT HOLDER</span><br><span class="line"><span class="number">1</span>	<span class="number">71053</span>	WHITE METAL LANTERN</span><br><span class="line"><span class="number">2</span>	<span class="number">84406</span>B	CREAM CUPID HEARTS COAT HANGER</span><br><span class="line"><span class="number">3</span>	<span class="number">84029</span>G	KNITTED UNION FLAG HOT WATER BOTTLE</span><br><span class="line"><span class="number">4</span>	<span class="number">84029</span>E	RED WOOLLY HOTTIE WHITE HEART.</span><br></pre></td></tr></table></figure></p>
<p>接下来需要做的是</p>
<ul>
<li><p>对于某个用户，将他购买的相同物品的数目进行求和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cleaned_retail[<span class="string">'CustomerID'</span>] = cleaned_retail.CustomerID.astype(int)</span><br><span class="line">cleaned_retail = cleaned_retail[[<span class="string">'StockCode'</span>, <span class="string">'Quantity'</span>, <span class="string">'CustomerID'</span>]]</span><br><span class="line">grouped_cleaned = cleaned_retail.groupby([<span class="string">'CustomerID'</span>, <span class="string">'StockCode'</span>]).sum().reset_index()</span><br></pre></td></tr></table></figure>
</li>
<li><p>将求和结果中的0置为1，数目为0的情况往往是顾客进行了退货，这种情况我们也当作顾客购买了商品，而不是当作没购买的情况处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grouped_cleaned.Quantity.loc[grouped_cleaned.Quantity == <span class="number">0</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>去除求和结果中小于0的记录</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grouped_purchased = grouped_cleaned.query(<span class="string">'Quantity &gt; 0'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>建立ALS算法中输入的数据矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">customers = list(np.sort(grouped_purchased.CustomerID.unique()))  </span><br><span class="line">products = list(grouped_purchased.StockCode.unique())</span><br><span class="line">quantity = list(grouped_purchased.Quantity)</span><br><span class="line">rows = grouped_purchased.CustomerID.astype(<span class="string">'category'</span>, categories=customers).cat.codes</span><br><span class="line">cols = grouped_purchased.StockCode.astype(<span class="string">'category'</span>, categories=products).cat.codes</span><br><span class="line">purchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))</span><br><span class="line"></span><br><span class="line">&lt;<span class="number">4338</span>x3664 sparse matrix of type <span class="string">'&lt;class '</span>numpy.int64<span class="string">'&gt;'</span></span><br><span class="line">	<span class="keyword">with</span> <span class="number">266723</span> stored elements <span class="keyword">in</span> Compressed Sparse Row format&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>构建的输入矩阵维度是$4338\ast 3664$，其中有266723个非空值。</p>
<h4 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h4><p>接下来需要把输入数据划分为训练集和测试集，传统的划分方式如图所示：</p>
<p><img src="https://s2.ax1x.com/2019/08/07/e5hMTK.png" alt="划分数据"></p>
<p>然而这种方式对于推荐系统是不适用的，因为矩阵分解时需要用上所有的用户-物品数据，更好的方法是随机隐藏输入矩阵中的某些观测值，将隐藏好的矩阵作为训练数据，将完整的矩阵作为测试矩阵，来判断推荐的物品用户是否会购买。</p>
<p><img src="https://s2.ax1x.com/2019/08/07/e54m9g.png" alt="划分数据"></p>
<p>为了对比推荐的效果，我们可以和另一种推荐方法作对比，即只推荐最流行的物品。</p>
<ul>
<li>划分数据集<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_train</span><span class="params">(ratings, pct_test)</span>:</span></span><br><span class="line">        test_set = ratings.copy()</span><br><span class="line">        test_set[test_set != <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        training_set = ratings.copy()</span><br><span class="line">        nonzero_inds = training_set.nonzero()</span><br><span class="line">        nonzero_pairs = list(zip(nonzero_inds[<span class="number">0</span>], nonzero_inds[<span class="number">1</span>]))</span><br><span class="line">        random.seed(<span class="number">0</span>)</span><br><span class="line">        num_samples = int(np.ceil(pct_test * len(nonzero_pairs)))</span><br><span class="line">        samples = random.sample(nonzero_pairs, num_samples)</span><br><span class="line">        user_inds = [index[<span class="number">0</span>] <span class="keyword">for</span> index <span class="keyword">in</span> samples]</span><br><span class="line">        item_inds = [index[<span class="number">1</span>] <span class="keyword">for</span> index <span class="keyword">in</span> samples]</span><br><span class="line">        training_set[user_inds, item_inds] = <span class="number">0</span></span><br><span class="line">        training_set.eliminate_zeros()</span><br><span class="line">        <span class="keyword">return</span> training_set, test_set, list(set(user_inds))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="ALS算法"><a href="#ALS算法" class="headerlink" title="ALS算法"></a>ALS算法</h4><p>下一步就是实现论文所用的ALS算法，具体数学公式见上面的推导：</p>
<ul>
<li>ALS算法<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">implicit_weighted_ALS</span><span class="params">(training_set, lambda_val=<span class="number">0.1</span>, alpha=<span class="number">40</span>, iterations=<span class="number">10</span>, rank_size=<span class="number">20</span>, seed=<span class="number">0</span>)</span>:</span></span><br><span class="line">    conf = (alpha * training_set)</span><br><span class="line">    num_user = conf.shape[<span class="number">0</span>]</span><br><span class="line">    num_item = conf.shape[<span class="number">1</span>]</span><br><span class="line">    rstate = np.random.RandomState(seed)</span><br><span class="line">    X = sparse.csr_matrix(rstate.normal(size=(num_user, rank_size)))</span><br><span class="line">    Y = sparse.csr_matrix(rstate.normal(size=(num_item, rank_size)))</span><br><span class="line">    X_eye = sparse.eye(num_user)</span><br><span class="line">    Y_eye = sparse.eye(num_item)</span><br><span class="line">    lambda_eye = lambda_val * sparse.eye(rank_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter_step <span class="keyword">in</span> range(iterations):  </span><br><span class="line">        yTy = Y.T.dot(Y)</span><br><span class="line">        xTx = X.T.dot(X)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> range(num_user):</span><br><span class="line">            conf_samp = conf[u, :].toarray()</span><br><span class="line">            pref = conf_samp.copy()</span><br><span class="line">            pref[pref != <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            CuI = sparse.diags(conf_samp, [<span class="number">0</span>])</span><br><span class="line">            yTCuIY = Y.T.dot(CuI).dot(Y)  </span><br><span class="line">            yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T)</span><br><span class="line">            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_item):</span><br><span class="line">            conf_samp = conf[:, i].T.toarray()</span><br><span class="line">            pref = conf_samp.copy()</span><br><span class="line">            pref[pref != <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            CiI = sparse.diags(conf_samp, [<span class="number">0</span>])</span><br><span class="line">            xTCiIX = X.T.dot(CiI).dot(X)  </span><br><span class="line">            xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T)</span><br><span class="line">            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> X, Y.T</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>我们可以举一个例子来看看效果：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_vecs, item_vecs = implicit_weighted_ALS(product_train, lambda_val = <span class="number">0.1</span>, alpha = <span class="number">15</span>, iterations = <span class="number">1</span>, rank_size = <span class="number">20</span>)</span><br><span class="line">print(user_vecs[<span class="number">0</span>,:].dot(item_vecs).toarray()[<span class="number">0</span>,:<span class="number">5</span>])</span><br><span class="line">[ <span class="number">0.00644811</span>, <span class="number">-0.0014369</span> ,  <span class="number">0.00494281</span>,  <span class="number">0.00027502</span>,  <span class="number">0.01275582</span> ]</span><br></pre></td></tr></table></figure></p>
<p>对第一个用户来说，前五个物品中第五个物品的得分最高，因此会被用于推荐，这只是一次迭代的结果，迭代多次效果会更好，但原始ALS的算法计算过程太慢，我们需要对它加速，可以使用github上star数上千的python ALS加速版本，所用时间要少得多：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> implict</span><br><span class="line">alpha = <span class="number">15</span></span><br><span class="line">product_train, product_test, product_users_altered = make_train(purchases_sparse, pct_test=<span class="number">0.2</span>)</span><br><span class="line">user_vecs, item_vecs = implicit.alternating_least_squares((product_train * alpha).astype(<span class="string">'double'</span>), factors=<span class="number">20</span>, regularization=<span class="number">0.1</span>, iterations=<span class="number">50</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以直观地感觉到计算速度大大加快了，用兴趣的可以去github上看看<a href="https://github.com/benfred/implicit" target="_blank" rel="noopener">源代码</a></p>
<h4 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h4><p>数据集有了，推荐系统也搭好了，那下一步就是对我们的推荐系统进行评估，看看它的表现。在划分训练集和测试集时，有这么一步：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_set[test_set != <span class="number">0</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>于是推荐就变成了一个二分类问题，购买1或不购买0，这时就可以引入分类系统的评测指标：ROC(Receiver Operating Characteristic)曲线与AUC(Area Under the Curve)值。它的介绍可以看另一篇博客<a href="http://www.bithub00.com/2019/08/07/ROC%E4%B8%8EAUC/" target="_blank" rel="noopener">ROC与AUC</a>。接下来是编写一个函数计算AUC值以进行比较：</p>
<ul>
<li>AUC<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auc_score</span><span class="params">(predictions, test)</span>:</span></span><br><span class="line">    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)</span><br><span class="line">    <span class="keyword">return</span> metrics.auc(fpr, tpr)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>基于上面这个函数，我们为训练集中每一个被隐藏了至少一条记录的用户计算AUC值，随后求平均值并与“推荐最流行物品”的策略进行比较：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_mean_auc</span><span class="params">(training_set, altered_users, predictions, test_set)</span>:</span></span><br><span class="line">    store_auc = []</span><br><span class="line">    popularity_auc = []</span><br><span class="line">    pop_items = np.array(test_set.sum(axis=<span class="number">0</span>)).reshape(<span class="number">-1</span>)</span><br><span class="line">    item_vecs = predictions[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> altered_users:</span><br><span class="line">        training_row = training_set[user, :].toarray().reshape(<span class="number">-1</span>) </span><br><span class="line">        zero_inds = np.where(training_row == <span class="number">0</span>)</span><br><span class="line">        user_vec = predictions[<span class="number">0</span>][user, :]</span><br><span class="line">        pred = user_vec.dot(item_vecs).toarray()[<span class="number">0</span>, zero_inds].reshape(<span class="number">-1</span>)</span><br><span class="line">        actual = test_set[user, :].toarray()[<span class="number">0</span>, zero_inds].reshape(<span class="number">-1</span>)</span><br><span class="line">        pop = pop_items[zero_inds]</span><br><span class="line">        store_auc.append(auc_score(pred, actual))</span><br><span class="line">        popularity_auc.append(auc_score(pop, actual))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> float(<span class="string">'%.3f'</span> % np.mean(store_auc)), float(<span class="string">'%.3f'</span> % np.mean(popularity_auc))</span><br><span class="line"></span><br><span class="line">print(calc_mean_auc(product_train, product_users_altered,[sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], product_test))</span><br><span class="line">(<span class="number">0.87</span>, <span class="number">0.814</span>)</span><br></pre></td></tr></table></figure></p>
<p>从结果可以看出，使用论文提出的推荐模型的效果是比单纯推荐最流行物品的效果要好的。</p>
<h4 id="实例观察"><a href="#实例观察" class="headerlink" title="实例观察"></a>实例观察</h4><p>AUC值的大小比较还是比较抽象不够直观，下一步选取一个用户查看他购买过的物品，以及模型所推荐的物品来直观地感受推荐的效果。首先编写一个函数，根据customer_id来获得他所购买的商品的一个列表：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_items_purchased</span><span class="params">(customer_id, mf_train, customers_list, products_list, item_lookup)</span>:</span></span><br><span class="line">    cust_ind = np.where(customers_list == customer_id)[<span class="number">0</span>][<span class="number">0</span></span><br><span class="line">    purchased_ind = mf_train[cust_ind,:].nonzero()[<span class="number">1</span>]</span><br><span class="line">    prod_codes = products_list[purchased_ind]</span><br><span class="line">    <span class="keyword">return</span> item_lookup.loc[item_lookup.StockCode.isin(prod_codes)]</span><br><span class="line"></span><br><span class="line">customers_arr = np.array(customers)</span><br><span class="line">products_arr = np.array(products)</span><br><span class="line">print(get_items_purchased(<span class="number">12346</span>, product_train, customers_arr, products_arr, item_lookup))</span><br><span class="line"></span><br><span class="line">	StockCode	Description</span><br><span class="line"><span class="number">61619</span>	<span class="number">23166</span>	MEDIUM CERAMIC TOP STORAGE JAR</span><br></pre></td></tr></table></figure></p>
<p>根据结果显示，这位顾客曾经购买了一个中等大小的陶瓷罐用于装东西，我们的推荐系统会推荐什么样的物品给他呢？<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rec_items</span><span class="params">(customer_id, mf_train, user_vecs, item_vecs, customer_list, item_list, item_lookup, num_items=<span class="number">10</span>)</span>:</span></span><br><span class="line">    cust_ind = np.where(customer_list == customer_id)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    pref_vec = mf_train[cust_ind, :].toarray()</span><br><span class="line">    pref_vec = pref_vec.reshape(<span class="number">-1</span>) + <span class="number">1</span> </span><br><span class="line">    pref_vec[pref_vec &gt; <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    rec_vector = user_vecs[cust_ind, :].dot(item_vecs.T)</span><br><span class="line">    min_max = MinMaxScaler()</span><br><span class="line">    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(<span class="number">-1</span>, <span class="number">1</span>))[:, <span class="number">0</span>]</span><br><span class="line">    recommend_vector = pref_vec * rec_vector_scaled</span><br><span class="line">    product_idx = np.argsort(recommend_vector)[::<span class="number">-1</span>][:num_items]</span><br><span class="line">    rec_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> product_idx:</span><br><span class="line">        code = item_list[index]</span><br><span class="line">        rec_list.append([code, item_lookup.Description.loc[item_lookup.StockCode == code].iloc[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    codes = [item[<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> rec_list]</span><br><span class="line">    descriptions = [item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> rec_list]</span><br><span class="line">    final_frame = pd.DataFrame(&#123;<span class="string">'StockCode'</span>: codes, <span class="string">'Description'</span>: descriptions&#125;)</span><br><span class="line">    <span class="keyword">return</span> final_frame[[<span class="string">'StockCode'</span>, <span class="string">'Description'</span>]]</span><br><span class="line"></span><br><span class="line">print(rec_items(<span class="number">12353</span>, product_train, user_vecs, item_vecs, customers_arr,products_arr, item_lookup, num_items=<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">      StockCode	       Description</span><br><span class="line"><span class="number">0</span>	<span class="number">23167</span>	SMALL CERAMIC TOP STORAGE JAR</span><br><span class="line"><span class="number">1</span>	<span class="number">23165</span>	LARGE CERAMIC TOP STORAGE JAR</span><br><span class="line"><span class="number">2</span>	<span class="number">22963</span>	JAM JAR WITH GREEN LID</span><br><span class="line"><span class="number">3</span>	<span class="number">23294</span>	SET OF <span class="number">6</span> SNACK LOAF BAKING CASES</span><br><span class="line"><span class="number">4</span>	<span class="number">22980</span>	PANTRY SCRUBBING BRUSH</span><br><span class="line"><span class="number">5</span>	<span class="number">23296</span>	SET OF <span class="number">6</span> TEA TIME BAKING CASES</span><br><span class="line"><span class="number">6</span>	<span class="number">23293</span>	SET OF <span class="number">12</span> FAIRY CAKE BAKING CASES</span><br><span class="line"><span class="number">7</span>	<span class="number">22978</span>	PANTRY ROLLING PIN</span><br><span class="line"><span class="number">8</span>	<span class="number">23295</span>	SET OF <span class="number">12</span> MINI LOAF BAKING CASES</span><br><span class="line"><span class="number">9</span>	<span class="number">22962</span>	JAM JAR WITH PINK LID</span><br></pre></td></tr></table></figure></p>
<p>从推荐结果中选取了前10个得分最高的物品，它们看起来和这个顾客购买的商品都比较相关，可解释性好，要知道推荐系统是完全不知道陶瓷罐代表什么含义的，而可解释性有时会让顾客更加信服得到的推荐结果。可以选另外的customer_id来继续验证。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>引申阅读：</p>
<ul>
<li>显式反馈推荐系统：</li>
</ul>
<ol>
<li><a href="http://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/" target="_blank" rel="noopener">Alternating Least Squares Method for Collaborative Filtering</a></li>
<li><a href="http://blog.ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/" target="_blank" rel="noopener">Explicit Matrix Factorization: ALS, SGD, and All That Jazz</a></li>
</ol>
<ul>
<li>混合推荐系统(显式/隐式)</li>
</ul>
<ol>
<li><a href="https://github.com/lyst/lightfm" target="_blank" rel="noopener">LightFM</a></li>
</ol>
]]></content>
      <tags>
        <tag>推荐系统</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>L1与L2范数</title>
    <url>/2019/04/03/%E8%8C%83%E6%95%B0/</url>
    <content><![CDATA[<p>看到的一篇很好的介绍L1与L2范数的文章，mark下来</p>
<a id="more"></a>
<p><img src="https://s2.ax1x.com/2019/04/03/Accl6O.png" alt=""></p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>算法基本思想</title>
    <url>/2019/09/21/%E7%AE%97%E6%B3%95%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3/</url>
    <content><![CDATA[<p>算法设计常见的5种基本技巧，有贪婪算法、分治算法、动态规划、随机化算法和回溯算法。<br><a id="more"></a></p>
<h3 id="贪婪算法"><a href="#贪婪算法" class="headerlink" title="贪婪算法"></a>贪婪算法</h3><p>虽然每次的选择都是局部最优，当在算法结束的时候，其期望是全局最优才是正确的。不过有时，在不同条件与要求下时，最优解的答案可能不止有一个或不一样，而贪婪算法也可以得出一个近似的答案。</p>
<h3 id="分治算法"><a href="#分治算法" class="headerlink" title="分治算法"></a>分治算法</h3><p>分治算法不是简单的递归，而是将大的问题递归解决较小的问题，然后从子问题的解构建原问题的解。比如，快速排序和归并排序算分治算法，而图的递归深度搜索和二叉树的递归遍历则不是分治算法的运用。</p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>动态规划与分治算法的区别是，两种算法同样是将较大的问题分解成较小问题，而动态规划对这些较小的问题并不是对原问题明晰的分割，其中一部分是被重复求解的，因此动态规划将较小问题的解记录下来，使得在处理较大问题的时候，可以不用重复去处理较小的问题，而是直接利用所记录的较小问题的答案来求解。</p>
<h3 id="随机化算法"><a href="#随机化算法" class="headerlink" title="随机化算法"></a>随机化算法</h3><p>随机化算法的一个应用是在快速排序中对枢纽元素的选择，使得算法的运行时间不止依赖于特定的输入，还而且还依赖于所出现的随机数。虽然一个随机化算法的最坏情形运行时间常常与非随机化算法的最坏情形运行时间相同，但两者还是有区别的，好的随机化算法没有坏的输入，而只有坏的随机数。</p>
<h3 id="回溯算法"><a href="#回溯算法" class="headerlink" title="回溯算法"></a>回溯算法</h3><p>回溯算法相当于穷举搜索的巧妙实现，对比蛮力的穷举搜索，回溯算法可以对一些不符合要求的或者是重复的情况进行裁剪，不再对其进行搜索，以减少搜索的工作量提高效率。比如，在图运用回溯算法的深度优先搜索遍历中，会对已搜索遍历过的顶点进行标记，避免下次的回溯搜索中对再次出现的该顶点进行重复遍历。</p>
]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>计量经济学课程笔记</title>
    <url>/2019/11/18/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/</url>
    <content><![CDATA[<p>本篇博客记录计量经济学课程学习过程中的笔记<br><a id="more"></a></p>
<h2 id="OLS估计"><a href="#OLS估计" class="headerlink" title="OLS估计"></a>OLS估计</h2><h3 id="模型假设"><a href="#模型假设" class="headerlink" title="模型假设"></a>模型假设</h3><ul>
<li>SLR.1 被解释变量y与解释变量x之间的关系满足</li>
</ul>
<script type="math/tex; mode=display">y=\beta_0+\beta_1x+u</script><ul>
<li>SLR.2 $(x_1,y_1),…,(x_n,y_n)是(x,y)的一组随机样本$</li>
<li>SLR.3 $x_1,…,x_n$不完全相同</li>
<li>SLR.4 随机误差满足$E(u|x)=0$</li>
<li>SLR.5 同方差假设</li>
</ul>
<script type="math/tex; mode=display">Var(u|x)=\sigma^2</script><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>假设样本$(x_1,y_1)…(x_n,y_n)$近似分布在一条直线两侧，模型为</p>
<script type="math/tex; mode=display">y_i=\beta_0+\beta_1x_i+u_i</script><p>直观上看最佳的拟合应该使数据点都尽量靠近所拟合的直线，转化为数学表达式就是对于每个样本点的实际观测值与拟合值之间的差$\hat{u_i}=y_i-\hat{y_i}$应尽可能小，其中$\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$为拟合值。<br>为了避免符号导致求和时进行相互抵消，使用残差平方和来表示实际观测值与拟合值之间的差：</p>
<script type="math/tex; mode=display">S(\hat{\beta_0},\hat{\beta_1})=\sum_{i=1}^{n}\hat{u_i}^2=\sum_{i=1}^{n}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2</script><p>则最小二乘法的目的就是最小化这个残差平方和来估计参数$\hat{\beta_0}$和$\hat{\beta_1}$。记$S(\hat{\beta_0},\hat{\beta_1})=\sum(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2$，则目标变成求解这个二元函数的最小值点，由微积分中求函数最值的办法进行求导：</p>
<script type="math/tex; mode=display">
\begin{cases}
\frac{\partial S(\hat{\beta_0},\hat{\beta_1})}{\partial\hat{\beta_0}} = 0 \\\\
\frac{\partial S(\hat{\beta_0},\hat{\beta_1})}{\partial\hat{\beta_1}} = 0 
\end{cases}</script><p>得到如下方程组：</p>
<script type="math/tex; mode=display">
\begin{cases}
\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)=0 \\\\
\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)x_i=0
\end{cases}</script><p>由第一个式子得到：</p>
<script type="math/tex; mode=display">\hat{\beta_0}=\bar{y}-\hat{\beta_1\bar{x}}</script><p>将它代入第二个式子，得到：</p>
<script type="math/tex; mode=display">\hat{\beta_1}=\frac{\sum x_iy_i-n\bar{x}\bar{y}}{\sum x_i^2-n(\bar{x})^2}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}=\frac{S_{xy}}{S_x^2}</script><p>这样我们就得到了两个未知参数的回归估计。</p>
<h3 id="拟合结果"><a href="#拟合结果" class="headerlink" title="拟合结果"></a>拟合结果</h3><p>对于简单线性模型:</p>
<script type="math/tex; mode=display">y_i=\beta_0+\beta_1x_i+u_i</script><p>通过最小二乘法得到了一个拟合模型：</p>
<script type="math/tex; mode=display">\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i</script><p>自然而然地，我们想知道这个模型的拟合效果如何，解释变量在多大程度上解释了被解释变量？<br>总的平方和：</p>
<script type="math/tex; mode=display">TSS=\sum_{i=1}^n(y_i-\bar{y})^2</script><p>解释平方和：</p>
<script type="math/tex; mode=display">ESS=\sum_{i=1}^n(\hat{y_i}-\bar{y})^2</script><p>残差平方和：</p>
<script type="math/tex; mode=display">RSS=\sum_{i=1}^n(y_i-\hat{y})^2</script><p>三者的关系为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
TSS=\sum_{i=1}^n(y_i-\bar{y})^2
  &=\sum_{i=1}^n(y_i-\hat{yi}+\hat{yi}-\bar{y})^2 \\
&= RSS+ESS+2\sum_{i=1}^n\hat{u_i}(\hat{yi}-\bar{y}) \\
&= RSS+ESS
\end{aligned}</script><p>判定系数定义为：</p>
<script type="math/tex; mode=display">R^2=\frac{ESS}{TSS}=1-\frac{RSS}{TSS}</script><p>可以表明解释变量多大程度上解释了被解释变量。</p>
<h3 id="期望与方差"><a href="#期望与方差" class="headerlink" title="期望与方差"></a>期望与方差</h3><h4 id="OLS估计的期望："><a href="#OLS估计的期望：" class="headerlink" title="OLS估计的期望："></a>OLS估计的期望：</h4><p>记离差$d_i=x_i-\bar{x}$，$TSS_x=\sum_{i=1}^n(x_i-\bar{x})^2$，有</p>
<script type="math/tex; mode=display">\sum^n_{i=1}d_i=0，\sum^n_{i=1}d_ix_i=\sum^n_{i=1}d_i^2=TSS_x</script><p>于是有</p>
<script type="math/tex; mode=display">\hat{\beta_1}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}=\frac{1}{TSS_x}\sum^n_{i=1}d_iy_i</script><p>代入$y_i=\beta_0+\beta_1x_i+u_i$，得：</p>
<script type="math/tex; mode=display">\hat{\beta_1}=\beta_1+\frac{1}{TSS_x}\sum_{i=1}^nd_iu_i</script><p>所以</p>
<script type="math/tex; mode=display">E(\hat{\beta_1}|x_1,...,x_n)=\beta_1+\frac{1}{TSS_x}\sum_{i=1}^nd_iE(u_i)=\beta_1</script><p>即OLS估计是无偏的，$\hat{\beta_0}$的证明同理</p>
<h4 id="OLS估计的方差："><a href="#OLS估计的方差：" class="headerlink" title="OLS估计的方差："></a>OLS估计的方差：</h4><p>前面已经得到$\hat{\beta_1}=\beta_1+\frac{1}{TSS_x}\sum_{i=1}^nd_iu_i$，则</p>
<script type="math/tex; mode=display">Var(\hat{\beta_1}|x_1,...,x_n)=\frac{1}{TSS_x^2}\sum^n_{i=1}d_i^2Var(u_i)=\frac{\sigma^2}{TSS_x}</script><p>同理</p>
<script type="math/tex; mode=display">Var(\hat{\beta_0}|x_1,...,x_n)=\frac{\sum^n_{i=1}x_i^2}{n\sum^n_{i=1}(x_i-\bar{x})^2}\sigma^2</script><p>则OLS估计方差依赖于三个因素：</p>
<ul>
<li>随机误差的方差</li>
<li>自变量的方差</li>
<li>样本量</li>
</ul>
]]></content>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>高斯先验分布与L2正则</title>
    <url>/2019/10/21/%E9%AB%98%E6%96%AF%E5%85%88%E9%AA%8C%E4%B8%8EL2%E6%AD%A3%E5%88%99/</url>
    <content><![CDATA[<p>本篇博客探讨一下为什么说L2正则等同于高斯先验分布<br><a id="more"></a></p>
<p>首先我们假设现在需要从一些样本点$(x_1,y_1)···(x_N,y_N)$中来估计参数 $\beta$，假设输出$y$与输入$x$之间线性相关，并且受噪声$\epsilon$影响:</p>
<script type="math/tex; mode=display">y_n=\beta x_n+\epsilon</script><p>这里的$\epsilon$服从均值为0，方差为$\sigma^2$的高斯分布，问题转换为如下的似然估计式子：</p>
<script type="math/tex; mode=display">\prod_{n=1}^N N(y_n|\beta x_n, \sigma^2)</script><p>接下来我们引入高斯先验分布$N(\beta|0, \frac{1}{\lambda})$，这里的$\lambda$是一个正数值，把这个先验分布与上面的似然估计相结合，我们得到：</p>
<script type="math/tex; mode=display">\prod_{n=1}^N N(y_n|\beta x_n, \sigma^2)N(\beta|0, \frac{1}{\lambda})</script><p>对这个式子取对数，并且对常数进行化简，因为它们不影响优化的结果，我们得到：</p>
<script type="math/tex; mode=display">\sum_{n=1}^N-\frac{1}{\sigma^2}(y_n-\beta x_n)^2-\lambda \beta^2+const</script><p>根据最大似然的原则，我们需要最大化上面这个式子，这时就可以看出来为什么说高斯先验分布与L2正则等价了。</p>
]]></content>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>论文学习</title>
    <url>/2020/09/25/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>对看过的论文做一个记录</p>
<a id="more"></a>
<h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><h2 id="ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19"><a href="#ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19" class="headerlink" title="ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]"></a>ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>给定一个有向图$G=(V,E)$以及一系列顶点对$(u,v)$,判断两个顶点之间是否连通，对应两种查询情况：  </p>
<ul>
<li>Chained Queries：查询路径上每条边开始于上一条边结束，并且总时间在规定的范围内</li>
<li>Snapshot Queries：对于一个动态变化的图，在给定的时间范围内至少在$c$个快闪图中存在连通</li>
</ul>
<h3 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h3><ol>
<li>以起始顶点和目的顶点各自进行多次固定长度的随机漫步构建两个集合，两个集合的交集非空说明连通</li>
<li>对于随机漫步的长度及次数的选取给出了理论证明<br> 2.1 随机漫步的长度：有向图中最长的最短路径的长度，通过10次的深度优先搜索得到<br> 2.2 随机漫步的次数：类比于扔球问题，给定$n$个篮子和数量相等的红球与蓝球，需要扔多少个球来保证有一个篮子中同时有红球与蓝球的概率高？红球可以看作起始顶点$u$可以到达的顶点，蓝球可以看作目的顶点$v$可以到达的顶点</li>
<li>模型的一个假设前提是构建的两个反向的随机漫步的平稳分布应尽可能接近，这对应于正向随机漫步选定一个出边的概率与反向随机漫步选定一个入边的概率相等，而这个概率恰等于顶点度的倒数。使用同配性(assortativity)作为这两个平稳分布接近程度的指标。</li>
</ol>
<h2 id="Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems-SIGIR’19"><a href="#Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems-SIGIR’19" class="headerlink" title="Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR’19]"></a>Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR’19]</h2><h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FEXIPRO[SIGMOD’17]中的对IPR问题的求解较慢，可以使用GPU进行并行加速。</p>
<h4 id="IPR问题"><a href="#IPR问题" class="headerlink" title="IPR问题"></a>IPR问题</h4><p>给定一个用户矩阵$Q\in \mathbb{R}^{d\times m}$以及一个物品矩阵$P\in \mathbb{R}^{d\times n}$，对于$Q$中的每一个用户$q$，返回内积$q^TP$中的前k个$q^Tp$对应的物品列表$p$</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="行文逻辑"><a href="#行文逻辑" class="headerlink" title="行文逻辑"></a>行文逻辑</h4><p>作者首先画出四个数据集上，SeqScan与FEXIPRO中两个步骤（内积计算与Top-k物品获取）的运行时间占比，发现内积计算占了总开销的90%以上，促使他提出方法加速这一步骤。接下来介绍GPU加速CPU程序的流程，提出了第一个改进方法，即分batch将矩阵送入GPU并行地计算内积。下一步同样地画出它各个步骤的运行时间占比，发现现在top-k物品的获取以及将内积结果从GPU内存复制到CPU内存这两个步骤变成了时间开销的大头。于是顺着分析结果提出了两个改进方法针对性地减小这两个步骤的时间开销。</p>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>前后提出了三个改进方法：GPU-IP、GPU-IPR、GPU-IPRO，分别为：<br>GPU-IP：1<br>GPU-IPR：1+2<br>GPU-IRPO：1+2+3</p>
<ol>
<li><p>并行计算$Q^TP$，并且提出了一种新的矩阵分割方法以充分利用GPU内存，从而加速内积的计算  </p>
<blockquote>
<p>给定GPU内存为$M$，各自选取用户矩阵与物品矩阵的子集$Q_s\in Q,P_s\in P$使得$Size(Q_s^TP_s)\le M$，论文的做法是取$Q_s=Q$，通过$Size(Q^TP_s)=M$来选取$P_s$的大小</p>
</blockquote>
</li>
<li>为每一个用户指定最佳的内积数量$g_s$为1024，从这1024个计算结果中返回top-k，减少了待排序的数据规模<blockquote>
<p>内积数量会严重影响下一步的Bitonic排序的性能。选取的依据是它应该满足每一个线程组的共享内存大小因为它会在GPU缓存层级关系中带来最小的缓存访问延迟(The size of $g_s$ should fit in the shared<br>memory of each threads group as it incurs minimum cache access latency in GPU cache hierarchy.)</p>
</blockquote>
</li>
<li><p>提出了一种剪枝方法来提前结束计算进程，减少了许多内积计算<br>假设用户$u$与其第$k$大的物品的内积为$S_k$，且$||q||\cdot||p||\le S_k$，则有  $q^Tp \le ||q||\cdot||p||\le S_k$，因为目的是得到top-k物品，满足上述不等式的物品已经被排除在top-k之外，不需要送入下一次迭代进行内积计算</p>
<blockquote>
<p>使用这种剪枝方法后，在四个数据集的前10次迭代中，分别减少了98.88%、76.61%、88.69%以及1.57%的用户数量。</p>
</blockquote>
</li>
</ol>
<h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1><h2 id="Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17"><a href="#Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17" class="headerlink" title="Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]"></a>Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]</h2><h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将神经网络应用在图结构数据上？</p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>给定以下输入：  </p>
<ol>
<li>图中顶点的特征矩阵$H\in \mathbb{R}^{n\times F}$，其中$n$为顶点数量，$F$为特征数量  </li>
<li>图的结构信息，如邻接矩阵$A$  </li>
</ol>
<p>输出：  </p>
<ol>
<li>图中顶点新的的特征表示$H’\in \mathbb{R}^{n\times F’}$，即</li>
</ol>
<script type="math/tex; mode=display">
H'=\text{GCN(H)}=g(AHW^T+b)</script><p>如果套用神经网络模型，每一层可以用一个非线性函数进行表示：</p>
<script type="math/tex; mode=display">
H^{(l+1)}=f(H^{(l)},A)</script><p>其中$H^{(0)}=X,H^{(L)}=Z$，问题在于如何选取函数$f(.,.)$</p>
<h3 id="做法及创新-1"><a href="#做法及创新-1" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>对于函数$f(.,.)$的选取，论文中提出了一种可能的函数形式：  </p>
<script type="math/tex; mode=display">
f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}) \\</script><p>其中$\hat{A}=A+I$，因为与矩阵$A$相乘表示对于每个顶点，我们对除了自身外所有邻居顶点的特征向量进行求和，因此加上单位矩阵是为了引入自环。而正则化是避免与矩阵$A$相乘改变特征向量的规模。实际在论文中只使用两层网络就达到了很好的效果，表示为：</p>
<script type="math/tex; mode=display">
Z_{\text{GCN}}=\text{softmax}\big(\hat{A'}\text{ReLU}\big(\hat{A'}XW_0\big)W_1\big)</script><p>其中$W_0、W_1$为这两层网络的参数，$\hat{A’}=\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$，$Z\in \mathbb{R}^{n\times c}$为预测的顶点标签，$c$为类别数目，毕竟论文解决的就是一个分类问题。</p>
<p>更一般地，<a href="https://arxiv.org/abs/1810.00826v3" target="_blank" rel="noopener">GIN</a>将使用邻域信息的图神经网络形式概括为：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\text{COMBINE}^{(l)}(h_v^{(l-1)},a_v^{(l)}),\quad a_v^{(l)}=\text{AGGREGATE}^{(l)}(\{h_u^{(l-1)}:u\in N(v) \})</script><p>其中$W_l$是第$l$层网络的权重矩阵，$\text{AGGREGATE}$是与特定模型相关的聚合函数，$h_v^{(l)}$是顶点$v$在第$l$层的隐层特征表示。论文中使用的网络形式如下，只是用了一个两层网络就达到了很好的效果：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\text{ReLU}\Big(W_l·\text{MEAN}\big\{h_u^{(l-1)}:u\in N(v)\cup\{v\} \big\}\Big)</script><p><a href="https://papers.nips.cc/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf" target="_blank" rel="noopener">AS-GCN</a>中对这篇论文的模型形式描述如下：</p>
<script type="math/tex; mode=display">
h_{v_i}^{(l)}=\sigma\Big(W_l·\sum_{j=1}^Na(v_i,u_j)·h_{u_j}^{(l-1)}\Big),i=1,\dots,N</script><p>这里$A=(a(v_i,u_j))\in \mathbb{R}^{N\times N}$对应前面一种写法的正则化邻接矩阵$\hat{A’}$，表面上看对于顶点$v_i$，需要考虑将图中剩下的所有顶点的上一时刻的隐层表示做加权和，来作为它当前时刻的隐层表示，因为$j$的取值范围为$[1,N]$，$N$就是图中顶点的数量。但实际上，大多数顶点因为与$v_i$并无边相连，所以邻接矩阵中对应的值为0，意味着在加权和中的权重为0，相当于加权和时只会考虑有边相连的顶点，这同样是考虑邻域，只不过跟上面那种写法不同。</p>
<h2 id="Neural-Graph-Collaborative-Filtering-SIGIR’19"><a href="#Neural-Graph-Collaborative-Filtering-SIGIR’19" class="headerlink" title="Neural Graph Collaborative Filtering[SIGIR’19]"></a>Neural Graph Collaborative Filtering[SIGIR’19]</h2><h3 id="解决的问题-3"><a href="#解决的问题-3" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在现有的推荐模型中，用户和物品的embedding只考虑了它们自身的特征，没有考虑用户-物品的交互信息</p>
<h3 id="做法及创新-2"><a href="#做法及创新-2" class="headerlink" title="做法及创新"></a>做法及创新</h3><script type="math/tex; mode=display">
\hat{y}_{NGCF}(u,i)={e^*_u}^Te^*_i \\
e^*_u = e_u^{(0)}||\dotsb||e_u^{(L)} \\
e^*_i = e_i^{(0)}||\dotsb||e_i^{(L)} \\
e_u^{(l)}=LeakyReLU(m^{(l)}_{u\leftarrow u}+\sum_{i\in N_u}m^{(l)}_{u\leftarrow i}) \\
\begin{cases}
m^{(l)}_{u\leftarrow i}=p_{ui}(W_1^{(l)}e_i^{(l-1)}+W_2^{(l)}(e_i^{(l-1)}\odot e_u^{(l-1)})) \\\\
m^{(l)}_{u\leftarrow u}=W_1^{(l)}e_u^{(l-1)}
\end{cases} \\
m_{u\leftarrow i}=\frac{1}{\sqrt{|N_u||N_i|}}(W_1e_i+W_2(e_i\odot e_u))</script><ol>
<li><p>通过堆叠$l$层embedding传播层，一个用户（物品）可以获得它的$l$跳邻居所传播的信息，如下图所示，通过这种方法来建模用户-物品交互信息中的高阶connectivity，下图展示的是一个三阶的例子:</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg" width="400" height="200" alt="0ZY5mF.jpg" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0ePimF.png" width="68%" alt="0ePimF.png" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0Z2M40.jpg" width="400" height="200" alt="0Z2M40.jpg" border="0">
</div>
</li>
<li><p>传统GCN推荐方法中，message embedding只考虑物品embedding$e_i$，论文中将用户embedding与物品embedding的交互也纳入考虑，解释为“This makes the message dependent on the affinity between $e_i$ and $e_u$, e.g., passing more messages from the similar items.”</p>
</li>
<li><p>两个层面上的dropout：message &amp; node dropout。前者表示在第$l$层传播层中，只有部分信息会对最后的表示有贡献；后者表示在第$l$层传播层中，随机地丢弃一些顶点。</p>
</li>
</ol>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-book</p>
<h2 id="LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20"><a href="#LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20" class="headerlink" title="LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]"></a>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]</h2><h3 id="解决的问题-4"><a href="#解决的问题-4" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在协同过滤中，图卷积网络中的特征转换与非线性激活对提升模型表现贡献很小，甚至有负面影响。</p>
<blockquote>
<p>在半监督顶点分类问题中，每个顶点有充分的语义特征作为输入，例如一篇文章的标题与摘要词，这种情况下加入多层的非线性特征转换能够有助于学习特征。而在协同过滤任务中，每个顶点（用户或商品）没有这么充分的语义特征，因此没有多大的作用。</p>
</blockquote>
<h3 id="做法及创新-3"><a href="#做法及创新-3" class="headerlink" title="做法及创新"></a>做法及创新</h3><div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0evf4P.png" alt="0evf4P.png" width="80%" border="0">
</div>

<script type="math/tex; mode=display">
\hat{y}_{ui}=e_u^Te_i \\
e_u=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_i=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_u^{(k+1)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(k)} \\
e_i^{(k+1)}=\sum_{i\in N_i}\frac{1}{\sqrt{|N_i||N_u|}}e_u^{(k)}</script><ol>
<li><p>仅考虑图卷积网络中的neighborhood aggregation，通过在用户-物品交互网络中线性传播来学习用户和物品的embedding，再通过加权和将各层学习的embedding作为最后的embedding</p>
</li>
<li><p>通过减少不必要的架构，相较于NGCF大大减少了需要训练的参数量。唯一需要训练的模型参数是第0层的embedding，即$e_u^{(0)}$与$e_i^{(0)}$，当它们两个给定后，后续层的embedding可以通过传播规则直接进行计算</p>
<blockquote>
<p>以加权和的方式结合各层的embedding等价于带自连接的图卷积</p>
</blockquote>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
E^{(K)}&=(A+I)E^{(K-1)}=(A+I)^KE^{(0)}\\
&=C_K^0E^{(0)}+C_K^1AE^{(0)}+C_K^2A^2E^{(0)}+\dots+C_K^KA^KE^{(0)}
\end{aligned}</script><ol>
<li>模型的可解释性更强，以二层网络为例:</li>
</ol>
<script type="math/tex; mode=display">
e_u^{(2)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(1)}=\sum_{i\in N_u}\frac{1}{|N_i|}\sum_{v\in N_i}\frac{1}{\sqrt{|N_u||N_v|}}e_v^{(0)}</script><p>如果另一个用户$v$与目标用户$u$有关联，则影响以下面的系数表示：</p>
<script type="math/tex; mode=display">
c_{v\rightarrow u}=\frac{1}{\sqrt{|N_u||N_v|}}\sum_{i\in N_u\cap N_v}\frac{1}{|N_i|}</script><p>可解释为:</p>
<ul>
<li>共同交互过的物品越多系数越大 $i\in N_u\cap N_v$</li>
<li>物品流行度越低系数越大$\frac{1}{|N_i|}$</li>
<li>用户$v$越不活跃系数越大$\frac{1}{|N_v|}$</li>
</ul>
<h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-Book</p>
<h2 id="Simplifying-Graph-Convolutional-Networks-PMLR’19"><a href="#Simplifying-Graph-Convolutional-Networks-PMLR’19" class="headerlink" title="Simplifying Graph Convolutional Networks[PMLR’19]"></a>Simplifying Graph Convolutional Networks[PMLR’19]</h2><h3 id="解决的问题-5"><a href="#解决的问题-5" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>图卷积网络中可能引入了一些不必要的复杂性及冗余的计算</p>
<h3 id="做法及创新-4"><a href="#做法及创新-4" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><img src="https://s1.ax1x.com/2020/10/04/0JtcnS.png" alt="0JtcnS.png" border="0">  </p>
<ol>
<li>移除图卷积网络各层之间的非线性关系，合并各层之间的权重矩阵</li>
</ol>
<h4 id="原始图卷积网络"><a href="#原始图卷积网络" class="headerlink" title="原始图卷积网络"></a>原始图卷积网络</h4><p>对于一个输入的图，图卷积网络利用多层网络为每个顶点的特征$x_i$学习一个新的特征表示，随即输入一个线性分类器。对第$k$层网络，输入为$H^{(k-1)}$，输出为$H^{(k)}$，其中$H^{(0)}=X$。一个$K$层的图卷积网络等价于对图中每个顶点的特征向量$x_i$应用一个$K$层感知机，不同之处在于顶点的隐层表示local averaging：</p>
<script type="math/tex; mode=display">
h_i^{(k)}\leftarrow \frac{1}{d_i+1}h_i^{(k-1)}+\sum^n_{j=1}\frac{a_{ij}}{\sqrt{(d_i+1)(d_j+1)}}h_j^{(k-1)}</script><p>矩阵形式：</p>
<script type="math/tex; mode=display">
S=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>其中$A=A+I$，则隐层表示用矩阵的形式表示为：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow SH^{(k-1)}</script><p>Local averaging：this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes  </p>
<p>$\Theta^{(k)}$为第$K$层网络的权重矩阵：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow \text{ReLU}(H^{(k)}\Theta^{(k)})</script><p>$Y\in \mathbb{R}^{n\times C}$，$y_{ic}$表示第$i$个顶点属于类别$C$的概率</p>
<script type="math/tex; mode=display">
Y_{GCN}=\text{softmax}(SH^{(K-1)}\Theta^{(K)})</script><h4 id="简化图卷积网络"><a href="#简化图卷积网络" class="headerlink" title="简化图卷积网络"></a>简化图卷积网络</h4><blockquote>
<p>在传统的多层感知机中，多层网络可以提高模型的表现力，是因为这样引入了特征之间的层级关系，例如第二层网络的特征是以第一层网络为基础构建的。而在图卷积网络中，这还有另外一层含义，在每一层中顶点的隐层表示都是以一跳的邻居进行平均，经过$K$层之后，一个顶点就能获得$K$跳邻居的特征信息。这类似于在卷积网路中网络的深度提升了特征的receptive field。</p>
</blockquote>
<p>保留local averaging，移除了非线性激活函数：</p>
<script type="math/tex; mode=display">
Y=\text{softmax}(S^KX\Theta^{(1)}\dots \Theta^{(K)})</script><p>其中$S^K$可以预先进行计算，大大减少了模型的训练时间</p>
<p>论文中证明了简化后的图卷积网络等价于谱空间的一个低通滤波器，它通过的低频信号对应于图中平滑后的特征</p>
<h3 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、Reddit</p>
<h2 id="Inductive-Representation-Learning-on-Large-Graphs-NIPS’17"><a href="#Inductive-Representation-Learning-on-Large-Graphs-NIPS’17" class="headerlink" title="Inductive Representation Learning on Large Graphs[NIPS’17]"></a>Inductive Representation Learning on Large Graphs[NIPS’17]</h2><h3 id="解决的问题-6"><a href="#解决的问题-6" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>对于学习图上顶点的embedding，现有的方法多为直推式学习，学习目标是直接生成当前顶点的embedding，不能泛化到未知顶点上</p>
<h3 id="做法及创新-5"><a href="#做法及创新-5" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出一种归纳式学习方法GrdaphSAGE，不为每个顶点学习单独的embedding，而是学习一种聚合函数$\text{AGGREGATE}$，从一个顶点的局部邻域聚合特征信息，为未知的顶点直接生成embedding，因此旧的顶点只要邻域发生变化也能得到一个新的embedding</p>
<blockquote>
<p>GCN不是归纳式，因为每次迭代会用到整个图的邻接矩阵$A$；而GraphSAGE可以对GCN做了精简，每次迭代只抽样取直接相连的邻居</p>
</blockquote>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol>
<li>给定顶点$v$及其特征$x_v$,作为它的初始表示$h_v^0=x_v$。</li>
<li>计算邻域向量$h^k_{N(v)}=\text{AGGREGATE}({h_u^{(k-1)}}, \forall u\in N(v))$，当前层顶点的邻居从上一层采样，且邻居个数固定，非所有邻居，这样每个顶点和采样后邻居的个数都相同，可以直接拼成一个batch送到GPU中进行批训练</li>
<li>将邻域向量与自身上一层的表示拼接，通过非线性激活函数$\sigma$后作为这一层的表示$h_v^k=\sigma(W^k\text{CONCAT}(h_v^{(k-1)},h^k_{N(v)})$</li>
<li>标准化 $h_v^k=h_v^k/||h_v^k||_2$</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/06/0tja9K.png" alt="0tja9K.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/06/0tvRaR.jpg" alt="0tvRaR.jpg" width="50%" border="0">
</div>

<h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><ol>
<li>MEAN</li>
</ol>
<script type="math/tex; mode=display">
h_v^k=\sigma(W·\text{MEAN}(\{h_v^{k-1}\}\cup\{h_u^{k-1},\forall u\in N(v) \})</script><ol>
<li>LSTM</li>
<li>Pooling<br>GraphSAGE采用的max-pooling策略能够隐式地选取领域中重要的顶点：</li>
</ol>
<script type="math/tex; mode=display">
h^k_{N(v)}=\text{AGGREGATE}_k^{pool}=\text{max}(\{\sigma(W_{pool}h_u^k + b),\forall u\in N(v)\})</script><h3 id="数据集-3"><a href="#数据集-3" class="headerlink" title="数据集"></a>数据集</h3><p>BioGRID、Reddit</p>
<h2 id="Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16"><a href="#Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16" class="headerlink" title="Learning Convolutional Neural Networks for Graphs[ICML’16]"></a>Learning Convolutional Neural Networks for Graphs[ICML’16]</h2><h3 id="解决的问题-7"><a href="#解决的问题-7" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积神经网络都是应用在图像数据上，如何将它有效地应用于图类型的数据上。</p>
<p>对于图像数据，应用一个卷积神经网络可以看成将receptive field（图中为$3\times3$）以固定的步长将图像遍历，因为图像中像素点的排列有一定的次序，receptive field的移动顺序总是从上到下，从左到右。这也唯一地决定了receptive field对一个像素点的遍历方式以及它如何被映射到向量空间中。</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WKA5n.png" alt="0WKA5n.png" border="0" width="65%">
</div>

<p>然而对于图结构数据这种隐式的结构特征很多时候是缺失的，而且当给定不止一张图时，各个图之间的顶点没有必然的联系。因此，在将卷积神经网络应用在图数据上时，需要解决下面两个问题：</p>
<ol>
<li><p>决定邻域中顶点的产生次序</p>
</li>
<li><p>计算一个将图映射到向量空间的映射方法</p>
</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/12/0W1Zo4.png" alt="0W1Zo4.png" border="0" width="80%"></p>
<h3 id="做法及创新-6"><a href="#做法及创新-6" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出方法的流程如下：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0W3X5Q.png" alt="0W3X5Q.png" border="0" width="60%"></p>
<h4 id="Node-Sequence-Selection"><a href="#Node-Sequence-Selection" class="headerlink" title="Node Sequence Selection"></a>Node Sequence Selection</h4><p>从图中选取固定数量$w$的顶点，它类比于图像的宽度，而选出的顶点就是卷积操作中小矩形的中心顶点。$w$就是在这个图上所做的卷积操作的个数。如下图所示，$w=6$，代表需要从图中选择6个顶点做卷积操作。论文中选取顶点的方式为$\text{DFS}$，关键点在于图标签函数$l$，这个函数的作用是决定选取顶点的次序，可以选区的函数为between centrality与WL算法等等</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WGInP.png" alt="0WGInP.png" border="0"></p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WyWOU.png" alt="0WyWOU.png" border="0"></p>
<h4 id="Neighborhood-Assembly"><a href="#Neighborhood-Assembly" class="headerlink" title="Neighborhood Assembly"></a>Neighborhood Assembly</h4><p>选取完顶点后，下一步是为它们构建receptive field，类似于第一张图中的$3\times3$矩阵。选取的方式为，以顶点$v$为中心，通过$\text{BFS}$添加领域顶点，直到满足receptive field长度$k$：</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WDBw9.png" alt="0WDBw9.png" border="0" width="60%">
</div>

<p><img src="https://s1.ax1x.com/2020/10/12/0W6V0g.png" alt="0W6V0g.png" border="0" width="80%"></p>
<h4 id="Graph-Normalization"><a href="#Graph-Normalization" class="headerlink" title="Graph Normalization"></a>Graph Normalization</h4><p>在选取了满足数量的邻域顶点后，下一步是通过图标签函数$l$为这些顶点赋予一个次序，目的在于将无序的领域映射为一个有序的向量：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0Wy1dH.png" alt="0Wy1dH.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0W6rnO.png" alt="0W6rnO.png" border="0" width="60%">
</div>

<h4 id="Convolutional-Architecture"><a href="#Convolutional-Architecture" class="headerlink" title="Convolutional Architecture"></a>Convolutional Architecture</h4><p>最后一步就是应用卷积层提取特征，顶点和边的属性对应于传统图像CNN中的channel：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpKeK.png" alt="0fpKeK.png" border="0" width="75%"></p>
<p>假设顶点特征的数目为$a_v$，边的特征个数为$a_e$，$w$为选取的顶点个数，$k$为receptive field中的顶点个数，则对于输入的一系列图中的每一个，可以得到两个张量维度分别为$(w,k,a_v)、(w,k,k,a_e)$，可以变换为$(wk,a_v)、(wk^2,a_e)$，其中$a_v$与$a_e$可以看成是传统图像卷积中channel的个数，对它们做一维的卷积操作，第一个的receptive field的大小为$k$，第二个的receptive field的大小为$k^2$。</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpWwT.png" alt="0fpWwT.png" border="0" width="75%"></p>
<p>整体卷积结构：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpbOx.png" alt="0fpbOx.png" border="0" width="75%"></p>
<h3 id="数据集-4"><a href="#数据集-4" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI、D&amp;D</p>
<h2 id="Graph-Attention-Networks-ICLR’18"><a href="#Graph-Attention-Networks-ICLR’18" class="headerlink" title="Graph Attention Networks[ICLR’18]"></a>Graph Attention Networks[ICLR’18]</h2><h3 id="解决的问题-8"><a href="#解决的问题-8" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将attention机制应用于图类型的数据上。</p>
<h3 id="做法及创新-7"><a href="#做法及创新-7" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><div align="center">
<img src="https://s1.ax1x.com/2020/10/16/07O1IK.png" alt="1" border="0" width="60%">
<img src="https://s1.ax1x.com/2020/10/16/07O8PO.png" alt="2" border="0" width="60%">
</div>

<p>给定一个含$n$个顶点的图，其中顶点的特征构成的集合为$(\overrightarrow{h_1},\overrightarrow{h_2},\dots,\overrightarrow{h_n})$，$\overrightarrow{h_i}\in \mathbb{R}^F$且邻接矩阵为$A$。一个图卷积层根据已有的顶点特征和图的结构来计算一个新的特征集合$(\overrightarrow{h_1’},\overrightarrow{h_2’},\dots,\overrightarrow{h_n’})$，$\overrightarrow{h_i’}\in \mathbb{R}^{F’}$</p>
<p>每个图卷积层首先会进行特征转换，以特征矩阵$W$表示，$W\in \mathbb{R}^{F’\times F}$它将特征向量线性转换为$\overrightarrow{g_i}=W\overrightarrow{h_i}$，再将新得到的特征向量以某种方式进行结合。为了利用邻域的信息，一种典型的做法如下：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_i}'=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}\overrightarrow{g_j}\bigg)</script><p>其中$N_i$表示顶点$i$的邻域（典型的构造方式是选取直接相连的顶点，包括自身），$\alpha_{ij}$表示顶点$j$的特征对于顶点$i$的重要程度，也可以看成一种权重。</p>
<p>现有的做法都是显式地定义$\alpha_{ij}$，本文的创新之处在于使用attention机制隐式地定义$\alpha_{ij}$。所使用的attention机制定义为$a:R^{F’}\times \mathbb{R}^{F’} \rightarrow \mathbb{R}$，以一个权重向量$\overrightarrow{a}\in \mathbb{R}^{2F’}$表示，对应于论文中的self-attention。  </p>
<h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><ol>
<li>基于顶点的特征计算系数$e_{ij}$</li>
</ol>
<script type="math/tex; mode=display">
e_{ij}=a(W\overrightarrow{h_i},W\overrightarrow{h_j})</script><ol>
<li>以顶点的邻域将上一步计算得到的系数正则化，这么做能引入图的结构信息：</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_{ij}&=\text{softmax}_j(e_{ij})=\frac{\exp(e_{ij})}{\sum_{k\in N_i}\exp(e_{ik})}\\
&=\frac{\exp(\text{LeakyReLU}(\overrightarrow{a}^T[W\overrightarrow{h}_i||W\overrightarrow{h}_j]))}{\sum_{k\in N_i}\exp(\text{LeakyReLU}(\overrightarrow{a}^T[W\overrightarrow{h}_i||W\overrightarrow{h}_k]))}
\end{aligned}</script><p><img src="https://s1.ax1x.com/2020/10/16/0H5cX6.png" alt="0H5cX6.png" border="0" width="30%"></p>
<blockquote>
<p>次序不变性：给定$(i,j),(i,k),(i’,j),(i’,k)$表示两个顶点间的关系，可以为边或自环。$a$为对应的attention系数，如果$a_{ij}&gt;a_{ik}$，则有$a_{i’j}&gt;a_{i’k}$</p>
</blockquote>
<p>​    <a href="https://dl.acm.org/doi/10.1145/3219819.3220077" target="_blank" rel="noopener">DeepInf</a>中给出了证明：</p>
<p>​    将权重向量$\overrightarrow{a}\in \mathbb{R}^{2F’}$重写为$\overrightarrow{a}=[p^T，q^T]$，则有</p>
<script type="math/tex; mode=display">
e_{ij}=\text{LeakyReLU}(p^TWh_i+q^TWh_j)</script><p>​    由softmax与LeakyReLU的单调性可知，因为$a_{ij}&gt;a_{ik}$，有$q^TWh_j&gt;q^TWh_k$，类似地就可以得到$a_{i’j}&gt;a_{i’k}$。</p>
<p>​    这意味着，即使每个顶点都只关注于自己的邻域，但得到的attention系数却具有全局性。</p>
<ol>
<li>以上一步得到的系数$\alpha_{ij}$作为顶点$j$的特征对顶点$i$的重要程度，将领域中各顶点的特征做一个线性组合以作为顶点$i$最终输出的特征表示：</li>
</ol>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}W\overrightarrow{h_j}\bigg)</script><h4 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><p>为了稳定self-attention的学习过程，论文引入了multi-head attention，即由$K$个相互独立的self-attention得到各自的特征，再进行拼接：</p>
<p><img src="https://s1.ax1x.com/2020/10/16/0HbZlj.png" alt="0HbZlj.png" border="0" width="60%"></p>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\Vert_{k=1}^K\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)</script><p>其中$\alpha_{ij}^k$是第$k$个attention机制$(a^k)$计算出来的正则化系数，$W^k$是对应的将输入进行线性转化的权重矩阵。论文选取的拼接操作为求平均：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\sigma\bigg(\frac{1}{K}\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)</script><h3 id="数据集-5"><a href="#数据集-5" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、PPI</p>
<h2 id="Representation-Learning-on-Graphs-with-Jumping-Knowledge-Networks-ICML’18"><a href="#Representation-Learning-on-Graphs-with-Jumping-Knowledge-Networks-ICML’18" class="headerlink" title="Representation Learning on Graphs with Jumping Knowledge Networks[ICML’18]"></a>Representation Learning on Graphs with Jumping Knowledge Networks[ICML’18]</h2><h3 id="解决的问题-9"><a href="#解决的问题-9" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>当图卷积网络GCN的层数超过两层时模型的表现会变差，这使得GCN只能作为浅层模型使用，且在对邻域节点的信息进行聚合时，即使同样是采用$k$层网络来聚合$k$跳邻居的信息，有着不同局部结构的顶点获得的信息也可能完全不同，以下图为例：</p>
<div align="center">
<a href="https://imgchr.com/i/BpCLz8" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpCLz8.jpg" alt="BpCLz8.jpg" border="0" width="70%"></a>
</div>

<p>图$(a)$中的顶点位于核心区域，因此采用$4$层网络把几乎整个图的信息都进行聚合了，而不是它的邻域，这会导致过度平滑，而图$(b)$中顶点位于图边缘的一个树状结构中，采取同样的$4$层网络只囊括了一小部分顶点的信息，只有在第$5$层囊括了核心顶点之后才有效地囊括了更多顶点的信息。</p>
<p>所以，对于处于核心区域的顶点，GCN中每多一层即每多一次卷积操作，节点的表达会更倾向全局，这导致核心区域的很多顶点的表示到最后没有区分性。对于这样的顶点应该减少GCN的层数来让顶点更倾向局部从而在表示上可以区分；而处于边缘的顶点，即使更新多次，聚合的信息也寥寥无几，对于这样的顶点应该增加GCN的层数，来学习到更充分的信息。因此，对于不同的顶点应该选取不同的层数，传统做法对于所有顶点都用一个值会带来偏差。</p>
<h3 id="做法及创新-8"><a href="#做法及创新-8" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>理论部分，论文主要讨论的问题是，在一个$k$层的GCN中，顶点$x$对顶点$y$的影响程度，即顶点$x$输入特征的改变，会对顶点$y$在最后一层得到的表示产生多大的变化，也可以说是顶点$y$对于顶点$x$有多敏感。假设输入的特征为$X\in \mathbb{R}^{n\times f}$，输出的预测标签为$Z\in \mathbb{R}^{n\times c}$，其中$n$为图中顶点数目，$c$为类别数目，$f$为特征数目，则这种影响程度可以表示为$I(x,y)=\sum_i\sum_j\frac{\partial Z_{yi}}{\partial X_{xj}}$。</p>
<p>更特别地，论文证明了这个影响程度与从顶点$x$开始的$k$步随机漫步的分布有关，如果对$k$取极限$k\rightarrow \infty$，则随机漫步的分布会收敛到$P_{lim}(\rightarrow y)$。详细论证过程可见原文。这说明，结果与随机漫步的的起始顶点$x$没有关系，通过这种方法来得到$x$的邻域信息是不适用的。</p>
<p>另一种说法是，一个$k$层的图卷积网络等同于一个$k$阶的多项式过滤器，其中的系数是预先确定的<a href="https://arxiv.org/abs/2007.02133v1" target="_blank" rel="noopener">SDC</a>。这么一个过滤器与随机漫步类似，最终会收敛到一个静态向量，从而导致过度平滑。</p>
<p>实践部分，论文提出JK-Net，通过Layer aggregation来让顶点最后的表示自适应地聚合不同层的信息，局部还是全部，让模型自己来学习：</p>
<div align="center">
<a href="https://imgchr.com/i/BpEvLT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpEvLT.jpg" alt="BpEvLT.jpg" border="0" width="50%"></a>
</div>

<p>论文的重点在于最后的Layer aggregation层，可选的三种操作为：Concat、Max-pooing以及LSTM-attn。</p>
<ol>
<li><p>Concat</p>
<p>将各层的表示直接拼接在一起，送入Linear层。对于小数据集及结构单一的图这种聚合方式会更好，因为它们不需要顶点在聚合邻域的顶点信息时具有什么自适应性。</p>
</li>
<li><p>Max-pooling</p>
<p>选取各层的表示中包含信息量最多的作为顶点的最终表示，在多层结构中，低层聚合更多局部信息，而高层会聚合更多全局信息，因此对于核心区域内的顶点可能会选取高层表示而边缘顶点选取低层表示。</p>
</li>
<li><p>LSTM-attention</p>
<p>对于各层的表示，attention机制通过计算一个系数$s_v^{(l)}$来表示各层表示的重要性，其中$\sum_ls_v^{(l)}=1$，顶点最终的表示就是各层表示的一个加权和：$\sum_ls_v^{(l)}·h_v^{(l)}$。</p>
<blockquote>
<p>$s_v^{(l)}$的计算：将$k$层网络各层的表示$h_v^{(1)},\dots,h_v^{(k)}$输入一个双向LSTM中，同时生成各层$l$的前向LSTM与反向LSTM的隐式特征，分别表示为$f_v^{(l)}、b_v^{(l)}$，拼接后将$|f_v^{(l)}||b_v^{(l)}|$送入一个Linear层，将Linear层的结果进行Softmax归一化操作就得到了系数$s_v^{l}$。</p>
</blockquote>
</li>
</ol>
<h3 id="数据集-6"><a href="#数据集-6" class="headerlink" title="数据集"></a>数据集</h3><p>Citeseer、Cora、Reddit、PPI</p>
<h2 id="Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19"><a href="#Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19" class="headerlink" title="Session-Based Recommendation with Graph Neural Networks[AAAI’19]"></a>Session-Based Recommendation with Graph Neural Networks[AAAI’19]</h2><h3 id="解决的问题-10"><a href="#解决的问题-10" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\rightarrow b \rightarrow  c$，只考虑$a\rightarrow b$或者$b\rightarrow c$</p>
<h3 id="做法及创新-9"><a href="#做法及创新-9" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BF3uuT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF3uuT.png" alt="BF3uuT.png" border="0"></a></p>
<h4 id="Session-Graph-Modeling"><a href="#Session-Graph-Modeling" class="headerlink" title="Session Graph Modeling"></a>Session Graph Modeling</h4><p>将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵：</p>
<div align="center">
<a href="https://imgchr.com/i/BF17nO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF17nO.png" alt="BF17nO.png" border="0" width="80%"></a>
</div>

<p>上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\in \mathbb{R}^{n\times 2n}$，其中的一行$A_{s,i:}\in \mathbb{R}^{1\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$：</p>
<div align="center">
<a href="https://imgchr.com/i/BFGCkQ" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFGCkQ.png" alt="BFGCkQ.png" border="0" width="80%"></a>
</div>

<h4 id="Node-Representation-Learning"><a href="#Node-Representation-Learning" class="headerlink" title="Node Representation Learning"></a>Node Representation Learning</h4><p>论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。</p>
<h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><p>一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate：</p>
<div align="center">
<a href="https://imgchr.com/i/BFaaAf" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFaaAf.png" alt="BFaaAf.png" border="0" width="60%"></a>
</div>

<p>直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\in \mathbb{R}^{n\times d}$，上一时刻隐层表示$H_{t-1}\in \mathbb{R}^{n\times h}$，重置门与更新门的输出由下式计算得到：</p>
<script type="math/tex; mode=display">
R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\
Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)</script><p>式中的$W$与$b$分别为权重与偏置参数。</p>
<h5 id="Reset-Gate"><a href="#Reset-Gate" class="headerlink" title="Reset Gate"></a>Reset Gate</h5><p>传统RNN网络的隐式状态更新公式为：</p>
<script type="math/tex; mode=display">
H_t=\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)</script><p>如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\tilde{H_t}$称为候选状态：</p>
<script type="math/tex; mode=display">
\tilde{H_t}=\tanh(X_tW_{xh}+(R_t\odot{H_{t-1}})W_{hh}+b_h)</script><h5 id="Update-Gate"><a href="#Update-Gate" class="headerlink" title="Update Gate"></a>Update Gate</h5><p>更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\tilde{H_t}决定$：</p>
<script type="math/tex; mode=display">
H_t=Z_t\odot H_{t-1}+(1-Z_t)\odot \tilde{H_t}</script><p>介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习：</p>
<p><a href="https://imgchr.com/i/BkiNUU" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BkiNUU.png" alt="BkiNUU.png" border="0"></a></p>
<p>最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。</p>
<h4 id="Session-Representation-Generation"><a href="#Session-Representation-Generation" class="headerlink" title="Session Representation Generation"></a>Session Representation Generation</h4><p>现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\text{s}=[v_{s,1},v_{s,2},\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\in \mathbb{R}^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。</p>
<p>局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣：</p>
<script type="math/tex; mode=display">
s_l=v_n</script><p>全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\text{soft-attention}$机制来计算得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_g&=\sum_{i=1}^{n}\alpha_iv_i\\
\alpha_i&=q^T\sigma(W_1v_n+W_2v_i+c)
\end{aligned}</script><p>最后使用一个$\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量：</p>
<script type="math/tex; mode=display">
s_h=W_3[s_l;s_g]</script><h4 id="Making-Recommendation"><a href="#Making-Recommendation" class="headerlink" title="Making Recommendation"></a>Making Recommendation</h4><p>对于一个待推荐物品$v_i\in V$，计算它在序列$s$中作为下一个被点击物品的概率：</p>
<script type="math/tex; mode=display">
\hat{y_i}=\text{softmax}(s_h^Tv_i)</script><h3 id="数据集-7"><a href="#数据集-7" class="headerlink" title="数据集"></a>数据集</h3><p>Yoochoose、Diginetica</p>
<h2 id="KGAT-Knowledge-Graph-Attention-Network-for-Recommendation-KDD’19"><a href="#KGAT-Knowledge-Graph-Attention-Network-for-Recommendation-KDD’19" class="headerlink" title="KGAT: Knowledge Graph Attention Network for Recommendation[KDD’19]"></a>KGAT: Knowledge Graph Attention Network for Recommendation[KDD’19]</h2><h3 id="解决的问题-11"><a href="#解决的问题-11" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在推荐系统中，如何将用户-物品交互信息与物品自身的属性相结合以做出更好的推荐，从另一个角度来说，即如何融合用户-物品交互图与知识图谱</p>
<div align="center">
<a href="https://imgchr.com/i/BnaHGn" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnaHGn.png" alt="BnaHGn.png" border="0" width="65%"></a>
</div>

<p>以上面的图为例，在电影推荐场景中，用户对应于观众，物品对应于电影，实体Entities可以有多种含义，例如导演、演员、电影类别等，对应的就会有多种关系，对应图中的$r_1-r_4$。对于用户$u_1$，协同过滤更关注于他的相似用户，即同样看过$i_1$的$u_4$与$u_5$；而有监督学习方法例如因子分解机等会更关注物品之间的联系，例如$i_1$与$i_2$同样有着属性$e_1$，但它无法进一步建模更高阶的关系，例如图中黄色圈内的用户$u_2$与$u_3$观看了同一个导演$e_1$的电影$i_2$，而这名导演$e_1$又作为演员参演了灰色圈内的电影$i_3$与$i_4$。图中上半部分对应于用户-物品交互图，下半部分对应于知识图谱。</p>
<h3 id="做法及创新-10"><a href="#做法及创新-10" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BnDu0f" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnDu0f.png" alt="BnDu0f.png" border="0"></a></p>
<h4 id="CKG-Embedding-Layer"><a href="#CKG-Embedding-Layer" class="headerlink" title="CKG Embedding Layer"></a>CKG Embedding Layer</h4><p>知识图谱的一般形式可以表示为三元组的集合$\{(h,r,t)\}$，表示头实体$h$与尾实体$t$之间有关系$r$，例如$\text{(Hugh Jackman,ActorOf,Logan)}$表示狼叔是电影罗根的演员，这是一种主动的关系，自然就有逆向的被动关系。而对于用户-物品交互信息来说，通常的表示形式为一个矩阵$R$，$R_{ui}$表示用户$u$与物品$i$的关系，有交互则值为1，否则为0。因此，为了统一两种表示形式，论文中将用户-物品交互信息同样改成三元组的集合$\text$，这样一来得到的统一后的新图称之为Collaborative Knowledge Graph(CKG)。</p>
<p>第一个步骤是对CKG做embedding，得到图中顶点和边的向量表示形式。论文使用了知识图谱中常用的一个方法$\text{TransR}$，即对于一个三元组$(h,r,t)$，目标为：</p>
<script type="math/tex; mode=display">
e_h^r+e_r\approx e_t^r</script><p>其中$e_h,e_t\in \mathbb{R}d、e_r\in \mathbb{R}k$分别为$h、t、r$的embedding，而$e_h^r,e_t^r$为$e_h、e_t$在$r$所处空间中的投影，损失函数定义为：</p>
<script type="math/tex; mode=display">
g(h,r,t)=||W_re_h+e_r-W_re_t||^2_2</script><p>值越小说明该三元组在知识图谱中更可能存在，即头实体$h$与尾实体$t$之间更可能有关系$r$。经过这一步骤之后，CKG中所有的顶点及边我们都得到了它们的embedding。</p>
<h4 id="Attentive-Embedding-Propagation-Layers"><a href="#Attentive-Embedding-Propagation-Layers" class="headerlink" title="Attentive Embedding Propagation Layers"></a>Attentive Embedding Propagation Layers</h4><p>第二个步骤直接用的GCN与GAT的想法，在一层embedding propagation layer中，利用图卷积网络在邻域中进行信息传播，利用注意力机制来衡量邻域中各邻居顶点的重要程度。再通过堆叠$l$层来聚合$l$阶邻居顶点的信息。</p>
<p>在每一层中，首先将顶点$h$的邻域以向量形式表示，系数$\pi(h,r,t)$还会进行$\text{softmax}$归一化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_{N_h}&=\sum_{(h,r,t)\in N_h}\pi(h,r,t)e_t \\
\pi(h,r,t)&=(W_re_t)^T\text{tanh}\big(W_re_h+e_r\big)
\end{aligned}</script><p>通过堆叠$l$层来聚合$l$阶邻居顶点的信息：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_h^{(l)}&=f\big( e_h^{(l-1)},e_{N_h}^{(l-1)} \big) \\
&=\text{LeakyReLU}\big( W_1(e_h+e_{N_h})\big)+\text{LeakyReLU}\big( W_2(e_h\odot e_{N_h})\big)
\end{aligned}</script><p>论文中所使用的聚合函数$f$在GCN与GraphSage的基础上，还额外地引入了第二项中$e_h$与$e_{N_h}$的交互，这使得聚合的过程对于两者之间的相近程度更为敏感，会在更相似的顶点中传播更多的信息。</p>
<h4 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h4><p>在得到$L$层embedding propagation layer的表示后，使用JK-Net中的LSTM-attention进行聚合，在通过点积的形式给出预测分数：</p>
<script type="math/tex; mode=display">
e_u^*=\text{LSTM-attention}(e_u^{(0)},e_u^{(L)})\\e_i^*=\text{LSTM-attention()}e_i^{(0)}||\dots||e_i^{(L)}\\
\hat{y}(u,i)={e_u^*}^Te_i^*</script><h3 id="数据集-8"><a href="#数据集-8" class="headerlink" title="数据集"></a>数据集</h3><p>Amazon-book、Last-FM、Yelp2018</p>
<h2 id="DeepInf-Social-Influence-Prediction-with-Deep-Learning-KDD’18"><a href="#DeepInf-Social-Influence-Prediction-with-Deep-Learning-KDD’18" class="headerlink" title="DeepInf: Social Influence Prediction with Deep Learning[KDD’18]"></a>DeepInf: Social Influence Prediction with Deep Learning[KDD’18]</h2><h3 id="解决的问题-12"><a href="#解决的问题-12" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何在图结构的社交数据中预测顶点的影响力。</p>
<p>在图中，给定顶点$v$与它的邻域以及一个时间段，通过对开始时各顶点的状态进行建模，来对结束时顶点$v$的状态进行预测（是否被激活）。</p>
<h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><ul>
<li>邻域：给定图$G=(V,E)$，顶点$v$的邻域定义为$N_v^r=\{u:d(u,v)\le r\}$，是一个顶点集合，不包含顶点$v$自身</li>
<li>中心网络：由邻域中的顶点及边所组成的网络，以$G_v^r$表示</li>
<li>用户行为：以$s_v^t$表示，用户对应于图中的顶点，对于一个时刻$t$，如果顶点$v$有产生动作，例如转发、引用等，则$s_v^t=1$</li>
</ul>
<p>给定用户$v$的中心网络、邻域中用户的行为集合$S_v^t=\{s_i^t:i\in N_v^r\}$，论文想解决的问题是，在一段时间$Δt$后，对用户$v$的行为的预测：</p>
<script type="math/tex; mode=display">
P(s_v^{t+Δt}|G_v^r,S_v^t)</script><h3 id="做法及创新-11"><a href="#做法及创新-11" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p><a href="https://imgchr.com/i/BGDfOO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGDfOO.png" alt="BGDfOO.png" border="0"></a></p>
<p>数据预处理方面，论文通过带重启的随机漫步来为图中的每个顶点$v$获取固定大小$n$的中心网络$G_v^r$，接着使用$\text{DeepWalk}$来得到图中顶点的embedding，最后进行归一化。通过这几个步骤对图中的特征进行提取后，论文还进一步添加了几种人工提取的特征，包括用户是否活跃等等：</p>
<div align="center">
<a href="https://imgchr.com/i/BGyXX6" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGyXX6.png" alt="BGyXX6.png" border="0" width="70%"></a>
</div>

<blockquote>
<p>摘要里说传统的影响力建模方法都是人工提取图中顶点及结构的特征，论文的出发点就是自动学习这种特征表示，结果在预处理的最后还是添加了几种人工提取的特征，这不是自相矛盾吗？</p>
</blockquote>
<p>经过上面的步骤后，最后得到包含所有用户特征的一个特征矩阵$H\in \mathbb{R}^{n\times F}$，每一行$h_i^T$表示一个用户的特征，$F$等同于$\text{DeepWalk}$长度加上人工特征长度。</p>
<h4 id="影响力计算"><a href="#影响力计算" class="headerlink" title="影响力计算"></a>影响力计算</h4><p>这一步纯粹是在套GAT的框架，没什么可以说的，计算如下：</p>
<script type="math/tex; mode=display">
H'=\text{GAT}(H)=g(A_{\text{GAT}}(G)HW^T+b)\\
A_{\text{GAT}}(G)=[a_{ij}]_{n\times n}</script><p>其中$W\in \mathbb{R}^{F’\times F}, b\in \mathbb{R}^{F’}$是模型的参数，$a_{ij}$的计算在GAT论文的笔记中有记录，不再赘述。</p>
<h3 id="数据集-9"><a href="#数据集-9" class="headerlink" title="数据集"></a>数据集</h3><p>OAG、Digg、Twitter、Weibo</p>
<h2 id="Predict-then-Propagate-Graph-Neural-Networks-meet-Personalized-PageRank-ICLR’19"><a href="#Predict-then-Propagate-Graph-Neural-Networks-meet-Personalized-PageRank-ICLR’19" class="headerlink" title="Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR’19]"></a>Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR’19]</h2><h3 id="解决的问题-13"><a href="#解决的问题-13" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GCN层数增加后性能反而变差，如何加深GCN的层数。</p>
<p>根据GCN的定义，每一层网络用来捕获一跳邻居的信息，例如一个三层的GCN网络捕获的就是一个顶点三跳邻居以内的信息，而现在如果只能用浅层模型，表示只能捕获有限跳内的邻域信息，而有时候要多几跳才能捕获到有用的信息，例如<a href="#Representation Learning on Graphs with Jumping Knowledge Networks[ICML&#39;18]">JK-Net</a>中的例子。</p>
<h3 id="做法及创新-12"><a href="#做法及创新-12" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>这一篇论文的工作其实是接着JK-Net继续往下，在那篇论文中，作者分析了GCN中信息传递这个过程与随机漫步之间的关系，论证了当层数加深之后，GCN会收敛到这个随机漫步的极限分布，而这个极限分布只与图的全局属性有关，没有把随机漫步的起始顶点，或者说是GCN中从邻域中传递和聚合信息的根顶点考虑在内，这么一来，层数加深之后每个顶点聚合出来的样子都差不多，无法区分从而导致性能变差，另一个看待的角度是，因为原始GCN是对所有聚合的信息做平均操作，层数加深之后各个顶点的邻域都变得跟整张图差不多，既然每个顶点的邻域都变得差不多，做的又是平均操作，每个顶点聚合出来的样子就会都差不多。</p>
<p>论文提出的解决办法是引入PageRank的思想，这也是从JK-Net中的结论观察出来的。JK-Net中所说的GCN会收敛到的极限分布的计算方法如下：</p>
<script type="math/tex; mode=display">
\pi_{lim}=\hat{A}\pi_{lim}</script><p>而PageRank的计算方法如下：</p>
<script type="math/tex; mode=display">
\pi_{pr}=A_{rw}\pi_{pr}</script><p>其中$A_{rw}=AD^{-1}$，两个计算方法明显地相似，区别在于，PageRank中邻接矩阵$A$没有考虑根顶点自身，而极限分布的计算里$\hat{A}$是引入了自环的。而Personalized PageRank通过引入自环而考虑了根顶点自身，论文的想法就是将随机漫步的极限分布用Personalized PageRank来代替，它的计算方法为：</p>
<script type="math/tex; mode=display">
\pi_{ppr}(i_x)=(1-\alpha)\hat{A}\pi_{ppr}(i_x)+\alpha i_x \\
\rightarrow \pi_{ppr}(i_x)=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}i_x</script><p>其中$i_x$是一个one_hot指示向量，用来从根顶点重新启动。</p>
<blockquote>
<p>Personalized PageRank算法的目标是要计算所有节点相对于用户u的相关度。从用户u对应的节点开始游走，每到一个节点都以α的概率停止游走并从u重新开始，或者以1-α的概率继续游走，从当前节点指向的节点中按照均匀分布随机选择一个节点往下游走。这样经过很多轮游走之后，每个顶点被访问到的概率也会收敛趋于稳定，这个时候我们就可以用概率来进行排名了。</p>
</blockquote>
<p>相较于原始的GCN模型，现在根顶点$x$对顶点$y$的影响程度$I(x,y)$，变得与$\pi_{ppr}(i_x)$中的第$y$个元素相关，这个影响程度对于每个根顶点都有不同的取值：</p>
<script type="math/tex; mode=display">
\require{cancel}
I(x,y)\propto \prod_{ppr}^{(yx)},\prod_{ppr}^{(yx)}=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}\cancel{I_{n}}</script><h4 id="PPNP"><a href="#PPNP" class="headerlink" title="PPNP"></a>PPNP</h4><p>经过上面的铺垫与介绍，论文提出的模型PPNP可以表示为：</p>
<script type="math/tex; mode=display">
Z_{PPNP}=\text{softmax}\Big(\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}H\Big),H_{i,:}=f_{\theta}(X_i,:)</script><p>其中$X$为特征矩阵，$f_{\theta}$是一个参数为$\theta$的神经网络，用来产生预测类别$H\in \mathbb{R}^{n\times c}$。</p>
<div align="center">
<a href="https://imgchr.com/i/ravXN9" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/20/ravXN9.png" alt="ravXN9.png" border="0" width="90%"></a>
</div>
由公式和图中都可以看到，PPNP其实是由两部分组成，左边的神经网络与右边的信息传递网络，神经网络部分就类似于在[GCN](#Semi-Supervised Classification with Graph Convolutional Network [ICLR'17])中介绍的，输入顶点特征与图的结构信息（邻接矩阵），输出顶点新的特征表示。信息传递网络部分，在PPNP中通过它来得到预测标签，而原始GCN的做法是$Z_{GCN}=\text{softmax}(\hat{A}HW)$，其中$W$是每层网络的参数。

#### APPNP

从前面的构造方式可以看到，矩阵$\prod_{ppr}$将会有$\mathbb{R}^{n\times n}$大小，会带来时间和空间上的复杂度。因此论文提出了一种近似的计算方法APPNP，计算方式如下：
$$
\begin{aligned}
Z^{(0)}&=H=f_{\theta}(X) \\
Z^{(k+1)}&=(1-\alpha)\hat{A}Z^{(k)}+\alpha H \\
Z^{(K)}&=\text{softmax}\Big((1-\alpha)\hat{A}Z^{(K-1)}+\alpha H\Big)
\end{aligned}
$$
其中$K$为信息传递的跳数或者说是随机漫步的步数，$k\in[0,K-2]$，这样一来就不用构造一个$\mathbb{R}^{n\times n}$的矩阵了。（不知道为什么...）  

### 数据集

Citeseer、Cora-ML、Pubmed、MS Academic  

## Graph Neural Networks for Social Recommendation[WWW'19]

### 解决的问题

如何将GNN应用于社会化推荐任务上。

面临的挑战有三点：

1. 在一个社会化推荐任务中，输入的数据包括社会关系图和用户-物品交互图，将两张图的信息都聚合才能得到用户更好的一个表示，而此前的GNN只是在同一张图上对邻域内的信息聚合。
2. 在用户-物品交互图中，顶点与顶点之间的边也包含更多的信息，除了表示是否交互，还能表示用户对一个物品的偏好（喜爱还是厌恶），而此前的GNN只是将边用来表示是否交互。
3. 社会关系图中用户之间的纽带有强有弱，显然地，一个用户更可能与强纽带的其它用户有类似的喜好。如果将所有纽带关系都看成一样，会有偏差。

### 做法及创新

创新：

* 在不同图(user-user graph和user-item graph)上进行信息传递与聚合
* 除了捕获user-item间的交互关系，还利用了user对item的评分
* 用attention机制表示社交关系的重要性，用户纽带的强与弱

<div align="center">
<a href="https://imgchr.com/i/r0xT1A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/r0xT1A.png" alt="r0xT1A.png" border="0" width="90%"></a>
</div>
整个GraphRec框架由三个部分组成，分别为user modeling、item modeling和rating prediction。其中user modeling用来学习用户的特征表示，学习的方式是两个聚合：item aggregation和social aggregation，类似地item modeling用来学习物品的特征表示，学习的方式是一个聚合：user aggregation。

#### User Modeling

##### item aggregation

<div align="center">
<a href="https://imgchr.com/i/rBuFzt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBuFzt.png" alt="rBuFzt.png" border="0" width="40%"></a>
</div>

<p>item aggregation的目的是通过用户交互过的物品以及对这些物品的倾向，来学习物品侧的用户特征表示，数学表示为：</p>
<script type="math/tex; mode=display">
h_i^I=\sigma(W·Aggre_{items}(\{x_{ia},\forall a\in C(i)\})+b)</script><p>$C(i)$就表示用户交互过的物品的一个集合。这里的$x_{ia}$是一个表示向量，它应该能够同时表示交互关系和用户倾向。论文中的做法是通过一个MLP来结合物品的embedding和倾向的embedding，两者分别用$q_a$和$e_r$表示。倾向的embedding可能很难理解，以五分制评分为例，倾向的embedding表示为$e_r\in \mathbb{R}^d$，其中$r\in \{1,2,3,4,5\}$。</p>
<script type="math/tex; mode=display">
x_{ia}=g_v([q_a\oplus e_r])</script><p>定义好$x_{ia}$后，下一步就是如何选取聚合函数$Aggre$了。论文中使用的是attention机制，来源于<a href="#Graph Attention Networks[ICLR&#39;18]">GAT</a>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_i^I&=\sigma(W·\Big\{\sum_{a\in C(i)}
\alpha_{ia}x_{ia}\Big\}+b) \\
\alpha_{ia}'&=w_2^T·\sigma(W_1·[x_{ia}\oplus p_i]+b_1)+b_2 \\
\alpha_{ia}&=\frac{\exp(\alpha_{ia}')}{\sum_{a\in C(i)}\exp(\alpha_{ia}')}
\end{aligned}</script><p>这里的权重$\alpha_{ia}$考虑了$x_{ia}$和用户$u_i$的embedding $p_i$，使得权重能够与当前用户相关。</p>
<h5 id="social-aggregation"><a href="#social-aggregation" class="headerlink" title="social aggregation"></a>social aggregation</h5><div align="center">
<a href="https://imgchr.com/i/rBK7g1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBK7g1.png" alt="rBK7g1.png" border="0" width="40%"></a>
</div>
social aggregation中，同样地使用了attention机制，通过attention机制来选取强纽带的其它用户（表现为聚合时权重更大）并聚合他们的信息，聚合的就是物品侧的用户特征表示。
$$
\begin{aligned}
h_i^S&=\sigma(W·\Big\{\sum_{o\in N(i)}
\beta_{io}h_o^I\Big\}+b) \\
\beta_{io}'&=w_2^T·\sigma(W_1·[h_o^I\oplus p_i]+b_1)+b_2 \\
\beta_{io}&=\frac{\exp(\beta_{io}')}{\sum_{o\in N(i)}\exp(\beta_{io}')}
\end{aligned}
$$
这里跟item aggregation基本一模一样，就不多介绍了。

得到物品侧的用户特征表示$h_i^I$和社交侧的用户特征表示$h_i^S$后，用一个MLP将它们结合，得到用户最终的特征表示：
$$
\begin{aligned}
c_1&=[h_i^I\oplus h_i^S] \\
c_2&=\sigma(W_2·c_1+b_2) \\
&······ \\
h_i&=\sigma(W_l·c_{l-1}+b_l)
\end{aligned}
$$


#### Item Modeling

##### user aggregation

<div align="center">
<a href="https://imgchr.com/i/rBYtjH" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBYtjH.png" alt="rBYtjH.png" border="0" width="50%"></a>
</div>
Item modeling与User modeling的做法基本一模一样...公式都是一一对应的：
$$
\begin{aligned}
f_{jt}&=g_u([p_t\oplus e_r]) \\
z_j&=\sigma(W·\Big\{\sum_{t\in B(j)}
\mu_{jt}f_{jt}\Big\}+b) \\
\mu_{jt}'&=w_2^T·\sigma(W_1·[f_{jt}\oplus q_j]+b_1)+b_2 \\
\mu_{jt}&=\frac{\exp(\mu_{jt}')}{\sum_{a\in C(i)}\exp(\mu_{jt}')}
\end{aligned}
$$


#### Rating Prediction

最后来到评分预测部分，由上面两个部分我们得到了用户特征表示$h_i$与物品特征表示$z_j$，产生评分用的也是一个MLP：
$$
\begin{aligned}
g_1&=[h_i\oplus z_j] \\
g_2&=\sigma(W_2·g_1+b_2) \\
&······ \\
g_{l-1}&=\sigma(W_l·g_{l-1}+b_l) \\
r_{ij}&=w^T·g_{l-1}
\end{aligned}
$$

### 数据集

Ciao、Epinions

## Graph Convolutional Matrix Completion[KDD'18]

### 解决的问题

如何将图卷积网络应用于矩阵补全问题。

具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。如果将评分矩阵转换为一张图，转换方法在下面有进行介绍，这时矩阵补全问题也可以看成图上的边预测问题。要预测用户对一个物品的评分，就是预测图上两个对应顶点之间相连的边的权重。

### 做法及创新

论文通过一个编码器-解码器的架构来实现从已有评分到特征表示再到预测评分的过程。

<div align="center">
<a href="https://imgchr.com/i/sQUdAS" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUdAS.png" alt="sQUdAS.png" border="0" width="70%"></a>
</div>

<h4 id="Bipartite-Graph-Construction"><a href="#Bipartite-Graph-Construction" class="headerlink" title="Bipartite Graph Construction"></a>Bipartite Graph Construction</h4><p>首先是将推荐任务里的评分数据转化为一张图，具体做法是将用户和物品都看作图中的顶点，交互记录看作边，分数作为边的权重，如图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/su9fr4" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/08/su9fr4.png" alt="su9fr4.png" border="0" width="60%"></a>
</div>

<h4 id="Graph-Convolutional-Encoder"><a href="#Graph-Convolutional-Encoder" class="headerlink" title="Graph Convolutional Encoder"></a>Graph Convolutional Encoder</h4><p>上一步所构建的图的输入形式为邻接矩阵$A\in \mathbb{R}^{n\times n}$与图中顶点的特征矩阵$X\in \mathbb{R}^{n\times d}$。编码器在这一步的作用就是得到用户与物品的特征表示$A,X^u,X^v\rightarrow U,V$。</p>
<p>具体编码时，论文将不同的评分水平分开考虑$r\in \{1,2,3,4,5\}$，我的理解是它们类似于处理图像数据时的多个channel。以一个评分水平$r$为例，说明编码得到特征表示的过程。假设用户$u_i$对电影$v_j$评分为$r$，而这部电影的特征向量为$x_j$，那么这部电影对这个用户特征表示的贡献可以表示为下面的式子(1)，相当于对特征向量进行了一个线性变换。</p>
<div align="center">
<a href="https://imgchr.com/i/sQUHnx" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUHnx.png" alt="sQUHnx.png" border="0" width="80%"></a>
</div>
对当前评分水平下所有评过分的电影进行求和，再对所有评分水平求和拼接，经过一个非线性变换，就得到了用户$u_i$的特征表示$h_{u_i}$，物品的做法相同。

<div align="center">
<a href="https://imgchr.com/i/sQdv6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQdv6A.png" alt="sQdv6A.png" border="0" width="80%"></a>
</div>

<div align="center">
<a href="https://imgchr.com/i/sQwmmq" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQwmmq.png" alt="sQwmmq.png" border="0" width="80%"></a>
</div>

<h4 id="Bilinear-Decoder"><a href="#Bilinear-Decoder" class="headerlink" title="Bilinear Decoder"></a>Bilinear Decoder</h4><p>在分别得到用户与物品的特征表示$U$与$V$后，解码器计算出用户对物品评分为$r$的概率，再对每个评分的概率进行求和，得到最终预测的评分。</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P_r)_{ij}&=\frac{\exp(u_i^TQ_rv_j)}{\sum_{s\in R}\exp(u_i^TQ_sv_j)} \\
\hat{M}&=\sum_{r\in R}rP_r
\end{aligned}</script><h3 id="数据集-10"><a href="#数据集-10" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、MovieLens</p>
<h2 id="Variational-Graph-Auto-Encoders-NIPS’16"><a href="#Variational-Graph-Auto-Encoders-NIPS’16" class="headerlink" title="Variational Graph Auto-Encoders[NIPS’16]"></a>Variational Graph Auto-Encoders[NIPS’16]</h2><h3 id="解决的问题-14"><a href="#解决的问题-14" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在图结构数据上如何使用变分自编码器</p>
<h3 id="做法及创新-13"><a href="#做法及创新-13" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>将已知的图进行编码（图卷积）得到图中顶点向量表示的一个分布，在分布中采样得到顶点的向量表示，然后进行解码重新构建图。</p>
<h4 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h4><p>因为这篇论文做的是一个迁移的工作，变分自编码器的背景对于理解这篇论文来说十分重要，首先进行介绍。</p>
<p>变分自编码器是自编码器的一种，一个自编码器由编码器和解码器构成，编码器将输入数据转换为低维向量表示，解码器通过得到的低维向量表示进行重构。</p>
<div align="center">
<a href="https://imgchr.com/i/sl9ZxP" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9ZxP.jpg" alt="sl9ZxP.jpg" border="0" width="80%"></a>
<a href="https://imgchr.com/i/sl9G2q" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9G2q.jpg" alt="sl9G2q.jpg" border="0" width="65%"></a>
</div>

<p>这种结构的不足之处在于，只能产生与输入数据相似的样本，而无法产生新的样本，低维向量表示必须是有真实样本通过编码器得到的，随机产生的低维向量经过重构几乎不可能得到近似真实的样本。而变分自编码器可以解决这个问题。</p>
<p>变分自编码器将输入数据编码为一个分布，而不是一个个低维向量表示，然后从这个分布中随机采样来得到低维向量表示。一般假设这个分布为正态分布，因此编码器的任务就是从输入数据中得到均值$\mu$与方差$\sigma^2$。</p>
<div align="center">
<a href="https://imgchr.com/i/slCW60" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slCW60.jpg" alt="slCW60.jpg" border="0" width="80%"></a>
<a href="https://imgchr.com/i/slPZB8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPZB8.jpg" alt="slPZB8.jpg" border="0" width="80%"></a>
</div>

<p>然而，如果是将所有输入数据编码到同一个分布里，从这个分布中随机采样的样本$Z_i$无法与输入样本$X_i$一一对应，会影响模型的学习效果。所以，实际的变分自编码器结构如下图所示，为每一个输入样本学习一个正态分布：</p>
<div align="center">
<a href="https://imgchr.com/i/slPgED" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPgED.jpg" alt="slPgED.jpg" border="0" width="80%"></a>
</div>

<p>采样时常用”重参数”技巧(reparameterization trick)，从分布$N(\mu,\sigma^2)$中采样一个$Z$相当于从$N(0,1)$中采样一个$\epsilon$使得$Z=\mu+\sigma*\epsilon$。</p>
<h4 id="图变分自编码器"><a href="#图变分自编码器" class="headerlink" title="图变分自编码器"></a>图变分自编码器</h4><p>介绍完传统的变分自编码器，接下来就是介绍这篇论文的工作，如何将变分自编码器的思想迁移到图上。</p>
<p>针对图这个数据结构，输入的数据变为图的邻接矩阵$A$与特征矩阵$X$：<br>邻接矩阵$A$：</p>
<div align="center">
<a href="https://imgchr.com/i/slFHhQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFHhQ.jpg" alt="slFHhQ.jpg" border="0" width="60%"></a>
</div>

<p>特征矩阵$X$：</p>
<div align="center">
<a href="https://imgchr.com/i/slFz7T" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFz7T.jpg" alt="slFz7T.jpg" border="0" width="60%"></a>
</div>


<p>接下来的工作与变分自编码器相同，通过编码器（图卷积）学习图中顶点低维向量表示分布的均值$\mu$与方差$\sigma^2$，再通过解码器生成图。</p>
<div align="center">
<a href="https://imgchr.com/i/slk1gA" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slk1gA.jpg" alt="slk1gA.jpg" border="0" width="80%"></a>
</div>

<p>编码器采用两层结构的图卷积网络，第一层产生一个低维的特征矩阵：</p>
<script type="math/tex; mode=display">
\bar{X}=\text{GCN}(X,A)=\text{ReLU}(\tilde{A}XW_0)\\
\tilde{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>第二层得到分布的均值$\mu$与方差$\sigma^2$：</p>
<script type="math/tex; mode=display">
\mu=\text{GCN}_{\mu}(X,A)=\tilde{A}\bar{X}W_1\\
\log\sigma^2=\text{GCN}_{\sigma}(X,A)=\tilde{A}\bar{X}W_1</script><p>将两层网络的表达式合并可以得到编码器的表达式：</p>
<script type="math/tex; mode=display">
\text{GCN}(X,A)=\tilde{A}\text{ReLU}(\tilde{A}XW_0)W_1</script><p>同样地使用重参数技巧来得到低维向量表示$Z=\mu+\sigma*\epsilon$。  </p>
<p>编码器重构出图的邻接矩阵，从而得到一个新的图。之所以使用点积的形式来得到邻接矩阵，原因在于我们希望学习到每个顶点的低维向量表示$z$的相似程度，来更好地重构邻接矩阵。而点积可以计算两个向量之间的cosine相似度，这种距离度量方式不受量纲的影响。因此，重构的邻接矩阵可以学习到各个顶点之间的相似程度。</p>
<script type="math/tex; mode=display">
\hat{A}=\sigma(zz^T)</script><p>损失函数用于衡量生草样本与真是样本之间的差异，但如果只用距离度量作为损失函数，为了让编码器的效果最佳，模型会将方差的值学为0，这样从正态分布中采样出来的就是定值，有利于减小生成样本和真实样本之间的差异。但这样一来，就退化成了普通的自编码器，因此在构建损失函数时，往往还会加入各独立正态分布与标准正态分布的KL散度，来使得各个正态分布逼近标准正态分布：</p>
<script type="math/tex; mode=display">
L=E_{q(Z|X,A)}[\log p(A|Z)]-\text{KL}[q(Z|X,A)||p(Z)],\quad where\quad p(Z)=N(0,1)</script><h3 id="数据集-11"><a href="#数据集-11" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>
<h2 id="Inductive-Matrix-Completion-Based-on-Graph-Neural-Networks-ICLR’20"><a href="#Inductive-Matrix-Completion-Based-on-Graph-Neural-Networks-ICLR’20" class="headerlink" title="Inductive Matrix Completion Based on Graph Neural Networks[ICLR’20]"></a>Inductive Matrix Completion Based on Graph Neural Networks[ICLR’20]</h2><h3 id="解决的问题-15"><a href="#解决的问题-15" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何让矩阵补全方法中一个数据集得到的embedding，能够迁移到另一个数据集上，同时不依赖额外的信息。</p>
<p>与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>应用的问题相同，具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。传统的做法是将输入的评分矩阵分解成用户与物品的embedding，通过embedding重构评分矩阵，填补其中的缺失值，从而做出预测，如下图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/sd6O1S" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd6O1S.png" alt="sd6O1S.png" border="0" width="80%"></a>
</div>


<p>很多现有方法研究的都是如何得到更好的embedding，但它们都是直推式(transductive)而非启发式(inductive)的，意味着没法迁移，例如MovieLens数据集上得到的embedding就不能直接用于Douban数据集上，需要重新训练一个新的embedding。即使对于同一个数据集而言，如果加入新的评分记录，往往需要整个embedding重新训练。</p>
<h3 id="做法及创新-14"><a href="#做法及创新-14" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Enclosing-Subgraph-Extraction"><a href="#Enclosing-Subgraph-Extraction" class="headerlink" title="Enclosing Subgraph Extraction"></a>Enclosing Subgraph Extraction</h4><p>论文的做法是为每一个评分记录提取一个子图，并且训练一个图神经网络来将得到的子图映射为预测评分。要想为评分记录提取子图，首先要将评分矩阵转换为图，转换的方法与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>相同，博客中有具体介绍，这里就不重复说明了。论文中对子图的定义方式为，给定一个评分记录$(u,v)$，表示用户$u$给物品$v$评过分，那么这个评分记录提取的子图由该用户$u$、物品$v$以及它们各自的$h$跳邻域内的顶点构成。为了具体说明是怎么从一个评分记录提取出子图的，我从论文作者的视频中截取了这部分内容，如下图所示：</p>
<p>假设第一张图中深绿色的方格是缺失值，这里先填入了模型的预测评分，倒退着来说明预测评分是怎么通过子图得到的。我们首先找到这个用户评过分的其它物品，对应于第五个物品的四分与第八个物品的两分，如第二张图所示。下一步是找到为这个物品评过分的其他用户，对应于第三个用户的五分与第四个用户的五分。</p>
<div align="center">
<a href="https://imgchr.com/i/sdfNf1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sdfNf1.png" alt="sdfNf1.png" border="0"></a>
</div>


<p>通过图二和图三找到的关系，就可以提取出这个评分记录的子图了，如下图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/sd4urT" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd4urT.png" alt="sd4urT.png" border="0" width="90%"></a>
</div>


<p>可以看到，这个提取出的子图能提供许多有用的信息，例如用户平均评分、物品平均评分、物品累计评价次数以及基于路径的结构信息。论文希望通过这种结构信息来找到一些特征，从而做出预测，例如，如果用户$u_0$喜欢一个物品$v_0$，那么对于另一个与他品味相同的用户$u_1$，我们可能发现他也喜欢$v_0$。品味相同可以表示为两个用户都喜欢另一个物品$v_1$，这个特征可以表示为这么一条路径：$u_0\rightarrow_{like}v_1\rightarrow_{liked\ by}u_1\rightarrow_{like}v_0$，如果$u_0$与$v_0$之间存在多条这样的路径，那么我们就可以推测$u_0$喜欢$v_0$。类似这样的结构特征数不胜数。因此，与其人工来手动定义大量这样的启发式特征(heuristics)，不如直接将子图输入一个图神经网络，来自动学习更通用的、更有表达能力的特征。</p>
<h4 id="Node-Labeling"><a href="#Node-Labeling" class="headerlink" title="Node Labeling"></a>Node Labeling</h4><p>这一步给顶点打标签是为了让子图中的顶点有着不同的角色，例如区分哪个是需要预测的目标用户与目标物品，区分用户顶点与物品顶点。而论文中打标签的方式十分简单：</p>
<ul>
<li>目标用户与目标物品分别标记为0和1</li>
<li>对于$h$跳邻域内的顶点，如果是用户顶点标记为$2h$，物品顶点则标记为$2h+1$</li>
</ul>
<p>标记之后，我们就能知道哪个是需要预测的目标用户与目标物品、哪些是用户顶点，因为用户顶点的标签均为偶数，以及邻域内顶点距离目标顶点距离的远近。这些标签将转换为one-hot编码的形式作为图神经网络输入的初始特征$x_0$。</p>
<p>这一节的最后论文作者还说到了这种标记方式与<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>做法的不同之处。GCMC中同样是将标签转换为one-hot编码的形式作为GNN的初始特征，不同的是它用顶点在整个bipartite graph中的全局id作为它的标签，这等价于将GNN第一层信息传递网络的参数，转换为与每个顶点的全局id相关联的embedding函数，可以理解为一个embedding查找表，输入一个全局id，输出它对应的embedding。这显然是直推式的，对于不在查找表中的id，就无法得到它的embedding。这种情况对应于在小数据集上训练网络得到embedding，然后换到大数据集上，因为大数据集的顶点数量肯定要多于小数据集，这就会使得顶点的全局id范围变大，超出了训练出来的这个embedding查找表的范围。</p>
<h4 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h4><p>这一步的目的就是训练一个GNN来将提取出的子图映射成预测评分。论文所使用的GNN分为两个部分：信息传递层与池化层。前者的作用是得到子图中各顶点的特征向量，后者是根据得到的特征向量形成子图的一个特征表示。</p>
<p>信息传递部分使用的是<a href="https://arxiv.org/pdf/1703.06103v4.pdf" target="_blank" rel="noopener">R-GCN</a>：</p>
<script type="math/tex; mode=display">
x_i^{l+1}=W_0^lx_i^l+\sum_{r\in R}\sum_{j\in N_r(i)}\frac{1}{|N_r(i)|}W_r^lx_j^l</script><p>其中$x_i^l$表示第$i$个顶点在第$l$层的特征向量，$N_r(i)$表示评分水平$r$下顶点$i$的邻域，顶点$i$以不同的边权重$r$所连接的顶点$j$用不同的参数矩阵$W_r^l$来进行处理。通过堆叠$L$层网络可以得到顶点$i$的$L$个特征向量，通过拼接的方式得到它最终的特征表示$h_i$：</p>
<script type="math/tex; mode=display">
h_i=\text{concat}(x_i^1,x_i^2,\dots,x_i^L)</script><p>池化部分只选取子图中目标用户与目标顶点的特征向量进行拼接，来得到该子图的特征表示，这么做的原因是这两个顶点携带了最多的信息。</p>
<script type="math/tex; mode=display">
g=\text{concat}(h_u,h_v)</script><p>在得到子图的特征表示后，最后一步是通过一个MLP将它转换为一个预测评分$\hat{r}$：</p>
<script type="math/tex; mode=display">
\hat{r}=w^T\sigma(Wg)</script><h4 id="Adjacent-Rating-Regularization"><a href="#Adjacent-Rating-Regularization" class="headerlink" title="Adjacent Rating Regularization"></a>Adjacent Rating Regularization</h4><p>论文对于信息传递部分使用的R-GCN还提出了一点改进，在原始的R-GCN中，不同的评分水平是独立看待的，彼此之间没有关联，例如对于1、4、5这三个评分，显然地4和5都表示了用户的喜爱而1表示了用户的厌恶，同时4和5的相似程度要大于4和1，但这种次序关系及大小关系在原始的R-GCN中都被丢掉了。因此本论文添加了一个约束来引入这部分丢失的信息，具体做法也很简单，就是使得相邻的评分水平使用的参数矩阵更加相似：</p>
<script type="math/tex; mode=display">
L_{ARR}=\sum_{i=1,2,\dots,|R|-1}||W_{r_i+1}-W_{r_i}||_F^2</script><p>这里假设评分$r_1,r_2,\dots,r_{|R|}$表示了用户喜爱程度的递增，通过这个约束就保留了评分的次序信息，同时可以使得出现次数较少的评分水平可以从相邻的评分水平中迁移信息，来弥补数据不足带来的问题。</p>
<h4 id="Graph-level-GNN-vs-Node-level-GNN"><a href="#Graph-level-GNN-vs-Node-level-GNN" class="headerlink" title="Graph-level GNN vs Node-level GNN"></a>Graph-level GNN vs Node-level GNN</h4><p>这一节还是在于GCMC作比较。在GCMC中，采用的是顶点层面的图神经网络，它应用于图中的顶点来得到顶点的embedding，再通过embedding得到预测评分，如下右图所示。这么做的缺陷是它独立地学习两个顶点所关联的子树，而忽略了这两棵子树之间可能存在的联系。</p>
<div align="center">
<a href="https://imgchr.com/i/swYFij" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/15/swYFij.png" alt="swYFij.png" border="0" width="50%"></a>
</div>

<h3 id="数据集-12"><a href="#数据集-12" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、ML-100K、ML-1M</p>
<h2 id="Link-Prediction-Based-on-Graph-Neural-Networks-NIPS’18"><a href="#Link-Prediction-Based-on-Graph-Neural-Networks-NIPS’18" class="headerlink" title="Link Prediction Based on Graph Neural Networks[NIPS’18]"></a>Link Prediction Based on Graph Neural Networks[NIPS’18]</h2><h3 id="解决的问题-16"><a href="#解决的问题-16" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何能自动而非人工定义的方式来学习图中的结构信息，从而进行边预测。</p>
<p>边预测任务就是预测图中的两个顶点是否有可能有边相连。一种常用的方法为启发式方法(heuristic)，它根据定义的顶点相似度来判断这条边存在的概率有多大。几种定义相似度的方法可以根据需要使用的邻居顶点的跳数来分类，例如common neighbors与preferential attachment是一阶的，因为它们只需要一跳邻居的信息，而Adamic-Adar和resource allocation为二阶，Katz、rooted PageRank与SimRank是更高阶的相似度。</p>
<p>这种启发式方法的缺点在于，边存在的概率很大程度依赖于定义的顶点相似度。例如选取common neighbors这个相似度，在社交网络可能是成立的，因为如果两个人有很多共同的朋友，他们两个确实更有可能认识，但是在蛋白质交互网络截然相反，有越多相同邻居顶点的蛋白质反而越不可能建立联系。所以，与其预先定义一种相似度，不如根据网络的特点自动的学习出来。</p>
<p>另一个挑战是，高阶的相似度相较于低阶相似度往往能带来更好的表现，但是随着阶数越高，每个顶点所形成的子图会越来越逼近完整的图，这样会带来过高的时间复杂度与空间复杂度。本文的另一个贡献就在于，定义了一种逼近的方式，不需要$h$阶的子图也能近似的获取$h$阶子图中包含的信息，之间的误差有理论上限。</p>
<h3 id="做法及创新-15"><a href="#做法及创新-15" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>同ICLR20的论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样（毕竟是同一个作者），论文对子图的定义方式为，给定一对顶点$(x,y)$，它的子图为顶点$x$与$y$不高于$h$阶的邻域的一个并集，数学描述如下，也就是与顶点$x$或$y$的距离小于等于$h$所构成的点的集合：</p>
<blockquote>
<p>给定一个图$G=(V,E)$，以及图上两个顶点$x、y$，它的$h$阶围绕子图(enclosing subgraph)$G^h_{x,y}$为图$G$的一个子图，满足$\{i|d(i,x)\le h\ or\ d(i,y)\le h\}$.</p>
</blockquote>
<p>接下来是定义一个$\gamma$-decaying heuristic函数，它用来逼近$h$阶子图的信息而不需要实际计算$h$阶子图：</p>
<script type="math/tex; mode=display">
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>其中$\gamma$是一个位于$(0,1)$的衰减因子，$\eta$是一个正的常数或一个上界为常数的函数。因为这里的求和从1到$\infin$，接下来的定理说明可以用有限项去逼近$H(x,y)$，误差随着$h$的增加而指数下降：</p>
<blockquote>
<p>定理一：</p>
<p>如果函数$f(x,y,l)$满足：</p>
<ol>
<li>$f(x,y,l)\le \lambda^l$，其中$\lambda &lt;\frac{1}{\gamma}$</li>
<li>对于$l=1,2,\dots,g(h)$，$f(x,y,l)$能够从$h$阶子图$G^h_{x,y}$中计算得到，其中$g(h)=ah+b$，$a,b\in \N,\ a&gt;0$</li>
</ol>
</blockquote>
<p>证明的方法很容易理解：</p>
<blockquote>
<p>逼近项为：</p>
<script type="math/tex; mode=display">
\tilde{H}(x,y)=\eta\sum_{l=1}^{g(h)}\gamma^lf(x,y,l)</script><p>计算差值可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
|H(x,y)-\tilde{H}(x,y)|&=\eta\sum_{l=g(h)+1}^{\infin}\gamma^lf(x,y,l)\\
&\le \eta\sum_{l=ah+b+1}^{\infin}\gamma^l\lambda^l\\
&=\eta\frac{(\gamma \lambda)^{ah+b+1}}{1-\gamma \lambda}
\end{aligned}</script></blockquote>
<p>第一个不等式是根据定理一的第一个条件，最后一个等号是根据等比数列的求和公式，当项数$n\rightarrow \infin$且$q\in(0,1)$时，结果为$\frac{a_1}{1-q}$。</p>
<p>到这里可能还是不知道这个$H(x,y)$和图中$h$阶的信息有什么关系，下面就通过Katz、rooted PageRank和SimRank三个高阶相似度来具体说明怎么使用：</p>
<p>在说明之前，先介绍一个引理，接下来会用到，证明起来也很直观：</p>
<blockquote>
<p>顶点$x$与$y$之间任意一条长度$l$满足$l\le2h+1$的路径都被包含在子图$G^h_{x,y}$中</p>
</blockquote>
<p>证明：</p>
<blockquote>
<p>即证明给定一条长度为$l$的路径$w=<x,v_1,\dots,v_{l-1},y>$中的每一个顶点都在子图中。取其中任意一个顶点$v_i$，满足$d(v_i,x)\ge h$且$d(v_i,y)\ge h$，根据子图$G^h_{x,y}$的定义它不在其中。那么有：</x,v_1,\dots,v_{l-1},y></p>
<script type="math/tex; mode=display">
2h+1\ge l=|<x,v_1,\dots,v_i>|+|<v_i,\dots,v_{l-1},y>|\ge d(v_i,x)+d(v_i,y)=2h+2</script><p>矛盾，不等号是因为$d(x,y)$就是表示两个顶点之间的最短路径，所以有$d(v_i,x)&lt;h$或$d(v_i,y)&lt;h$，则顶点$v_i$在子图$G^h_{x,y}$中。</p>
</blockquote>
<h4 id="Katz-index"><a href="#Katz-index" class="headerlink" title="Katz index"></a>Katz index</h4><p>给定一对顶点$(x,y)$，Katz index定义为：</p>
<script type="math/tex; mode=display">
\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}</script><p>其中$\text{walk}^{<l>}(x,y)$是这两个顶点之间长度为$l$的路径构成的集合，$A^l$是邻接矩阵的$l$次幂。从表达式可以看到，长度越长的路径在计算时会被$\beta^l$衰减的越多$(0&lt;\beta&lt;1)$，短路径有更大的权重。</l></p>
<p>对比两式可以发现：</p>
<script type="math/tex; mode=display">
\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}\\
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>Katz index是论文中定义的$\gamma$-decaying heuristic函数的一种特殊形式，取$\eta=1,\gamma=\beta$，$f(x,y,l)=|\text{walks}^{<l>}(x,y)|=[A^l]_{x,y}$。根据引理，只要取长度小于2h+1的路径，其中的顶点就会全部被子图给包含，这也就满足了定理一的第2个“可计算”条件。对于第一个条件，可以通过数学归纳法说明Katz index的表达式同样满足：</l></p>
<blockquote>
<p>给定任意的顶点$i、j$，$[A^l]_{i,j}$的上限为$d^l$，其中$d$是网络中的最大顶点度</p>
</blockquote>
<p>数学归纳法证明：</p>
<blockquote>
<p>当$l=1$时，$A_{i,j}$退化成了顶点的度，那显然有$A_{i,j}\le d$成立。假设$k=l$时也成立$[A^l]_{i,j}\le d^l$，当$k=l+1$时：</p>
<script type="math/tex; mode=display">
[A^{l+1}]_{i,j}=\sum_{k=1}^{|V|}[A^l]_{i,k}A_{k,j}\le d^l\sum_{k=1}^{|V|}A_{k,j}\le d^ld=d^{l+1}</script></blockquote>
<p>第一个等式就是矩阵乘法的定义，因为$[A^{l+1}]$的含义就是$l+1$个邻接矩阵$A$相乘。因此，对比定理一的第一个条件，我们只要取$\lambda=d$，$d$满足$d&lt;\frac{1}{\beta}$就能够成立，这样一来两个条件都被满足了，这说明Katz index能够很好地从$h$阶子图中近似。</p>
<h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><p>rooted PageRank来源于这篇论文<a href="https://dl.acm.org/doi/10.1145/511446.511513" target="_blank" rel="noopener">Topic-sensitive PageRank</a>，它通过迭代计算PageRank向量$\pi_x$来得到某一点相对于其它顶点的相似度。具体来说，它计算一个从顶点$x$开始的随机漫步的平稳分布，这个随机漫步以概率$\alpha$移动到任一邻居上或以概率$1-\alpha$回到顶点$x$。这个平稳分布满足：</p>
<script type="math/tex; mode=display">
\pi_x=\alpha P\pi_x+(1-\alpha)e_x</script><p>其中$[\pi_x]_i$表示在这个平稳分布下漫步到顶点$i$的概率，$P$为转移矩阵，其中$P_{i,j}=\frac{1}{|\Gamma(v_j)|}$，这里的$\Gamma(v_j)$表示顶点$v_j$的一跳邻居构成的集合。如果一个顶点与五个顶点相连，那它转移到其中任意一个顶点的概率就是$\frac{1}{5}$。</p>
<p>rooted PageRank应用于边预测任务时，用来得到一对顶点$(x,y)$的分数，以$[\pi_x]_y$或$[\pi_x]_y+[\pi_y]_x$（对称）表示，分数越高越有可能有边相连。</p>
<p>接下来就要说明rooted PageRank如何能够同样以论文中提出的$\gamma$-decaying heuristic函数进行表示。根据<a href="http://infolab.stanford.edu/~glenj/spws.pdf" target="_blank" rel="noopener">inverse P-distance理论</a>，$[\pi_x]_y$能够等价地改写为：</p>
<script type="math/tex; mode=display">
[\pi_x]_y=(1-\alpha)\sum_{w:x\leadsto y}P[w]\alpha^{len(w)}</script><p> 这里的求和范围$w:x\leadsto y$表示所有从$x$开始结束于$y$的路径，$P[w]$定义为$\prod_{i=0}^{k-1}\frac{1}{|\Gamma(v_i)|}$，$k$是路径长度，$v_i$是路径中的顶点，通过这条路径来从$x$到$y$的概率就是漫步到路径中每一个顶点的概率的连乘。</p>
<p>接下来就是证明这个形式满足定理一的两个条件：</p>
<blockquote>
<p>首先进一步改写：</p>
<script type="math/tex; mode=display">
[\pi_x]_y=(1-\alpha)\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]\alpha^l\\
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>对比：取$\gamma=\alpha,\eta=(1-\alpha),f(x,y,l)=\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]$。因为这时候$f(x,y,l)$表示一个随机漫步恰好以$l$步从顶点$x$漫步到$y$的概率，有$\sum_{z\in V}f(x,z,l)=1$，则$f(x,y,l)\le1&lt;\frac{1}{\alpha}$，这样就满足了定理一，而根据引理，只要取长度小于等于2h+1的路径，路径中的点就会被全部包含在子图中，也就满足了第二个”可计算“条件。</p>
</blockquote>
<h4 id="SimRank"><a href="#SimRank" class="headerlink" title="SimRank"></a>SimRank</h4><p>SimRank的核心思想是，如果两个顶点的邻域相似，那它们也相似：</p>
<script type="math/tex; mode=display">
s(x,y)=\gamma \frac{\sum_{a\in\Gamma(x)}\sum_{b\in \Gamma(y)}s(a,b)}{|\Gamma(x)|·|\Gamma(y)|}</script><p>它有一个<a href="https://dl.acm.org/doi/10.1145/775047.775126" target="_blank" rel="noopener">等价定义形式</a>：</p>
<script type="math/tex; mode=display">
s(x,y)=\sum_{w:(x,y)\multimap (z,z)}P[w]\gamma^{len(w)}</script><p>其中$w:(x,y)\multimap (z,z)$表示从顶点$x$开始的随机漫步与从顶点$y$开始的随机漫步第一次相遇于顶点$z$。证明与rooted PageRank基本一致，可以见原论文。</p>
<p>总结来说，$\gamma$-decaying heuristic函数的思想是，对于远离目标顶点的结构信息通过指数衰减的方式给一个更小的权重，因为它们带来的信息十分有限。</p>
<h4 id="SEAL框架"><a href="#SEAL框架" class="headerlink" title="SEAL框架"></a>SEAL框架</h4><div align="center">
<a href="https://imgchr.com/i/ypCC6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/ypCC6A.png" alt="ypCC6A.png" border="0" width="80%"></a>
</div>


<p>这一节就是根据上面的理论分析建立一个用于边预测任务的框架。一个图神经网络的典型输入形式是$(A,X)$，在本论文中，$A$自然地被定义为子图$G^h_{x,y}$的邻接矩阵，子图的获取即来自正样本（已知边）也来自负样本（未知边）。接下来的部分就是介绍论文怎么定义顶点的特征矩阵$X$，它包含三个部分：structural node labels、node embeddings和node attributes。</p>
<h5 id="Node-labeling"><a href="#Node-labeling" class="headerlink" title="Node labeling"></a>Node labeling</h5><p>跟作者的另一篇论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样，通过给顶点打标签的方式来区别顶点在子图中的不同角色，这么做的意义在另一篇博客说过了这里就不写了，具体打标签的方式为：</p>
<ul>
<li>起始顶点$x$与目标顶点$y$的标签都为”1“</li>
<li>如果两个顶点$i、j$距离起始顶点与目标顶点的距离都相同，那么它们的标签一样</li>
<li>$(d(i,x),d(i,y))=(a,b)\rightarrow label:a+b$</li>
</ul>
<p>将顶点的标签进行one-hot编码后作为结构特征。</p>
<h5 id="Node-embeddings-Node-attributes"><a href="#Node-embeddings-Node-attributes" class="headerlink" title="Node embeddings + Node attributes"></a>Node embeddings + Node attributes</h5><p>Node attributes一般数据集直接给定，而Node embeddings是通过一个GNN得到，具体做法是：给定正样本$E_p\in E$，负样本$E_n$，$E_p\and E_n=\empty$，在这么一个图$G’=(V,E\and E_n)$上生成embeddings，防止过拟合。</p>
<h3 id="数据集-13"><a href="#数据集-13" class="headerlink" title="数据集"></a>数据集</h3><p>USAir、NS、PB、Yeast、C.ele、Power、Router、E.coli</p>
<h2 id="GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training-KDD’20"><a href="#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training-KDD’20" class="headerlink" title="GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD’20]"></a>GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD’20]</h2><h3 id="解决的问题-17"><a href="#解决的问题-17" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将自监督学习的思想应用与图表示学习，通过预训练图神经网络从而仅需要微调就可以应用于新的数据集。</p>
<p>图表示学习目前受到了广泛关注，但目前绝大多数的图表示学习方法都是针对特定领域的图进行学习和建模，训练出的图神经网络难以迁移。</p>
<h3 id="做法及创新-16"><a href="#做法及创新-16" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h4><p>对比学习是自监督学习思想的一种典型框架，一个典型的例子如下图所示：</p>
<div align="center">
  <a href="https://imgchr.com/i/yiECBF" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiECBF.png" alt="yiECBF.png" border="0" width="80%"></a>
</div>

<p>对比学习的思想是：尽管我们已经见过钞票很多次，能够轻易地分辨出一张钞票，我们也很少能画出一张完美无缺的钞票。<strong>表示学习算法不需要关注到样本的每一个细节，只要学到的特征能够将用来区分其它样本即可</strong>。不需要模型能够生成一匹栩栩如生的马之后它才能去分辨一张图片里的动物是不是马，这就是对比学习和生成对抗网络的一个区别。</p>
<p>既然是表示学习，核心就是通过一个函数把样本$x$转换成特征表示$f(x)$，而对比学习作为一种表示学习方法，它的思想是满足下面这个式子：</p>
<script type="math/tex; mode=display">
s(f(x),f(x^+))\gg s(f(x),f(x^-))</script><p>使得类似样本之间的相似度要远大于非类似样本之间的相似度，这样才能够进行区分。</p>
<h4 id="图表示学习"><a href="#图表示学习" class="headerlink" title="图表示学习"></a>图表示学习</h4><p>具体到论文的图表示学习任务中，论文的一个重要假设是，具有典型性的图结构在不同的网络之间是普遍存在而且可以迁移的（Representative graph structural patterns are universal and transferable across networks）。受对比学习在计算机视觉和自然语言处理领域的成功应用，论文想把对比学习（contrastive learning）的思想放在图表示学习中。通过预训练一个图神经网络，它能够很好地区分这些典型性的图结构，这样它的表现就不会仅仅局限于某个特定的数据集。</p>
<div align="center">
<a href="https://imgchr.com/i/y9x4KI" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/y9x4KI.png" alt="y9x4KI.png" border="0" width="80%"></a>
</div>

<p>论文首先将现有工作对顶点相似度的衡量分为了三类：</p>
<ol>
<li><p>邻域相似度</p>
<p>核心思想：越近的两个顶点之间相似度越高，包括有Jaccard、RWR、SimRank以及LINE、DeepWalk、node2vec。</p>
</li>
<li><p>结构相似度</p>
<p>核心思想：有相似的局部结构的两个顶点之间相似度更高。不同于邻域相似度，结构相似度不需要两个顶点之间有路径相连。常用的局部结构包括vertex degree、structural diversity、structural hole、k-core、motif等。</p>
</li>
<li><p>属性相似度</p>
<p>当数据集中顶点有许多标签信息时，可以将标签作为顶点的特征来衡量它们之间的相似度。</p>
</li>
</ol>
<p>在对比学习中，给定一个查询表示$q$以及一个包含$K+1$个键表示${k_0,\dots,k_K}$的字典，我们希望找到一个能与$q$匹配的键$k_+$。所以，论文优化的损失函数来自于InfoNCE：</p>
<script type="math/tex; mode=display">
L=-\log \frac{\exp(q^Tk_+\tau)}{\sum_{i=0}^K\exp(q^Tk_i/\tau)}</script><p>其中$f_q、f_k$是两个图神经网络，分别将样本$x^q$和$x^k$转换为低维表示$q$与$k$。</p>
<h4 id="正负样本获取"><a href="#正负样本获取" class="headerlink" title="正负样本获取"></a>正负样本获取</h4><p>因为查询和键可以是任意形式，具体到本论文里，定义每一个样本都是一个从特定顶点的$r$阶邻居网络中采样的子图，这里的子图定义和其它论文一致：$S_v=\{u:d(u,v)&lt;r \}$，距离顶点$v$最短路径距离小于$r$的顶点构成的集合。既然是最短路径，给定$r$那么这个集合也基本确定了，这种情况下得到的子图数量有限，在计算机视觉领域，当输入用于训练的图片数量有限时，往往会使用反转、旋转等方式对图片进行变换，以扩充训练图片的数量，这里论文也想采取类似的做法，对得到的子图$x$进行变换，来得到对比学习中的类似$x^+$与非类似样本$x^-$，具体做法如下：</p>
<ol>
<li><strong>带重启动的随机漫步</strong>。首先从子图的中心顶点$v$开始随机漫步，每一步时都有一定概率重新回到中心顶点，而漫步到任一邻居顶点的概率与当前顶点的出度有关。</li>
<li><strong>子图推演</strong>。随机漫步可以得到一系列顶点，它们构成的集合记为$\tilde{S_v}$，所形成的子图记作$\tilde{G_v}$，它就可以看作子图$S_v$的一个变换。</li>
<li><strong>匿名化</strong>。重新定义$\tilde{G_v}$中的顶点的标签，将$\{1,2,\dots,|\tilde{S_v} |\}$的顺序随机打乱作为重新定义后的标签。</li>
</ol>
<div align="center">
    <a href="https://imgchr.com/i/yiFHv8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiFHv8.png" alt="yiFHv8.png" border="0" width="70%"></a>
</div>

<p>论文对于每个子图都进行两次上述变换，而变换后的子图显然会与原子图相似，这样就有了一组相似的子图$(x^q,x^{k_+})$。要得到不相似的子图也很容易，不是同一个子图变换得到的子图就定义为不相似：$(x^q,x^k),k\not =k_+$。在上图的例子中，$x^q$和$x^{k_0}$是从红色的中心顶点采样得到的子图，我们认为它是一对正样本，而$x^{k_1}$和$x^{k_2}$作为从蓝色的中心顶点采样得到的子图，则被作为负样本。在变换时之所以要做最后一步，是为了防止图神经网络在判断两个子图是否相似时，仅仅是通过判断对应顶点的标签是不是一样，这样显然没有学到任何有用的结构信息。这里有一个小结论：</p>
<blockquote>
<p>绝大多数图神经网络对于输入图中顶点的顺序的随机扰动有稳定性</p>
</blockquote>
<p>现在有了正样本和负样本，下一步就是训练一个图神经网络对它们加以区分了，论文选取的是GIN。这就是自监督学习的思想，对比学习就是这种思想的一种典型框架。因为现有的图神经网络框架都需要额外的顶点特征作为输入，论文提出了一种位置embedding来作为其中特征：$I-D^{-1/2}AD^{-1/2}=U\Lambda U^T$，矩阵$U$中排序靠前的特征向量作为embedding。其它特征还包括顶点度的one-hot编码和中心顶点的指示向量。</p>
<h4 id="模型学习"><a href="#模型学习" class="headerlink" title="模型学习"></a>模型学习</h4><p>在模型学习时采用了何凯明组的MoCo框架的思想：</p>
<div align="center">
  <a href="https://imgchr.com/i/yimVaD" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yimVaD.png" alt="yimVaD.png" border="0" width="80%"></a>
</div>

<p>在对比学习中，我们需要维护一个大小为$K$的字典和编码器，要计算上面定义的损失函数，理想的情况是把所有负样本加入字典中进行计算，这会导致$K$很大字典难以维护。在MoCo的方法中，为了增大字典大小$K$，需要维护一个负样本的队列，队列中包含此前训练过的batch的样本作为负样本。在更新参数时，只有$q$的编码器图神经网络$f_q$中的参数通过反向传播进行更新，而$k$的编码器$f_k$中的值通过一种动量法进行更新：$\theta_k\leftarrow m\theta_k+(1-m)\theta_q$。</p>
<h3 id="数据集-14"><a href="#数据集-14" class="headerlink" title="数据集"></a>数据集</h3><p>Academia、DBLP(SNAP)、DBLP(NetRep)、IMDB、Facebook、LiveJournal</p>
<h2 id="How-Powerful-are-Graph-Neural-Networks-ICLR’19"><a href="#How-Powerful-are-Graph-Neural-Networks-ICLR’19" class="headerlink" title="How Powerful are Graph Neural Networks?[ICLR’19]"></a>How Powerful are Graph Neural Networks?[ICLR’19]</h2><h3 id="解决的问题-18"><a href="#解决的问题-18" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GNN性能表现好的原因是什么？</p>
<h4 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h4><ol>
<li>证明了GNN的性能上限是Weisfeiler-Lehman (WL) test，最多只和它一样有效</li>
<li>给出了GNN在什么条件下能够和WL test一样有效</li>
<li>指明了主流GNN框架如GCN、GraphSage无法区分的图结构，以及它们能够区分的图结构的特点</li>
<li>提出了一个简单有效的框架GIN，能够与WL test一样有效</li>
</ol>
<h3 id="做法及创新-17"><a href="#做法及创新-17" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>首先是介绍现有GNN框架的做法及图同构测试的定义，还有WL test的做法。</p>
<h4 id="GNN与WL-test"><a href="#GNN与WL-test" class="headerlink" title="GNN与WL test"></a>GNN与WL test</h4><p>论文认为主流的GNN框架可以分为下面这三步：</p>
<ol>
<li><p><strong>Aggregate</strong>：聚合邻域内的信息</p>
<script type="math/tex; mode=display">
a_v^{(k)}=\text{AGGREGATE}^{(k)}(\{h_u^{(k-1)}:u\in N(v) \})</script></li>
<li><p><strong>Combine</strong>：将聚合后的邻域信息与当前顶点信息结合</p>
<script type="math/tex; mode=display">
h_v^{(k)}=\text{COMBINE}^{(k)}(h_v^{(k-1)},a_v^{(k)})</script></li>
<li><p><strong>Readout</strong>：通过图中的每个顶点的表示得到图的表示</p>
<script type="math/tex; mode=display">
h_G=\text{READOUT}({h_v^{(K)}|v\in G})</script></li>
</ol>
<p>图同构测试就是判断两张图是否在拓扑结构上相同。而WL test的做法是迭代地进行以下步骤：</p>
<ul>
<li>聚合顶点及其邻域的标签信息</li>
<li>将聚合后的标签集合哈希成唯一的新标签</li>
</ul>
<p>如果经过若干次迭代后，两张图中的顶点的标签出现了不同则判断为不同构。基于WL test有一种核函数被提出以计算图之间的相似性。直观上来说，如下图所示，一个顶点在第$k$次迭代时的标签，实际表示了一颗以该顶点为根顶点高度为$k$的子树。</p>
<div align="center">
  <a href="https://imgchr.com/i/yVPBNQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/31/yVPBNQ.png" alt="yVPBNQ.png" border="0" width="80%"></a>
</div>


<p>而GNN同样是通过迭代地更新图中每个顶点的特征向量来捕捉图的结构信息以其周围顶点的特征，这里的结构特征同样可以是上图的根子树rooted subtree。如果给每个顶点的特征向量一个唯一的标签例如{a,b,c,…}，那一个顶点的邻域中所有顶点的特征向量可以构成一个Multiset，它的定义基本和C++中的Multiset一样，是一个Set的同时里面的元素还可以重复例如{a,a,b,c}。论文中对Multiset给出的数学定义是：$X=(S,m)$，其中$S$由Multiset中的非重复元素构成，$m$表示$S$中的元素在$X$中的频数。</p>
<p>直观上来说，一个有效的GNN应该只有在两个顶点对应的根子树结构相同，且其中对应顶点的特征向量也相同时，才将这两个顶点在特征空间中映射成相同的表示。也就是永远不会将不同的两个Multiset映射成同一个特征表示（因为Multiset中的顶点也是根子树中的顶点，既然它们都是通过聚合邻域得到的）。这也就意味着GNN中使用的聚合函数必须是单射的，对值域内的每一个$y$，存在最多一个定义域内的$x$使得$f(x)=y$。有下面这么一个引理：</p>
<blockquote>
<p>设$G_1$和$G_2$是两个非同构图，如果一个图神经网络$A:G\rightarrow \mathbb{R}^d$将$G_1$和$G_2$映射成不同的embedding，那么WL test同样会判断这两个图为非同构。</p>
</blockquote>
<p>引理表明在图区分任务上，一个图神经网络的表现最多和WL test一样好。而一样好的条件是，这个图神经网络的邻居聚合函数和图表示函数都是单射的。这里的一个局限是，函数考虑的定义域和值域都是离散集合。</p>
<p>图神经网络相较于WL test的另一个好处是，WL test输入的顶点特征向量都是one-hot编码，这无法捕捉到子树之间的结构相似度：</p>
<blockquote>
<p>Note that node feature vectors in the WL test are essentially one-hot encodings and thus cannot capture the similarity between subtrees. In contrast, a GNN satisfying the criteria in Theorem 3 generalizes the WL test by learning to embed the subtrees to low-dimensional space. This enables GNNs to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures.</p>
</blockquote>
<h4 id="图同构网络GIN"><a href="#图同构网络GIN" class="headerlink" title="图同构网络GIN"></a>图同构网络GIN</h4><p>基于上面介绍的引理和结论，论文提出的GIN框架如下：</p>
<script type="math/tex; mode=display">
h_v^{(k)}=\text{MLP}^{(k)}\Big((1+\epsilon^{(k)})·h_v^{(k-1)}+\sum_{u\in N(v)}h_u^{(k-1)} \Big)</script><p>对比一开始论文给出的GNN主流框架，可以看到是Aggregate函数选取了求和函数，Combine函数选取了MLP+(1+$\epsilon$)的形式。常见的Aggregate函数包括求和Sum、最大值Max和平均值Mean，论文花了一部分篇幅来说明求和相较于其他两个函数的好处：</p>
<div align="center">
  <a href="https://imgchr.com/i/yGIv5D" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGIv5D.png" alt="yGIv5D.png" border="0" width="80%"></a>
  <a href="https://imgchr.com/i/yGoirt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGoirt.png" alt="yGoirt.png" border="0" width="80%"></a>
</div>

<p>上面两幅图分别说明这几种聚合函数的特点及何种场景下会导致误差。第一幅图中即使减少了顶点的数量但对于取平均和最大值函数来说得到的信息保持不变，第二幅图也是想说明同样的问题，例如取平均，两个一样的顶点与三个一样的顶点取平均出来结构都是一样的，但它们分别对应的局部结构是不相同的。</p>
<p>对于顶点分类及边预测这类下游任务，只要得到顶点的embedding即可。而对于图分类任务，还需要根据所有顶点的embedding来得到图的一个表示，也就是前面提到的主流GNN框架做法的第三步Readout函数。论文的做法类似于<a href="http://www.bithub00.com/2020/12/22/JK-Net[ICML18]/" target="_blank" rel="noopener">JK-Net</a>，将所有层的表示都考虑进来，不过没有具体说是怎么做的。</p>
<p>最后，论文还探讨了那些不满足上面定理的GNN框架如GCN、GraphSAGE等，这些框架都采用一层感知机如ReLU来将Multiset映射成特征表示，而不像论文的做法采用多层感知机，而ReLU存在将不同的Multiset表示成同一种特征表示的情况，即$\exist X_1 \not=X_2,\ s.t. \ \sum_{x\in X_1}\text{ReLU}(Wx)=\sum_{x\in X_2}\text{ReLU}(Wx)$。论文中直接给了一个简单的例子：$X_1=\{1,1,1,1,1\},X_2=\{2,3\}$，因为有$\sum_{x\in X_1}\text{ReLU}(Wx)=\text{ReLU}(W\sum_{x\in X_1}x)$。</p>
<h3 id="数据集-15"><a href="#数据集-15" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI1、PROTEINS、COLLAB、IMDB-BINARY、IMDB-MULTI、REDDIT-BINARY、REDDIT-MULTI5K</p>
<h2 id="SPAGAN-Shortest-Path-Graph-Attention-Network"><a href="#SPAGAN-Shortest-Path-Graph-Attention-Network" class="headerlink" title="SPAGAN Shortest Path Graph Attention Network"></a>SPAGAN Shortest Path Graph Attention Network</h2><h3 id="解决的问题-19"><a href="#解决的问题-19" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GAT中只是中心顶点与邻域顶点的顶点间attention聚合，本文关注的是路径对中心顶点的attention聚合。一条路径往往会包含很多个顶点，怎么能够对中心顶点做到“多对一”的聚合呢？在于一条路径$p^c_{ij}$的表示$\phi(p^c_{ij})$是通过路径中所有顶点的特征取平均得到，这样一来就变成“一对一”的聚合，和GAT中的顶点间attention聚合相同。</p>
<div align="center">
<a href="https://imgtu.com/i/2Az8VP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2Az8VP.png" alt="2Az8VP.png" border="0" width="60%"></a>
</div>


<h3 id="做法及创新-18"><a href="#做法及创新-18" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Shortest-Path-Generation"><a href="#Shortest-Path-Generation" class="headerlink" title="Shortest Path Generation"></a>Shortest Path Generation</h4><p>计算最短路径使用Dijkstra，而顶点间边的权重由它们之间的attention系数决定：</p>
<script type="math/tex; mode=display">
W_{ij}=\frac{1}{K}\sum_{k=1}^K\alpha_{ij}^{(k)}</script><h4 id="Path-Sampling"><a href="#Path-Sampling" class="headerlink" title="Path Sampling"></a>Path Sampling</h4><p>这一阶段的核心思想是，对于有着相同长度的几条最短路径，代价最小的与中心顶点的相关性更高，代价指的就是路径上边的权重的求和。记$p_{ij}^c$为顶点i到j长度为c的一条最短路径，$P^c$表示所有$p_{ij}^c$形成的集合，取样：</p>
<script type="math/tex; mode=display">
N_i^c=top_k(P^c),k=degree_i*r</script><h4 id="Hierarchical-Path-Aggregation"><a href="#Hierarchical-Path-Aggregation" class="headerlink" title="Hierarchical Path Aggregation"></a>Hierarchical Path Aggregation</h4><p>这一阶段的层次路径聚合分为两层，第一层聚合相同长度的路径，第二层聚合不同长度的路径。</p>
<p>加权系数$\alpha_{ij}^{(k)}$为中心顶点i的特征$h_i’$与路径的表示$\phi(p^c_{ij})$的attention系数：</p>
<script type="math/tex; mode=display">
l_i^c=\sum_{k=1}^K\{\sum_{p^c_{ij}\in N_i^c}\alpha_{ij}^{(k)}\phi(p^c_{ij}) \}</script><p>在第二层，进一步聚合不同长度的路径，第一层已经得到以顶点i为中心长度为c的所有路径形成的一个特征表示$l_i^c$：</p>
<script type="math/tex; mode=display">
h_i=\sigma\{\sum_{c=2}^C\beta_cl_i^c\}</script><p>这样一来就通过路径得到了中心顶点i的新的特征表示$h_i$。</p>
<h3 id="数据集-16"><a href="#数据集-16" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>
<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1>]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读点滴</title>
    <url>/2019/08/27/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>这里摘录自己看过的书里受启发的一些句子以及自己的所想<br><a id="more"></a></p>
<h3 id="《扫起落叶好过冬》—-林达"><a href="#《扫起落叶好过冬》—-林达" class="headerlink" title="《扫起落叶好过冬》— 林达"></a>《扫起落叶好过冬》— 林达</h3><ul>
<li><p>林奇堡：私刑的发源地<br>对于民众激情的过度赞美是危险的。“人民”和“暴民”之间，并没有一条不可逾越的鸿沟。在失控的人群中，“人民法庭”随意蹂躏和消灭一个生命的情况，就很容易发生。聚集的人群在心理上一旦放任自己，就容易在处置罪犯的借口下，忽略个体生命，放大自己的权力。程序自然会迅速简化，刑罚必然就日趋严峻。而契约规定的严格的司法程序正是对草菅人命的有效约束。</p>
</li>
<li><p>阿米绪人的故事<br>一个社会要发动成千上百的人并不难，要达到多数人的一致也不难，难的是公正善待只有百分之几的少数。有时候，少数显得如此人微言轻，他们的生死存亡是如此地微不足道，可是，能否保证这微乎其微地少数得到公平地善待，恰恰是检验文明和人道的试金石，也是决定能否长治久安的一个关键。</p>
</li>
<li><p>一个历史学家和他的小镇<br>我们在自己短暂的一生里，所看到的社会、所看到的人和人之间的关系，和历史长河里大时段大范围里呈现的图景是有所不同的，甚至会有很大的差别。短暂一生里，更多的机会是看到了人性之恶，是令人失望的现实。即使是在我们的上半辈子，我们也看到不知有多少人是怀着对人类、对国家、对社会的彻底绝望离开这个世界的。这样的事情，自古以来不知发生了多少。只有在读历史的时候，你能在纸页间经历几百年几千年，你才能看到进步、改善，你才会庆幸自己生活在此时此刻。【我们为什么要读历史】</p>
</li>
<li><p>各有一番风景<br>美国宪法之父詹姆斯·麦迪逊认为，政治之所以经常败坏人性，和人们在政治中拉帮结派有关。他认为，单独的个人都有一定得道德要求。独立的个人，须对自己的行为主张负责，更容易有道德心。但是，当一些人结成宗派，就会人为制造虚幻正义，把个人的自私，在虚幻正义下掩盖起来，互相提供行为正当性的保证。所以，小宗派的道德水准，通常低于个人道德。单个好人，会在拉帮结派中放任自己的私心，甚至做出坏事来。</p>
</li>
<li><p>科尔曼报告<br>它向国会证明，教育机会的平等，不仅要从教育的投入来考察，即考察学生能够获得的公共教育资源，而且更重要的是，要从教育的结果来考察，因为正是对受教育结果的期望，影响了学生的自我评估，决定了学生的学习状态，也造成了种族、肤色、宗教等因素下，弱势人群教育机会的实质不平等。</p>
</li>
<li><p>两千年前那个叫西塞罗的老头儿<br>两千年前的时候，罗马人已经有了人类的初始民主，民主决定的法律，总不算错了吧？西塞罗想想觉得还是不对，假如大众通过一项法律说，现在可以抢劫了，难道就真的能出去抢东西了吗？西塞罗琢磨着，人应该有一种“本性”的东西，它不会“屈从愚氓者的意见和命令”。于是他就找到如伊甸园里那种人的最初状态去了。<br>一旦进了伊甸园，我发现西塞罗还是很有道理。仔细打量的话，人和人之间，真的就有非常近似的那一部分。只要这么一想就明白了：所有的人，都有一些绝对不愿意发生在自己身上的事情。比如说，只要是个正常人，就没人愿意自己被杀被抢的，没人愿意别人骑在自己头上作威作福的，这才是人“自然本源”的状态。人要维护自己这样的生存状态，就是维护人的“自然权利”，这权利与生俱来。就刚才那简单的几个“不愿意”，已经隐含了生命的权利、平等的权利、人身自由的权利等等。维护自然权利的法，就是自然法。<br>所以另一个比西塞罗还要早的罗马老头儿狄摩西尼说，“每一种法律都是另一种发现”。法律不是胡编乱造、随心所欲的，正义的法律是对自然法的发现。<br>【柏拉图的理型论】</p>
</li>
<li><p>陪审团已经作出了判决<br>法制制度最关键的一步，就是这个文化本身必须建立起绝对尊重司法的传统。因为论“硬件”来说，司法分支是最弱的一环，它在相当程度上是必须依靠社会共识来维持的。宪政国家的产生，就其历史发展来说，是一个社会依据其长期的经验，首先得出对司法之崇高地位的认可。缺少这种文化上的认可，司法是很容易被破坏的。<br>一旦陪审团宣布被告无罪，任何人，即使是总统和最高法院大法官，都没有权力改变。假如不是这样，那就将是一个打不开的死结，就会引出打着伸张正义旗号的民众私刑，就会走向暴力和血腥。假如不是这样，司法就失去权威，整个法制制度都将崩溃。新奥尔良私刑事件中的意大利裔受难者们，用他们的鲜血，用他们被民众的子弹打得残碎的躯体，为后代美国人重申了非常简单却至为重要的道理，这就是今日美国人在法庭大门口经常听到的话：“陪审团已经作出了判决，我们的制度要求我们，必须尊重陪审团。</p>
</li>
<li><p>非法之法不是法<br>实际上，美国的领袖们对立法议会的警惕非常容易理解。读读权利法案那短短十条就明白，它要保障的，是民众个人的权利，是一个一个具体的、分散的、个人的权利，听起来是一回事，只是局部和整体的关系，在实际生活中却根本就是两回事。因为所谓人民的权利，在组成政府的时候，就已经委托给了政府，变成了政府之权力。建国领袖们所担心的，是作为这些个人之集合体的人民，通过他们选出的代表，在具体事务上，侵犯一部分民众的个人权利。<br>多数的暴政和绝对个人专权的暴政，可以在顷刻间转换。美国的建国领袖和同时代的法国革命者不同的是，在他们看来，“多数”并不天然地蕴含着“正确”，多数民众对少数人地镇压，并没有想象中地合理性。所以，对当年的宪法起草者来说，保障民众的个人权利，即使是保障少数人甚至一个人的权利，和防止暴政，特别是多数的暴政，就是同一回事。</p>
</li>
<li><p>星期日早晨的谋杀案<br>哪怕再合理的推论，也不足以定罪。定罪必须有超越“合理怀疑”的确凿无疑的证据。所以，被告的辩护律师们常常说的一句话是：”合理怀疑“是我们的救星。在一般人看来，假如被告被发现有强烈的作案动机，应该对被告是不利的，但是在律师看来，远非如此。因为作案动机的存在，通常会引出人们逻辑合理的推论。就可能在这种强烈的逻辑力量下忽略证据，甚至自然而然地就以推理取代证据。这个时候，距离辩护取胜、被告被开释，也就不远了。<br>在这里经常出现的、对种族问题的此类不确切描述，起因于人们尤其是知识阶层，有很强烈的、要表达自己对弱者深切同情，以及要挺身为底层代言的倾向。这是知识阶层由来已久、经久不息的一个情结。这也恰恰旁证了知识阶层和底层事实上的本质差异。这种差异给知识阶层带来越多的不安，他们产生这种表达的意向就越趋强烈。无疑，贫穷与恶劣的生活状态导致罪恶。可是，对这张联系的探究，应该引出的是社会学意义上的如何消除贫困、消灭罪恶根源的研究和行动，而不是对已经结出的罪恶之果表达泛滥的同情，不论这个恶果是个别的罪犯或是群体的暴民。道理很简单：任何罪行都是有受害者的。而知识阶层假如放弃面对犯罪行为的道德立场，甚至提供过分的借口和“理解”，不仅无助于弱势群体自身的演进，甚至可能将他们带入更为危险的困惑和歧途。</p>
</li>
<li><p>汉娜的手提箱<br>人类历史有大量的负面经验，即使是在和平时期，每个国家也都有大量负面的现实。人们需要历史的传承，汲取历史的教训，需要面对现实。而与此同时，作为儿童和青少年教育，又要警惕大规模的心理伤害。悲和愤等感情，是正义感的基础，可是一旦过度，很容易走向极端，产生对理性的摒弃。历史教育的目的，是带来一个健康的社会，让新的一代有幸福的生活、健康的心态。他们应该是幽默、睿智、快乐、自尊、富有想象力的一代，而不是一代悲壮的愤怒青年。</p>
</li>
<li><p>哪怕在奥斯维辛，绘画依然是美丽的<br>历史学家在摸索的，多是粗大的社会走向之脉络；文学艺术在细细解剖的，却是人们在不由自主中刻意藏匿的内心。在一定程度上，后者是理解前者必不可少的依托；前者又是后者无可离弃的基本背景。</p>
</li>
<li><p>《野火集》的启示<br>从中华文化圈里出来，很多人能够做到文字优美、内容正确、逻辑严密，可是，也许是我们习惯了这一文化中“士”的特殊位置，即使理解平等的意义，对自己的定位定调往往还是会“开低走高”。做社会批判时，会忘记自己也是社会的一员。批判的烈度越大，自我的位置就不断上升，不能持之以恒地维护和读者对话的平等。因为在我们的文化中，历来缺少平等地概念，我们自己地“低调”往往只是理智的产物，而不是本能的反应。</p>
</li>
<li><p>听一次演讲后的随想<br>他们大多是没有名气的，甚至是被误解的。然而，假如没有他们在一片需要改良的土地上默默耕耘，而只有燃野火者，那么当春天来临，也未见得就能够快快长出健康丰润的树苗来。</p>
</li>
</ul>
<h3 id="《刀锋》—-毛姆"><a href="#《刀锋》—-毛姆" class="headerlink" title="《刀锋》— 毛姆"></a>《刀锋》— 毛姆</h3><ul>
<li><p>你不觉得或许他正在追求一个深藏在一片未知的云朵中的理想——就像一位天文学家寻找某个仅仅通过数学计算才能知道其存在的星球吗？  </p>
</li>
<li><p>我们谁也不能两次涉足于同一条河流，然而，河水流去，随即流来的河水依旧沁人心脾。</p>
</li>
</ul>
<h3 id="《娱乐至死》—-尼尔·波兹曼"><a href="#《娱乐至死》—-尼尔·波兹曼" class="headerlink" title="《娱乐至死》— 尼尔·波兹曼"></a>《娱乐至死》— 尼尔·波兹曼</h3><ul>
<li><p>我时常想起萧伯纳著名的诗句，理智的人适应环境，而世上所有的进步都依赖不理智的人。麦克卢汉是不理智的，兰斯是不理智的，尼尔也是不理智的。因为这样，所有美好的事才发生了。  </p>
</li>
<li><p>根据柏拉图的观点，我们应该把焦点放在人类会话的形式上，而且假定我们会话的形式对于要表达的思想有重大的影响，而且容易表达出来的思想自然会成为文化的组成部分。 </p>
</li>
<li><p>犹太人的上帝存在于文字中，或者通过文字而存在，这需要人们进行最精妙的抽象思考。运用图像是亵渎神袛的表现，这样就防止了新的上帝进入。</p>
</li>
<li><p>随着印刷术影响的减退，政治、宗教、教育和任何其他构成公共事务的领域都要改变其内容，并且用最适用于电视的表达方式去重新定义。</p>
</li>
<li><p>如果我们能够意识到，我们创造的每一种工具都蕴含着超越其自身的意义，那么理解这些隐喻就会容易多了。例如，有人指出，12世纪眼镜的发明不仅使矫正视力成为可能，而且还暗示了人类可以不必把天赋或者缺陷视为最终的命运。 </p>
</li>
<li><p>马克思《德意志意识形态》：如果印刷机存在，这世上是否还可能有《伊利亚特》？他反问道：”有了印刷机，那些吟唱、传说和思考难道还能继续吗？这些史诗存在的必备条件难道不会消失吗？“</p>
</li>
<li><p>道格拉斯：“在讨论这些问题的时候，沉默比掌声更得体，我希望你们能够用自己的评判力、理解力和良知来听我的演讲，而不是用你们的激情或热情。”……，对于读者更是如此，因为作者并不是一直值得信任的。他们撒谎他们陷入迷茫，他们过于笼统，他们滥用逻辑甚至常识。读者对此必须有备而来，用知识武装好自己。</p>
</li>
<li><p>这并不是说，印刷术时代发表的文字就一定都是真实的。文字不能保证内容的真实性，而是形成一个语境，让人们可以问“这是真的还是假的”。</p>
</li>
<li><p>“阐释年代”：所有成熟话语所拥有的特征，都被偏爱阐释的印刷术发扬光大：富有逻辑的复杂思维，高度的理性和秩序，对于自相矛盾的憎恶，超常的冷静和客观以及等待受众反应的耐心。</p>
</li>
<li><p>《瓦尔登湖》：我们匆匆地建起了从缅因州通往德克萨斯州的磁性电报，但是它们可能并没有什么重要的东西需要交流……我们满腔热情地在大西洋下开通隧道，把新旧世界拉近几个星期，但是到达美国人耳朵里的第一条新闻可能却是阿德雷德公主得了百日咳。</p>
</li>
<li><p>从此，来路不明、读者对象不定的新闻开始横扫整个国家。战争、犯罪、交通事故、火灾和水灾——大多是阿德莱德公主得百日咳新闻的社会版本和政治版本——开始成为所谓“今日新闻”的主要内容。……电报使脱离语境的信息合法化，也就是说，信息的价值不再取决于其在社会和政治对策和行动中所起的运用，而是取决于它是否新奇有趣。电报把信息变成了一种商品，一种可以置用处或意义于不顾而进行买卖的东西。</p>
</li>
<li><p>行为最终产生的结果并不能成为行为最初的目的；人本性就是自私的，只有判断他行为中私己成分的多与少。</p>
</li>
<li><p>马歇尔·麦克卢汉所说的”后视镜“思维：认为一种新媒介只是旧媒介的延伸和扩展，比如汽车只是速度更快的马，电灯是功率更大的蜡烛。他们犯的错误就是完全误解了电视如何重新定义公众话语的意义。</p>
</li>
<li><p>宗教改革运动的发起者马丁·路德：如果每个家庭的餐桌上都有上帝的文字，基督徒就不需要教皇来为他们释义了。</p>
</li>
<li><p>萧伯纳第一次看见百老汇和四十二街上夜间闪烁的霓虹灯时发表的评论：如果你不识字，这些灯光无疑是美丽的。</p>
</li>
<li><p>思考不是表演艺术，而电视需要的是表演艺术…….人们看的以及想要看的是有动感的画面—-成千上万的图片，稍纵即逝却斑斓夺目。正是电视本身的这种性质决定了他必须舍弃思想，来迎合人们都视觉快感的需求，适应娱乐业的发展。</p>
</li>
<li><p>和他们一样，你要把注意力放在如何最大限度地实现节目的娱乐价值上。例如，你要为节目选择一个音乐主题。所有的电视新闻节目的开始、结束或者中间都要插入一段音乐。我发现很少有美国人会觉得这样的做法奇怪，这足以证明严肃的公众话语和娱乐之间存在的分界线已经荡然无存…….制造一种情绪，为娱乐提供一个主题，提醒观众使用一个合适的情绪。</p>
</li>
<li><p>假信息并不意味着错误的信息，而是意味着使人产生误解的信息——没有依据、毫无关联、支离破碎或流于表面的信息——这些信息让人产生错觉，以为自己知道了很多事实，其实却离事实的真相越来越远。当新闻被包装成一种娱乐形式时，它提供给观众的是娱乐而不是信息。</p>
</li>
<li><p>电视信息的娱乐化：节目中加入音乐引导观众情绪  | 新闻通常不超过1分钟，几乎不可能报导一个完整的严肃性新闻  |  紧跟着播放的一系列广告会在瞬间消解新闻的重要性 | 电视屏幕上的图像源源不断出现，不留有时间供观众思考 | 播音员播报任何新闻时保持一种固定不变的、得体的热情，不为播报的新闻内容所影响 | 播音员的容貌须得体，以及讲述者的可信度决定了事件的真实性。</p>
</li>
<li><p>不论是释迦牟尼、摩西、耶稣还是穆罕默德、路德，从来没有那个伟大的宗教领袖会给人们他们想要的东西，他们给的是人们应该具备的东西。但电视传教士的不成文规则是：“只有给观众他们想要的东西，你才可以得到市场占有率。”</p>
</li>
<li><p>真正的危险不在于宗教已经成为电视节目的内容，而在于电视节目可能会成为宗教的内容。</p>
</li>
<li><p>那些经营电视的人从来没有限制我们获得信息，而是不断扩大我们获得信息的途径。也就是说，通过制造大量的娱乐性的、无目的性、无意义的信息，来使真正应该被引起注意的新闻被淹没其中。不采用限制的手段却达到了使特定的信息在操纵下无法流通。</p>
</li>
<li><p>杜威《经验与教育》：也许人们对于教育最大的错误认识是，一个人学会的只有他当时正在学习的东西。其实，伴随学习的过程形成持久的态度········也许比拼写课或地理历史课更为重要·······因为这些态度才是在未来发挥重要作用的东西。</p>
</li>
<li><p>有两种方法可以让文化精神枯萎，一种是奥威尔式的——文化成为一个监狱，另一种是赫胥黎式的——文化成为一种滑稽戏。</p>
</li>
<li><p>奥威尔预言的世界比赫胥黎预言的世界更容易辨认，也更有理由去反对。我们的生活经历已经能够让我们认识监狱，并且知道在监狱大门即将关上的时候要奋力反抗。在弥尔顿、培根、伏尔泰、歌德和杰弗逊这些前辈的精神的激励下，我们一定会拿起武器保卫和平。但是，如果我们没有听到痛苦的哭声呢？谁会拿起武器去反对娱乐？</p>
</li>
<li><p>那些提出这些问题的人是得出和我一样的答案还是和马歇尔·麦克卢汉一样的答案并不重要，能够提出答案就行了，提出了问题就是破除了禁忌。</p>
</li>
</ul>
<h3 id="《如彗星划过夜空》—-林达"><a href="#《如彗星划过夜空》—-林达" class="headerlink" title="《如彗星划过夜空》— 林达"></a>《如彗星划过夜空》— 林达</h3><ul>
<li><p>这是一件非常有意思的事情，人类文明可能在一部分人中间先创造出来，而它被称为是“文明”的原因之一，就是它有能力超越自身利益的局限，有了抽象的人道、人权的思维，而且，还在设计“制度”，保障这样的权益。</p>
</li>
<li><p>华盛顿将军：“先生们，请等我戴上眼镜。这么些年，我的头发白了，眼神也不济了。”在华盛顿将军心中，“枪杆子”只是带来了追求自由的一个可能。唯有民众的授权，才是政府权力的合法来源。</p>
</li>
<li><p>欠债的农夫一多，就群起要求州议会通过立法，允许他们缓偿债务，要求州里加印纸币，还要求立法强立债权人接受纸币作为还款…….欠债的农夫们走投无路，开始造反，最震动的一次冲突是美国历史上有名的谢思暴动。</p>
</li>
<li><p>熟悉英国议会制度的代表们，在费城会议中也设立了全体委员会，作用却不一样。它只是一个矛盾的缓冲设置。这时的表决结果，只相当于委员会推荐方案，是对可行方案的试探。</p>
</li>
<li><p>今天人们在讨论的民主，往往都是指政治制度。为什么要去牵扯出“民主是一种思维方式”或者非政治领域的“生活方式”呢？我想，政治制度，其实时需要一个相应的社会文明程度去配合的。在民主制度自然生成的国家，是文明的土壤长出了这颗制度之树，而不是相反。</p>
</li>
<li><p>费城制宪会议的另一个规则，是一条条地分别表决提案，任何一条通过之后，都可以再返回来提出异议，要求重新表决。</p>
</li>
<li><p>民主意识的一个重要来源是人性的觉醒，从而自然地引发出对底层悲惨状况的同情和不平，进而为他们争取权益。正由于这种同情大多发自有比较优越的社会地位、文明程度较高的阶层，或者说发自知识阶层，因此他们的民主意识从起源来说，都是带着原罪负担的。他们非常容易进入的一个误区，就是会不由自主地要美化底层，以平衡自己的原罪意识。他们会在表达对底层苦难同情的时候，在赞美底层的时候，表现得煽情和夸张，以支撑自己的道德感。他们中的一部分，会要求竭力降低自己的文明水准甚至在行为上表现出反文明和粗俗…….这种倾向符合道德出发点的原始冲动。从法国大革命对平民杀贵族的支持，到一代代的民粹倾向，直至现代美国走到极端的“政治正确”，都是源于同样的出发点。在原罪负担之下，承担原罪感的人群往往是不自信的，他们需要他人对自己做出道德上的肯定。结果就是以过激的平民认同和平民倾向，来达到心理和道德需求上的平衡。</p>
</li>
<li><p>所以，我以为知识阶层的所谓道德勇气，一部分应该是表现在对强权的批判上，但更为困难更难做到的，是表现在他不迎合、不取悦于民众上。前者是很容易理解也相对更容易做到的。可是，只有非常少的人，能够有智慧有勇气做到，对强权和民众，都保持应有的独立和批判。这和他是否同情弱者，是否保护弱势人群，其实是两回事。</p>
</li>
<li><p>这是一个自然形成的利益诉求。个人在人群中非常弱下，可能被强者吃掉。他因此需要一个社会保护层，有法律抵挡强人，有地方可以申诉，有政府力量的保护，如同在身上加一个保护性的外壳。因此，人的联合、政府组织自然形成。</p>
</li>
<li><p>麦迪逊认为：结派会导致人的道德水平下降。个人作为个人行动的时候，都会对自己有一定的道德要求，有人之常情，有恻隐之心，会自觉地压抑人性中自私和恶的一面。可是一群人结成一派行动的时候，就会互相提供行为的正当性，提供派别内部的互相暗示，自我道德要求就会下降，甚至做出在一个人的时候不会做的坏事。</p>
</li>
<li><p>只有获得充分信息的持批判态度的大众意见，能够保护民主政府的价值体系。所以，警觉的、无所不晓的、自由的新闻界本身，对实现宪法第一修正案的目的是最为重要的。他说：“没有一个自由的、获得了充分信息的新闻界，就不可能有脱离蒙昧的人民。”</p>
</li>
<li><p>在这里，人们仍然有一种信念，他们相信，他们可能要走一段弯路，可能有一段倒退，可是任何威胁只能阻碍人们追求自由的道路，却不可能堵死它。也许今夜没有星辰，可是，他们相信，在云霭之上，仍然有群星在太空闪亮。</p>
</li>
</ul>
<h3 id="《美丽新世界》—-赫胥黎"><a href="#《美丽新世界》—-赫胥黎" class="headerlink" title="《美丽新世界》— 赫胥黎"></a>《美丽新世界》— 赫胥黎</h3><ul>
<li><p>直到最后，孩童的心灵就是这些暗示，而这些暗示的总和也就是孩童的心灵。不仅仅是孩童的心灵而已。成年人的心灵亦复如此······终其一生而不渝。那用来判断、希冀和下决心的心灵——都由这些暗示所组成。而所有这些暗示都是我们的暗示！</p>
</li>
<li><p>长久的追悔，是最可厌的一种情绪，这是所有道德家都同意的。如果你犯了错，就忏悔、努力改正，争取下回做好就是了。绝对不要沉溺在自己的错失里。在污泥中打滚可不是最好的净身方法。</p>
</li>
</ul>
<h3 id="《乌合之众》—-古斯塔夫·勒庞"><a href="#《乌合之众》—-古斯塔夫·勒庞" class="headerlink" title="《乌合之众》— 古斯塔夫·勒庞"></a>《乌合之众》— 古斯塔夫·勒庞</h3><ul>
<li><p>群体不善推理，却急于采取行动。他们目前的组织赋予了他们巨大的力量。我们目睹其诞生的那些教条，很快也会具有旧式教条的威力，也就是说，不容讨论的专横武断的力量。群众的神权就要取代国王的神权了。  </p>
</li>
<li><p>只有环境的单一性，才能造成明显的性格单一性。我曾在其它著作中指出，一切精神结构都包含着各种性格的可能性，环境的突变就会使这种可能性表现出来。这解释了法国国民公会中最野蛮的成员为何原来都是些谦和的公民。 </p>
</li>
<li><p>他很难约束自己不产生这样的念头：群体是个无名氏，因此也不必承担责任。这样一来，总是约束着个人的责任感便彻底消失了。  </p>
</li>
<li><p>群体在智力上总是低于孤立的个人，但是从感情及其激起的行动这个角度看，群体可以比个人表现得更好或更差，这全看环境如何。一切取决于群体所接受的暗示具有什么性质……刺激群体的因素多种多样，群体总是屈从于这些刺激，因此群体也极为多变。群体很容易做出刽子手的举动，同样也很容易慷慨赴义。</p>
</li>
<li><p>孤立的个人很清楚，在孤身一人时，他不能焚烧宫殿或洗劫商店，即使受到这样做的诱惑，他也很容易抵制这种诱惑。但是在成为群体的一员时，他就会意识到人数赋予他的力量，这足以让他生出杀人劫掠的念头，并且会立刻屈从于这种诱惑。</p>
</li>
<li><p>群体中的某个人对真相的第一次歪曲，是传染性暗示过程的起点……被派往寻找失散巡洋舰的护航舰“贝勒·波拉号”上，执勤兵突然发出了有一艘遇难船只的信号。而下船去营救落难士兵的官员们发现，那只不过是几根长满树叶的树枝。在这个实例中，可以清楚地看到我们已经解释过的集体幻觉的作用机制。一方面，我们看到一个在期待中观望的群体；另一方面，是执勤者发出海上有遇难船只的信号这样一个暗示。</p>
</li>
<li><p>群体推理的特点，是把彼此不同、只在表面上相似的事物搅在一起，而且立刻把具体的事物普遍化。知道如何操纵群体的人，给他们提供的也正是这种论证。包含一系列环节的逻辑论证，对群体来说完全是不可理解的。对于演讲家来说，20本滔滔不绝的长篇论证，尽管它们是认真思考的产物，还不如几句对群众有号召力的口号。</p>
</li>
<li><p>拿破仑对国会说：“我通过改宗天主教，终止了旺代战争；通过变成一个穆斯林教徒，在埃及站住了脚；通过成为一名信奉教皇至上的人，赢得了意大利神父的支持。如果我去统治一个犹太人的国家，我也会重修所罗门的神庙。”</p>
</li>
<li><p>持政府和帝国的具体工作就是用新的名称把大多数过去的制度重新包装一遍，这就是说，用新名称代替那些能够让群众想起不利形象的名称，因为它们的新鲜能防止这种联想。如商号和行会的税款变成了执照费等…名称的威力如此强大，如果选择得当，它足以使最可恶的事情改头换面，变得能被民众所接受。</p>
</li>
<li><p>让人们怀抱着那些希望和幻想吧，不然他们是活不下去的。这就是存在着诸神、英雄和诗人的原因。科学承担起这一任务已有50年的时间，但是在渴望理想的心灵里，科学是有所欠缺的，因为它不敢作出过于慷慨的承诺，因为它不能撒谎。</p>
</li>
<li><p>然而群众无论付出多大的代价，他们必须拥有自己的幻想，于是他们便像趋光的昆虫一样，本能地转向那些迎合他们需要的巧舌如簧者。</p>
</li>
<li><p>群众从来就没有渴望过真理，面对那些不合口味的证据，他们会拂袖而去，假如谬论对他们有诱惑力，他们更容易崇拜谬论。凡是能向他们供应幻觉的，也可以很容易地成为他们的主人。</p>
</li>
<li><p>要让群体相信什么，首先得搞清楚让他们兴奋的感情，并且装出自己也有这种感情的样子……那些知道如何影响他们的演说家，总是诉诸他们的感情而不是他们的理性。逻辑定律对群体不起作用……若演讲者遵循的是自己的思路而不是听众的思路，仅仅这一个事实就会使他不可能产生任何影响。</p>
</li>
<li><p>一个人占据着某种位置，拥有一定的财富或头衔，仅仅这些事实，就能使他享有名望，不管他本人多么没有价值。帕斯卡尔十分正确地指出，法袍和假发使法官必不可少的行头。没了这些东西，他们的权威就会损失一半。</p>
</li>
<li><p>巴特农神庙按其现存的状态，不过是一堆非常没有意思的破败废墟，但是它的巨大名望却使它看起来不是那个样子，而是与所有的历史记忆联系在一起。</p>
</li>
<li><p>各民族一直清楚获得普遍信念的好处，他们本能地知道，这种信念的消失是他们衰败的信号。使罗马人能够征服世界的信念，是他们对罗马的狂热崇拜；当这种信念寿终正寝时，罗马也注定衰亡。</p>
</li>
<li><p>每个时代的人都是在一个由相似的传统、意见和习惯组成的基本环境中成长，他们不能摆脱这些东西的桎梏。人的行为首先是受他们的信念支配，也受由这些信念所形成的习惯支配。</p>
</li>
<li><p>利用密谋可以推翻一个暴君，而反对牢固的信念又有什么可资利用？人类所知道的唯一真正的暴君，历来就是他们对死人的怀念或他们为自己编织出来的幻觉……躺在坟墓深处的摩西、佛祖、耶稣和默罕穆德，对人类实行着更深刻的转制统治。</p>
</li>
<li><p>至于过去引导意见的报业，就像政府一样，它在群众势力面前也变得屈尊俯就。当然，它仍然有相当大的影响，然而这不过是因为它只一味反映群众的意见及其不断的变化。报业既然成了仅仅提供信息的部门，它便放弃了让人接受某种观念或学说的努力。它在公众思想的变化中随波逐流，出于竞争的必要，它也只能这样做，因为它害怕失去自己的读者。最有价值的新闻被夹在各种轻松话题、社会见闻和金融谎言之间。</p>
</li>
<li><p>人们的意见还大致存在着一般趋势，它们的产生是因为接受了一些基本的信仰，只根据某人是个君主制的拥护者这一事实，即可断定他持有某些明确的历史观和科学观；只根据某人是共和主义者，便可以说他有着完全相反的观点。</p>
</li>
<li><p>候选人可以毫无惧色地承诺最重要地改革。这些夸张能够产生巨大的效果，但它们对未来并没有约束力，因为这需要不断地进行观察，而选民绝对不想为这事操心，他并不想知道自己支持的候选人在实行他所赞成的竞选纲领上走了多远，虽然他以为正是这个纲领使他的选择有了保证。</p>
</li>
</ul>
<h3 id="《达摩流浪者》—-杰克·凯鲁亚克"><a href="#《达摩流浪者》—-杰克·凯鲁亚克" class="headerlink" title="《达摩流浪者》— 杰克·凯鲁亚克"></a>《达摩流浪者》— 杰克·凯鲁亚克</h3><ul>
<li><p>在钢铁工厂和飞机场遍布的美国，会出现这样一号人物，更是奇上加奇。有贾菲这样的人在，表示着世界还不算太没有希望。我为此而感到高兴。我全身的肌肉都酸痛得要死，而肚子也饿得要命，不过，能够坐在这里和另一个充满热情的年轻人为这个世界祷告，这件事所带给我的安抚，就足以胜过一千个吻和一千句柔情话。</p>
</li>
<li><p>一想到这个，我就不禁噗嗤一声笑了出来。我想不出来，除了露宿、攀火车和做自己想做的事情以外，还有什么生活是值得过的，难道是在精神病院里和其它一百个病人聚精会神地盯着电视看吗？</p>
</li>
<li><p>你知道吗，我常常开着这辆大东西，在俄亥俄和洛杉矶之间没命地跑来跑去，而我跑一趟的钱，说不定要比你当流浪汉一辈子能赚的还要多。但你不必工作，不需要多少钱，却可以享受人生。到你是你聪明还是我聪明，我实在说不上来。</p>
</li>
<li><p>你和我都不是那种愿意为了过优裕的生活而践踏别人的人。我们的理想是找一个安静的地方，永远为所有的有情祷告，而只要等我们都变得够强壮，就可以付诸实行。</p>
</li>
</ul>
<h3 id="《僧侣与哲学家》—-让-弗朗索瓦·何维勒"><a href="#《僧侣与哲学家》—-让-弗朗索瓦·何维勒" class="headerlink" title="《僧侣与哲学家》— 让-弗朗索瓦·何维勒"></a>《僧侣与哲学家》— 让-弗朗索瓦·何维勒</h3><ul>
<li><p>用同样的方式，我们正眼去看一个念头，追溯它的起源，一样找不到任何具体的东西。当你发现这一点的那一刹那，那个念头就会消失掉。这称为“通过认识念头的本质来解放它”，意思是要认出念头的“空性”。我们一旦解放一个念头，就不会产生连环作用，反而像是从天空中飞过去的一只鸟，消失了却不留痕迹。</p>
</li>
<li><p>一个不存在的自我其实没有办法真正被“毁灭”，但是我们可以认出它的不存在性。我们想毁灭的就是那个幻觉，那个错误，那原来并非存在的东西。关于这一点经常有如下的隐喻：如果你进到一个房间，在阴暗的光线下看到一条绳子，以为是一条蛇，你会害怕，你会想逃走，或者想拿一根棍棒把它赶跑。这时候一个人突然把灯打开，你马上看到了，那根本不是一条蛇。事实上，在这个过程之中，什么都没有发生；你也没有“消灭”那条蛇，因为它从来就没有存在过，你只不过是驱除了一个幻觉。</p>
</li>
<li><p>我们需要再度分辨科学的种类，一方面是历史科学、心灵科学和人文科学，另外一边是“硬”的科学。后者得到的证据是任何人，不管他的意见是什么，都必须接受的。而前者不断累积见证，增加真理的可能性，但是有一个永远属于“绝对确定”的门槛，是没有办法跨越的。</p>
</li>
<li><p>我向你保证，我们无法让一位新几内亚热带雨林的居民相信百分之一的科学发现。我们说话的对象必须有一个相称的心理架构才行。我们必须许多年用一种特定的方式来教育他。同样地，没有打开自己的心来研究修行的人士，也不可能接受这样的成果。这也是需要教育的。“硬”的科学，也就是需要可复制性证据的科学，目标其实不是要解决形而上的问题，也不是要让生命有意义，而是希望用最准确的方式来描述物质世界，如果现实可以被浓缩成物质，意识只不过是神经系统的一个作用，这只是在替科学的运作下一个定义。修行的生物也有它的规则，修行在心中所带出的深沉信仰力量，可以和物质世界进行的实验相当。从一个纯粹修行的观点来观察心的本性，所带来的确定性等同于观察一个因地心引力而坠落的物体。</p>
</li>
<li><p>如果内省作为一种科学方法在西方心理学中已经失败而且被抛弃了，那是因为使用它的人没有适当的工具来进行实验。他们完全没有训练，完全没有心灵领域的知识，也完全不知道如何让心平静的技巧，让心背后的本性能够被观察出来。这像一个人用一个不稳定的电表来测电，最后得到的结论是：电是无法测量的。心灵技巧的学习需要精进，你不能一挥手就说不算，就因为它们不属于西方世界的主流思想。怀疑是容易理解的，但是缺乏兴趣，或不愿去印证另外一种方式这才是难以理解的。事实上，反过来说，也会遭遇同样的问题：我认识的一些西藏人绝对不愿意相信人类已经登陆了月球！</p>
</li>
<li><p>褊狭有两种主要形态：第一种是当人们没有深入自己宗教的深层意义时，不用纯正的方式实践它，反而拿它作为一面镜子，用来煽动地方性、民族性或国家性的情绪；第二种就是诚恳实行自己宗教的人们深信所信仰的真理，到他们认为可以用任何方式把这些信仰强加到别人身上的地步，因为他们认为这样做是在帮助别人的情况。</p>
</li>
<li><p>我们对现象世界的觉知是通过感觉器官，再由一串意识的刹那来感受这些器官送出来的资讯，并加以诠释。于是我们并不在觉知真正的世界，我们觉知的只不过是在我们意识中反映出来的意向。</p>
</li>
<li><p>如果一个囚犯想要释放和他同样受苦的伙伴，他必须先想办法挣脱自己的枷锁……如果要有能力帮助众生，你所教导的就是你所实行的。</p>
</li>
<li><p>丹增仁波切经常说：“我来西方的目的并不是为了要多创造一两个佛教徒，而是要分享我的经验，关于佛教这几世纪所发展出来的智慧。”他每一次演讲完都会说：“如果你举得我说的任何话有用，请用它，不然就忘掉它吧！”他甚至劝告到其他国家旅行的西藏喇嘛们不要过度强调佛教的教义，而要以一个人的身份对另外一个人提供自己的经验。</p>
</li>
<li><p>佛教谈到三种懒惰。第一种很简单，就是把所有时间用在吃饭和睡觉上。第二种就是告诉自己，“像我这样的人绝对不可能达到完美”。佛教的观点中，这种懒惰会让你觉得就算努力也没有意义，你永远无法达到任何心灵上的成就，这种懒惰让自己灰心，反而令自己试都不去试。第三种，也是在这里最切题的，就是把生命浪费在次等重要的工作上，永远不去面对最精要的问题，所有时间都花在解决次要问题上，一个个接一个，在一个永无止境的顺序中，像湖上的涟漪一样。你告诉自己，当你完成了这件或那件事情之后，你会开始寻找你生命的意义。</p>
</li>
<li><p>佛说：“不要因为对我的尊敬而相信我的教导。检查它，让你自己重新发掘出真理。”他又说：“我已经为你指引出道路，要不要走下去是你的事。”佛陀的教导就像一本导游手册，解释他自己走向智慧的道路。真正要成为一个佛教徒，我们必须皈依佛陀，并不是要把他当做一个神，而是把他当作一个导游，以及证悟的象征。同时我们要皈依他的教导，就是佛法，但佛法不是教条，而是一条道路。</p>
</li>
<li><p>不要只顾着梯子，要记得你要爬向哪里。</p>
</li>
</ul>
<h3 id="《魔鬼经济学》"><a href="#《魔鬼经济学》" class="headerlink" title="《魔鬼经济学》"></a>《魔鬼经济学》</h3><ul>
<li><p>值得注意的是，这些网站仅仅是将报价罗列了出来，并未参与保险的销售。因此，它们所经手的并非保险，而是信息……三K党的机密情报被爆料给广播和报纸媒体后，申请入会人数开始减少，机要信息如内部的等级分级制度及阶层名称也成为了供人讥讽的笑料……最高法院大法官路易斯·D·布兰戴斯曾写道：“据说阳光是最好的杀菌剂。”</p>
</li>
<li><p>信息就是互联网的货币。作为一种媒介，互联网行之有效地将信息从占有方转移到需求方。通常，一如定期人寿保险费的例子，信息以极端零碎的方式存在。在此情况下，互联网就像一块巨大的马蹄形磁铁，将沉入大海的绣花针一根根收集起来。互联网有一项成就，就连最热心积极的消费者保护团体也望尘莫及：它大幅缩小了专家与公众之间的信息差。</p>
</li>
<li><p>报纸上的言论听起来更加顺耳——犯罪率的下降要归功于新型的治安策略、灵活的枪支管制和良好的经济形势。我们越来越习惯于从我们触手可及的事物上寻找因果联系，而忽视年代久远、难以理解的现象。我们尤其迷信短期可见的原因，多数时候，这种推断都是正确的。但探讨因果关系时，这种一概而论的思维却常常存在陷阱。</p>
</li>
<li><p>这其实是个滑坡谬论——看似毫无意义的个人行为积少成多，就会产生客观的影响。并不是说有选民认识到自己的选票无法左右选举结果而不去投票，会使得选举不复存在。滑坡谬论，是一种非形式谬论，使用连串的因果推论，却夸大了每个环节的因果强度，因此得到不合理的结论。</p>
</li>
<li><p>那说90年代的经济繁荣导致了犯罪率的下降，似乎也顺理成章。然而，二者存在相关关系，并不等同于一者导致了另一者。相关关系仅表示，两个因素——姑且称之为X和Y——之间存在某种关系，但你无从判断孰因孰果。</p>
</li>
</ul>
<h3 id="《通往维根码头之路》—-乔治·奥威尔"><a href="#《通往维根码头之路》—-乔治·奥威尔" class="headerlink" title="《通往维根码头之路》— 乔治·奥威尔"></a>《通往维根码头之路》— 乔治·奥威尔</h3><ul>
<li><p>这个地方开始让我抑郁。不仅仅是因为那些灰尘，异味，和令人生厌的食物，更是由于那种一潭死水又毫无意义的衰退的感觉，那些在地底下工作的人散发出的味道如爬虫一般，那是一种被禁锢在充满污秽的劳作和刻薄的抱怨的泥沼之中，一圈一圈兜兜转转无法消散的气息。像布鲁克夫妇这样的人，最让人忍无可忍的是他们对于同一件事情一遍又一遍发着牢骚的方式。这让你觉得他们并不是活生生的人，而像是某种神神叨叨反复呢喃胡言乱语的幽魂。最后布鲁克夫人那千篇一律、自怜自艾的话语永远都是以“这真的太难了，不是吗？”结尾，一遍又一遍地重复着。</p>
</li>
<li><p>北方商人抱着其可恶的“要么成功要么滚蛋”的哲学理念成为十九世纪的主导力量，并至今阴魂不散地统治着我们。这是一类受阿诺德·本涅特启发的人——一类从半个克朗起家，到头来赢得五万英镑的人，他们最可吹嘘之处就是赚了钱之后变得比以前更粗鲁。他们唯一的优点就是会赚钱。我们被教导要推崇这样的人，仅仅是因为他们尽管心胸狭窄、卑鄙、愚昧、贪婪又粗鲁，却有“胆识”，他“成功”了。</p>
</li>
<li><p>接下来，你要面对的是西方社会阶级差异的真正秘密……下等人臭，这就是我们被教导的。没有什么喜恶之感能像生理感受这般切肤。种族厌恶、宗教憎恨、教育差异、性格、智力、甚至道德观的差异都可以被跨越，唯有生理上的排斥无从化解。</p>
</li>
<li><p>但是你的愿景苍白无力，除非你能抓紧它所牵扯进来的东西，即一个你不得不面对的现实，想要废除阶级差异，等于废除一部分的自我。像我这样，一个典型的中产阶级的一员，叫我说出我想让阶级差异消亡殆尽简直易如反掌。但我所有的行之所至，思之所及都来源于这阶级特质。我所有的观念，我的善与恶、悲与喜、笑与怒、美与丑，本质上都是中产阶级所拥有的概念；我读书、穿衣、饮食的品味，我的幽默，我的礼节，甚至我的一言一行、一平一仄，都是一种在社会阶层中自抬身价的特殊举措。当我意识到这点，我也同样意识到了我在无产阶级背后所给予他们的鼓励和平等的徒负虚名；若我果真想和他接触，我必须彻底改头换面，以至于最终从前的那个自我荡然无存，无人能辨。这里牵涉到的不仅仅是改善工人阶级的生活条件，也不仅仅是避免愚蠢的自命不凡的姿态，而是彻底抛弃中上层阶级的生活态度。</p>
</li>
</ul>
<h3 id="《人性的枷锁》—-毛姆"><a href="#《人性的枷锁》—-毛姆" class="headerlink" title="《人性的枷锁》— 毛姆"></a>《人性的枷锁》— 毛姆</h3><ul>
<li><p>自己读完的感受：【艺术性地夸大欲望的力量，将人的本性暴露得一览无遗】</p>
</li>
<li><p>他明明愿意放弃一切来再和罗斯成为朋友的。他恨自己和罗斯吵架，现在他看到自己让他痛苦了他觉得十分抱歉。但是刚刚那一刻仿佛不是自己了，就好像魔鬼抓住了他，强迫着他违背自己的意愿对罗斯说一些尖酸刻薄的话，尽管他现在恨不得立刻冲去和罗斯握手言和。但是他报仇雪恨的渴望太强烈了。他想要为自己曾经受到的痛苦和侮辱报复罗斯。这是他的骄傲：当然他也知道这很蠢，因为罗斯根本就不会在乎，而他自己却会因此二痛苦。</p>
</li>
<li><p>等你长大一点儿你就会明白要让这个世界变成一个尚可容忍的生活场所，第一件事情就是要承认人类的自私是不可避免的。你要求别人对你无私实际上就很荒谬，因为这就是在要求他们为了你而牺牲自己的欲望。他们凭什么要那么做呢？如果你甘心接受世界上每个人都是为了自己而活的，你反而就会对别人要求变少了。那么他们也不会让你失望，而你也会更加宽恕地看待他们。人们在世上只追求一件事情——那就是他们自己的愉悦。</p>
</li>
<li><p>菲利普突然意识到了他一直在欺骗自己：并不是自我牺牲逼得他想到了结婚，而是对于一位妻子一个家和爱情的渴望；而现在这一切似乎都在他的手上溜走了，他感到十分的绝望。他想要那些，比世界上任何东西都想要。他有什么好在乎西班牙和西班牙的那些城市呢，什么科尔德瓦，什么多莱多，什么莱昂；那些缅甸的宝塔和南海群岛的环礁湖又对他有什么意义呢？此时此地就是美洲啊。在他看来他以前的所有人生都在追随着其他人通过言语或者文字灌输给他的想法，他从来就没有过自己心底的渴望。他的轨迹永远是被他认为应该做的事情所动摇，而不是被他的整颗心想要做的事情所动摇。他现在不耐烦的把这些所有的一切都抛弃了。他永远都活在未来，而当下永远，永远地都从他的指尖溜走。他自己的想法？他想到了他想要从纷繁复杂又毫无意义地生活中编出一个美丽而复杂的图案来的愿望：他不也见过了那些最简单的图案了吗？那种一个人出生、工作、结婚、生子然后死亡的图案，那不也是最完美的图案吗？屈服于幸福也许就是承认失败，但是这种失败却比千百次的成功都更好啊</p>
</li>
</ul>
<h3 id="《沉默的大多数》—-王小波"><a href="#《沉默的大多数》—-王小波" class="headerlink" title="《沉默的大多数》— 王小波"></a>《沉默的大多数》— 王小波</h3><ul>
<li><p>假设有某君思想高尚，我是十分敬佩的；可是如果你因此想把我的脑子挖出来扔掉，换上他的，我绝不肯。人既然活着，就有权保证他思想的连续性，到死方休。假如我全盘接受，无异于请那些善良的思想母鸡到我脑子里下蛋，而我总不肯相信，自己的脖子上方，原来是长了一座鸡窝……假设我相信上帝，并且正在为善恶不分而苦恼，我就会请求上帝让我聪明到足以明辨是非的程度，而绝不会请他让我愚蠢到让人家给我灌输善恶标准的程度。假若上帝要我负起灌输的任务，我就要请求他让我在此项任务和下地狱中作一选择，并且我坚定不移的决心是：选择后者。</p>
</li>
<li><p>假如要我举出一生最善良的时刻，那我就要举出刚当知青时，当时我一心想要解放全人类，丝毫也没有想到自己。同时我也要承认，当时我愚蠢得很，所以不仅没干成什么事情，反而染上了一身病，丢盔卸甲地讨回城里。现在我认为，愚蠢是一种极大的痛苦；降低人类的智能，乃是一种最大的罪孽。所以，以愚蠢教人，那是善良的人所能犯下的最严重地罪孽。从这个意义上说，我们决不可对善人放松警惕。</p>
</li>
<li><p>中国的人文知识分子，有种以天下为己任的使命感，总觉得自己该搞出些给老百姓当信仰的东西。这种想法的古怪之处在于，他们不仅是想当牧师、想当神学家，还想当上帝（中国话不叫上帝，叫“圣人”）。可惜的是，老百姓该信什么，信到哪种程度，你说了并不算哪，这是令人遗憾的。还有一条不令人遗憾，但却要命：你自己也是老百姓；所以弄得不好，就会自己屙屎自己吃。</p>
</li>
<li><p>我对国学的看法是：这种东西实在厉害。最可怕之处就在那个“国”字。顶着这个字，谁还敢有不同意见？这种套子套上脖子，想把它再扯下来是枉然的；否则也不至于套了好几千年。它的诱人之处也在这个“国”字，抢到这个制高点，就可以压制一切不同意见；所以它对一切想在思想领域里巧取豪夺的不良分子都有莫大的诱惑力。你说它是史学也好，哲学也罢，我都不反对——倘若此文对正经史学家哲学家有了得罪之处，我深表歉意——但你不该否认它有成为棍子的潜力。想当年，像姚文元之类的思想流氓拿阶级斗争当棍子，打死打伤了无数人。现在有人又在造一根漂亮棍子。它实在太漂亮了，简直是完美无缺。我怀疑除了落进思想流氓手中变成一种凶器之外，它还能有什么用场。鉴于有这种危险，我建议大家都不要做上帝梦，也别做圣人梦，以免头上鲜血淋漓。</p>
</li>
<li><p>作为一个知识分子，我对信念的看法是：人活在世上，自会形成信念。对我本人来说，学习自然科学、阅读文学作品、看人文科学的书籍，乃至旅行、恋爱，无不有助于形成我的信念，构造我的价值观。一种学问、一本书，假如不对我的价值观发生作用（姑不论其大小，我要求它是有作用的），就不值得一学，不值得一看。有一个公开的秘密就是：任何一个知识分子，只要他有了成就，就会形成自己的哲学、自己的信念。托尔斯泰是这样，维纳也是这样。到目前为止，我还看不出自己有要死的迹象，所以不想最终皈依什么——这块地方我给自己留着，它将是我一生事业的终结之处，我的精神墓地。不断地学习和追求，这可是人生在世最有趣的事啊，要把这件趣事从生活中去掉，倒不如把我给阉了……你有种美好的信念，我很尊重，但要硬塞给我，我就不那么乐意：打个粗俗的比方，你的把把不能代替我的把把，更不能代替天下人的把把啊。这种看法会遭到反对，你会说：有些人就是笨，老也形不成信念，也管不了自己，就这么浑浑噩噩地活着，简直是种灾难！所以，必须有种普遍适用的信念，我们给它加点压力，灌到他们脑子里！你倒说说看，这再不叫意识形态，什么叫意识形态？</p>
</li>
<li><p>有关理性，哲学家有很多讨论，但根据我的切身体会，它的关键是：凡不可信的东西就不信，像我姥姥当年对待亩产三十万斤粮的态度，就叫做有理性。但这一点有时候不容易做到，因为会导致悲观和消极，从理性和乐观两样东西里选择理性颇不容易。理性就像贞操，失去了就不会再有；只要碰上了开心的事，乐观还会回来的。不过这一点很少有人注意到。从逻辑上说，从一个错误的前提什么都能推出来；从实际上看，一个扯谎的人什么都能编出来。所以假如你失去了理性，就会遇到大量令人诧异的新鲜事物，从此迷失在万花筒里，直到碰上了钉子。假如不是遇到了林彪事件，我至今还以为自己真能保卫毛主席哩。</p>
</li>
<li><p>我上大学时，有一次我的数学教授在课堂上讲到：我现在所教的数学，你们也许一生都用不到，但我还要教，因为这些知识是好的，应该让你们知道。这位老师的胸襟之高远，使我终生佩服。</p>
</li>
<li><p>我认为像我这样的人不在少数：我们热爱艺术、热爱科学，认为它们是崇高的事业，但是不希望这些领域里的事同我为人处世的态度、我对别人的责任、我的爱憎感情发生关系，更不愿因此触犯社会的禁忌。这是因为，这两个方面不在一个论域里，而且后一个论域比前者要严重。打个比方，我像本世纪初年的一个爪哇土著人，此种人生来勇敢，不畏惧战争，但是更重视清洁。换言之，生死和清洁两个领域里，他们更看重后者；因为这个缘故，他们敢于面对枪林弹雨猛冲，却不敢朝着秽物冲杀。荷兰殖民军和他们作战时，就把屎橛子劈面掷去，使他们望风而逃。当我和别人讨论文化问题时，我以为自己的审美情趣、文化修养在经受挑战，这方面的反对意见就如飞来的子弹，不能使我惧怕；而道德方面的非难就如飞来的粪便那样使我胆寒。我的意思当然不是说现在文化的领域是个屎橛纷飞的场所，臭气熏天——决不是的；我只是说，它还有让我胆寒的气味。所以，假如有人以这种态度论争，我要做的第一件事，就是逃到安全距离之外，然后再好言相劝：算了吧，何必呢？</p>
</li>
<li><p>前年夏天，我到外地开一个会——在此声明，我很少去开会，这个会议的伙食标准也不高——看到一位男会友穿了一件文化衫，上面用龙飞凤舞的笔迹写着一串英文：OK, Let’s pee！总的来说，这个口号让人振奋，因为它带有积极、振奋的语调，这正是我们都想听到的。但是这个pee是什么意思不大明白，我觉得这个字念起来不大对头。回来一查，果不出我所料，是尿尿的意思。搞明白了全句的意思，我就觉得这话不那么激动人心了。众所周知，我们已过了要人催尿的年龄，在小便这件事上无须别人的鼓励。<br>我提到这件事，不是要讨论如何小便的问题，而是想指出，在做一件事之前，首先要弄明白是在干什么，然后再决定是不是需要积极和振奋。</p>
</li>
<li><p>现在有人说，同性恋是一种社会丑恶现象，我反对这种说法，但不想在此详加讨论——我的看法是，同性恋是指一些人和他们的生活，说人家是种社会现象很不郑重。我要是说女人是种社会现象，大家以为如何？——我只想转述一位万事通先生在澡堂里对这个问题发表的宏论，他说：“同性恋那是外国的高级玩意儿，我们这里有些人就会赶时髦……这艾滋病也不是谁想得就配得的！”在他说这些话时，我的一位调查对象就在一边坐着。后者告诉我说，他的同性恋倾向是与生俱来的。他既不是想赶时髦，也不是想得艾滋病。他还认为，生为一个同性恋者，是世间最沉重的事。</p>
</li>
<li><p>我想，假如这位万事通先生知道这一切，也不会对同性恋作出轻浮、赶时髦这样的价值评判，除非他对自己说出的话是对是错也不关心。我举这个例子是想说明：伦理道德的论域也和其他论域一样，你也需要先明白有关事实才能下结论，而并非像某些人想象的那样，只要你是个好人，或者说，站对了立场，一切都可以不言自明。不管你学物理也好，学数学也罢，都得想破了脑袋，才能得到一点成绩；假设有一个领域，你在其中想都不用想就能得到大批的成绩，那倒是很开心的事。不过，假如我有了这样的感觉，一定要先去看看心理医生。</p>
</li>
<li><p>现在可以说，孔孟程朱我都读过了。虽然没有很钻进去，但我也怕钻进去就爬不出来。如果说，这就是中华文化遗产的主要部分，那我就要说，这点东西太少了，拢共就是人际关系里那么一点事，再加上后来的阴阳五行。这么多读书人研究了两千年，实在太过分。我们知道，旧时的读书人都能把四书五经背得烂熟，随便点出两个字就能知道它在书中什么地方。这种钻研精神虽然可佩，这种做法却十足是神经病。显然，会背诵爱因斯坦原著，成不了物理学家；因为真正的学问不在字句上，而在于思想。就算文科有点特殊性，需要背诵，也到不了这个程度。因为“文革”里我也背过毛主席语录，所以以为，这个调调我也懂——说是诵经念咒，并不过分。</p>
</li>
<li><p>“二战”期间，有一位美国将军深入敌后，不幸被敌人堵在了地窖里，敌人在头上翻箱倒柜，他的一位随行人员却咳嗽起来。将军给了随从一块口香糖让他嚼，以此来压制咳嗽。但是该随从嚼了一会儿，又伸手来要，理由是：这一块太没味道。将军说：没味道不奇怪，我给你之前已经嚼了两个钟头了！我举这个例子是要说明，四书五经再好，也不能几千年地念；正如口香糖再好吃，也不能换着人地嚼。当然，我没有这样地念过四书，不知道其中的好处。有人说，现代的科学、文化，林林总总，尽在儒家的典籍之中，只要你认真钻研。这我倒是相信的，我还相信那块口香糖再嚼下去，还能嚼出牛肉干的味道，只要你不断地嚼。我个人认为，我们民族最重大的文化传统，不是孔孟程朱，而是这种钻研精神。过去钻研四书五经，现在钻研《红楼梦》。我承认，我们晚生一辈在这方面差得很远，但也未尝不是一件好事。四书也好，《红楼梦》也罢，本来只是几本书，却硬要把整个大千世界都塞在其中。我相信世界不会因此得益，而是因此受害。</p>
</li>
<li><p>任何一门学问，即便内容有限而且已经不值得钻研，但你把它钻得极深极透，就可以挟之以自重，换言之，让大家都佩服你；此后假如再有一人想挟这门学问以自重，就必须钻得更深更透。此种学问被无数的人这样钻过，会成个什么样子，实在难以想象。那些钻进去的人会成个什么样子，更是难以想象。古宅闹鬼，树老成精，一门学问最后可能变成一种妖怪。就说国学吧，有人说它无所不包，到今天还能拯救世界，虽然我很乐意相信，但还是将信将疑。</p>
</li>
<li><p>拉封丹寓言里，有一则《大山临盆》，内容如下：大山临盆，天为之崩，地为之裂，日月星辰，为之无光。房倒屋坍，烟尘滚滚，天下生灵，死伤无数……最后生下了一只耗子。中国的人文学者弄点学问，就如大山临盆一样壮烈。当然，我说的不只现在，而且有过去，还有未来。</p>
</li>
<li><p>大概把对自己所治之学的狂热感情视作学问本身乃是一种常见的毛病，不独中国人犯，外国人也要犯。…躲在人文学科的领域之内，享受自满自足的大快乐，在目前还是可以的；不过要有人养。在自然科学里就不行：这世界上每年都有人发明永动机，但谁也不能因此发财。</p>
</li>
<li><p>我知道，这哲人王也不是谁想当就能当，他必须是品格高洁之士，而且才高八斗，学富五车。在此我举中国古代的哲人王为例——这只是为了举例方便，毫无影射之意——孔子是圣人，也很有学问。夏礼、周礼他老人家都能言之。但假如他来打量我，我就要抱怨说：甭管您会什么礼，千万别来打量我。再举孟子为例，他老人家善养浩然之气，显然是品行高洁，但我也要抱怨道：您养正气是您的事，打量我干什么？这两位老人家的学养再好，总不能构成侵犯我的理由。特别是，假如学养的目的是要打量人的话，我对这种学养的性质是很有看法的。比方说，朱熹老夫子格物、致知，最后是为了齐家、治国、平天下。因为本人不姓朱，还可以免于被齐，被治和被平总是免不了的。假如这个逻辑可以成立，生活就是很不安全的。很可能在我不知道的地方，有一位我全然不认识的先生在努力地格、致，只要他功夫到家，不管我乐意不乐意，也不管他打算怎样下手，我都要被治和平，而且根本不知自己会被修理成什么模样。</p>
</li>
<li><p>顺便说一句，有些话只有哲人才能说得出来，比如尼采说：到女人那里去不要忘了带上鞭子。我要替女人说上一句：我们招谁惹谁了。至于这类疯话气派很大，我倒是承认的。总的来说，哲人王藐视人类，比牢头禁子有过之无不及。主张信任哲人王的人会说：只有藐视人类的人才能给人类带来更大利益。我又要说：只有这种人才能给人类带来最大的祸害。从常理来说，倘若有人把你当做了nothing，你又怎能信任他们？</p>
</li>
<li><p>让我们像奥威尔一样，想想什么是一加一等于二，七十年代对于大多数中国人来说，是个极痛苦的年代。很多年轻人作出了巨大的自我牺牲，而且这种牺牲毫无价值。想清楚了这些事，我们再来谈谈崇高的问题。就七十年代这个例子来说，我认为崇高有两种：一种是当时的崇高，领导上号召我们到农村去吃苦，说这是一种光荣。还有一种崇高是现在的崇高，忍受了这些痛苦、作出了自我牺牲之后，我们自己觉得这是崇高的。我觉得这后一种崇高比较容易讲清楚。弗洛伊德对受虐狂有如下的解释：假如人生活在一种无力改变的痛苦之中，就会转而爱上这种痛苦，把它视为一种快乐，以便使自己好过一些。对这个道理稍加推广，就会想到：人是一种会自己骗自己的动物。我们吃了很多无益的苦，虚掷了不少年华，所以有人就想说，这种经历是崇高的。这种想法可以使他自己好过一些，所以它有些好作用。很不幸的是它还有些坏作用：有些人就据此认为，人必须吃一些无益的苦、虚掷一些年华，用这种方法来达到崇高。这种想法不仅有害，而且是有病。</p>
</li>
<li><p>说到吃苦、牺牲，我认为它是负面的事件。吃苦必须有收益，牺牲必须有代价，这些都属一加一等于二的范畴。我个人认为，我在七十年代吃的苦、作出的牺牲是无价值的，所以这种经历谈不上崇高；这不是为了贬低自己，而是为了对现在和未来发生的事件有个清醒的评价。逻辑学家指出，从正确的前提能够推导出正确的结论，但从一个错误的前提就什么都能够推导出来。把无价值的牺牲看做崇高，也就是接受了一个错误的前提。此后你就会什么鬼话都能说出口来，什么不可信的事都肯信。</p>
</li>
<li><p>人有权拒绝一种虚伪的崇高，正如他有权拒绝下水去捞一根稻草。假如这是对的，就对营造或提倡社会伦理的人提出了更高的要求：不能只顾浪漫煽情，要留有余地；换言之，不能够只讲崇高，不讲道理。举例来说，孟子发明了一种伦理学，说亲亲敬长是人的良知良能，孝敬父母、忠君爱国是人间的大义。所以，臣民向君父奉献一切，就是崇高之所在。孟子的文章写得很煽情，让我自愧不如，他老人家要是肯去作诗，就是中国的拜伦；只可惜不讲道理。臣民奉献了一切之后，靠什么活着？再比方说，在七十年代，人们说，大公无私就是崇高之所在。为公前进一步死，强过了为私后退半步生。这是不讲道理的：我们都死了，谁来干活呢？在煽情的伦理流行之时，人所共知的虚伪无所不在；因为照那些高调去生活，不是累死就是饿死——高调加虚伪才能构成一种可行的生活方式。</p>
</li>
<li><p>《廊桥遗梦》上演之前，有几位编辑朋友要我去看，看完给他们写点小文章。现在电影都演过去了，我还没去看。这倒不是故作清高，主要是因为围绕着《廊桥遗梦》有种争论，使我觉得很烦，结果连片子都懒得看了。有些人说，这部小说在宣扬婚外恋，应该批判。还有人说，这部小说恰恰是否定婚外恋的，所以不该批判。于是，《廊桥遗梦》就和“婚外恋”焊在一起了。我要是看了这部电影，也要对婚外恋作一评判，这是我所讨厌的事情。</p>
</li>
<li><p>在一生的黄金时代里，我们没有欣赏到别的东西，只看了八个戏。现在有人说，这些戏都是伟大的作品，应该列入经典作品之列，以便流传到千秋万代。这对我倒是种安慰——如前所述，这些戏到底有多好我也不知道，你怎么说我就怎么信，但我也有点怀疑，怎么我碰到的全是经典？就说《红色娘子军》吧，作曲的杜鸣心先生显然是位优秀的作曲家，但他毕竟不是柴可夫斯基，……芭蕾和京剧我不懂，但概率论我是懂的。这辈子碰上了八个戏，其中有两个是芭蕾舞剧，居然个个是经典，这种运气好得让人起疑。根据我的人生经验，假如你遇到一种可疑的说法，这种说法对自己又过于有利，这种说法准不对，因为它是编出来自己骗自己的。</p>
</li>
<li><p>作品里的艺术性，或则按事急从权的原则，最低限度地出现；或则按得到最高格调的原则，合理地搭配。比如说，径直去写男女之爱，得分为一，搭配成革命的爱情故事，就可以得到一百零一分。不管怎么说，最后总要得到高大全。<br>我反对把一切统一到格调上，这是因为它会把整个生活变成一种得分游戏。一个得分游戏不管多么引人入胜，总不能包容全部生活，包容艺术，何况它根本就没什么意思。假如我要写什么，我就根本不管它格调不格调，正如谈恋爱时我决不从爱祖国谈起。</p>
</li>
<li><p>所谓幽闭类型的小说，有这么个特征：那就是把囚笼和噩梦当做一切来写。或者当媳妇，被人烦；或者当婆婆，去烦人；或者自怨自艾；或者顾影自怜；总之，是在不幸之中品来品去。<br>这种想法我很难同意。我原是学理科的，学理科的不承认有牢不可破的囚笼，更不信有摆不脱的噩梦；人生唯一的不幸就是自己的无能……由此得出结论，要努力去做事情，拼命地想问题，这才是自己的救星。</p>
</li>
<li><p>总而言之，当一种现象（不管是社会现象还是文学现象）开始贫了的时候，就该兜头给它一瓢凉水。要不然它还会贫下去，就如美国人说的，散发出屁眼气味——我是福尔斯先生热烈的拥护者。我总觉得文学的使命就是制止整个社会变得无趣……当然，你要说福尔斯是反色情的义士，我也没什么可说的。你有权利把任何有趣的事往无趣处理解。     </p>
</li>
<li><p>如果说贫穷是种生活方式，捡垃圾和挑大粪只是这种方式的契机。生活方式像一个曲折漫长的故事，或者像一座使人迷失的迷宫。很不幸的是，任何一种负面的生活都能产生很多乱七八糟的细节，使它变得蛮有趣的；人就在这种趣味中沉沦下去，从根本上忘记了这种生活需要改进。用文化人类学的观点来看，这些细节加在一起，就叫做“文化”。有人说，任何一种文化都是好的，都必须尊重。就我们谈的这个例子来说，我觉得这解释不对。在萧伯纳的《英国佬的另一个岛》里，有一位年轻人这么说他的穷父亲：“一辈子都在弄他的那片土、那只猪；结果自己也变成了一片土、一只猪。”要是一辈子都这么兴冲冲地弄一堆垃圾、一桶屎，最后自己也会变成一堆垃圾、一桶屎。所以，我觉得总要想出些办法，别和垃圾、大粪直接打交道才对。</p>
</li>
<li><p>他老人家还说，须知参差多态，乃是幸福的本源。反过来说，呆板无趣就是不幸福——正是这句话使我对他有了把握。一般来说，主张扼杀有趣的人总是这么说的：为了营造至善，我们必须做出这种牺牲。但却忘记了让人们活着得到乐趣，这本身就是善；因为这点小小的疏忽，至善就变成了至恶……</p>
</li>
<li><p>我常听人说：这世界上哪有那么多有趣的事情。人对现实世界有这种评价、这种感慨，恐怕不能说是错误的。问题就在于应该做点什么。这句感慨是个四通八达的路口，所有的人都到达过这个地方，然后在此分手。有些人去开创有趣的事业，有些人去开创无趣的事业。前者以为，既然有趣的事不多，我们才要做有趣的事。后者经过这一番感慨，就自以为知道了天命，此后板起脸来对别人进行说教。我以为自己是前一种人，我写作的起因就是：既然这世界上有趣的书是有限的，我何不去试着写几本——至于我写成了还是没写成，这是另一个问题，我很愿意就这后一个问题进行讨论，但很不愿有人就头一个问题来和我商榷。</p>
</li>
<li><p>对我自己来说，心胸是我在生活中想要达到的最低目标。某件事有悖于我的心胸，我就认为它不值得一做；某个人有悖于我的心胸，我就觉得他不值得一交；某种生活有悖于我的心胸，我就会以为它不值得一过。罗素先生曾言，对人来说，不加检点的生活，确实不值得一过。我同意他的意见：不加检点的生活，属于不能接受的生活之一种。人必须过他可以接受的生活，这恰恰是他改变一切的动力。</p>
</li>
<li><p>知识另有一种作用，它可以使你生活在过去、未来和现在，使你的生活变得更充实、更有趣。这其中另有一种境界，非无知的人可解。不管有没有直接的好处，都应该学习——持这种态度来求知更可取。大概是因为我曾独自一人度过了求知非法的长夜，所以才有这种想法……当然，我这些说明也未必能服人。反对我的人会说，就算你说的属实，但我就愿意只生活在现时现世！我就愿意得些能见得到的好处！有用的我学，没用的我不学，你能奈我何？……假如执意这样放纵自己，也就难以说服。罗素曾经说：对于人来说，不加检点的生活，确实不值得一过。他的本意恰恰是劝人不要放弃求知这一善行。抱着封闭的态度来生活，活着真的没什么意思。</p>
</li>
<li><p>有一种说法是这样的：人在年轻时，心气总是很高的，最后总要向现实投降。我刚刚过了44岁生日，在这个年龄上给自己做结论似乎还为时过早。但我总觉得，我这一生决不会向虚无投降。我会一直战斗到死。</p>
</li>
</ul>
<h3 id="《美丽新世界》—-奥尔德斯·赫胥黎"><a href="#《美丽新世界》—-奥尔德斯·赫胥黎" class="headerlink" title="《美丽新世界》— 奥尔德斯·赫胥黎"></a>《美丽新世界》— 奥尔德斯·赫胥黎</h3><ul>
<li><p>完全组织化的社会、科学式的等级体制、以系统培育泯灭自由意志、通过定期服用化学药物产生快感而接受奴役、利用夜间睡眠教育灌输正统理念——这些事情将会发生</p>
</li>
<li><p>《一九八四》的寓言所描写的社会是一个几乎完全通过惩罚和对惩罚的恐惧实施控制的社会。而在我所想的世界里，惩罚并不经常发生，而且大体上很温和。由政府实施的近乎完美的控制是通过多种多样的几乎非暴力的身心控制、基因标准化和对合乎要求的行为进行系统性的强化而实现的。</p>
</li>
<li><p>科学或许可以被定义为“将多样性归纳为统一性”。它的宗旨是通过忽略个别事件的独特性，专注于事件之间的共性</p>
</li>
<li><p>城市生活是匿名性的，也是抽象的。人们彼此之间的关系不是基于完整的人格，而是经济功能的体现，当他们不工作时，就只是不负责任的贪图享乐的人。过着这样的生活，个体会感受到孤单和无足轻重。他们的存在失去了任何意义。</p>
</li>
<li><p>在生物学的意义上，人有一定的群居性，但不是彻底的社会性动物——他更像是一头狼或一头大象，而不是一只蜜蜂或一只蚂蚁。原始形态的人类社会根本不像蚂蚁穴或白蚁窝，他们只是组成了族群而已。做一个简单的类比，文明是从原始的族群转变到社会性昆虫式的有机体的过程。</p>
</li>
<li><p>《美丽新世界》所描述的社会是一个世界统一的国家，战争已经被消除，统治者的首要目标是不惜一切代价让被统治者不制造麻烦。这一点他们通过合法化一定程度的性自由（通过废除家庭）以及其他手段，基本上保证生活在美丽新世界里的人不会有任何形式的毁灭性（或创造性）的情感压力。在《一九八四》里，权力欲通过施加痛苦而得到满足，而在《美丽新世界》里，则是通过给予快乐，但同样是对人的侮辱。</p>
</li>
<li><p>在西方，它们正构成威胁，将维护个体自由和民主制度的理性宣传淹没在无关紧要的琐事的海洋中。</p>
</li>
</ul>
<h3 id="《爱的进化论》-—-阿兰·德波顿"><a href="#《爱的进化论》-—-阿兰·德波顿" class="headerlink" title="《爱的进化论》 — 阿兰·德波顿"></a>《爱的进化论》 — 阿兰·德波顿</h3><ul>
<li>他无力表达自己的愤怒；她认识到，他在把困难转化成一种麻木和自我厌恶。</li>
<li>耻辱感与压抑的冲动不只为人类祖先和某些内敛的宗教——出于鲜为人知也并无必要的原因——所尊崇：它们注定亘古长存；从而，在某些特殊时刻（也许一生寥寥可数），当陌生者邀请我们卸下防御，坦然面对隐藏在内心的那些令人内疚的欲望时，给予我们力量。</li>
<li>他们不只有了性交；他们已经将彼此的感受——欣赏、柔情、感激和征服——翻译成肉体语言。</li>
<li>人们认为，肉体的交融令人兴奋沸腾，但实质上，它也许是暗指我们欣喜于自己获允展现隐秘的自我——欣喜于发现，爱人丝毫未被真实的我们所惊扰，反以鼓励与支持回应我们。</li>
<li>若他声明并不真正了解自己求婚的缘由，并不存在理性且思路清晰、可与持怀疑或探寻态度的第三方分享的诸多动机，这并非有失敬意。他有的不是理论依据，而是感觉，是丰富的感觉。</li>
<li>他无法解释为何大半婚姻都终于失败，这种一无所知和和对婚姻参与者的想象缺失，让他免于了信心的丧失。</li>
<li>这关爱之举那么自然，充满温柔，让拉比不经意间感受到自己孑然孤影，仿佛胸口遭到一记重击；在这凡尘，无人关注他的生存与命运，然后，他意识到，这形单影只不可为继。</li>
<li>遗憾的是，婚姻的魅力，一定程度上，归结于单身枯乏无趣。</li>
<li>一旦自由放纵的学习时代结束，陪伴与温情便再难觅寻；社交生活再不能避开为人夫妻者；再无人可电话联系或陪逛。于是，即便对方差强人意，我们也可能敞怀相迎。在旧时代，当婚姻（原则上）成为床底之欢的必要条件时，明理者便意识到，这可能导致错误的结婚动机，于是坚决主张取消有关婚前性行为的戒律，以便让年轻人冷静，少作冲动的抉择。</li>
<li>连续五十二个周日的独挨，可能严重损害人该有的谨慎。孤独也可能激发无谓的冲动，消除对潜在配偶的犹疑。任何一段关系的成功，不应单取决于夫妻共处的幸福指数，双方对于该种关系缺失的担忧，也该是判断标准之一。</li>
<li>在于他相信，自己必是极为坦诚的生活伴侣——这是孤身多年的又一个间接恶果。单身状态会令人惯于将错误的自我形象升格为正常。拉比内心混乱时极为追求外在整洁，他惯以工作排解焦虑，他心有愁绪时便有表述障碍，他不能找到合乎心意的T恤时便愤怒万分——所有这些怪癖，都可毫无痕迹地得以掩盖，只要无人在他身边目睹这一切。</li>
<li>目击者的缺失，会令他产生幻觉，以为只要觅得佳偶，自己便是极易和谐相处之人。</li>
<li>由此，一个符合逻辑的事实便是，长大成人的我们之所以拒绝某些候选对象，原因并不在于他们有过错，而在于他们总无过错——貌似极度稳重、成熟、善解人意和可靠——在我们内心深处，如此毫无差池，令人感觉陌生、失真。我们追寻其他更令我们兴奋的人，并非因为笃信与其携手的人生会更和谐，而是潜意识里认定它的挫折模式为我们熟知，令我们安心。</li>
<li>按理，他们已然抵达故事落幕的节点。罗曼蒂克部分的挑战已不复存在。从此，生活将维持稳定而重复的节奏，他们会发现，经年岁月，皆是一般模样，很少有不同凡响的事务值得一提。今后，他们只是需要在生活的溪流中伫立得久些，然后用更小号的网筛，去捕捉点点滴滴的兴致。</li>
<li>人们认可世间万物纷繁复杂，因而对于生活中绝大多数的宏大领域：国际贸易、移民、肿瘤学······能接受分歧，容忍冲突。但一旦分歧涉及家事，人们便容易作出决定性判断，并因此痛恶旷日持久的协商。</li>
<li>没有耐心协商，痛苦——无名怒火——便就随之而生。唠叨的一方要立作了断，却懒于说明缘由；被唠叨的一方，也再无心情解释自己的抵触是基于合理的反驳，或是出于令人同情，也许甚至值得原谅的个性缺陷。</li>
<li>愠怒者迫切需要对方理解，却又丝毫不帮助对方理解。正是对解释的需求，形成了侮辱的核心：如果对方尚需解释方可领悟，那么显然，他们不配得到解释。我们还需补充一句：这是愠怒者的特权，它代表他们足够尊重、信任我们，认为我们应该领会他们没有言说的伤害。这是爱情古怪的馈赠之一。</li>
<li>“在内心深处，我依然是个婴童，此刻我需要你变身高堂。我需要你准确地猜测出我痛苦的真实原由，就如我尚在襁褓，我对爱的概念初生成时那样。”若将爱人的气恼视作婴童的耍小性子，这便是给予最大可能的善解人意。我们过于敏感地认为，被视作少不经事，乃是对方居高临下之态；我们却忘了，人们间或会忽视我们的成人身份，只为与我们内在的那个失望、愤怒、口齿不清的幼童和谐相处并原谅他。</li>
<li>因为相比于我个人的舒适，更为重要的是，我有能力自如应对真实的你。刚刚在幻想安东内拉的人儿，正是与我在因弗内斯成婚的人，也是在我们抽屉柜顶部的那张照片里瞪着眼睛的小男孩。尽管他的想法有时可能令我不安，但他是我爱的人，我不愿把他想得太坏。你是我最亲密的朋友，我想要了解你的思想，不论它们多么冷僻怪异，我都可安然接受。我永远无法完全按照你的意愿行事，或成为你想要的任何模样，反之亦然；但我想我们可以成为敢于将真实的自己告知彼此的那种人。另有一种选择是沉默和谎言，但它们是爱情真正的敌人。</li>
<li>当伴侣很少再提及那些令我们害怕、震惊或厌恶之事，便是我们需要开始警觉之时，因为它也许便是最明确的信号，不管对方是因为善良，还是出于令人动容的担心、担心失去爱情，都表明我们不再被坦诚相见，或已被屏蔽在幻想之外。它也可能意味着对于有悖期望、因此越发危及期望的信息，我们已经不由自主地充耳不闻。</li>
<li>基于现状的行为分析，意义实则甚微。就好比当下境况的某些方面，其实自有其他原由；它似乎在不知不觉中触发一方长久存在的一种行为模式——此时为了应对特别的威胁，下意识被唤醒。将根源于过往的某种情绪，转嫁给当下也许全然无辜的受众，此类过激反应者，需要对心理学术语所描述的这种“移情”负责。</li>
</ul>
]]></content>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
</search>
