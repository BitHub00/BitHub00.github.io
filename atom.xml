<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>原力小站</title>
  
  <subtitle>扎导的原版正联出了吗？</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://Bithub00.com/"/>
  <updated>2021-11-16T07:23:05.923Z</updated>
  <id>http://Bithub00.com/</id>
  
  <author>
    <name>Mr.shuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>增量学习笔记</title>
    <link href="http://Bithub00.com/2021/11/16/%E5%A2%9E%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://Bithub00.com/2021/11/16/增量学习笔记/</id>
    <published>2021-11-16T07:23:19.151Z</published>
    <updated>2021-11-16T07:23:05.923Z</updated>
    
    <content type="html"><![CDATA[<p>借这篇博客记录增量学习的笔记。<br><a id="more"></a></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><a href="https://imgtu.com/i/Ie0eFs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/Ie0eFs.png" alt="Ie0eFs.png"></a></p><p>增量学习研究的是在一个连续的信息流中持续进行学习的问题，它的目标是将吸收新知识的同时要保留甚至整合、优化旧知识。之所以要研究增量学习，是因为在机器学习领域，模型训练有一个普遍缺陷，称为「灾难性遗忘」，即一个模型在新任务上训练时，在旧任务上的表现会显著下降。</p><p>造成灾难性遗忘的一个主要原因是<strong>「传统模型假设数据分布是固定或平稳的，训练样本是独立同分布的」</strong>，所以模型可以一遍又一遍地看到所有任务相同的数据，但当数据变为连续的数据流时，训练数据的分布就是非平稳的，模型从非平稳的数据分布中持续不断地获取知识时，新知识会干扰旧知识，从而导致模型性能的快速下降，甚至完全覆盖或遗忘以前学习到的旧知识。</p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>增量学习的方法大体上可以分为如下三个类别：</p><h3 id="回放"><a href="#回放" class="headerlink" title="回放"></a>回放</h3><p>回放类方法的思想是<strong>「温故而知新」</strong>，也就是在训练新任务时，模型会保留一部分具有代表性的旧数据来避免对旧任务的遗忘。因此这类方法的核心是<strong>「保留旧任务的哪部分数据，以及如何利用旧数据与新数据一起训练模型」</strong>。</p><h4 id="iCaRL-CVPR’17"><a href="#iCaRL-CVPR’17" class="headerlink" title="iCaRL[CVPR’17]"></a><a href="https://arxiv.org/abs/1611.07725v2" target="_blank" rel="noopener">iCaRL</a>[CVPR’17]</h4><p>iCaRL是回放类方法的代表，它为每个旧任务保留了一部分有代表性的数据（iCaRL假设越接近类别特征均值的样本越具有代表性）。给定一个包含不同类别的数据流$X^1,X^2,\dots$其中$X^y=\{x_1^y,\dots\}$中的样本都属于同一个类别。iCaRL学习的是一个分类器和一种特征表示的映射方法。iCaRL对一个新输入的待分类样本的流程如下：</p><div align="center"><a href="https://imgtu.com/i/Ie03mF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/Ie03mF.png" border="0" width="65%/"></a></div><p>关键在于第二行的样本集$P$和第三行的特征映射方法$\phi$。样本集$P$中包含$t$个类别各自的代表性样本，将一个类别中的所有样本都经过$\phi$得到它们的特征表示，然后取平均作为这个类别的特征表示，随后将新输入的样本也经过$\phi$得到它的特征表示，将这个特征表示与所有类别的特征表示进行比较，将最相近的作为新样本的分类标签。</p><div align="center"><a href="https://imgtu.com/i/Ie0o7Q" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/Ie0o7Q.png" border="0" width="50%/"></a></div><p>其中$X^s,…,X^t $表示新增加每一类对应的样本集；$K$表示存储的大小，即所有样本集的样本总数；$θ$表示当前的模型参数；$P$为当前的样本集。算法2代表的样本集的构建步骤如下：</p><ol><li>将新得到新类样本和之前存储的旧类样本集共同加到卷积神经网络中训练，来更新当前的模型参数$θ$</li><li>$K$是固定值，增加新类别后，样本集中每个类别应该保留的图片数应该进行调整，对旧任务$1,…,s-1$每个类别的图片数减少到$m$</li><li>对新任务构建新的样本集$P_y$，其中$y = s,…,t$，每个类别分别选择$m$张，最后将其加入到总的样本集$P$中</li></ol><p>第一步中的卷积神经网络仅是用做表征学习的，而不是做最后的分类任务。</p><div align="center"><a href="https://imgtu.com/i/IeB9AJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/IeB9AJ.png" alt="IeB9AJ.png" border="0" width="50%/"></a></div><p>算法3代表的特征学习步骤如下：</p><ol><li>首先将新任务的数据集$X_s,…,X_t$和$P$一起进行数据增广，构建最终的训练集$D$</li><li>然后将增广后的数据集$D$中旧任务的部分通过神经网络，提取它们的特征向量并存储</li><li>网络在$D$中新任务的部分上训练，loss函数前半部分为新任务的分类损失，后半部分为旧任务的蒸馏损失。</li></ol><p>通过算法3得到一个用于特征表示的映射的网络，通过算法2构建了旧任务的样本集，最后的步骤就是通过算法1对新样本进行分类。</p><h4 id="GEM-NIPS’17"><a href="#GEM-NIPS’17" class="headerlink" title="GEM[NIPS’17]"></a><a href="https://arxiv.org/abs/1706.08840v5" target="_blank" rel="noopener">GEM</a>[NIPS’17]</h4><p>iCaRL可能会对保留的旧任务的样本产生过拟合，GEM通过一个约束优化问题改善了这种情况。GEM以不等式约束的方式修正新任务的梯度更新方向，从而希望模型在不增大旧任务的损失的同时尽量最小化新任务的损失值，只更新新任务的参数而不干扰旧任务的参数。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化类方法的思想是<strong>「通过给新任务的损失函数施加约束的方法来保护旧知识不被新知识覆盖」</strong>，这类方法在学习新任务时不需要旧任务的数据。</p><h4 id="LWF-ECCV’16"><a href="#LWF-ECCV’16" class="headerlink" title="LWF[ECCV’16]"></a><a href="https://arxiv.org/abs/1606.09282" target="_blank" rel="noopener">LWF</a>[ECCV’16]</h4><p>LWF是正则化类方法的典型代表。下面以一张不同增量学习方法之间的对比来说明LWF：</p><div align="center"><a href="https://imgtu.com/i/IeBaNj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/IeBaNj.png" alt="IeBaNj.png" border="0" width="85%/"></a></div><p>图(a)是一个CNN网络，它的参数分别以$θ_s$和$θ_o$来表示。$θ_s$表示网络的卷积层 + 全连接层，它与特定的任务无关，是网络的共享参数。而最后一层是与类别相关的输出层，其参数单独用$θ_o$表示。若加入一个有新类别的分类任务，就将新任务新增的参数先随机初始化，表示为$θ_n$。</p><p>目前基于已有的$θ_s$来学习$θ_n$主要有以下三种方法：</p><ul><li><strong>微调（Fine-tuning）</strong>：$θ_s$和$θ_o$都可以进行更新，在新任务上一般采用一个较小的学习率对在旧任务上训练的网络进行调整，有时$\theta_s$会被固定防止过拟合等。微调的方法会因为没有旧任务样本的指导而在旧任务上表现变差。</li><li><strong>特征提取（Feature Extraction）</strong>：$θ_s$和$θ_o$都不变，从网络中的一个或多个中间层的输出被用来训练新任务的参数$θ_n$。特征提取通常在新任务上表现不佳，因为共享参数不能表示一些新任务独有的特征表示。</li><li><strong>联合训练（Joint Training）</strong>：所有的参数可以都进行学习优化，通常这个方法产生的结果是最优的，所以一般视为增量学习方法的性能上界（upper bound）。</li></ul><p>而LWF可以看作知识蒸馏与微调的结合。它只使用新任务的训练样本和标签，通过学习$\theta_n$来使得网络在新旧任务上都有好的表现。微调是使用一个已经训练完成的网络参数来对新的网络进行初始化，并在一个低的学习率下，更新参数，在新任务数据中重新找到一个局部最优解。而知识蒸馏则可以训练一个小的网络在源数据集上或者大量无标签的数据集上达到一个复杂网络的性能。</p><div align="center"><a href="https://imgtu.com/i/IeB2E4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/IeB2E4.png" alt="IeB2E4.png" border="0" width="65%/"></a></div><p>LWF的步骤如下：</p><ol><li>将新样本通过由旧样本训练好的网络并记录网络的输出$y_o$，新类别中的样本被添加到输出层，权重随机初始化为$\theta_n$，这里新加入的参数量等于新的类别数乘上其中的样本数。</li><li>首先固定$\theta_s$和$\theta_o$来更新$\theta_n$直到收敛，然后再一起更新所有参数直到收敛。避免新任务的训练过分调整旧模型的参数而导致新模型在旧任务上性能的下降。</li></ol><p>LWF中的损失函数同样由新任务的分类损失以及旧任务的知识蒸馏损失组成，分类损失是多分类任务常见的logistic损失，而旧任务的知识蒸馏损失是一个交叉熵的形式，它使得新网络在新任务上的预测和旧网络在新任务上的预测相近：</p><script type="math/tex; mode=display">\mathcal{L}_{n e w}\left(\mathbf{y}_{n}, \hat{\mathbf{y}}_{n}\right)=-\mathbf{y}_{n} \cdot \log \hat{\mathbf{y}}_{n}</script><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\text {old }}\left(\mathbf{y}_{o}, \hat{\mathbf{y}}_{o}\right) &=-H\left(\mathbf{y}_{o}^{\prime}, \hat{\mathbf{y}}_{o}^{\prime}\right) \\&=-\sum_{i=1}^{l} y_{o}^{\prime(i)} \log \hat{y}_{o}^{\prime(i)}\end{aligned}</script><script type="math/tex; mode=display">y_{o}^{(i)}=\frac{\left(y_{o}^{(i)}\right)^{1 / T}}{\sum_{j}\left(y_{o}^{(j)}\right)^{1 / T}}, \quad \hat{y}_{o}^{\prime(i)}=\frac{\left(\hat{y}_{o}^{(i)}\right)^{1 / T}}{\sum_{j}\left(\hat{y}_{o}^{(j)}\right)^{1 / T}}</script><blockquote><p>我们知道对于一个复杂网络来说往往能够得到很好的分类效果，错误的概率比正确的概率会小很多很多，但是对于一个小网络来说它是无法学成这个效果的。我们为了去帮助小网络进行学习，就在小网络的softmax加一个T参数，加上这个T参数以后错误分类再经过softmax以后输出会变大,同样的正确分类会变小。这就人为的加大了训练的难度，一旦将T重新设置为1，分类结果会非常的接近于大网络的分类效果。</p></blockquote><p>LWF的缺点是，它很依赖新旧任务之间的相关性，而当数据分布的相关性很弱时，并不能保证不丢失先前任务的重要信息。而且将新样本通过由旧样本训练好的网络并记录网络的输出会带来额外的开销。</p><h4 id="EBLL-ICCV’17"><a href="#EBLL-ICCV’17" class="headerlink" title="EBLL[ICCV’17]"></a><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Rannen_Encoder_Based_Lifelong_ICCV_2017_paper.pdf" target="_blank" rel="noopener">EBLL</a>[ICCV’17]</h4><p>EBLL在LWF的基础上保留旧任务中重要的特征从而保留其中的重要信息，来达到避免遗忘的目的。LWF中使用知识蒸馏的思路不代表没有信息的丢失，因此，提出通过auto-encoder，来学习一个新的子特征空间，包含旧任务中中最重要的特征表示。</p><h4 id="MAS-ECCV’18"><a href="#MAS-ECCV’18" class="headerlink" title="MAS[ECCV’18]"></a><a href="https://arxiv.org/abs/1711.09601" target="_blank" rel="noopener">MAS</a>[ECCV’18]</h4><p>MAS的核心思路是对每个任务，在训练完该任务之后，计算网络中每个参数$\theta_{ij}$对于该任务的重要性$\Omega_{ij}$并沿用到训练后续的任务中去。</p><p>每当进来一个新任务对其进行训练时，对于$\Omega_{ij}$大的参数$\theta_{ij}$，在梯度下降时尽量减少它的改变幅度，因为该参数对过去的某个任务很重要，需要保留它的值来避免遗忘。对于$\Omega_{ij}$比较小的参数$\theta_{ij}$，我们可以以较大的幅度对其进行梯度更新，以得到在该新任务上较好的性能或者准确率。在具体训练过程中，重要性$\Omega_{ij}$以正则项的形式添加到损失函数中。</p><p>关键在于如何计算每个参数$\theta_{ij}$对于该任务的重要性$\Omega_{ij}$。对于一个网络模型来说，它收敛以后我们可以将它的前向传递过程视为对一个真实函数$F$的近似，在此每个参数对于一个任务的重要性相当于该函数对于该参数的敏感程度。比如当一个$\theta$进行很小的改变后，却引起了网络输出很大的变化，就可以认为该参数是重要的，或者当前函数对该参数敏感。对于每个数据点$x_k$，有：</p><script type="math/tex; mode=display">F\left(x_{k} ; \theta+\delta\right)-F\left(x_{k} ; \theta\right) \approx \sum_{i, j} g_{i j}\left(x_{k}\right) \delta_{i j}</script><script type="math/tex; mode=display">g_{i j}\left(x_{k}\right)=\frac{\partial\left(F\left(x_{k} ; \theta\right)\right)}{\partial \theta_{i j}}</script><p>这就是函数在该数据点上对每个参数的偏导数。随后对于每个数据点，MAS求得它们的偏导的均值作为最后的重要性$\Omega_{ij}$，其中$N$是当前任务中的样本总数：</p><script type="math/tex; mode=display">\Omega_{i j}=\frac{1}{N} \sum_{k=1}^{N}\left\|g_{i j}\left(x_{k}\right)\right\|</script><p>当函数$F$的输出是多维度的时候，这时候对于每个维度都需要进行一次计算，才能得到最终的$\Omega_{ij}$。这里MAS用L2范数的平方作为代替，相当于把输出的所有维度合为一个标量，最终只需要进行一次计算即可：</p><script type="math/tex; mode=display">\Omega_{i j}=\frac{1}{N} \sum_{k=1}^{N} g_{i j}\left(x_{k}\right)=2 * \frac{1}{N} \sum_{k=1}^{N} y_{i}^{k} * y_{j}^{k}</script><p>MAS的损失函数为如下的形式，每当新进来一个任务时，在其任务的原损失函数上添加一个正则项来限制各个参数的更新幅度。其中$\theta_{ij}^*$是由前n-1个任务训练得到的模型，同时也是用于训练第n个任务的初始模型的参数。每当训练完一个任务之后， $\Omega_{ij}$都会进行更新，先计算对最新的任务（刚训练完成的任务）的重要性，然后累加到之前的重要性上。</p><script type="math/tex; mode=display">L(\theta)=L_{n}(\theta)+\lambda \sum_{i, j} \Omega_{i j}\left(\theta_{i j}-\theta_{i j}^{*}\right)^{2}</script><p>该方法的优点包括：</p><ol><li><p><strong>Constant Memory</strong>：模型占用的内存不随着任务的数量进行增加。</p></li><li><p><strong>Problem Agnostic：</strong>MAS方法是场景无关的，因为在具体代码中，该模型只对网络的backbone部分计算重要性，对于每个新的任务来说，模型会新建一个head（比如在分类中就是最后一个全连接层）进行训练。</p></li><li><p><strong>On Pretrained：</strong>给定一个预训练好的模型，可以在其top上再进行改动，然后添加新任务。</p></li><li><p><strong>Unlabelled Data：</strong>能够从无标签数据中学习，这个属性使得该方法能用应用在没有训练数据的场景下。</p></li></ol><h3 id="参数分离"><a href="#参数分离" class="headerlink" title="参数分离"></a>参数分离</h3><p>参数分离类方法的思想是<strong>「训练不同任务时使用模型的不同参数」</strong>，来达到避免遗忘的目的。通常的做法是模型的大小保持不变，不随任务数量的增加而变大，而在训练新任务时对旧任务的参数进行mask。</p><h4 id="PackNet-CVPR’18"><a href="#PackNet-CVPR’18" class="headerlink" title="PackNet[CVPR’18]"></a><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PackNet</a>[CVPR’18]</h4><p>PackNet是参数分离方法的典型代表，也是这篇survey中表现最好的方法。它最基本的想法就是在固定网络大小不变的情况下，通过剪枝使一些参数成为 free parameters，以便能使它们被用来训练新的任务。</p><p><a href="https://imgtu.com/i/IeBTKK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/11/04/IeBTKK.png" alt="IeBTKK.png"></a></p><p>以一个 5×5 的 filter 为例说明PackNet的训练过程。(a) 在任务 I 上进行初始训练，学习得到一个完整的filter；(b) 裁剪掉 60% (15/25) 再重新训练，得到了一个对应于任务 I 的稀疏的 filter，其中白圆圈代表0值权重，对应于任务 I 的权重将被固定，并且不会在后续过程中被裁剪；© 被剪枝的权重可以用于学习任务 II；(d) 再减枝33%(5/15)并且重新训练，橘色的圆圈即代表对应于任务 II 的权重，随即也被固定，并且之后不会被剪枝。这个过程将会随着学习完所有任务或者使用完所有filter 而终止。在推断时，根据所选的任务使用对应的mask。</p><p>例图已经说明了PackNet的步骤：</p><ol><li><p>我们从为初始任务学习的一个标准网络开始，称作 Task I。filter 的初始权重在 (a) 中用灰色表示。然后，我们裁剪掉一部分网络权重（即：将其置为0）。由于网络连接的突然变化，裁剪网络会导致性能下降。当裁剪率很高时，这一点尤其明显。为了在裁剪后恢复准确率，我们需要对网络进行重新训练，所需的 迭代次数要比初始训练时所需的少。经过一轮裁剪和重新训练后，我们获得了一个具有稀疏 filter 的网络，并且对 Task I 的性能影响降到了最低。Task I的surviving parameters（未被裁剪的参数，即 (b) 中灰色的参数）此后保持不变。</p></li><li><p>下一步，我们为新任务 Task II 训练网络，使裁剪后的权重从0恢复，获得 (c) 所示的橙色权重。注意，Task II的 filters 同时使用了灰色和橙色权重——即，重复使用了属于先前任务的权重。我们再次裁剪网络，释放掉一些仅用于 Task II 的参数，并重新训练 Task II 以从裁剪中恢复。 最终的 filters 如 (d) 所示。此后，Task I 和 Task II 的权重保持固定，然后将可被裁剪的参数用于学习又一个新任务，从而得到如 (e) 所示的绿色权重。重复此过程，直到完成了所有任务或没有更多可用的 free parameters 为止。</p></li></ol><p>值得注意的是，PackNet每次裁剪的比例是固定的，而不是自适应的；在每一轮裁剪中，从每个卷积层和全连接层中删除固定百分比的可被裁剪的权重。同一层中的权重按其绝对大小排序，并选择最低的50％或75％予以裁剪。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;借这篇博客记录增量学习的笔记。&lt;br&gt;
    
    </summary>
    
    
      <category term="增量学习" scheme="http://Bithub00.com/tags/%E5%A2%9E%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>工业界推荐系统小综述</title>
    <link href="http://Bithub00.com/2021/11/16/%E5%B7%A5%E4%B8%9A%E7%95%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>http://Bithub00.com/2021/11/16/工业界推荐系统/</id>
    <published>2021-11-16T07:18:21.512Z</published>
    <updated>2021-11-16T07:18:21.513Z</updated>
    
    <content type="html"><![CDATA[<p>借这篇博客记录看工业界推荐系统论文的心得。</p><a id="more"></a><h2 id="Logistics-Regression"><a href="#Logistics-Regression" class="headerlink" title="Logistics Regression"></a>Logistics Regression</h2><p>本节主要参考<a href="https://zhuanlan.zhihu.com/p/151036015" target="_blank" rel="noopener">刘启林的推荐系统</a></p><p>逻辑回归是推荐领域的经典模型之一，回归是指将值回归到[0,1]区间。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>LR将线性回归模型与Sigmoid函数相结合，线性回归模型的常见形式为$y=w^Tx+b$，为了表达形式上的统一，常将$w_0=b,x_0=1$，则有下图的$y=\sum_{i=0}^nw_ix_i=w^Tx$：</p><p><a href="https://imgtu.com/i/4BEwJs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BEwJs.jpg" alt="4BEwJs.jpg"></a></p><p>LR按照输出y的取值可以分为$y\in\{0,1\}、y\in\{-1,1\}$两种形式：</p><ul><li><p>CTR预估（0：曝光后未被点击，1：曝光后被点击）</p><blockquote><p>伯努利分布：随机变量X只能取0和1两个值：</p><script type="math/tex; mode=display">P(X=k)=p^k(1-p)^{1-k},~k={0,1}</script></blockquote><p><a href="https://imgtu.com/i/4BVanK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BVanK.jpg" alt="4BVanK.jpg"></a></p></li><li><p>分类预估（-1：负类，1：正类）</p><p><a href="https://imgtu.com/i/4BVNX6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BVNX6.jpg" alt="4BVNX6.jpg"></a></p></li></ul><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><a href="https://imgtu.com/i/4BZZUe" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZZUe.jpg" alt="4BZZUe.jpg"></a></p><p><a href="https://imgtu.com/i/4BZnCd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZnCd.jpg" alt="4BZnCd.jpg"></a><br><a href="https://imgtu.com/i/4BZVED" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZVED.jpg" alt="4BZVED.jpg"></a></p><p><a href="https://imgtu.com/i/4BZgPJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZgPJ.jpg" alt="4BZgPJ.jpg"></a><br><a href="https://imgtu.com/i/4BZ654" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BZ654.jpg" alt="4BZ654.jpg"></a></p><p><a href="https://imgtu.com/i/4Besyt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4Besyt.jpg" alt="4Besyt.jpg"></a><br><a href="https://imgtu.com/i/4BerQI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BerQI.jpg" alt="4BerQI.jpg"></a><br><a href="https://imgtu.com/i/4BeDSA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BeDSA.jpg" alt="4BeDSA.jpg"></a></p><h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><ul><li>适合离散特征；增加、减少特征容易，易于拟合、快速迭代</li><li>特征空间大，容易过拟合；</li><li>去掉高度相关特征；</li></ul><h2 id="Wide-amp-Deep-Learning-for-Recommender-Systems-DLRS’16"><a href="#Wide-amp-Deep-Learning-for-Recommender-Systems-DLRS’16" class="headerlink" title="Wide &amp; Deep Learning for Recommender Systems[DLRS’16]"></a>Wide &amp; Deep Learning for Recommender Systems[DLRS’16]</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>解决推荐时Memorization和Generalization无法兼顾的问题。</p><h4 id="Memorization"><a href="#Memorization" class="headerlink" title="Memorization"></a>Memorization</h4><p>面对拥有大规模离散sparse特征的CTR预估问题时，可以通过将特征之间进行叉乘来捕捉特征之间的相关性，典型代表如LR模型，使用原始sparse特征和叉乘特征作为输入。但缺点是特征的叉乘需要人工进行设计，而且对于训练数据中未曾出现过的特征对，模型中对应项的权重也会为0.</p><h4 id="Generalization"><a href="#Generalization" class="headerlink" title="Generalization"></a>Generalization</h4><p>Generalization为sparse特征学习低维的dense embeddings来捕获相关性，它相较于Memorization涉及更少的人工设计以及更好的泛化能力，即使训练数据中未曾出现的特征对，对应的权重也会因为各自的dense embeddings而非零。但缺点也是会带来过度泛化，当user-item矩阵非常稀疏时，例如小众爱好的user和冷门商品，这时大部分user-item应该是没有关联的，但dense embedding还是能得到非零预测，导致推荐不怎么相关的商品，这时Memorization更好，因为它可以记忆这些特殊的特征组合。</p><p>Memorization根据历史行为数据，产生的推荐通常和用户已有行为的物品直接相关的物品。而Generalization会学习新的特征组合，提高推荐物品的多样性。 论文作者结合两者的优点，提出了一个新的学习算法——Wide &amp; Deep Learning，其中Wide &amp; Deep分别对应Memorization &amp; Generalization。</p><p><a href="https://imgtu.com/i/4KtfW8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/17/4KtfW8.png" alt="4KtfW8.png"></a></p><h3 id="做法及创新-1"><a href="#做法及创新-1" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><strong>Wide</strong>部分是一个广义线性模型，其中$x$和$\phi(x)$表示原始特征和叉乘特征：</p><script type="math/tex; mode=display">y=w^T[x,\phi(x)]+b</script><p>原始特征$x=[x_1,x_2,\cdots,x_d]$有$d$维，叉乘特征的构造方式为：$\phi_k(x)=\Pi_{i=1}^dx_i^{c_{ki}},~c_{ki}\in\{0,1\}$</p><p>其实就是用一个布尔变量来标示哪些特征进行了叉乘。</p><p><strong>Deep</strong>部分是前馈神经网络，它会对一些sparse特征（如ID类特征）学习一个dense embeddings，维度在O(10)到O(100)之间。</p><script type="math/tex; mode=display">a^{l+1}=f(W^la^l+b^l)</script><p><strong>损失函数</strong>选取的是logistic损失函数，模型最后的预测输出为，其中$a^{l_f}$是神经网络最后一层的激活值：</p><script type="math/tex; mode=display">p(y=1|x)=\sigma(w^T_{wide}[x,\phi(x)]+w^T_{deep}a^{l_f}+b)</script><p><strong>联合训练</strong>时Wide部分只需要做一小部分的特征叉乘来弥补Deep部分的不足，不需要一个完整的Wide模型。优化方法使用的是mini-batch随机梯度下降，Wide部分是带L1正则的FTRL算法，Deep部分是AdaGrad算法。</p><p><a href="https://imgtu.com/i/4KB1ts" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/17/4KB1ts.png" alt="4KB1ts.png"></a></p><p>实验部分采取的模型设置如上图所示，其中的细节为：</p><ul><li>连续型特征会被归一化到[0,1]之间</li><li>离散型特征映射到32维embeddings，与原始连续特征共1200维作为网络输入</li><li>Wide部分只有一组特征叉乘，被推荐的App×用户下载的App，希望Wide部分能发现这样的规则：用户安装了应用A，此时曝光应用B，用户安装的B概率大。</li><li>线上模型更新时，用上次的embeddings和模型参数进行”热启动“</li></ul><h4 id="实践细节"><a href="#实践细节" class="headerlink" title="实践细节"></a>实践细节</h4><ol><li><p>为什么Wide部分要用L1 FTRL训练？</p><p>FTRL的介绍可见<a href="https://github.com/wzhe06/Ad-papers/blob/master/Optimization%20Method/%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3(Online%20Optimization" target="_blank" rel="noopener">文章</a>-%E5%86%AF%E6%89%AC.pdf)。这种方式注重模型的稀疏性，能让Wide部分变得更加稀疏，大部分权重都为0。</p></li><li><p>为什么Deep部分不考虑稀疏性的问题？</p><p>Deep部分的输入，要么是Age，#App Installs这些数值类特征，要么是已经降维并稠密化的Embeddings向量。所以Deep部分不存在严重的特征稀疏问题，自然可以使用精度更好，更适用于深度学习训练的AdaGrad去训练。</p></li></ol><h2 id="Factorization-Machines"><a href="#Factorization-Machines" class="headerlink" title="Factorization Machines"></a>Factorization Machines</h2><p>本节内容主要参考<a href="https://zhuanlan.zhihu.com/p/145436595" target="_blank" rel="noopener">刘启林的推荐系统</a></p><h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>因子分解机在线性回归模型中加入了特征的交互，来建模特征的相关性，并且解决数据的稀疏性以及特征空间维度过高的问题。</p><p>对于常见的categorical特征，经过one-hot编码以后，样本的数据就会变得很稀疏。举例来说，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间也会一下子暴增一百万。</p><h3 id="做法及创新-2"><a href="#做法及创新-2" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h4><p>线性回归模型假设特征之间相互独立：</p><script type="math/tex; mode=display">y=w_0+\sum_{i=1}^nw_ix_i</script><p>而现实场景中，特征之间是有相关性的，例如&lt;程序员&gt;与&lt;计算机类书籍&gt;，因此需要在线性回归模型中加入特征组合项。最简单的组合方式是两两组合，变为二阶多项式回归模型，多出$\frac{n(n-1)}{2}$项：</p><script type="math/tex; mode=display">y=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j\ge i}^nw_{ij}x_ix_j</script><p>这样做的局限是对于样本中没出现过交互的特征组合，就无法对相应的参数进行估计，而且时间复杂度上升到了$O(n^2)$。</p><p>上式中的二项式参数$w_{ij}$可以组成一个对称矩阵$W$，根据Cholesky分解可以分解成：</p><blockquote><p>Cholesky分解：将一个对称正定矩阵化为一个下三角矩阵与其共轭转置矩阵的积</p></blockquote><script type="math/tex; mode=display">W=VV^T</script><p>二次项参数转化为$w_{ij}=<v_i,v_j>=\sum_{f=1}^kv_{i,f}v_{j,f}$，此时隐向量的特征维度$k$一般远小于原始特征维度$n$。$v_i\in \mathbb{R}^k$是特征i的嵌入向量。FM的假设是，特征两两相关。</v_i,v_j></p><h4 id="计算化简"><a href="#计算化简" class="headerlink" title="计算化简"></a>计算化简</h4><p>FM的计算复杂度可以化简为线性复杂度：</p><p><a href="https://imgtu.com/i/4ahAk6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4ahAk6.jpg" alt="4ahAk6.jpg"></a></p><p><a href="https://imgtu.com/i/4ahMnA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4ahMnA.jpg" alt="4ahMnA.jpg"></a></p><p><a href="https://imgtu.com/i/4ahhH1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4ahhH1.jpg" alt="4ahhH1.jpg"></a></p><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>FM模型可用于回归（Regression）、二分类（Binary classification）、排名（Ranking）任务，其对应的损失函数如下：</p><p><a href="https://imgtu.com/i/4a4d2D" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4a4d2D.jpg" alt="4a4d2D.jpg"></a></p><p><a href="https://imgtu.com/i/4a5Uwn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4a5Uwn.jpg" alt="4a5Uwn.jpg"></a></p><p>以随机梯度下降训练为例：</p><p><a href="https://imgtu.com/i/4aIt1O" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aIt1O.jpg" alt="4aIt1O.jpg"></a></p><p><a href="https://imgtu.com/i/4aovQS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aovQS.jpg" alt="4aovQS.jpg"></a></p><h4 id="特征工程-1"><a href="#特征工程-1" class="headerlink" title="特征工程"></a>特征工程</h4><p>FM模型对特征两两自动组合，不需要人工参与，类别特征One-Hot化，以一个电影数据集为例：</p><p><a href="https://imgtu.com/i/4aT2wj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aT2wj.jpg" alt="4aT2wj.jpg"></a></p><p><a href="https://imgtu.com/i/4aTRTs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aTRTs.jpg" alt="4aTRTs.jpg"></a></p><p><a href="https://imgtu.com/i/4aTIpV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4aTIpV.jpg" alt="4aTIpV.jpg"></a></p><h2 id="Field-aware-Factorization-Machines-for-CTR-Prediction-RecSys’16"><a href="#Field-aware-Factorization-Machines-for-CTR-Prediction-RecSys’16" class="headerlink" title="Field-aware Factorization Machines for CTR Prediction[RecSys’16]"></a>Field-aware Factorization Machines for CTR Prediction[RecSys’16]</h2><p>本节主要参考<a href="https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">深入FFM原理与实践</a>、<a href="https://www.jianshu.com/p/781cde3d5f3d" target="_blank" rel="noopener">FFM模型理论和实践</a></p><h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FM在遇到one-hot类型的特征时遇到的数据稀疏性问题。</p><h3 id="做法及创新-3"><a href="#做法及创新-3" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="核心思想-2"><a href="#核心思想-2" class="headerlink" title="核心思想"></a>核心思想</h4><p>FFM模型中引入了域（field）的概念，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个域，包括用户国籍，广告类型，日期等等，以一条CTR点击数据为例，说明FFM与FM的区别：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Clicked</th><th style="text-align:center">Publisher(P)</th><th style="text-align:center">Advertiser(A)</th><th style="text-align:center">Gender(G)</th></tr></thead><tbody><tr><td style="text-align:center">Yes</td><td style="text-align:center">ESPN</td><td style="text-align:center">Nike</td><td style="text-align:center">Male</td></tr></tbody></table></div><p>对于FM，只会考虑二次交叉项：</p><script type="math/tex; mode=display">\phi_{FM}=V_{ESPN}\cdot V_{Nike}+V_{ESPN}\cdot V_{Male}+V_{Nike}\cdot V_{Male}</script><p>因为Nike与Male显然属于不同的field，所以（ESPN，Nike）和（ESPN，Male）的隐含含义也可能是不同的，而FM只用一个隐向量$V_{ESPN}$来统一表示ESPN与Nike和Maled的交互作用，不够准确。而在FFM中，域之间的交互作用是不同的，每个特征有k个隐向量个数，k为其余特征field的个数：</p><script type="math/tex; mode=display">\phi_{FFM}=V_{ESPN,A}\cdot V_{Nike,P}+V_{ESPN,G}\cdot V_{Male,P}+V_{Nike,G}\cdot V_{Male,A}</script><p>所以FFM的数学表达式为：</p><script type="math/tex; mode=display">\phi_{FFM}(w,x)=\sum_{i=1}^n\sum_{j\ge i}^nw_{i,f_j}\cdot w_{j,f_i}x_ix_j</script><p>FFM的参数个数为kfn，FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。值得注意的是，由于隐向量与field相关，所以FFM中的二次项不能够化简，它的时间复杂度为$O(kn^2)$。</p><h4 id="实践细节-1"><a href="#实践细节-1" class="headerlink" title="实践细节"></a>实践细节</h4><ol><li>样本归一化。FFM默认是进行样本数据的归一化，否则容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。</li><li>特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到[0,1]是非常必要的。</li><li>省略零值特征。零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</li></ol><h2 id="DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction-IJCAI’17"><a href="#DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction-IJCAI’17" class="headerlink" title="DeepFM: A Factorization-Machine based Neural Network for CTR Prediction[IJCAI’17]"></a>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction[IJCAI’17]</h2><p>本节主要参考<a href="https://static001.geekbang.org/con/33/pdf/1511951900/file/%E6%9C%80%E7%BB%88%E7%89%88-%E5%BC%A0%E4%BF%8A%E6%9E%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%9A%84%E6%8A%80%E6%9C%AF%E8%BF%9B%E5%B1%95%E5%8F%8A%E5%BE%AE%E5%8D%9A%E7%9A%84%E5%BA%94%E7%94%A8.pdf" target="_blank" rel="noopener">深度学习在推荐的技术进展及微博的应用</a>、<a href="https://www.jianshu.com/p/6f1c2643d31b" target="_blank" rel="noopener">DeepFM模型理论与实践</a></p><h3 id="解决的问题-3"><a href="#解决的问题-3" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>对于一个基于CTR预估的推荐系统，最重要的是学习到用户点击行为背后隐含的特征组合。举例来说，在主流的app市场上，我们发现，用户喜欢在用餐时间下载送餐app， 说明二阶交叉特征“app类别-时间戳” 可以作为CTR预估的重要特征。另一个发现， 男青年喜欢射击游戏和RPG游戏，因此，三阶交叉特征“app类别-用户性别-用户年龄”也可以作为CTR预估的一个特征。但是这种交叉特征往往需要专家知识，类似“尿布-啤酒”这种经典的例子。</p><p>前面提到的FM虽然理论上来讲可以对高阶特征组合进行建模，但实际上因为计算复杂度的原因一般都只用到了二阶特征组合。而多层神经网络能够学习复杂的交叉特征。</p><h3 id="做法及创新-4"><a href="#做法及创新-4" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="DNN与高维特征"><a href="#DNN与高维特征" class="headerlink" title="DNN与高维特征"></a>DNN与高维特征</h4><p>虽然DNN能够学习复杂的特征组合，但直接应用于CTR预告等问题上时会在离散型特征上遇到阻碍，对于离散型特征典型的做法是进行one-hot编码，这会导致输入的数据维度非常高：</p><p><a href="https://imgtu.com/i/4wrxln" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wrxln.png" alt="4wrxln.png"></a></p><p><a href="https://imgtu.com/i/4wsVp9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wsVp9.png" alt="4wsVp9.png"></a></p><p>类似于FFM中将特征按域来进行分类，可以将输入的one-hot数据按照域形成对应的dense vector，来避免数据稀疏性的问题：</p><p><a href="https://imgtu.com/i/4wsmOx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wsmOx.png" alt="4wsmOx.png"></a></p><p><a href="https://imgtu.com/i/4wsGpd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wsGpd.png" alt="4wsGpd.png"></a></p><p><a href="https://imgtu.com/i/4wscXq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wscXq.png" alt="4wscXq.png"></a></p><p><a href="https://imgtu.com/i/4wyp3d" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/4wyp3d.png" alt="4wyp3d.png"></a></p><p>也就是希望能将DNN与FM进行一个融合，而融合的形式总的来说分为串行与并行，本节介绍的DeepFM以及前面介绍过的Wide&amp;Deep都为典型的并行结构。</p><p><a href="https://imgtu.com/i/40FPt1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/40FPt1.png" alt="40FPt1.png"></a></p><h4 id="核心思想-3"><a href="#核心思想-3" class="headerlink" title="核心思想"></a>核心思想</h4><p><a href="https://imgtu.com/i/40AJoj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/40AJoj.png" alt="40AJoj.png"></a></p><p>首先来看DeepFM的结构，FM与DNN分别负责提取低阶与高阶特征，这两部分<strong>共享输入</strong>：</p><p><a href="https://imgtu.com/i/40Eihn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/23/40Eihn.png" alt="40Eihn.png"></a></p><p>FM部分与标准的FM并无不同，而DNN部分，在进入第一层隐藏层之前，首先通过一个嵌入层来将输入的高维特征压缩到低维稠密向量。这一部分的两个特点为：</p><ol><li>尽管不同域的特征维度不同，在经过压缩后的维度均为k。</li><li>FM部分的隐向量V现在作为DNN的嵌入层权重。这样一来就不需要通过FM对隐向量V进行预训练之后对DNN的嵌入层进行初始化，而是在训练DNN时对V一起进行学习，做到端到端。</li></ol><h2 id="Deep-Neural-Networks-for-YouTube-Recommendations-RecSys’16"><a href="#Deep-Neural-Networks-for-YouTube-Recommendations-RecSys’16" class="headerlink" title="Deep Neural Networks for YouTube Recommendations[RecSys’16]"></a>Deep Neural Networks for YouTube Recommendations[RecSys’16]</h2><p>本节要介绍的是Youtube出品的经典工业界论文，主要内容参考<a href="https://zhuanlan.zhihu.com/p/52169807" target="_blank" rel="noopener">王喆的机器学习笔记1</a>、<a href="https://zhuanlan.zhihu.com/p/61827629" target="_blank" rel="noopener">王喆的机器学习笔记2</a>、<a href="https://www.jianshu.com/p/8fa4dcbd5588" target="_blank" rel="noopener">深度学习遇上推荐系统</a>。</p><h3 id="解决的问题-4"><a href="#解决的问题-4" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>作为全球最大的UGC的视频网站，Youtube需要在百万量级的视频规模下进行个性化推荐。由于候选视频集合过大，考虑online系统延迟问题，不宜用复杂网络直接进行推荐，所以Youtube采取了两层深度网络完成整个推荐过程：</p><ol><li>第一层是<strong>Candidate Generation Model</strong>完成候选视频的快速筛选，这一步候选视频集合由百万降低到了百的量级（粗排）。</li><li>第二层是用<strong>Ranking Model</strong>完成几百个候选视频的排序（精排）。</li></ol><div align="center"><a href="https://imgtu.com/i/4BqpB6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BqpB6.png" alt="4BqpB6.png" border="0" width="75%"></a></div><h4 id="做法及创新-5"><a href="#做法及创新-5" class="headerlink" title="做法及创新"></a>做法及创新</h4><h4 id="粗排模型"><a href="#粗排模型" class="headerlink" title="粗排模型"></a>粗排模型</h4><div align="center"><a href="https://imgtu.com/i/4BOiSH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BOiSH.png" alt="4BOiSH.png" border="0" width="65%/"></a></div><p>自底向上看粗排模型的结构，最底层的输入是用户观看过的video的embedding向量，以及搜索词的embedding向量，由word2vec得到。特别地，历史搜索的query分词后的token的embedding向量进行平均，能够反映用户的整体搜索历史状态。其它的特征向量还包括了用户的地理位置的embedding，年龄，性别等。然后把所有这些特征concatenate起来，输入上层的ReLU神经网络。</p><p><strong>引入fresh content的bias的作用？</strong></p><p>这里比较特别的一个特征是”example age“。每一秒中，YouTube都有大量视频被上传，推荐这些最新视频对于YouTube来说是极其重要的。同时，通过观察历史数据发现，用户更倾向于推荐相关度不高但最新（fresh）的视频。视频的点击率实际上都会受fresh的影响，训练的时候加入example age ，为的就是“显式”的告诉模型example age对点击的影响。在预测的时候，example age置0，就排除了这个特征对模型的影响。类似于广告，广告在展示列表中的位置，对广告的点击概率有非常大影响，排名越靠前的广告，越容易被点击，在产生训练样本的时候，把展示位置作为特征放在样本里面，并且在使用模型的时候，把展示位置特征统一置为0。</p><p>假设一个视频是十天前发布的，许多用户在当前观看了该视频，那么在当天会产生许多Sample Log，而在后面的九天里，观看记录不多，Sample Log也很少。如果我们没有加入Example Age这个特征的话，无论何时训练模型，这个视频对应的分类概率都是差不多的，但是如果我们加入这个特征，模型就会知道，如果这条记录是十天前产生的话，该视频会有很高的分类概率，如果是最近几天产生的话，分类概率应该低一些，这样可以更加逼近实际的数据。实验结果也证明了这一点：</p><div align="center"><a href="https://imgtu.com/i/4BzKQe" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4BzKQe.png" alt="4BzKQe.png" border="0" width="65%/"></a></div><p>训练样本的产生方面，正样本是用户所有完整观看过的视频，其余可以视作负样本。同时，针对每一个用户的观看记录，都生成了固定数量的训练样本，这样，每个用户在损失函数中的地位都是相等的，防止一小部分超级活跃用户主导损失函数。</p><p>在对待用户的搜索和观看历史时，Youtube并没有选择时序模型，而是完全摒弃了序列关系，采用求平均的方式对历史记录进行了处理。这是因为考虑时序关系，用户的推荐结果将过多受最近观看或搜索的一个视频的影响。文章中给出一个例子，如果用户刚搜索过“taylor swift”，你就把用户主页的推荐结果大部分变成taylor swift有关的视频，这其实是非常差的体验。为了综合考虑之前多次搜索和观看的信息，YouTube丢掉了时序信息，将用户近期的历史纪录等同看待。</p><p><a href="https://imgtu.com/i/4DSqNq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4DSqNq.png" alt="4DSqNq.png" border="0"></a></p><p>在处理测试集时，Youtube采用的是图(b)的方式。图(a)是held-out方式，利用上下文信息预估中间的一个视频；图(b)是predicting next watch的方式，则是利用上文信息，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。而且只留最后一次观看行为做测试集主要是为了避免引入future information，产生与事实不符的数据穿越。</p><p>输出方面，因为Youtube将推荐问题建模成一个“超大规模多分类”问题。即在时刻t，用户U（上下文信息C）会观看视频i的概率（每个具体的视频视为一个类别，i即为一个类别），所以输出应该是一个在所有candidate video上的概率分布，自然是一个多分类问题。</p><p>同时，输出分为线上和离线训练两个部分。离线训练阶段输出层为softmax层，输出3.1中公式表达的概率。对于在线服务来说，有严格的性能要求，Youtube没有重新跑一遍模型，而是通过保存用户的embedding和视频的embedding，通过最近邻搜索的方法得到top N（approx topN，使用hash的方法来得到近似的topN）的结果。</p><h4 id="精排模型"><a href="#精排模型" class="headerlink" title="精排模型"></a>精排模型</h4><div align="center"><a href="https://imgtu.com/i/4D9nzT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/24/4D9nzT.png" alt="4D9nzT.png" border="0" width="75%/"></a></div>排序过程是对生成的候选集做进一步细粒度的排序，模型架构与粗排模型基本一致，区别在于特征工程部分，图中从左至右的特征依次是：1. **impression video ID embedding**: 当前要计算的video的embedding2. **watched video IDs average embedding**: 用户观看过的最后N个视频embedding的average pooling3. **language embedding**: 用户语言的embedding和当前视频语言的embedding4. **time since last watch**: 自上次观看同channel视频的时间5. **previous impressions**: 该视频已经被曝光给该用户的次数后面两个特征很好地引入了对用户行为的观察，第4个特征是用户上次观看同频道时间距现在的时间间隔,从用户的角度想一想，假如我们刚看过“DOTA经典回顾”这个channel的视频，我们很大概率是会继续看这个channel的视频的，那么该特征就很好的捕捉到了这一用户行为。第5个特征previous impressions则一定程度上引入了exploration的思想，避免同一个视频持续对同一用户进行无效曝光。尽量增加用户没看过的新视频的曝光可能性。在**特征处理**部分分为离散与连续变量：**离散变量*** 在进行video embedding的时候，只保留用户最常点击的N个视频的embedding，剩余的长尾视频的embedding直接用0向量代替。把大量长尾的video截断掉，主要还是为了节省online serving中宝贵的内存资源。当然从模型角度讲，低频video的embedding的准确性不佳是另一个“截断掉也不那么可惜”的理由。* 对于相同域的特征可以共享embedding，比如用户点击过的视频ID，用户观看过的视频ID，用户收藏过的视频ID等等，这些公用一套embedding可以使其更充分的学习，同时减少模型的大小，加速模型的训练。**连续变量*** 主要是归一化处理，同时还把归一化后的的根号和平方作为网络输入，以期能使网络能够更容易得到特征的次线性（sub-linear）和（super-linear）超线性函数。（引入了特征的非线性）。在精排模型的**训练**阶段，模型采用了用户的期望观看时间作为优化目标，所以如果简单使用LR就无法引入正样本的观看时间信息。因此采用weighted LR，将观看时间$T_i$作为正样本的权重，对于负样本，权重是单位权重(可以认为是1)。在线上serving中使用$e^{w^Tx+b}$做预测可以直接得到expected watch time的近似。这里引出一个问题：1. 在模型serving过程中又为何没有采用sigmoid函数预测正样本的probability，而是使用$e^{w^Tx+b}$这一指数形式预测用户观看时长？   > 回到LR的定义：   > $$   > y=\frac{1}{1+e^{-w^Tx}}   > $$   > 对于二分类问题：   > $$   > P(y=1|x)=\sigma(x) \\   > P(y=0|x)=1-\sigma(x)   > $$   > 一件事情的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是p，那么该事件的odds是$\frac{p}{1-p}$，对于LR而言：   > $$   > \frac{\frac{1}{1+e^{-w^Tx}}}{1-\frac{1}{1+e^{-w^Tx}}}=e^{w^Tx}   > $$   > 所以$e^{w^Tx+b}$求的就是LR形式下的odds。   >   > Weighted LR中的单个样本的weight，并不是让这个样本发生的概率变成了weight倍，而是让这个样本，对预估的影响(也就是loss)提升了weight倍。因为观看时长的几率=$\frac{\sum T_i}{N-k}$，其中k为正样本的个数，非wieght的odds可以直接看成N+/N-，因为wieghted的lr中，N+变成了weight倍，N-没变，还是1倍，所以直接可得后来的odds是之前odds的weight倍。   >   > 也就是说样本i的odds变成了下面的式子，由于在视频推荐场景中，用户打开一个视频的概率p往往是一个很小的值，且YouTube采用了用户观看时长$T_i$作为权重，$w_i=T_i$，所以有：   > $$   > odds(i)=\frac{w_ip}{1-w_ip}\approx w_ip=T_ip   > $$   > 这就是用户观看某视频的期望时长的计算式。所以模型serving部分使用的是这个形式，经历了$e^{w^Tx+b}\rightarrow odds\rightarrow 用户期望观看时长$的过程。## Deep & Cross Network for Ad Click Predictions[ADKDD'17]本节主要参考[玩转企业级Deep&Cross Network模型你只差一步](https://zhuanlan.zhihu.com/p/43364598)、[揭秘 Deep & Cross : 如何自动构造高阶交叉特征](https://zhuanlan.zhihu.com/p/55234968)### 解决的问题这篇论文是Google对 Wide & Deep工作的一个后续研究，文中提出 Deep & Cross Network，将Wide部分替换为由特殊网络结构实现的Cross，**自动构造有限高阶的交叉特征**，并学习对应权重，从而在一定程度上告别人工特征叉乘，说一定程度是因为文中出于模型复杂度的考虑，仍是仅对sparse特征对应的embedding作自动叉乘，但这仍是一个有益的创新。Wide & Deep 的结构能同时实现Memorization与Generalization，但是在Wide部分，仍然需要人工地设计特征叉乘。面对高维稀疏的特征空间、大量的可组合方式，基于人工先验知识虽然可以缓解一部分压力，但仍需要不小的人力和尝试成本，并且很有可能遗漏一些重要的交叉特征。FM可以自动组合特征，但也仅限于二阶叉乘。能否告别人工组合特征，并且自动学习高阶的特征组合呢？Deep & Cross 即是对此的一个尝试。### 做法及创新#### 核心思想DCN的结构如下图所示，由嵌入和堆叠层、交叉网络、深度网络以及组合输出网络四部分构成：<a href="https://imgtu.com/i/4skSbT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/25/4skSbT.png" alt="4skSbT.png" border="0"></a><div align="center"><a href="https://imgtu.com/i/4rJKYR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/25/4rJKYR.png" alt="4rJKYR.png" border="0" width="75%/"></a></div><p><strong>嵌入和堆叠层</strong></p><p>这部分和前面介绍的模型做法大同小异，就是对于one-hot编码的离散型特征，通过嵌入来将输入的高维特征压缩到低维稠密向量，最后将嵌入向量与归一化的连续型特征进行堆叠，形成模型的输入。</p><p><strong>交叉网络</strong></p><p>交叉网络的每一层形式为：</p><script type="math/tex; mode=display">x_{l+1}=x_0x^T_lw_l+b_l+x_l=f(x_l,w_l,b_l)+x_l</script><ol><li>每层的神经元个数都相同，都等于输入$x_0$的维度$d$，也即每层的输入输出维度都是相等的。</li><li>受残差网络（Residual Network）结构启发，每层的函数f拟合的是$x_{l+1}-x_l$的残差，残差网络有很多优点，其中一点是处理梯度消失的问题，使网络可以“更深”。</li></ol><p>那么交叉网络为什么能够自动构造有限高阶的交叉特征呢？以一个二层的交叉网络为例，其中$x_0=[x_{0,1};x_{0,2}]$，另各层的$b_i=0$：</p><p><a href="https://imgtu.com/i/4rtbWR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/25/4rtbWR.png" alt="4rtbWR.png" border="0"></a></p><p>最后得到$y_{cross}=x_2^T*w_{cross}$，可以看到$x_1$包含了原始特征 $x_{0,1}$、$x_{0,2}$从一阶到二阶的所有可能叉乘组合，而 $x_2$包含了其从一阶到三阶的所有可能叉乘组合。从这个例子可以看出DCN的特点：</p><ul><li><strong>有限高阶</strong>：叉乘<strong>阶数由网络深度决定</strong>，深度$L_c$对应最高阶$L_c+1$的叉乘</li><li><strong>自动叉乘</strong>：Cross输出包含了原始特征从一阶（即本身）到$L_c+1$阶的<strong>所有叉乘组合，</strong>而模型参数量仅仅随输入维度成<strong>线性增长</strong>：$2<em>d</em>L_c$</li><li><strong>参数共享</strong>：不同叉乘项对应的权重不同，但并非每个叉乘组合对应独立的权重（指数数量级）， 通过参数共享，Cross有效<strong>降低了参数量</strong>。此外，参数共享还使得模型有更强的<strong>泛化性</strong>和<strong>鲁棒性</strong>。例如，如果独立训练权重，当训练集中$x_i\not =0 \land x_j\not =0$这个叉乘特征没有出现 ，对应权重肯定是零，而参数共享则不会，类似地，数据集中的一些噪声可以由大部分正常样本来纠正权重参数的学习</li></ul><p>训练部分，模型的Deep 部分如上图右侧部分所示，DCN拼接Cross和Deep的输出，采用logistic loss作为损失函数，进行联合训练，这些细节与Wide &amp; Deep几乎是一致的，在这里不再展开论述。另外，文中也在目标函数中加入L2正则防止过拟合。</p><h2 id="Attentional-Factorization-Machines-IJCAI’17"><a href="#Attentional-Factorization-Machines-IJCAI’17" class="headerlink" title="Attentional Factorization Machines[IJCAI’17]"></a>Attentional Factorization Machines[IJCAI’17]</h2><p>本节主要参考<a href="https://zhuanlan.zhihu.com/p/395140453" target="_blank" rel="noopener">推荐算法精排模型AFM</a></p><h3 id="解决的问题-5"><a href="#解决的问题-5" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FM在做特征交互时，对所有交叉项赋予相同的权重，这可能是不够准确的，不相关的特征的交叉项可能还会引来噪声，论文通过attention机制学习各特征交叉项的重要程度进行加权求和。</p><h3 id="做法及创新-6"><a href="#做法及创新-6" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>前面介绍DeepFM时说到它是典型的一种DNN与FM融合的并行结构，而本节的AFM就是典型的一种串行结构：</p><p><a href="https://imgtu.com/i/4skFPJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/25/4skFPJ.png" alt="4skFPJ.png" border="0"></a></p><h4 id="核心思想-4"><a href="#核心思想-4" class="headerlink" title="核心思想"></a>核心思想</h4><p>AFM的目的也是像FFM一样区分同一特征与不同特征组合时的相互作用，只是不再划分为field而是通过一个注意力网络学习得到权重，总体参数量增加不明显。</p><p>回顾FM的预测公式：</p><script type="math/tex; mode=display">\hat{y}(x):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}<v_{i}, v_{j}>x_{i} x_{j}</script><p>其中需要学习的参数为：</p><script type="math/tex; mode=display">w_{0} \in \mathbb{R}, \quad w \in \mathbb{R}^{n}, \quad V \in \mathbb{R}^{n \times k}</script><p>它们的含义为：</p><ul><li>$w_0$表示全局的偏差；</li><li>$w_i$表示第i个特征的强度；</li><li>$w_{ij}$表示第i个特征和第j个特征之间的交互，在实际参数学习中不是直接学习交互特征的权重参数$w_{ij}$的，而是通过<strong>学习因式分解参数</strong>来学习交互特征的参数。</li></ul><p><a href="https://imgtu.com/i/4sk3RA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/25/4sk3RA.png" alt="4sk3RA.png" border="0"></a></p><p>输入层和embedding层与FM模型是一样的，其中对于输入特征都采取了稀疏表示，即将所有的非零特征都嵌入到dense特征。嵌入后的表示为$v_ix_i$，而FM中二次项的系数分解为两个特征i和j的嵌入向量的叉乘，$w_{ij}=v_i^Tv_j$，这里$v_i$就是特征i的嵌入向量。</p><p><strong>Pair-wise交互层</strong></p><p>这一层的目的是在神经网络中表达FM的计算逻辑：</p><script type="math/tex; mode=display">\hat{y}=\mathbf{p}^{T} \sum_{(i, j) \in \mathcal{R}_{x}}\left(\mathbf{v}_{i} \odot \mathbf{v}_{j}\right) x_{i} x_{j}+b</script><p>向量p置为全1以及b设为0就回退到传统的FM模型。</p><p><strong>Attention-based池化层</strong></p><script type="math/tex; mode=display">f_{\text {Att }}\left(f_{P I}(\mathcal{E})\right)=\sum_{(i, j) \in \mathcal{R}_{x}} a_{i j}\left(\mathbf{v}_{i} \odot \mathbf{v}_{j}\right) x_{i} x_{j}</script><p>跟传统attention一样，$a_{ij}$就是表示特征i和j的交互在进行预测时的重要程度，可以直接通过最小化loss函数去学习$a_{ij}$，虽然看起来是可行的，但是这又会碰到之前的问题：当某个交互特征没有出现在样本中时，就没法学习某个交互特征的attention分数了。为了解决这个泛化能力方面的问题，我们使用MLP网络去参数化这个attention分数，该MLP网络称之为attention network。attention network的输入是嵌入后的两个特征的交互向量：</p><script type="math/tex; mode=display">\begin{aligned}a_{i j}^{\prime} &=\mathbf{h}^{T} ReL U\left(\mathbf{W}\left(\mathbf{v}_{i} \odot \mathbf{v}_{j}\right) x_{i} x_{j}+\mathbf{b}\right) \\a_{i j} &=\frac{\exp \left(a_{i j}^{\prime}\right)}{\sum_{(i, j) \in \mathcal{R}_{x}} \exp \left(a_{i j}^{\prime}\right)}\end{aligned}</script><p>Attention-based 池化层的输出是一个k维的向量，它在embedding空间中通过区分出各特征交互的重要性，来压缩所有的特征交互，然后将这些映射到最终的预测结果上面。</p><p>AFM模型在防止过拟合上的做法：</p><ul><li>dropout方式是通过防止神经元之间的共现性从而防止过拟合。由于AFM模型中会学习所有的特征之间的二阶交互特征，因此更加容易导致模型学习特征之间的共现性从而更容易导致过拟合，因此在pair-wise交互层使用了dropout方法来避免共现性。</li><li>对于AFM模型中的attention network，它是一个单层的MLP网络，这里使用L2正则化来防止过拟合，对于attention network，不选择dropout防止过拟合。</li></ul><h2 id="Practical-Lessons-from-Predicting-Clicks-on-Ads-at-Facebook-ADKDD’14"><a href="#Practical-Lessons-from-Predicting-Clicks-on-Ads-at-Facebook-ADKDD’14" class="headerlink" title="Practical Lessons from Predicting Clicks on Ads at Facebook[ADKDD’14]"></a>Practical Lessons from Predicting Clicks on Ads at Facebook[ADKDD’14]</h2><h3 id="解决的问题-6"><a href="#解决的问题-6" class="headerlink" title="解决的问题"></a>解决的问题</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;借这篇博客记录看工业界推荐系统论文的心得。&lt;/p&gt;
    
    </summary>
    
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode笔记</title>
    <link href="http://Bithub00.com/2021/09/07/Leetcode%E7%AC%94%E8%AE%B0/"/>
    <id>http://Bithub00.com/2021/09/07/Leetcode笔记/</id>
    <published>2021-09-07T07:59:25.753Z</published>
    <updated>2021-09-28T13:49:50.786Z</updated>
    
    <content type="html"><![CDATA[<p>借这篇博客记录并复习Leetcode做题时的笔记。</p><a id="more"></a><h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><h3 id="模板及细节"><a href="#模板及细节" class="headerlink" title="模板及细节"></a>模板及细节</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">0</span>, right = ...;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(...) &#123;</span><br><span class="line">        <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] == target) &#123;</span><br><span class="line">            ...</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; target) &#123;</span><br><span class="line">            left = ...</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &gt; target) &#123;</span><br><span class="line">            right = ...</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ...;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>while循环中&lt;与&lt;=的选择</p><p>初始化right赋值是nums.length-1时，说明搜索区间为[left,right]左闭右闭区间，初始化right赋值是nums.length时，说明搜索区间为[left,right)左闭右开区间，因为nums.length是越界的。当while(left&lt;right)退出时，left==right对应的索引漏搜索了一次，所以需要在退出循环后额外进行一次判断。</p></li><li><p>搜索区间的收缩</p><p>当搜索区间为[left,right]时，如果索引mid不是要找的target时，需要将它从搜索区间中去除，下一个要搜索的区间自然变成了[left,mid-1]和[mid+1,right]。当搜索区间为[left,right)时，去掉mid后的下一个搜索区间变成了[left,mid-1]和[mid+1,right)，所以此时left=mid+1而right=mid。</p></li></ul><h3 id="在排序数组中查找元素的第一个和最后一个位置"><a href="#在排序数组中查找元素的第一个和最后一个位置" class="headerlink" title="在排序数组中查找元素的第一个和最后一个位置"></a>在排序数组中查找元素的第一个和最后一个位置</h3><p><a href="https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/" target="_blank" rel="noopener">题目</a>：给定一个按照升序排列的整数数组nums，和一个目标值target。找出给定目标值在数组中的开始位置和结束位置。如果数组中不存在目标值 target，返回 [-1, -1]。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这道题目是要寻找给定元素在数组中的左侧边界和右侧边界</span></span><br><span class="line"><span class="comment">//因为元素在数组中可能重复，在nums[mid]==target时不能马上break，如果是寻找左侧边界就令right=mid在[left,mid)中继续寻找，如果是寻找右侧边界就令left=mid+1在[mid+1,right)中继续寻找</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">left_bound</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.size();</span><br><span class="line">        <span class="comment">//根据left和right的取值搜索区间为[left,right)</span></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] == target) &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt; target) &#123;</span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &gt; target) &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//搜索完left==right，取值范围为[0,nums.size()]，需要判断没找到target的情况</span></span><br><span class="line">        <span class="comment">//返回值left可以理解为在target左侧有left个元素</span></span><br><span class="line">        <span class="comment">//特别如果target大于数组中所有元素，left==nums.size()，需要先判断越界</span></span><br><span class="line">        <span class="keyword">if</span>(left == nums.size() || nums[left] != target) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">right_bound</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.size();</span><br><span class="line">        <span class="comment">//根据left和right的取值搜索区间为[left,right)</span></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] == target) &#123;</span><br><span class="line">                <span class="comment">//while循环结束时，nums[left]一定不等于target，而nums[left-1]可能是target</span></span><br><span class="line">                left = mid+<span class="number">1</span>; </span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt; target) &#123;</span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &gt; target) &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//搜索完left==right，取值范围为[0,nums.size()]，需要判断没找到target的情况</span></span><br><span class="line">        <span class="comment">//返回值left可以理解为在target左侧有left个元素</span></span><br><span class="line">        <span class="comment">//特别如果target小于数组中所有元素，left==0，需要先判断越界</span></span><br><span class="line">        <span class="keyword">if</span>(left == <span class="number">0</span> || nums[left<span class="number">-1</span>] != target) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">return</span> left<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; searchRange(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">        <span class="keyword">int</span> left = left_bound(nums, target);</span><br><span class="line">        <span class="keyword">int</span> right = right_bound(nums, target);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&#123;left, right&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="搜索插入位置"><a href="#搜索插入位置" class="headerlink" title="搜索插入位置"></a>搜索插入位置</h3><p><a href="https://leetcode-cn.com/problems/search-insert-position/" target="_blank" rel="noopener">题目</a>：给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">searchInsert</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.size();</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] == target) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &gt; target) &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt; target) &#123;</span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> right;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="x的平方根"><a href="#x的平方根" class="headerlink" title="x的平方根"></a>x的平方根</h3><p><a href="https://leetcode-cn.com/problems/sqrtx/" target="_blank" rel="noopener">题目</a>：给你一个非负整数 x ，计算并返回 x 的平方根。由于返回类型是整数，结果只保留整数部分，小数部分将被舍去。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始搜索区间为[1,x]左闭右闭，因此while循环条件为left&lt;=right</span></span><br><span class="line"><span class="comment">//循环跳出时left=right+1，right*right为小于x的最大平方数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">1</span>, right = x;</span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">long</span> <span class="keyword">long</span> guess = (<span class="keyword">long</span> <span class="keyword">long</span>)mid*mid;</span><br><span class="line">            <span class="keyword">if</span>(guess == x) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(guess &gt; x) &#123;</span><br><span class="line">                right = mid<span class="number">-1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(guess &lt; x) &#123;</span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> right;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="山脉数组的峰顶索引"><a href="#山脉数组的峰顶索引" class="headerlink" title="山脉数组的峰顶索引"></a>山脉数组的峰顶索引</h3><p><a href="https://leetcode-cn.com/problems/peak-index-in-a-mountain-array/" target="_blank" rel="noopener">题目</a>：给你由整数组成的山脉数组 arr，返回任何满足 arr[0] &lt; arr[1] &lt; … arr[i - 1] &lt; arr[i] &gt; arr[i + 1] &gt; … &gt; arr[arr.length - 1] 的下标i。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始搜索区间为[1,arr.size()-1]左闭右闭，因此while循环条件为left&lt;=right</span></span><br><span class="line"><span class="comment">//山峰左侧满足单调递增，右侧满足单调递减</span></span><br><span class="line"><span class="comment">//如果左侧不满足单调递增，将搜索区间收缩为[left,mid]，右侧同理，收缩为[mid,right]</span></span><br><span class="line"><span class="comment">//不排除mid是因为mid虽然自身做不了山峰但可以判断下一处山峰，例如[3,5,3,2,0]，以左侧为例，第一次mid=2，arr[1]&gt;arr[2]，如果right=mid-1区间只剩下[3,5]，这时候5无法作为山峰，右侧同理</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">peakIndexInMountainArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = arr.size()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> answer;</span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(arr[mid] &lt; arr[mid<span class="number">-1</span>]) right = mid;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(arr[mid] &lt; arr[mid+<span class="number">1</span>]) left = mid;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                answer = mid;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> answer;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="旋转数组的最小数字"><a href="#旋转数组的最小数字" class="headerlink" title="旋转数组的最小数字"></a>旋转数组的最小数字</h3><p><a href="https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/" target="_blank" rel="noopener">题目</a>：把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转，输出旋转数组的最小元素。例如，数组[3,4,5,1,2]为[1,2,3,4,5]的一个旋转，该数组的最小值为1。</p><div align="center"><a href="https://imgtu.com/i/42vUu6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/09/27/42vUu6.png" alt="42vUu6.png" border="0" width="65%/"></a></div><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//寻找旋转数组的最小元素即为寻找右排序数组的首个元素</span></span><br><span class="line"><span class="comment">//当nums[mid]&gt;nums[right]时，一定在左排序数组里，旋转点一定在[mid+1,right]区间内</span></span><br><span class="line"><span class="comment">//当nums[mid]&lt;nums[right]时，一定在右排序数组里，旋转点一定在[left,mid]区间内</span></span><br><span class="line"><span class="comment">//当nums[mid]==nums[right]时，无法判断，缩小搜索区间为[left,right-1]</span></span><br><span class="line"><span class="comment">//与right比较是因为right初始值肯定在右排序数组中，而left如果原数组未旋转在右排序，旋转则在左排序</span></span><br><span class="line"><span class="comment">//初始搜索区间为[1,numbers.size()-1]左闭右闭，因此while循环条件为left&lt;=right</span></span><br><span class="line"><span class="comment">//退出循环时left=right+1，此时numbers[right]一定不是旋转点，right指向的是旋转点左侧的位置</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; numbers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = numbers.size()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(numbers[mid] &gt; numbers[right]) left = mid+<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(numbers[mid] &lt; numbers[right]) right = mid;</span><br><span class="line">            <span class="keyword">else</span> right--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> numbers[left];<span class="comment">//return numbers[right+1]</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="0-n-1中缺失的数字"><a href="#0-n-1中缺失的数字" class="headerlink" title="0~n-1中缺失的数字"></a>0~n-1中缺失的数字</h3><p><a href="https://leetcode-cn.com/problems/que-shi-de-shu-zi-lcof/" target="_blank" rel="noopener">题目</a>：一个长度为n-1的递增排序数组中的所有数字都是唯一的，并且每个数字都在范围0～n-1之内。在范围0～n-1内的n个数字中有且只有一个数字不在该数组中，请找出这个数字。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缺失数字的右侧满足nums[mid]&gt;mid，左侧满足nums[mid]=mid</span></span><br><span class="line"><span class="comment">//初始搜索区间为[1,numbers.size())左闭右开，因此while循环条件为left&lt;right</span></span><br><span class="line"><span class="comment">//当mid位于右侧时，缩小搜索区间为[left,mid)，当mid位于左侧时，缩小搜索区间为[mid+1,right)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">missingNumber</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.size();</span><br><span class="line">        <span class="keyword">while</span>(left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; mid) &#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt;= mid) &#123;</span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始搜索区间为[1,numbers.size()-1]左闭右闭，因此while循环条件为left&lt;=right</span></span><br><span class="line"><span class="comment">//当mid位于右侧时，缩小搜索区间为[left,mid-1]，当mid位于左侧时，缩小搜索区间为[mid+1,right]</span></span><br><span class="line"><span class="comment">//退出循环时left==right+1，根据它们的更新式，此时nums[right]&lt;=mid，而nums[left]&gt;mid，right指向缺失数字左边的位置，而left正好指向缺失数字的位置</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">missingNumber</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.size()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; mid) &#123;</span><br><span class="line">                right = mid<span class="number">-1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[mid] &lt;= mid) &#123;</span><br><span class="line">                left = mid+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="搜索旋转排序数组"><a href="#搜索旋转排序数组" class="headerlink" title="搜索旋转排序数组"></a>搜索旋转排序数组</h3><p><a href="">题目</a>：整数数组nums按升序排列，数组中的值互不相同。 在传递给函数之前，nums在预先未知的某个下标 k（0 &lt;= k &lt; nums.length）上进行了旋转。给你旋转后的数组nums和一个整数target ，如果 nums中存在这个目标值 target ，则返回它的下标，否则返回 -1</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="搜索旋转数组"><a href="#搜索旋转数组" class="headerlink" title="搜索旋转数组"></a>搜索旋转数组</h3><p><a href="https://leetcode-cn.com/problems/search-rotate-array-lcci/" target="_blank" rel="noopener">题目</a>：给定一个排序后的数组，包含n个整数，但这个数组已被旋转过很多次了，次数不详。请编写代码找出数组中的某个元素，假设数组元素原先是按升序排列的。若有多个相同元素，返回索引值最小的一个。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//旋转数组的特点是，数组中至少有一半的元素是有序的，每次二分时只需要判断target是否在有序的一侧</span></span><br><span class="line"><span class="comment">//初始搜索区间为[1,arr.size()-1]左闭右闭，因此while循环条件为left&lt;=right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">search</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> right  = arr.size() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right)&#123;</span><br><span class="line">            <span class="comment">//当target不存在</span></span><br><span class="line">            <span class="keyword">if</span>(left == right &amp;&amp; arr[left] != target)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">            &#125; </span><br><span class="line">            <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(arr[mid] == target)&#123;</span><br><span class="line">              <span class="comment">//找到target的同时还需要找到其中索引值最小的一个</span></span><br><span class="line">                <span class="keyword">while</span>(mid &gt; <span class="number">0</span> &amp;&amp; arr[mid] == arr[mid - <span class="number">1</span>])&#123;</span><br><span class="line">                    mid--;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(arr[<span class="number">0</span>] == target) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//arr[mid] == arr[left] 则缩短数组左边界</span></span><br><span class="line">            <span class="keyword">while</span>(left &lt; mid &amp;&amp; arr[mid] == arr[left])&#123;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//arr[mid] == arr[right] 则缩短数组右边界</span></span><br><span class="line">            <span class="keyword">while</span>(right &gt; mid &amp;&amp; arr[mid] == arr[right])&#123;</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//左边有序</span></span><br><span class="line">            <span class="keyword">if</span>(arr[mid] &gt;= arr[left])&#123;</span><br><span class="line">                <span class="comment">//目标在左边部分，缩减搜索区间为[left,mid-1]</span></span><br><span class="line">                <span class="keyword">if</span>(target &gt;= arr[left] &amp;&amp; target &lt;= arr[mid])&#123;</span><br><span class="line">                    right = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//目标在右边部分，缩减搜索区间为[mid+1,right]</span></span><br><span class="line">                    left = mid + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">              <span class="comment">//右边有序，arr[mid] &lt;= arr[right]</span></span><br><span class="line">                <span class="comment">//目标在右边部分，缩减搜索区间为[mid+1,right]</span></span><br><span class="line">                <span class="keyword">if</span>(target &gt;= arr[mid] &amp;&amp; target &lt;= arr[right])&#123;</span><br><span class="line">                    left = mid + <span class="number">1</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//目标在左边部分，缩减搜索区间为[left,mid-1]</span></span><br><span class="line">                    right = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><h3 id="中缀表达式转后缀表达式"><a href="#中缀表达式转后缀表达式" class="headerlink" title="中缀表达式转后缀表达式"></a>中缀表达式转后缀表达式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1. 数字直接输出</span></span><br><span class="line"><span class="comment">//2. 运算符号</span></span><br><span class="line"><span class="comment">//若当前运算符优先度较高，进栈，否则栈顶出栈输出，直到当前运算符优先度高于栈顶运算符，再进栈</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; RPN;<span class="comment">//后缀表达式（波兰表达式）</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isNumber</span><span class="params">(<span class="built_in">string</span>&amp; c)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> !((c == <span class="string">"+"</span>) || (c == <span class="string">"-"</span>) || (c == <span class="string">"*"</span>) || (c == <span class="string">"/"</span>));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InToRPN</span><span class="params">(<span class="built_in">string</span>&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="built_in">string</span>&gt; cs;</span><br><span class="line">    <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; record(&#123;&#123;<span class="string">"+"</span>, <span class="number">1</span>&#125;, &#123;<span class="string">"-"</span>, <span class="number">1</span>&#125;, &#123;<span class="string">"*"</span>, <span class="number">2</span>&#125;, &#123;<span class="string">"/"</span>, <span class="number">2</span>&#125;&#125;);</span><br><span class="line">    <span class="function"><span class="built_in">istringstream</span> <span class="title">iss</span><span class="params">(s)</span></span>;</span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">char</span> op;</span><br><span class="line">    <span class="keyword">while</span>(iss &gt;&gt; num) &#123;</span><br><span class="line">        RPN.push_back(to_string(num));</span><br><span class="line">        iss &gt;&gt; op;</span><br><span class="line">        <span class="keyword">if</span>(!iss) <span class="keyword">break</span>;<span class="comment">//读取完表达式最后一个整数，这时的op不需要再使用</span></span><br><span class="line">        <span class="function"><span class="built_in">string</span> <span class="title">token</span><span class="params">(<span class="number">1</span>, op)</span></span>;</span><br><span class="line">        <span class="keyword">if</span>(cs.empty()) cs.push(token);</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">int</span> priority = record[token];</span><br><span class="line">            <span class="keyword">if</span>(priority &gt; record[cs.top()]) cs.push(token);</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">while</span>(!cs.empty() &amp;&amp; priority &lt;= record[cs.top()]) &#123;</span><br><span class="line">                    RPN.push_back(cs.top());</span><br><span class="line">                    cs.pop();</span><br><span class="line">                &#125;</span><br><span class="line">                cs.push(token);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(!cs.empty()) &#123;</span><br><span class="line">        RPN.push_back(cs.top());</span><br><span class="line">        cs.pop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = nums.size();</span><br><span class="line">        <span class="keyword">int</span> i, j;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> temp = nums[i];</span><br><span class="line">            <span class="keyword">for</span>(j = i; j &gt; <span class="number">0</span> &amp;&amp; nums[j<span class="number">-1</span>] &gt; temp; j--) &#123;</span><br><span class="line">                nums[j] = nums[j<span class="number">-1</span>];<span class="comment">//升序排列，将比temp大的数往数组后面移动</span></span><br><span class="line">            &#125;</span><br><span class="line">            nums[j] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="基尔排序"><a href="#基尔排序" class="headerlink" title="基尔排序"></a>基尔排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = nums.size();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> gap = size / <span class="number">2</span>; gap &gt; <span class="number">0</span>; gap /= <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = gap; i &lt; size; i++) &#123;</span><br><span class="line">                <span class="keyword">int</span> temp = nums[i];</span><br><span class="line">                <span class="keyword">int</span> j;</span><br><span class="line">                <span class="keyword">for</span>(j = i; j &gt;= gap &amp;&amp; nums[j-gap] &gt; temp; j-= gap) &#123;</span><br><span class="line">                    nums[j] = nums[j-gap];</span><br><span class="line">                &#125;</span><br><span class="line">                nums[j] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = nums.size();</span><br><span class="line">        <span class="keyword">bool</span> bubble = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">while</span>(bubble) &#123;</span><br><span class="line">            bubble = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size<span class="number">-1</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[i] &gt; nums[i+<span class="number">1</span>]) &#123;</span><br><span class="line">                    swap(nums[i], nums[i+<span class="number">1</span>]);</span><br><span class="line">                    bubble = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列</span></span><br><span class="line"><span class="comment">//2. 设定两个指针，最初位置分别为两个已经排序序列的起始位置</span></span><br><span class="line"><span class="comment">//3. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置</span></span><br><span class="line"><span class="comment">//4. 重复步骤3直到某一指针到达序列尾</span></span><br><span class="line"><span class="comment">//5. 将另一序列剩下的所有元素直接复制到合并序列尾</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> l, <span class="keyword">int</span> m, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i, j, k;</span><br><span class="line">        <span class="keyword">int</span> n1 = m-l+<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> n2 = r-m;</span><br><span class="line">        <span class="keyword">int</span> L[n1], R[n2];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n1; i++)</span><br><span class="line">            L[i] = nums[l+i];</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; n2; j++)</span><br><span class="line">            R[j] = nums[m+<span class="number">1</span>+j];</span><br><span class="line">        </span><br><span class="line">        i = j = <span class="number">0</span>;</span><br><span class="line">        k = l;</span><br><span class="line">        <span class="keyword">while</span>(i &lt; n1 &amp;&amp; j &lt; n2) &#123;</span><br><span class="line">            <span class="keyword">if</span>(L[i] &lt;= R[j]) </span><br><span class="line">                nums[k++] = L[i++];</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                nums[k++] = R[j++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(i &lt; n1) nums[k++] = L[i++];</span><br><span class="line">        <span class="keyword">while</span>(j &lt; n2) nums[k++] = R[j++];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">int</span> m = l + (r-l) / <span class="number">2</span>;</span><br><span class="line">            mergeSort(nums, l, m);</span><br><span class="line">            mergeSort(nums, m+<span class="number">1</span>, r);</span><br><span class="line">            merge(nums, l, m, r);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = nums.size();</span><br><span class="line">        mergeSort(nums, <span class="number">0</span>, size<span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p><img src="https://pic2.zhimg.com/80/v2-5d6119c9801eadb83402ef68f6d4b689_720w.jpg" alt="img"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//最大堆中的最大元素值出现在根结点（堆顶），且堆中每个父节点的元素值都大于等于其孩子结点</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> n, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> largest = i;<span class="comment">//将最大元素设为堆顶元素</span></span><br><span class="line">        <span class="keyword">int</span> l = <span class="number">2</span>*i + <span class="number">1</span>;<span class="comment">//左孩子结点</span></span><br><span class="line">        <span class="keyword">int</span> r = <span class="number">2</span>*i + <span class="number">2</span>;<span class="comment">//右孩子结点</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果left比root大的话</span></span><br><span class="line">        <span class="keyword">if</span>(l &lt; n &amp;&amp; nums[l] &gt; nums[largest])</span><br><span class="line">            largest = l;</span><br><span class="line">        <span class="comment">//如果right比right大的话</span></span><br><span class="line">        <span class="keyword">if</span>(r &lt; n &amp;&amp; nums[r] &gt; nums[largest])</span><br><span class="line">            largest = r;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(largest != i) &#123;</span><br><span class="line">            swap(nums[i], nums[largest]);</span><br><span class="line">            heapify(nums, n, largest);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size();</span><br><span class="line">        <span class="comment">//建立堆</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = n/<span class="number">2</span><span class="number">-1</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">            heapify(nums, n, i);</span><br><span class="line">        <span class="comment">//把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = n<span class="number">-1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            swap(nums[<span class="number">0</span>], nums[i]);</span><br><span class="line">            heapify(nums, i, <span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1. 从数列中挑出一个元素，称为 "基准"（pivot）;</span></span><br><span class="line"><span class="comment">//2. 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；</span></span><br><span class="line"><span class="comment">//3. 递归地把小于基准值元素的子数列和大于基准值元素的子数列排序；</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size();</span><br><span class="line">        <span class="keyword">int</span> i = low + (rand() % (high-low+<span class="number">1</span>));<span class="comment">//直接以第一个元素为pivot会超时</span></span><br><span class="line">        swap(nums[low], nums[i]);</span><br><span class="line">        <span class="keyword">int</span> pivot = nums[low];</span><br><span class="line">        <span class="keyword">while</span>(low &lt; high) &#123;</span><br><span class="line">            <span class="keyword">while</span>(low &lt; high &amp;&amp; nums[high] &gt;= pivot) high--;</span><br><span class="line">            nums[low] = nums[high];</span><br><span class="line">            <span class="keyword">while</span>(low &lt; high &amp;&amp; nums[low] &lt;= pivot) low++;</span><br><span class="line">            nums[high] = nums[low];</span><br><span class="line">        &#125;</span><br><span class="line">        nums[low] = pivot;</span><br><span class="line">        <span class="keyword">return</span> low;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(low &lt; high) &#123;</span><br><span class="line">            <span class="keyword">int</span> pivot = Partition(nums, low, high);</span><br><span class="line">            quickSort(nums, low, pivot<span class="number">-1</span>);</span><br><span class="line">            quickSort(nums, pivot+<span class="number">1</span>, high);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sortArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size();</span><br><span class="line">        quickSort(nums, <span class="number">0</span>, n<span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">return</span> nums;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;借这篇博客记录并复习Leetcode做题时的笔记。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://Bithub00.com/tags/Leetcode/"/>
    
      <category term="c++" scheme="http://Bithub00.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>高效计算Personalized PageRank小综述</title>
    <link href="http://Bithub00.com/2021/07/04/PPR%E5%B0%8F%E7%BB%BC%E8%BF%B0/"/>
    <id>http://Bithub00.com/2021/07/04/PPR小综述/</id>
    <published>2021-07-04T13:59:07.681Z</published>
    <updated>2021-07-05T02:18:14.833Z</updated>
    
    <content type="html"><![CDATA[<p>本学期最后一次组会我做了这两三年近似计算Personalized PageRank（PPR）的小综述，包含理论及图神经网络上的应用。</p><a id="more"></a><p>这里我直接把组会分享的PPT和讲稿放上来了。</p><h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>大家好，今天我在组会本来要分享ICDE20年一篇高效计算个性化PageRank的论文，但因为论文涉及过多方法上的细节而大家都不是PageRank近似计算的研究方向，所以我想转为介绍一下近几年顶会上对近似计算个性化 pagerank这个问题是怎么做的以及有哪些应用场景，涉及下面这些论文，其中前面四篇为理论而后面两篇是在GNN上的应用。</p><p>首先再简单回顾一下个性化PageRank。个性化PageRank简称PPR，它来自于Google创始人于1998年提出的PageRank。PageRank用于度量web网络中各网页的重要性，它的核心思想有两点：被很多网页所链接的网页重要性较高；被重要的网页所链接的网页重要性较高。原始的PageRank的计算方式是一个迭代的过程。$\pi$是n维的PageRank向量，每一维对应一个顶点的PageRank分数，下面的例子是说明PageRank的计算过程以及式子中两部分的由来，之前介绍过这里就不重复了。因为PageRank 相当于站在全局视角评价所有节点的重要程度，必须遍历所有网络上的节点，实际中很难做到，而且为了更个性化的评价，就有了PPR。它是PageRank的一种特殊形式，用于衡量图上顶点关于某一顶点的相对重要性，两者之间的关系为下面的式子。一个是相对重要性一个是全局重要性。</p><p>因为PPR的计算复杂度较高，所以有一些对它的近似估计方法，主要是下面这三种，它们作为后面要介绍的几篇论文的基本方法，这里大致地介绍一下。首先是蒙特卡洛模拟。它作为一种采样的方法，是通过以随机游走的概率来估计PPR值，至于为什么可以这样来估计，主要来自PPR的另一个定义，即两点之间的PPR值等价于从起始顶点出发的随机游走终止于目标顶点的概率。从大数定律知道，次数足够多时频率可以近似于概率，蒙特卡洛的做法就是进行若干次随机游走，将其中终止于目标顶点的比例作为PPR的近似值。做法很简单，关键在于随机游走的次数怎么选取来保证近似的精度。这里直接给出结论，证明过程要用到切尔诺夫界限，这里不展开了。</p><p>下一个是Forward Push，它为每个顶点维护两个变量，残差值和$\pi$值。在每轮迭代时，对于满足条件的顶点，首先将出度邻居的残差值增加一定比例，随后增加自己的$\pi$值并将残差值置零。直到所有顶点都不满足条件时迭代中止，将$\pi$值作为近似的PPR值。有前向类似地就有后向，做法基本一致，区别在于后向考虑的是入度邻居。两个方法的联系在于，前向的输入是源顶点s，考虑的是出度邻居；而后向的输入是目标顶点t，考虑的是入度邻居。</p><p>从刚刚的介绍可以看到，蒙特卡洛准确但效率低，因为需要大量的随机游走；Forward Push效率高但无法保证准确度。因此有学者将两者结合起来，就有了KDD17年的这篇论文FORA。Forward Push之所以无法保证准确度，是因为它返回的近似值与真实值之间本来就不保证接近，这篇论文找到了两者之间的一个联系，通过新加的尾项来建立两个值之间的关系。因为新加的尾项中$\pi(v,t)$同样是未知的，所以论文通过随机游走的办法来近似得到这个值，这就是蒙特卡洛的部分。那既然是通过随机游走来进行近似，具体要游走多少次来保证近似的精度，以及在Forward Push中的终止条件怎么确定就是两个关键的问题。这两个参数的选取都涉及证明这里就略过了。看论文的算法流程图，就是分为两个部分。Forward Push得到一个近似值，再通过蒙特卡洛提高近似值的精度。蒙特卡洛部分，蓝色下划线都是参数的取值，关键在于蓝色框内这个增量的操作。刚才说到论文通过新添加了一个尾项来建立Forward Push近似值与真实值之间的联系，而这里算法中+=这部分增量的期望是等于这个尾项的。</p><p>到这里对KDD17年这篇论文的介绍就结束了，接下来是本来组会要分享的ICDE20年的这篇论文，它主要是针对上一篇中的一些不足进行了改进，整体的算法流程没有改变。提出的第一个改进是所谓的“残差累积”，目的是为了减少Forward Push中每个顶点的转移次数。拿图中的这个简单图为例子，对于每个节点的残差值，Forward Push中涉及的操作只有两个，就是图中用红框圈出来的部分。将一部分转移给出度邻居之后置零。以顶点v1为源顶点，初始时只有v1的残差值为1。因为它的出度邻居是v2和v3，所以第一步的时候把自己的残差值转移了一部分给v2和v3，随后置零。所以这里矩阵的第一行只有v2和v3的值非零。下一步到v2，同样地把残差值转移给唯一的出度邻居v4并置零，所以第二行只有v3和v4的值非零，以此类推，可以得到矩阵每一行的值。在这个过程中，会发现顶点v2的残差值转移了两次，v1转移给它后它转给了v4，v3转移给它后它也转给了v4，这篇论文就认为这种单个顶点重复的转移是冗余的，实际上可以v1和v3都转移给v2后v2再一次性转移给v4，也就是右下角这个矩阵，这时候得到相同的结果只需要三步就可以了，虽然相对于原来的做法只减少了一步，但如果在大图上节省的操作次数是比较可观的。这种不断累积最后一次性转移出去的操作就是论文提出的第一个改进“残差累积”。</p><p>第二个改进是针对图中有自环的情况，拿图中的简单图为例，在顶点s处有一个自环，如果是原始的Forward Push，残差值会沿着s-&gt;v1-&gt;v2-&gt;s这样不断循环下去，直到某个节点的残差值不满足转移条件才会终止。论文给出的解决方法是在顶点s第一次转移后，就只接受其他顶点转移过来的残差值进行累积，不再把自己的残差值转移出去，这样就切断了自环的循环。但是对于一个大图来讲，要让除了顶点s外的其他顶点都转移完是非常耗时间的，所以论文提出在s的一个子图上完成这个残差累积的过程。这个过程结束后，子图中其他顶点的残差值和$\pi$值能够通过顶点s的残差值直接计算得到，不需要再重复进行转移的操作。之所以能这么做是因为论文证明了不管是否进行残差累积，顶点s涉及的转移操作都是一样的，区别只在于转移的残差值是多少，而这个残差值在两种操作间有种比例关系，根据这个比例关系和顶点s累积后的残差值就能直接计算出来。这部分在论文中涉及的定理引理和证明比较多，我就不在这里介绍了，论文是为了说明这种残差累积能够切断自环情况的同时还能保证结果的准确。因为按照论文对子图的定义，子图外的顶点即使满足转移条件也是不能进行残差值的转移的，所以还有额外的一步是扫描子图外的顶点进行残差值转移的操作。我们看论文的算法流程图会发现，也是可以分为两个部分，Forward Push和蒙特卡洛，只是Forward Push在上一篇的基础上进行了两点改进。<br>刚刚介绍的这两篇都是围绕Forward Push+蒙特卡洛展开，那有没有办法将另外一个基本方法也考虑进来？SIGMOD18年的这篇论文就是结合了这三者，来解决topk PPR查询问题。结合的动机是通过Backward Search进一步提升蒙特卡洛的精度。把前向和后向两个方法的表达式放在一起，把后向的表达式直接带入前向的尾项中，就是对应标红的部分，就得到了这篇论文的一个表达式。在第三项中的真实值$\pi(u,v)$同样是未知的，这时候通过蒙特卡洛来近似这个值，所以就完成了对三个基本方法的结合。前面针对的的问题都是点对点的PPR查询，给定两个顶点s和t，希望知道它们之间的PPR值。实际应用中我们可能更想知道，对于一个顶点s来说，topk PPR的顶点是哪些，典型的应用就是社交网络上的好友推荐，希望找到和用户相关的好友，而不是查询用户和某个好友之间的亲密程度。对于topk PPR查询问题，最直观的想法就是做一次全图PPR查询之后排序取topk，但这么做显然复杂度很高。类似地，求解topk查询也是希望在较快的时间内给出一个精度较高的近似解，按照定义需要满足topk中的近似值与真实值之间的误差也小于某个界限，但界限中的PPR真实值是预先不知道的，就无法确定topk PPR的一个大概范围。回到刚才KDD17年的那篇论文，它对于topk问题给出的解决方案是试错然后调整。首先假设真实值是一个较大的值进行查询，如果查询结果不如假设的大，将假设值调低进行下一次迭代，直到查询结果满足要求。在算法流程图里这个试错的过程体现在第一个红框，将$\delta$的值从1/2一直设到1/n。在候选集C有了k个顶点后，为每个顶点计算一个上限和下限来判断近似的精度是否满足要求，判断条件是图中的两个红色下划线部分，如果两个条件均满足则返回当前结果作为topk查询的结果。上下限的定义及判断条件的构造也是涉及一些定理和证明，这里同样就略去了。回到SIGMOD18年的这篇论文，它的做法也是类似的，维护一个候选集C，为候选集C中的顶点计算一个界限，如果界限满足某个条件则把候选集C中的节点移入结果集Vk中，直到结果集中的数量满足要求。这个界限和判断条件也是涉及定理和证明这里一样略去了。对于候选集来说，里面的顶点大致可以分为三类，确定为topk的顶点，位于topk边界的顶点以及不可能为topk的顶点，对于第二类边界的顶点希望进行更深的Backward Search来得到更精确的一个近似值，做进一步的判断。</p><p>实验部分的话因为几种方法都有理论上证明近似的精确度，所以这里只放了效率之间的比较，最快的方法是ICDE2020年的ResAcc，其次是SIGMOD18的TopPPR以及KDD17的FORA。到这里就介绍完了所有的理论部分，最后介绍两篇将PPR应用于图神经网络上的论文，ICLR19年的这篇论文是最早将PPR引入GNN来解决GNN层数无法加深的问题，从最开始的介绍我们知道了PPR的原始定义式，一个迭代求解的过程，它的解可以表示为下面第二个式子，这篇论文就是将求解得到的PPR矩阵作为GNN中信息传递的模式，同时将预测和传递分离开来，解决传统GNN层数无法加深的问题。那么导致GNN无法加深的一个原因是所谓的过平滑现象，就是随着层数的加深，不同顶点经过网络学习得到的特征表示会趋于相同，导致顶点间没有区分度。拿下面的图为例，从左到右从上到下是网络层数加深后顶点特征表示的可视化结果，可以看到在六层的时候顶点几乎都已经混在了一起，远没有二三层时表现来的好。</p><p>导致这种过平滑现象的原因ICML18年的一篇论文从随机游走的角度给出了解释。一个K层的GNN相当于从源顶点出发到目标顶点的一个K步的随机游走，当K趋向于无穷即层数不断加深时, 顶点的极限分布与初始的顶点表示无关，只与图的结构相关。也就是说，随着层数加深聚合的信息越多，反而得到的表示与中心顶点无关。所以ICLR19年这篇论文引入PPR就是引入其中的重启概率$\alpha$，每一步时会以一定的概率重新回到初始顶点，这样来保证层数加深后与中心顶点间依然有联系。而且与GCN的表达式比较，这篇论文的表达式与层数l无关。最后是KDD20年的PPRGo，从表达式来看做法几乎和上一篇是一样的，不同之处在于这里用到的PPR矩阵是一个近似值，而且对每一行取了topk来控制信息传递时的规模。</p><h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/RfWGee" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWGee.png" alt="RfWGee.png"></a><br><a href="https://imgtu.com/i/RfW1sO" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW1sO.png" alt="RfW1sO.png"></a><br><a href="https://imgtu.com/i/RfW3LD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW3LD.png" alt="RfW3LD.png"></a><br><a href="https://imgtu.com/i/RfW0Qf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW0Qf.png" alt="RfW0Qf.png"></a><br><a href="https://imgtu.com/i/RfWlQK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWlQK.png" alt="RfWlQK.png"></a><br><a href="https://imgtu.com/i/RfWNFA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWNFA.png" alt="RfWNFA.png"></a><br><a href="https://imgtu.com/i/RfWaWt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWaWt.png" alt="RfWaWt.png"></a><br><a href="https://imgtu.com/i/RfWYod" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWYod.png" alt="RfWYod.png"></a><br><a href="https://imgtu.com/i/RfWUJI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWUJI.png" alt="RfWUJI.png"></a><br><a href="https://imgtu.com/i/RfWwSP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWwSP.png" alt="RfWwSP.png"></a><br><a href="https://imgtu.com/i/RfWBy8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWBy8.png" alt="RfWBy8.png"></a><br><a href="https://imgtu.com/i/RfWDOS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWDOS.png" alt="RfWDOS.png"></a><br><a href="https://imgtu.com/i/RfWseg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWseg.png" alt="RfWseg.png"></a><br><a href="https://imgtu.com/i/RfWywQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWywQ.png" alt="RfWywQ.png"></a><br><a href="https://imgtu.com/i/RfWgFs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWgFs.png" alt="RfWgFs.png"></a><br><a href="https://imgtu.com/i/RfW6oj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW6oj.png" alt="RfW6oj.png"></a><br><a href="https://imgtu.com/i/RfW2Yn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW2Yn.png" alt="RfW2Yn.png"></a><br><a href="https://imgtu.com/i/RfWhlV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWhlV.png" alt="RfWhlV.png"></a><br><a href="https://imgtu.com/i/RfWRWq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWRWq.png" alt="RfWRWq.png"></a><br><a href="https://imgtu.com/i/RfW4yT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW4yT.png" alt="RfW4yT.png"></a><br><a href="https://imgtu.com/i/RfWfS0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWfS0.png" alt="RfWfS0.png"></a><br><a href="https://imgtu.com/i/RfW5OU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW5OU.png" alt="RfW5OU.png"></a><br><a href="https://imgtu.com/i/RfWomF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWomF.png" alt="RfWomF.png"></a><br><a href="https://imgtu.com/i/RfWTw4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWTw4.png" alt="RfWTw4.png"></a><br><a href="https://imgtu.com/i/RfW7TJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfW7TJ.png" alt="RfW7TJ.png"></a><br><a href="https://imgtu.com/i/RfWbk9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWbk9.png" alt="RfWbk9.png"></a><br><a href="https://imgtu.com/i/RfWqYR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWqYR.png" alt="RfWqYR.png"></a><br><a href="https://imgtu.com/i/RfWLf1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWLf1.png" alt="RfWLf1.png"></a><br><a href="https://imgtu.com/i/RfWXSx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWXSx.png" alt="RfWXSx.png"></a><br><a href="https://imgtu.com/i/RfWjl6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWjl6.png" alt="RfWjl6.png"></a><br><a href="https://imgtu.com/i/RffSmD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffSmD.png" alt="RffSmD.png"></a><br><a href="https://imgtu.com/i/RfWv6K" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWv6K.png" alt="RfWv6K.png"></a><br><a href="https://imgtu.com/i/RfWxOO" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfWxOO.png" alt="RfWxOO.png"></a><br><a href="https://imgtu.com/i/Rffp0e" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rffp0e.png" alt="Rffp0e.png"></a><br><a href="https://imgtu.com/i/Rff9TH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rff9TH.png" alt="Rff9TH.png"></a><br><a href="https://imgtu.com/i/RffPkd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffPkd.png" alt="RffPkd.png"></a><br><a href="https://imgtu.com/i/RffitA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffitA.png" alt="RffitA.png"></a><br><a href="https://imgtu.com/i/RffFfI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffFfI.png" alt="RffFfI.png"></a><br><a href="https://imgtu.com/i/RffApt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffApt.png" alt="RffApt.png"></a><br><a href="https://imgtu.com/i/RffE1P" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffE1P.png" alt="RffE1P.png"></a><br><a href="https://imgtu.com/i/RffV6f" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffV6f.png" alt="RffV6f.png"></a><br><a href="https://imgtu.com/i/RffZX8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffZX8.png" alt="RffZX8.png"></a><br><a href="https://imgtu.com/i/RffmnS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffmnS.png" alt="RffmnS.png"></a><br><a href="https://imgtu.com/i/Rffu7Q" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rffu7Q.png" alt="Rffu7Q.png"></a><br><a href="https://imgtu.com/i/Rffn0g" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rffn0g.png" alt="Rffn0g.png"></a><br><a href="https://imgtu.com/i/RffMkj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffMkj.png" alt="RffMkj.png"></a><br><a href="https://imgtu.com/i/RffQts" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RffQts.png" alt="RffQts.png"></a></p><p><a href="https://imgtu.com/i/RfqIoV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqIoV.png" alt="RfqIoV.png"></a><br><a href="https://imgtu.com/i/Rfq7JU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfq7JU.png" alt="Rfq7JU.png"></a><br><a href="https://imgtu.com/i/RfqTiT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqTiT.png" alt="RfqTiT.png"></a><br><a href="https://imgtu.com/i/Rfqbz4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqbz4.png" alt="Rfqbz4.png"></a><br><a href="https://imgtu.com/i/RfqHWF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqHWF.png" alt="RfqHWF.png"></a><br><a href="https://imgtu.com/i/RfqLQJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqLQJ.png" alt="RfqLQJ.png"></a><br><a href="https://imgtu.com/i/RfqOy9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqOy9.png" alt="RfqOy9.png"></a><br><a href="https://imgtu.com/i/RfqXLR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfqXLR.png" alt="RfqXLR.png"></a><br><a href="https://imgtu.com/i/Rfqve1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqve1.png" alt="Rfqve1.png"></a><br><a href="https://imgtu.com/i/Rfqxdx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqxdx.png" alt="Rfqxdx.png"></a><br><a href="https://imgtu.com/i/Rfqzo6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/Rfqzo6.png" alt="Rfqzo6.png"></a><br><a href="https://imgtu.com/i/RfLCWD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfLCWD.png" alt="RfLCWD.png"></a><br><a href="https://imgtu.com/i/RfLpFK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfLpFK.png" alt="RfLpFK.png"></a><br><a href="https://imgtu.com/i/RfL9JO" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/07/04/RfL9JO.png" alt="RfL9JO.png"></a></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://wyydsb.xin/other/ppr.html" target="_blank" rel="noopener">大图中如何快速计算PPR</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本学期最后一次组会我做了这两三年近似计算Personalized PageRank（PPR）的小综述，包含理论及图神经网络上的应用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="高效计算" scheme="http://Bithub00.com/tags/%E9%AB%98%E6%95%88%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>SPAGAN Shortest Path Graph Attention Network</title>
    <link href="http://Bithub00.com/2021/05/18/SPAGAN%5BIJCAI&#39;19%5D/"/>
    <id>http://Bithub00.com/2021/05/18/SPAGAN[IJCAI&#39;19]/</id>
    <published>2021-05-18T15:04:54.239Z</published>
    <updated>2021-05-29T13:46:15.885Z</updated>
    
    <content type="html"><![CDATA[<p>IJCAI19一篇关注最短路径对中心顶点的attention聚合的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GAT中只是中心顶点与邻域顶点的顶点间attention聚合，本文关注的是路径对中心顶点的attention聚合。一条路径往往会包含很多个顶点，怎么能够对中心顶点做到“多对一”的聚合呢？在于一条路径$p^c_{ij}$的表示$\phi(p^c_{ij})$是通过路径中所有顶点的特征取平均得到，这样一来就变成“一对一”的聚合，和GAT中的顶点间attention聚合相同。</p><div align="center"><a href="https://imgtu.com/i/2Az8VP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2Az8VP.png" alt="2Az8VP.png" border="0" width="60%"></a></div><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Shortest-Path-Generation"><a href="#Shortest-Path-Generation" class="headerlink" title="Shortest Path Generation"></a>Shortest Path Generation</h4><p>计算最短路径使用Dijkstra，而顶点间边的权重由它们之间的attention系数决定：</p><script type="math/tex; mode=display">W_{ij}=\frac{1}{K}\sum_{k=1}^K\alpha_{ij}^{(k)}</script><h4 id="Path-Sampling"><a href="#Path-Sampling" class="headerlink" title="Path Sampling"></a>Path Sampling</h4><p>这一阶段的核心思想是，对于有着相同长度的几条最短路径，代价最小的与中心顶点的相关性更高，代价指的就是路径上边的权重的求和。记$p_{ij}^c$为顶点i到j长度为c的一条最短路径，$P^c$表示所有$p_{ij}^c$形成的集合，取样：</p><script type="math/tex; mode=display">N_i^c=top_k(P^c),k=degree_i*r</script><h4 id="Hierarchical-Path-Aggregation"><a href="#Hierarchical-Path-Aggregation" class="headerlink" title="Hierarchical Path Aggregation"></a>Hierarchical Path Aggregation</h4><p>这一阶段的层次路径聚合分为两层，第一层聚合相同长度的路径，第二层聚合不同长度的路径。</p><p>加权系数$\alpha_{ij}^{(k)}$为中心顶点i的特征$h_i’$与路径的表示$\phi(p^c_{ij})$的attention系数：</p><script type="math/tex; mode=display">l_i^c=\sum_{k=1}^K\{\sum_{p^c_{ij}\in N_i^c}\alpha_{ij}^{(k)}\phi(p^c_{ij}) \}</script><p>在第二层，进一步聚合不同长度的路径，第一层已经得到以顶点i为中心长度为c的所有路径形成的一个特征表示$l_i^c$：</p><script type="math/tex; mode=display">h_i=\sigma\{\sum_{c=2}^C\beta_cl_i^c\}</script><p>这样一来就通过路径得到了中心顶点i的新的特征表示$h_i$。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IJCAI19一篇关注最短路径对中心顶点的attention聚合的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>数据库的几个证明题</title>
    <link href="http://Bithub00.com/2021/05/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%81%E6%98%8E/"/>
    <id>http://Bithub00.com/2021/05/18/数据库证明/</id>
    <published>2021-05-18T13:15:35.239Z</published>
    <updated>2021-05-18T13:21:40.651Z</updated>
    
    <content type="html"><![CDATA[<p>数据库的几个证明题</p><a id="more"></a><p>一：证明R∈ 3NF，则R 也∈ 2NF。</p><p>证明：采用反证法。设R不是2NF，则有非主属性（Z）对码（X）存在部分函数依赖，即存在Y包含于X，X→Y，Y→Z，也就是Z传递依赖于X。这与3NF范式的定义相矛盾，所以如果R∈ 3NF，则R 也∈ 2NF。</p><p>二：证明R ∈ BCNF ，则 R 也∈ 3NF。</p><p>证明：采用反证法。设 R 不是 3NF ，则必然存在这样的码 X ，属性组 Y 和非主属性 Z （ Z 不∈ Y ），使得 X→Y （ Y 不 -&gt;X ）， Y→Z ，这样 Y→Z 函数依赖的决定因素 Y 不包含码，这与 BCNF 范式的定义相矛盾，所以如果 R ∈ BCNF ，则 R 也是 3NF 。</p><p>三：证明R ∈ BCNF，则 R 也∈ 2NF。</p><p>证明：采用反证法。假设R不属于2NF，存在X对Y的部分函数依赖，存在X的真子集X’有X’ -&gt; Y，又因为X’是码X的真子集，X’不能包含码，则X’-&gt;Y与R属于BCNF矛盾。</p><p>四：试由Armostrong公理系统推导出下面三条规则：</p><p>（1）合并规则：若X→Z，X→Y，有X→YZ</p><p>（2）伪传递规则：由X→Y，WY→Z，有XW→Z</p><p>（3）分解规则：X→Y，Z包含于Y，有X→Z</p><p>证明：</p><p>（1）已知X→Z，由增广律知XY→ZY，又因为X→Y，可得XX→XY→YZ，最后根据传递律得X→YZ。（因为XX等于X）</p><p>（2）已知X→Y，由增广律得XW→WY，又因为WY→Z，所以XW→WY→Z，通过传递律可知XW→Z。</p><p>（3）已知Z包含于Y，根据自反律得Y→Z，又因为X→Y，所以由传递律可知得X→Z。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据库的几个证明题&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="http://Bithub00.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Personalized PageRank to a Target Node, Revisited[KDD&#39;20]</title>
    <link href="http://Bithub00.com/2021/04/24/RBS%5BKDD20%5D/"/>
    <id>http://Bithub00.com/2021/04/24/RBS[KDD20]/</id>
    <published>2021-04-24T02:42:01.068Z</published>
    <updated>2021-07-05T11:32:35.806Z</updated>
    
    <content type="html"><![CDATA[<p>KDD20一篇近似计算单目标PPR相似度的论文</p><a id="more"></a><p>这里我直接把组会分享的PPT和讲稿放上来了。</p><h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>今天我要讲的是KDD20年的一篇论文，解决的是针对目标顶点的个性化PageRank查询。个性化PageRank简称PPR，它来自于Google创始人于1998年提出的PageRank。PageRank用于度量web网络中各网页的重要性，它的核心思想有两点：被很多网页所链接的网页重要性较高；被重要的网页所链接的网页重要性较高。如果将web网络转化为图结构G(V,E)，网页看作顶点，网页间的链接看作边，则PageRank的计算方式为下面的式子，$\pi$是PPR向量有n维，每一维对应一个顶点的PageRank分数，是一个迭代的计算过程。</p><p>它可以用于衡量各顶点的全局重要性，举个例子说明这个式子的含义，有A、B、C、D四个网页，假设一个上网的人在网页A，那他会以1/3的概率跳到B、C和D，访问一个网页的概率由链接到它的所有网页的概率来决定，例如A由B、C两个网页链接，就有第一个式子。各个网页间的转移可以用一个概率转移矩阵表示，初始时上网的人在每个网页停留的概率是相等的，右乘上转移矩阵后就得到了下一时刻对每个网页的访问概率。然后不断迭代直到收敛。实际上，如果存在一个网页只链接到自己，例如图里的C，那就会像一个陷阱一样，导致概率分布值全部转移到网页C上来，所以PageRank另一部分就是以一定的概率跳转到一个随机的网页，来避免这种问题。</p><p>而PPR是PageRank的一种特殊形式，它用于衡量图上顶点关于某一顶点的相对重要性，两者之间的关系为下面的式子。一个是相对重要性一个是全局重要性。这篇论文重点关注是单目标PPR的计算问题，即给定图上某一顶点t作为目标顶点，计算图上所有顶点关于t的PPR值，它希望找到使得目标顶点t重要性较高的一系列顶点。有别于我们传统上理解的单源PPR的计算问题，后者是希望找到相对源顶点较为重要的一系列顶点。</p><p>论文做的是一个近似求解，满足一定的误差要求来提升计算的效率。近似的方法是从PPR的另一个定义方式出发，即从源顶点出发的随机游走停止在目标顶点的概率。进一步地分解，就是以0步、1步到无穷步停止的概率进行求和，近似的做法就是进行一个截断，上限不需要是无穷，截断后满足误差小于给定极限即可。单目标PPR的计算问题相关研究较少，目前时间复杂度上最优的方法是Backward Search，为$O(\frac{\overline{d}}{\delta})$，</p><p>这篇论文的贡献就是在它的基础上引入随机性，使得复杂度降为$O(\frac{1}{\delta})$。这里先介绍Backward Search的做法，然后指出论文具体在哪几个步骤做了改动。Backward Search的做法是，给定一个目标顶点t，算法对图上的每个顶点u都维护两个变量residue和reserve，我这里简称为r值和$\pi$值，每次选取当前r值最大的顶点v，更新自己的$\pi$值，再将r值以一定比例push给所有的入度邻居并置零。最后每个顶点的$\pi$值就作为PPR值的近似计算结果。因为在push操作时需要遍历所有的入度邻居，所以复杂度上会带有一个$\overline{d}$，表示图中所有顶点的平均出度。</p><p>所以论文在push这一步做了改动，提出了自己的方法RBS。它不会push给所有的入度邻居，而是只push增长值超过阈值的部分。之所以这么做，是因为Backward Search里的增长值与入度邻居u的出度成反比，对于出度较大的这部分入邻居，这次push操作对它的\pi值改变不大，即使进行了本次push，它\pi值的变化幅度也小于误差要求，所以放弃对它的push操作以节省时间。</p><p>在RBS算法里，push操作有三种情况，当出度满足要求时才确定进行push操作，如果不满足条件，产生一个0、1之间的随机数，如果出度满足小于放宽后的要求，则push一个固定值，否则不进行本次push。这样，对于每个需要push操作的入度邻居，只确定地更新了一部分并且采样了一部分进行更新以保证结果的无偏。</p><p>在实际实现时不能遍历每个入度邻居u来判断它是否满足要求，这样同样会带来$\overline{d}$的复杂度，因为push操作的条件是逐渐递减的，只需要预先对所有的入度邻居按照出度进行排序，逐个扫描到临界条件后就可以放弃查看剩下的所有邻居，因为都不会满足条件。最后是实验部分，选用的几个图数据集描述如下，有无向图也有有向图，有小图也有大图，评测指标一个是最大计算误差，另外两个是推荐常用指标。</p><p>实验结果上，因为是基于Backward Search提出的，baseline只选了这一个。图中的横轴都是查询时间，纵轴分别是各个评测指标，结果上来说就是花费同等的时间，新的方法可以达到更好的结果，达到相同的结果新的方法只需要更少的时间。</p><p>论文所研究的单目标PPR计算虽然相对冷门，但实际应用可以结合许多问题，第一个相当于是该问题下的一个特例，对顶点的重要性有更高的要求，其次在图神经网络上也有利用PPR矩阵进行改进的工作，例如ICLR19年的这个方法。</p><p>最后想重点说一下的是与SimRank相似度的关系，因为前一篇讨论班论文我讲的就是ICDE20年一篇如何高效计算SimRank相似度的论文，里面提到SimRank相似度可以看成两条分别从源顶点和目标顶点出发的随机游走，以相同的步数相遇于同一个顶点的概率，因为定义上有相似之处，可以结合PPR来计算顶点u或v到这一系列顶点w的概率。</p><p>总结来说，这篇论文的贡献是在BackWard Search的基础上引进随机化来降低时间复杂度并且保证了理论上的近似精度，并且给出了数学证明说明做法的有效性。</p><h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/cjNkPH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNkPH.png" alt="cjNkPH.png"></a><br><a href="https://imgtu.com/i/cjNERA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNERA.png" alt="cjNERA.png"></a><br><a href="https://imgtu.com/i/cjNiIe" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNiIe.png" alt="cjNiIe.png"></a><br><a href="https://imgtu.com/i/cjNVxI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNVxI.png" alt="cjNVxI.png"></a><br><a href="https://imgtu.com/i/cjNAGd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNAGd.png" alt="cjNAGd.png"></a><br><a href="https://imgtu.com/i/cjNeMt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNeMt.png" alt="cjNeMt.png"></a><br><a href="https://imgtu.com/i/cjNmsP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNmsP.png" alt="cjNmsP.png"></a><br><a href="https://imgtu.com/i/cjNnqf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNnqf.png" alt="cjNnqf.png"></a><br><a href="https://imgtu.com/i/cjNKZ8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNKZ8.png" alt="cjNKZ8.png"></a><br><a href="https://imgtu.com/i/cjNMdS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNMdS.png" alt="cjNMdS.png"></a><br><a href="https://imgtu.com/i/cjNQIg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNQIg.png" alt="cjNQIg.png"></a><br><a href="https://imgtu.com/i/cjN3Gj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjN3Gj.png" alt="cjN3Gj.png"></a><br><a href="https://imgtu.com/i/cjN1iQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjN1iQ.png" alt="cjN1iQ.png"></a><br><a href="https://imgtu.com/i/cjNGzn" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNGzn.png" alt="cjNGzn.png"></a><br><a href="https://imgtu.com/i/cjN8Rs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjN8Rs.png" alt="cjN8Rs.png"></a><br><a href="https://imgtu.com/i/cjNNLV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNNLV.png" alt="cjNNLV.png"></a><br><a href="https://imgtu.com/i/cjNYMq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNYMq.png" alt="cjNYMq.png"></a><br><a href="https://imgtu.com/i/cjNts0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNts0.png" alt="cjNts0.png"></a><br><a href="https://imgtu.com/i/cjNaZT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNaZT.png" alt="cjNaZT.png"></a><br><a href="https://imgtu.com/i/cjNwoF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNwoF.png" alt="cjNwoF.png"></a><br><a href="https://imgtu.com/i/cjNddU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNddU.png" alt="cjNddU.png"></a><br><a href="https://imgtu.com/i/cjNszR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNszR.png" alt="cjNszR.png"></a><br><a href="https://imgtu.com/i/cjNBi4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNBi4.png" alt="cjNBi4.png"></a><br><a href="https://imgtu.com/i/cjNDJJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNDJJ.png" alt="cjNDJJ.png"></a><br><a href="https://imgtu.com/i/cjNrW9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/24/cjNrW9.png" alt="cjNrW9.png"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KDD20一篇近似计算单目标PPR相似度的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="高效计算" scheme="http://Bithub00.com/tags/%E9%AB%98%E6%95%88%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>CrashSim An Efficient Algorithm for Computing SimRank over Static and Temporal Graphs[ICDE&#39;20]</title>
    <link href="http://Bithub00.com/2021/04/10/CrashSim%5BICDE20%5D/"/>
    <id>http://Bithub00.com/2021/04/10/CrashSim[ICDE20]/</id>
    <published>2021-04-10T09:05:01.001Z</published>
    <updated>2021-04-10T09:20:01.407Z</updated>
    
    <content type="html"><![CDATA[<p>ICDE20一篇高效计算SimRank相似度的论文</p><a id="more"></a><pre><code>这里我直接把组会分享的PPT和讲稿放上来了。</code></pre><h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>​    大家好，今天我要讲的是ICDE2020年的一篇论文，“一种在静态和时序图上高效计算SimRank相似度的算法”。第一眼看到这个题目会想，首先什么是SimRank相似度，然后为什么它的计算是低效的，以及论文怎么进行高效计算。按照这个逻辑，首先介绍一下SimRank相似度，它是应用于图上的一种相似度计算方法，基本思想是，关联到相同顶点的两个顶点，相互之间具有相似性。以下面这个图片为例，两位教授相似度高因为他们来自同一所大学，而两位学生的相似度要相对低一些，在于他们并不直接来自于同一位教授。SimRank相似度原始的计算方式如下，它是一个迭代的过程。要计算顶点u和v的相似度，我们需要找到它们各自邻域中所有的顶点x和y，进行求和。前面的系数是为了避免热门顶点的影响而做的惩罚。因为是迭代计算，当图中顶点数量较多的时候计算就会变得低效。而VLDB17年的一篇论文证明了可以通过随机漫步近似计算SimRank相似度，这就为高效计算带来了新的思路。从随机漫步的角度来看，顶点u和v之间的SimRank相似度，可以看作从这两个顶点出发的随机漫步序列相交的概率。这个随机漫步的定义如下，以\sqrt{c}的概率漫步到下一个邻居，1-\sqrt{c}的概率终止于当前顶点。随机漫步可以看成是一种采样的方法，在图上的应用比较广泛，包括用来生成顶点的embedding如node2vec，或者在一个大图中构建子图捕获局部信息如GraphSAGE。既然是一个采样的方法，那么采样多少来保证获取到必要的信息就是一个关键的参数。同样地，在通过随机漫步近似计算SimRank相似度时，单次漫步的长度以及漫步的次数怎么选取，才能保证近似值与实际值之间的误差小于某个界限。论文的一个主要贡献就是给出了这两个参数的选取以及证明了它们的有效性。我对证明过程的每一步作了注解放在了ppt里，这里就不详细展开。主要是通过几何分布以及正态分布的3σ原则来证明。回到刚刚的式子，有了两个序列W(u)和W(v)，这时只剩下最后一步也就是怎么计算这里的概率。最直观的办法是通过蒙特卡洛模拟，给定一对查询顶点u和v，我们总共做n次随机漫步，用其中相遇的次数所占的比例作为概率的一个近似，然而这需要进行大量的尝试，直观但效率低。论文的改进做法是，与其去判断从u和v出发的随机漫步是否会相交，不如直接判断顶点v能否到达顶点u随机漫步的范围内。因为在静态图上固定了随机漫步的长度后，一个顶点能够漫步到的范围就是固定的，因此只需要在迭代之前计算一遍即可，不需要迭代时一次次地去计算。以左边的图为例，取漫步的长度$l_{max}=4$，在这个图中顶点A能够漫步到的范围就能用右边这颗树表示。因为随机漫步就是由概率决定是停止还是继续，所以只要将右边这棵树上的边用概率表示就可以进行近似计算。论文中给出的一个表示是下面这个式子，含义是这棵树每层之间边的一个关系。在实际编程实现时用的是一个二维矩阵$U\in \mathbb{R}^{l_{max}\times |\Omega|}$来表示这棵树，为什么这样的一个关系能够保证近似计算的有效性的证明我也放在了这里，同样不展开来讲了。单看表达式比较抽象，我就继续以刚才的例子说明这个计算过程，取参数$c=0.25$，初始时因为只能停留在起点A处，所以设这个概率值为1，$U(0,A)$表示以0步漫步到顶点A的概率。而一步能够漫步到的范围包括B、C两个顶点，以B为例，它的入度邻居数目为2，对应着上面的公式计算出它的概率值。所以漫步到顶点B的概率由先到顶点A的概率决定，这也符合随机漫步的定义。接下来以此类推。对于顶点A能够到达的顶点，它们在矩阵里对应位置的值都不为0。得到这么一个U矩阵后，迭代时不断地产生顶点v的随机漫步序列W(v)，根据这里的式子来近似计算SimRank相似度，直观上的理解是W(v)这个序列多大程度上走进了顶点u的这个漫步范围。还是刚才的例子，假设我们想得到顶点A和C之间的SimRank相似度，而某次迭代时顶点C产生的一条序列为$W(C)=(C,D,B,A)$，我们查阅刚刚得到的U矩阵中对应元素的值，进行累加，多次迭代后就得到了近似计算结果。回顾一下整个算法的流程，输入是单个顶点u和一系列顶点v，我们希望知道u和这些v之间的SimRank相似度大小。首先设定两个参数的取值，接下来在迭代前先预先计算顶点u的U矩阵，矩阵中的元素表示它漫步到某个顶点的概率。迭代过程中从顶点v产生一条随机漫步序列，通过U矩阵计算这条序列与u相遇的概率，多次取平均作为结果。到这里论文就完成了静态图部分的工作，接下来就是怎么将提出的方法继续用在时序图上。一个时序图通常由一系列快照图组成，每个快照图捕获了某个时间段内顶点之间的状态，图中就是一个包含三个快照图的时序图。联系现实可以拿微博举例子，假设我是顶点H，第一周关注了用户F，第二周取关了他，第三周用户G又新关注用户F。实际中因为用户的兴趣更新频率很快，所以时序图更能表达这种动态变化的兴趣。在时序图的情景下，最常见的两种SimRank查询分别为趋势查询和阈值查询。趋势查询的含义是，在给定的时间区间内，对于顶点u，我们希望找到一系列顶点v，它们与u的SimRank相似度在这个区间内是递增或递减的。而阈值查询是指在给定的时间区间内，它们与u的SimRank相似度大于某个阈值。因为时序图的每个快照图都可以看成静态图，最直观的想法就是把刚才的方法在每个快照图上算一遍，但这样只是照搬静态图的做法，没有用上时序图的特点，所以论文的后半部分针对时序图的特点提出了两个减少计算量的策略，分别是delta剪枝和差异剪枝，它们的思想也很直观，希望只对时序图中产生变化的顶点重新进行计算。回到刚才的例子图，会发现这三张快照图变的只是蓝框部分，红框部分一直维持不变。所以红框内的部分没必要每次都进行计算。具体到delta剪枝，一条新增或删除的边x-&gt;y影响的区域定义为下面两个部分，第一部分的意思是，如果顶点u能去的范围因为新增的边变大了，对应的这棵树也会改变。而第二部分的意思是，对于顶点y能到达的最远顶点$y_{lmax}$，反过来看顶点x刚好在它漫步范围之外，所以它在前后两个快照图里是不受影响的，涉及它的SimRank计算就可以省去。根据这个观察，delta剪枝的做法就是满足该前提的条件下，避免重计算未受影响区域的顶点。论证在下面主要从时间复杂度来说明，这里就省去了。而差异剪枝的想法要简单一些，因为近似计算依靠的就是随机漫步，如果前后两张快照图里漫步的范围没变，那计算结果也不会发生改变。所以差异剪枝的做法就是只要顶点u和v漫步范围不变，它们之间就不需要进行重计算。论证同样是通过复杂度说明。这两个策略具体放在流程里体现为图里的红框和蓝框，主要是判断是否达到对应的条件，然后删去不需要重计算的顶点来减少计算量。最后是实验部分，数据集和baseline的描述如下，结果主要是准确率和效率两方面，纵轴的ME表示近似值与真实值的最大误差，五个数据集上论文的方法都是又快又准，在时序图实验上也是一样。总结来说，论文的贡献是给出了一种高效计算SimRank相似度的方法并且证明了它的有效性，并且针对时序图的情景提出了两个优化策略。因为论文涉及比较多的证明和细节，看完可能不太明白这么计算的意义。图在许多领域应用广泛，而SimRank相似度既然是一种相似度，那它就可以应用在推荐系统、社交网络等一系列任务上，同时这些场景下往往有大量的用户，高效计算就有了实际的应用背景。不过这里也是我对论文有疑问的一个地方，因为实验用到的数据集都很小，最多的顶点也才三万多个，时序图也是以天为单位，而在现实生活中可能用户兴趣可能几个小时就发生改变了，例如微博热搜。而且即使在这么一个数据集上，论文的方法也要耗费75分钟的时间进行计算，感觉并不是很高效。</p><h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/cdSebj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSebj.png" alt="cdSebj.png"></a><br><a href="https://imgtu.com/i/cdSnVs" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSnVs.png" alt="cdSnVs.png"></a><br><a href="https://imgtu.com/i/cdSuan" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSuan.png" alt="cdSuan.png"></a><br><a href="https://imgtu.com/i/cdSVKg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSVKg.png" alt="cdSVKg.png"></a><br><a href="https://imgtu.com/i/cdSZrQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSZrQ.png" alt="cdSZrQ.png"></a><br><a href="https://imgtu.com/i/cdSlGV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSlGV.png" alt="cdSlGV.png"></a><br><a href="https://imgtu.com/i/cdSK5q" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSK5q.png" alt="cdSK5q.png"></a><br><a href="https://imgtu.com/i/cdSQP0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSQP0.png" alt="cdSQP0.png"></a><br><a href="https://imgtu.com/i/cdSYqJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSYqJ.png" alt="cdSYqJ.png"></a><br><a href="https://imgtu.com/i/cdS12T" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS12T.png" alt="cdS12T.png"></a><br><a href="https://imgtu.com/i/cdS3xU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS3xU.png" alt="cdS3xU.png"></a><br><a href="https://imgtu.com/i/cdSGMF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSGMF.png" alt="cdSGMF.png"></a><br><a href="https://imgtu.com/i/cdSJr4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSJr4.png" alt="cdSJr4.png"></a><br><a href="https://imgtu.com/i/cdSwPx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSwPx.png" alt="cdSwPx.png"></a><br><a href="https://imgtu.com/i/cdSNZ9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSNZ9.png" alt="cdSNZ9.png"></a><br><a href="https://imgtu.com/i/cdSaI1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSaI1.png" alt="cdSaI1.png"></a><br><a href="https://imgtu.com/i/cdSUaR" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSUaR.png" alt="cdSUaR.png"></a><br><a href="https://imgtu.com/i/cdS0G6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS0G6.png" alt="cdS0G6.png"></a><br><a href="https://imgtu.com/i/cdSBRK" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSBRK.png" alt="cdSBRK.png"></a><br><a href="https://imgtu.com/i/cdS6qH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS6qH.png" alt="cdS6qH.png"></a><br><a href="https://imgtu.com/i/cdS5z8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS5z8.png" alt="cdS5z8.png"></a><br><a href="https://imgtu.com/i/cdS7LQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS7LQ.png" alt="cdS7LQ.png"></a><br><a href="https://imgtu.com/i/cdSsMD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSsMD.png" alt="cdSsMD.png"></a><br><a href="https://imgtu.com/i/cdSgZd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSgZd.png" alt="cdSgZd.png"></a><br><a href="https://imgtu.com/i/cdS2dA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS2dA.png" alt="cdS2dA.png"></a><br><a href="https://imgtu.com/i/cdSRII" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSRII.png" alt="cdSRII.png"></a><br><a href="https://imgtu.com/i/cdSTsg" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSTsg.png" alt="cdSTsg.png"></a><br><a href="https://imgtu.com/i/cdShJP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdShJP.png" alt="cdShJP.png"></a><br><a href="https://imgtu.com/i/cdSfit" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSfit.png" alt="cdSfit.png"></a><br><a href="https://imgtu.com/i/cdSoQS" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSoQS.png" alt="cdSoQS.png"></a><br><a href="https://imgtu.com/i/cdS4Rf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdS4Rf.png" alt="cdS4Rf.png"></a><br><a href="https://imgtu.com/i/cdSbZj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSbZj.png" alt="cdSbZj.png"></a><br><a href="https://imgtu.com/i/cdSqds" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSqds.png" alt="cdSqds.png"></a><br><a href="https://imgtu.com/i/cdSLon" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSLon.png" alt="cdSLon.png"></a><br><a href="https://imgtu.com/i/cdSjJ0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSjJ0.png" alt="cdSjJ0.png"></a><br><a href="https://imgtu.com/i/cdSXiq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSXiq.png" alt="cdSXiq.png"></a><br><a href="https://imgtu.com/i/cdSxzT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdSxzT.png" alt="cdSxzT.png"></a><br><a href="https://imgtu.com/i/cdpSQU" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/04/10/cdpSQU.png" alt="cdpSQU.png"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICDE20一篇高效计算SimRank相似度的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="高效计算" scheme="http://Bithub00.com/tags/%E9%AB%98%E6%95%88%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>How Powerful are Graph Neural Networks?[ICLR&#39;19]</title>
    <link href="http://Bithub00.com/2021/01/31/GIN%5BICLR19%5D/"/>
    <id>http://Bithub00.com/2021/01/31/GIN[ICLR19]/</id>
    <published>2021-01-31T13:28:36.766Z</published>
    <updated>2021-02-09T04:36:31.688Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR19一篇从图同构测试（Graph Isomorphism Test）角度说明GNN性能表现的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GNN性能表现好的原因是什么？</p><h4 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h4><ol><li>证明了GNN的性能上限是Weisfeiler-Lehman (WL) test，最多只和它一样有效</li><li>给出了GNN在什么条件下能够和WL test一样有效</li><li>指明了主流GNN框架如GCN、GraphSage无法区分的图结构，以及它们能够区分的图结构的特点</li><li>提出了一个简单有效的框架GIN，能够与WL test一样有效</li></ol><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>首先是介绍现有GNN框架的做法及图同构测试的定义，还有WL test的做法。</p><h4 id="GNN与WL-test"><a href="#GNN与WL-test" class="headerlink" title="GNN与WL test"></a>GNN与WL test</h4><p>论文认为主流的GNN框架可以分为下面这三步：</p><ol><li><p><strong>Aggregate</strong>：聚合邻域内的信息</p><script type="math/tex; mode=display">a_v^{(k)}=\text{AGGREGATE}^{(k)}(\{h_u^{(k-1)}:u\in N(v) \})</script></li><li><p><strong>Combine</strong>：将聚合后的邻域信息与当前顶点信息结合</p><script type="math/tex; mode=display">h_v^{(k)}=\text{COMBINE}^{(k)}(h_v^{(k-1)},a_v^{(k)})</script></li><li><p><strong>Readout</strong>：通过图中的每个顶点的表示得到图的表示</p><script type="math/tex; mode=display">h_G=\text{READOUT}({h_v^{(K)}|v\in G})</script></li></ol><p>图同构测试就是判断两张图是否在拓扑结构上相同。而WL test的做法是迭代地进行以下步骤：</p><ul><li>聚合顶点及其邻域的标签信息</li><li>将聚合后的标签集合哈希成唯一的新标签</li></ul><p>如果经过若干次迭代后，两张图中的顶点的标签出现了不同则判断为不同构。基于WL test有一种核函数被提出以计算图之间的相似性。直观上来说，如下图所示，一个顶点在第$k$次迭代时的标签，实际表示了一颗以该顶点为根顶点高度为$k$的子树。</p><div align="center">  <a href="https://imgchr.com/i/yVPBNQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/31/yVPBNQ.png" alt="yVPBNQ.png" border="0" width="80%"></a></div><p>而GNN同样是通过迭代地更新图中每个顶点的特征向量来捕捉图的结构信息以其周围顶点的特征，这里的结构特征同样可以是上图的根子树rooted subtree。如果给每个顶点的特征向量一个唯一的标签例如{a,b,c,…}，那一个顶点的邻域中所有顶点的特征向量可以构成一个Multiset，它的定义基本和C++中的Multiset一样，是一个Set的同时里面的元素还可以重复例如{a,a,b,c}。论文中对Multiset给出的数学定义是：$X=(S,m)$，其中$S$由Multiset中的非重复元素构成，$m$表示$S$中的元素在$X$中的频数。</p><p>直观上来说，一个有效的GNN应该只有在两个顶点对应的根子树结构相同，且其中对应顶点的特征向量也相同时，才将这两个顶点在特征空间中映射成相同的表示。也就是永远不会将不同的两个Multiset映射成同一个特征表示（因为Multiset中的顶点也是根子树中的顶点，既然它们都是通过聚合邻域得到的）。这也就意味着GNN中使用的聚合函数必须是单射的，对值域内的每一个$y$，存在最多一个定义域内的$x$使得$f(x)=y$。有下面这么一个引理：</p><blockquote><p>设$G_1$和$G_2$是两个非同构图，如果一个图神经网络$A:G\rightarrow \mathbb{R}^d$将$G_1$和$G_2$映射成不同的embedding，那么WL test同样会判断这两个图为非同构。</p></blockquote><p>引理表明在图区分任务上，一个图神经网络的表现最多和WL test一样好。而一样好的条件是，这个图神经网络的邻居聚合函数和图表示函数都是单射的。这里的一个局限是，函数考虑的定义域和值域都是离散集合。</p><p>图神经网络相较于WL test的另一个好处是，WL test输入的顶点特征向量都是one-hot编码，这无法捕捉到子树之间的结构相似度：</p><blockquote><p>Note that node feature vectors in the WL test are essentially one-hot encodings and thus cannot capture the similarity between subtrees. In contrast, a GNN satisfying the criteria in Theorem 3 generalizes the WL test by learning to embed the subtrees to low-dimensional space. This enables GNNs to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures.</p></blockquote><h4 id="图同构网络GIN"><a href="#图同构网络GIN" class="headerlink" title="图同构网络GIN"></a>图同构网络GIN</h4><p>基于上面介绍的引理和结论，论文提出的GIN框架如下：</p><script type="math/tex; mode=display">h_v^{(k)}=\text{MLP}^{(k)}\Big((1+\epsilon^{(k)})·h_v^{(k-1)}+\sum_{u\in N(v)}h_u^{(k-1)} \Big)</script><p>对比一开始论文给出的GNN主流框架，可以看到是Aggregate函数选取了求和函数，Combine函数选取了MLP+(1+$\epsilon$)的形式。常见的Aggregate函数包括求和Sum、最大值Max和平均值Mean，论文花了一部分篇幅来说明求和相较于其他两个函数的好处：</p><p><div align="center">  <a href="https://imgchr.com/i/yGIv5D" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGIv5D.png" alt="yGIv5D.png" border="0" width="80%"></a>  <a href="https://imgchr.com/i/yGoirt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGoirt.png" alt="yGoirt.png" border="0" width="80%"></a></div><br>上面两幅图分别说明这几种聚合函数的特点及何种场景下会导致误差。第一幅图中即使减少了顶点的数量但对于取平均和最大值函数来说得到的信息保持不变，第二幅图也是想说明同样的问题，例如取平均，两个一样的顶点与三个一样的顶点取平均出来结构都是一样的，但它们分别对应的局部结构是不相同的。</p><p>对于顶点分类及边预测这类下游任务，只要得到顶点的embedding即可。而对于图分类任务，还需要根据所有顶点的embedding来得到图的一个表示，也就是前面提到的主流GNN框架做法的第三步Readout函数。论文的做法类似于<a href="http://www.bithub00.com/2020/12/22/JK-Net[ICML18]/" target="_blank" rel="noopener">JK-Net</a>，将所有层的表示都考虑进来，不过没有具体说是怎么做的。</p><p>最后，论文还探讨了那些不满足上面定理的GNN框架如GCN、GraphSAGE等，这些框架都采用一层感知机如ReLU来将Multiset映射成特征表示，而不像论文的做法采用多层感知机，而ReLU存在将不同的Multiset表示成同一种特征表示的情况，即$\exist X_1 \not=X_2,\ s.t. \ \sum_{x\in X_1}\text{ReLU}(Wx)=\sum_{x\in X_2}\text{ReLU}(Wx)$。论文中直接给了一个简单的例子：$X_1=\{1,1,1,1,1\},X_2=\{2,3\}$，因为有$\sum_{x\in X_1}\text{ReLU}(Wx)=\text{ReLU}(W\sum_{x\in X_1}x)$。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI1、PROTEINS、COLLAB、IMDB-BINARY、IMDB-MULTI、REDDIT-BINARY、REDDIT-MULTI5K</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICLR19一篇从图同构测试（Graph Isomorphism Test）角度说明GNN性能表现的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>GCC--Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD&#39;20]</title>
    <link href="http://Bithub00.com/2021/01/28/GCC%5BKDD20%5D/"/>
    <id>http://Bithub00.com/2021/01/28/GCC[KDD20]/</id>
    <published>2021-01-28T13:30:43.112Z</published>
    <updated>2021-01-29T15:11:31.339Z</updated>
    
    <content type="html"><![CDATA[<p>KDD20一篇将对比学习（contrastive learning）应用于图表示学习任务从而进行迁移的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将自监督学习的思想应用与图表示学习，通过预训练图神经网络从而仅需要微调就可以应用于新的数据集。</p><p>图表示学习目前受到了广泛关注，但目前绝大多数的图表示学习方法都是针对特定领域的图进行学习和建模，训练出的图神经网络难以迁移。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h4><p>对比学习是自监督学习思想的一种典型框架，一个典型的例子如下图所示：</p><div align="center">  <a href="https://imgchr.com/i/yiECBF" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiECBF.png" alt="yiECBF.png" border="0" width="80%"></a></div><p>对比学习的思想是：尽管我们已经见过钞票很多次，能够轻易地分辨出一张钞票，我们也很少能画出一张完美无缺的钞票。<strong>表示学习算法不需要关注到样本的每一个细节，只要学到的特征能够将用来区分其它样本即可</strong>。不需要模型能够生成一匹栩栩如生的马之后它才能去分辨一张图片里的动物是不是马，这就是对比学习和生成对抗网络的一个区别。</p><p>既然是表示学习，核心就是通过一个函数把样本$x$转换成特征表示$f(x)$，而对比学习作为一种表示学习方法，它的思想是满足下面这个式子：</p><script type="math/tex; mode=display">s(f(x),f(x^+))\gg s(f(x),f(x^-))</script><p>使得类似样本之间的相似度要远大于非类似样本之间的相似度，这样才能够进行区分。</p><h4 id="图表示学习"><a href="#图表示学习" class="headerlink" title="图表示学习"></a>图表示学习</h4><p>具体到论文的图表示学习任务中，论文的一个重要假设是，具有典型性的图结构在不同的网络之间是普遍存在而且可以迁移的（Representative graph structural patterns are universal and transferable across networks）。受对比学习在计算机视觉和自然语言处理领域的成功应用，论文想把对比学习（contrastive learning）的思想放在图表示学习中。通过预训练一个图神经网络，它能够很好地区分这些典型性的图结构，这样它的表现就不会仅仅局限于某个特定的数据集。</p><div align="center"><a href="https://imgchr.com/i/y9x4KI" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/y9x4KI.png" alt="y9x4KI.png" border="0" width="80%"></a></div><p>论文首先将现有工作对顶点相似度的衡量分为了三类：</p><ol><li><p>邻域相似度</p><p>核心思想：越近的两个顶点之间相似度越高，包括有Jaccard、RWR、SimRank以及LINE、DeepWalk、node2vec。</p></li><li><p>结构相似度</p><p>核心思想：有相似的局部结构的两个顶点之间相似度更高。不同于邻域相似度，结构相似度不需要两个顶点之间有路径相连。常用的局部结构包括vertex degree、structural diversity、structural hole、k-core、motif等。</p></li><li><p>属性相似度</p><p>当数据集中顶点有许多标签信息时，可以将标签作为顶点的特征来衡量它们之间的相似度。</p></li></ol><p>在对比学习中，给定一个查询表示$q$以及一个包含$K+1$个键表示${k_0,\dots,k_K}$的字典，我们希望找到一个能与$q$匹配的键$k_+$。所以，论文优化的损失函数来自于InfoNCE：</p><script type="math/tex; mode=display">L=-\log \frac{\exp(q^Tk_+\tau)}{\sum_{i=0}^K\exp(q^Tk_i/\tau)}</script><p>其中$f_q、f_k$是两个图神经网络，分别将样本$x^q$和$x^k$转换为低维表示$q$与$k$。</p><h4 id="正负样本获取"><a href="#正负样本获取" class="headerlink" title="正负样本获取"></a>正负样本获取</h4><p>因为查询和键可以是任意形式，具体到本论文里，定义每一个样本都是一个从特定顶点的$r$阶邻居网络中采样的子图，这里的子图定义和其它论文一致：$S_v=\{u:d(u,v)&lt;r \}$，距离顶点$v$最短路径距离小于$r$的顶点构成的集合。既然是最短路径，给定$r$那么这个集合也基本确定了，这种情况下得到的子图数量有限，在计算机视觉领域，当输入用于训练的图片数量有限时，往往会使用反转、旋转等方式对图片进行变换，以扩充训练图片的数量，这里论文也想采取类似的做法，对得到的子图$x$进行变换，来得到对比学习中的类似$x^+$与非类似样本$x^-$，具体做法如下：</p><ol><li><strong>带重启动的随机漫步</strong>。首先从子图的中心顶点$v$开始随机漫步，每一步时都有一定概率重新回到中心顶点，而漫步到任一邻居顶点的概率与当前顶点的出度有关。</li><li><strong>子图推演</strong>。随机漫步可以得到一系列顶点，它们构成的集合记为$\tilde{S_v}$，所形成的子图记作$\tilde{G_v}$，它就可以看作子图$S_v$的一个变换。</li><li><strong>匿名化</strong>。重新定义$\tilde{G_v}$中的顶点的标签，将$\{1,2,\dots,|\tilde{S_v} |\}$的顺序随机打乱作为重新定义后的标签。</li></ol><div align="center">    <a href="https://imgchr.com/i/yiFHv8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiFHv8.png" alt="yiFHv8.png" border="0" width="70%"></a></div><p>论文对于每个子图都进行两次上述变换，而变换后的子图显然会与原子图相似，这样就有了一组相似的子图$(x^q,x^{k_+})$。要得到不相似的子图也很容易，不是同一个子图变换得到的子图就定义为不相似：$(x^q,x^k),k\not =k_+$。在上图的例子中，$x^q$和$x^{k_0}$是从红色的中心顶点采样得到的子图，我们认为它是一对正样本，而$x^{k_1}$和$x^{k_2}$作为从蓝色的中心顶点采样得到的子图，则被作为负样本。在变换时之所以要做最后一步，是为了防止图神经网络在判断两个子图是否相似时，仅仅是通过判断对应顶点的标签是不是一样，这样显然没有学到任何有用的结构信息。这里有一个小结论：</p><blockquote><p>绝大多数图神经网络对于输入图中顶点的顺序的随机扰动有稳定性</p></blockquote><p>现在有了正样本和负样本，下一步就是训练一个图神经网络对它们加以区分了，论文选取的是GIN。这就是自监督学习的思想，对比学习就是这种思想的一种典型框架。因为现有的图神经网络框架都需要额外的顶点特征作为输入，论文提出了一种位置embedding来作为其中特征：$I-D^{-1/2}AD^{-1/2}=U\Lambda U^T$，矩阵$U$中排序靠前的特征向量作为embedding。其它特征还包括顶点度的one-hot编码和中心顶点的指示向量。</p><h4 id="模型学习"><a href="#模型学习" class="headerlink" title="模型学习"></a>模型学习</h4><p>在模型学习时采用了何凯明组的MoCo框架的思想：</p><div align="center">  <a href="https://imgchr.com/i/yimVaD" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yimVaD.png" alt="yimVaD.png" border="0" width="80%"></a></div><p>在对比学习中，我们需要维护一个大小为$K$的字典和编码器，要计算上面定义的损失函数，理想的情况是把所有负样本加入字典中进行计算，这会导致$K$很大字典难以维护。在MoCo的方法中，为了增大字典大小$K$，需要维护一个负样本的队列，队列中包含此前训练过的batch的样本作为负样本。在更新参数时，只有$q$的编码器图神经网络$f_q$中的参数通过反向传播进行更新，而$k$的编码器$f_k$中的值通过一种动量法进行更新：$\theta_k\leftarrow m\theta_k+(1-m)\theta_q$。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Academia、DBLP(SNAP)、DBLP(NetRep)、IMDB、Facebook、LiveJournal</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KDD20一篇将对比学习（contrastive learning）应用于图表示学习任务从而进行迁移的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Link Prediction Based on Graph Neural Networks[NIPS&#39;18]</title>
    <link href="http://Bithub00.com/2021/01/26/SEAL%5BNIPS18%5D/"/>
    <id>http://Bithub00.com/2021/01/26/SEAL[NIPS18]/</id>
    <published>2021-01-26T12:34:08.608Z</published>
    <updated>2021-02-01T13:44:37.333Z</updated>
    
    <content type="html"><![CDATA[<p>NIPS18一篇使用图神经网络来做图的边预测任务的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何能自动而非人工定义的方式来学习图中的结构信息，从而进行边预测。</p><p>边预测任务就是预测图中的两个顶点是否有可能有边相连。一种常用的方法为启发式方法(heuristic)，它根据定义的顶点相似度来判断这条边存在的概率有多大。几种定义相似度的方法可以根据需要使用的邻居顶点的跳数来分类，例如common neighbors与preferential attachment是一阶的，因为它们只需要一跳邻居的信息，而Adamic-Adar和resource allocation为二阶，Katz、rooted PageRank与SimRank是更高阶的相似度。</p><p>这种启发式方法的缺点在于，边存在的概率很大程度依赖于定义的顶点相似度。例如选取common neighbors这个相似度，在社交网络可能是成立的，因为如果两个人有很多共同的朋友，他们两个确实更有可能认识，但是在蛋白质交互网络截然相反，有越多相同邻居顶点的蛋白质反而越不可能建立联系。所以，与其预先定义一种相似度，不如根据网络的特点自动的学习出来。</p><p>另一个挑战是，高阶的相似度相较于低阶相似度往往能带来更好的表现，但是随着阶数越高，每个顶点所形成的子图会越来越逼近完整的图，这样会带来过高的时间复杂度与空间复杂度。本文的另一个贡献就在于，定义了一种逼近的方式，不需要$h$阶的子图也能近似的获取$h$阶子图中包含的信息，之间的误差有理论上限。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>同ICLR20的论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样（毕竟是同一个作者），论文对子图的定义方式为，给定一对顶点$(x,y)$，它的子图为顶点$x$与$y$不高于$h$阶的邻域的一个并集，数学描述如下，也就是与顶点$x$或$y$的距离小于等于$h$所构成的点的集合：</p><blockquote><p>给定一个图$G=(V,E)$，以及图上两个顶点$x、y$，它的$h$阶围绕子图(enclosing subgraph)$G^h_{x,y}$为图$G$的一个子图，满足$\{i|d(i,x)\le h\ or\ d(i,y)\le h\}$.</p></blockquote><p>接下来是定义一个$\gamma$-decaying heuristic函数，它用来逼近$h$阶子图的信息而不需要实际计算$h$阶子图：</p><script type="math/tex; mode=display">H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>其中$\gamma$是一个位于$(0,1)$的衰减因子，$\eta$是一个正的常数或一个上界为常数的函数。因为这里的求和从1到$\infin$，接下来的定理说明可以用有限项去逼近$H(x,y)$，误差随着$h$的增加而指数下降：</p><blockquote><p>定理一：</p><p>如果函数$f(x,y,l)$满足：</p><ol><li>$f(x,y,l)\le \lambda^l$，其中$\lambda &lt;\frac{1}{\gamma}$</li><li>对于$l=1,2,\dots,g(h)$，$f(x,y,l)$能够从$h$阶子图$G^h_{x,y}$中计算得到，其中$g(h)=ah+b$，$a,b\in \N,\ a&gt;0$</li></ol></blockquote><p>证明的方法很容易理解：</p><blockquote><p>逼近项为：</p><script type="math/tex; mode=display">\tilde{H}(x,y)=\eta\sum_{l=1}^{g(h)}\gamma^lf(x,y,l)</script><p>计算差值可以得到：</p><script type="math/tex; mode=display">\begin{aligned}|H(x,y)-\tilde{H}(x,y)|&=\eta\sum_{l=g(h)+1}^{\infin}\gamma^lf(x,y,l)\\&\le \eta\sum_{l=ah+b+1}^{\infin}\gamma^l\lambda^l\\&=\eta\frac{(\gamma \lambda)^{ah+b+1}}{1-\gamma \lambda}\end{aligned}</script></blockquote><p>第一个不等式是根据定理一的第一个条件，最后一个等号是根据等比数列的求和公式，当项数$n\rightarrow \infin$且$q\in(0,1)$时，结果为$\frac{a_1}{1-q}$。</p><p>到这里可能还是不知道这个$H(x,y)$和图中$h$阶的信息有什么关系，下面就通过Katz、rooted PageRank和SimRank三个高阶相似度来具体说明怎么使用：</p><p>在说明之前，先介绍一个引理，接下来会用到，证明起来也很直观：</p><blockquote><p>顶点$x$与$y$之间任意一条长度$l$满足$l\le2h+1$的路径都被包含在子图$G^h_{x,y}$中</p></blockquote><p>证明：</p><blockquote><p>即证明给定一条长度为$l$的路径$w=<x,v_1,\dots,v_{l-1},y>$中的每一个顶点都在子图中。取其中任意一个顶点$v_i$，满足$d(v_i,x)\ge h$且$d(v_i,y)\ge h$，根据子图$G^h_{x,y}$的定义它不在其中。那么有：</x,v_1,\dots,v_{l-1},y></p><script type="math/tex; mode=display">2h+1\ge l=|<x,v_1,\dots,v_i>|+|<v_i,\dots,v_{l-1},y>|\ge d(v_i,x)+d(v_i,y)=2h+2</script><p>矛盾，不等号是因为$d(x,y)$就是表示两个顶点之间的最短路径，所以有$d(v_i,x)&lt;h$或$d(v_i,y)&lt;h$，则顶点$v_i$在子图$G^h_{x,y}$中。</p></blockquote><h4 id="Katz-index"><a href="#Katz-index" class="headerlink" title="Katz index"></a>Katz index</h4><p>给定一对顶点$(x,y)$，Katz index定义为：</p><script type="math/tex; mode=display">\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}</script><p>其中$\text{walk}^{<l>}(x,y)$是这两个顶点之间长度为$l$的路径构成的集合，$A^l$是邻接矩阵的$l$次幂。从表达式可以看到，长度越长的路径在计算时会被$\beta^l$衰减的越多$(0&lt;\beta&lt;1)$，短路径有更大的权重。</l></p><p>对比两式可以发现：</p><script type="math/tex; mode=display">\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}\\H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>Katz index是论文中定义的$\gamma$-decaying heuristic函数的一种特殊形式，取$\eta=1,\gamma=\beta$，$f(x,y,l)=|\text{walks}^{<l>}(x,y)|=[A^l]_{x,y}$。根据引理，只要取长度小于2h+1的路径，其中的顶点就会全部被子图给包含，这也就满足了定理一的第2个“可计算”条件。对于第一个条件，可以通过数学归纳法说明Katz index的表达式同样满足：</l></p><blockquote><p>给定任意的顶点$i、j$，$[A^l]_{i,j}$的上限为$d^l$，其中$d$是网络中的最大顶点度</p></blockquote><p>数学归纳法证明：</p><blockquote><p>当$l=1$时，$A_{i,j}$退化成了顶点的度，那显然有$A_{i,j}\le d$成立。假设$k=l$时也成立$[A^l]_{i,j}\le d^l$，当$k=l+1$时：</p><script type="math/tex; mode=display">[A^{l+1}]_{i,j}=\sum_{k=1}^{|V|}[A^l]_{i,k}A_{k,j}\le d^l\sum_{k=1}^{|V|}A_{k,j}\le d^ld=d^{l+1}</script></blockquote><p>第一个等式就是矩阵乘法的定义，因为$[A^{l+1}]$的含义就是$l+1$个邻接矩阵$A$相乘。因此，对比定理一的第一个条件，我们只要取$\lambda=d$，$d$满足$d&lt;\frac{1}{\beta}$就能够成立，这样一来两个条件都被满足了，这说明Katz index能够很好地从$h$阶子图中近似。</p><h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><p>rooted PageRank来源于这篇论文<a href="https://dl.acm.org/doi/10.1145/511446.511513" target="_blank" rel="noopener">Topic-sensitive PageRank</a>，它通过迭代计算PageRank向量$\pi_x$来得到某一点相对于其它顶点的相似度。具体来说，它计算一个从顶点$x$开始的随机漫步的平稳分布，这个随机漫步以概率$\alpha$移动到任一邻居上或以概率$1-\alpha$回到顶点$x$。这个平稳分布满足：</p><script type="math/tex; mode=display">\pi_x=\alpha P\pi_x+(1-\alpha)e_x</script><p>其中$[\pi_x]_i$表示在这个平稳分布下漫步到顶点$i$的概率，$P$为转移矩阵，其中$P_{i,j}=\frac{1}{|\Gamma(v_j)|}$，这里的$\Gamma(v_j)$表示顶点$v_j$的一跳邻居构成的集合。如果一个顶点与五个顶点相连，那它转移到其中任意一个顶点的概率就是$\frac{1}{5}$。</p><p>rooted PageRank应用于边预测任务时，用来得到一对顶点$(x,y)$的分数，以$[\pi_x]_y$或$[\pi_x]_y+[\pi_y]_x$（对称）表示，分数越高越有可能有边相连。</p><p>接下来就要说明rooted PageRank如何能够同样以论文中提出的$\gamma$-decaying heuristic函数进行表示。根据<a href="http://infolab.stanford.edu/~glenj/spws.pdf" target="_blank" rel="noopener">inverse P-distance理论</a>，$[\pi_x]_y$能够等价地改写为：</p><script type="math/tex; mode=display">[\pi_x]_y=(1-\alpha)\sum_{w:x\leadsto y}P[w]\alpha^{len(w)}</script><p> 这里的求和范围$w:x\leadsto y$表示所有从$x$开始结束于$y$的路径，$P[w]$定义为$\prod_{i=0}^{k-1}\frac{1}{|\Gamma(v_i)|}$，$k$是路径长度，$v_i$是路径中的顶点，通过这条路径来从$x$到$y$的概率就是漫步到路径中每一个顶点的概率的连乘。</p><p>接下来就是证明这个形式满足定理一的两个条件：</p><blockquote><p>首先进一步改写：</p><script type="math/tex; mode=display">[\pi_x]_y=(1-\alpha)\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]\alpha^l\\H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>对比：取$\gamma=\alpha,\eta=(1-\alpha),f(x,y,l)=\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]$。因为这时候$f(x,y,l)$表示一个随机漫步恰好以$l$步从顶点$x$漫步到$y$的概率，有$\sum_{z\in V}f(x,z,l)=1$，则$f(x,y,l)\le1&lt;\frac{1}{\alpha}$，这样就满足了定理一，而根据引理，只要取长度小于等于2h+1的路径，路径中的点就会被全部包含在子图中，也就满足了第二个”可计算“条件。</p></blockquote><h4 id="SimRank"><a href="#SimRank" class="headerlink" title="SimRank"></a>SimRank</h4><p>SimRank的核心思想是，如果两个顶点的邻域相似，那它们也相似：</p><script type="math/tex; mode=display">s(x,y)=\gamma \frac{\sum_{a\in\Gamma(x)}\sum_{b\in \Gamma(y)}s(a,b)}{|\Gamma(x)|·|\Gamma(y)|}</script><p>它有一个<a href="https://dl.acm.org/doi/10.1145/775047.775126" target="_blank" rel="noopener">等价定义形式</a>：</p><script type="math/tex; mode=display">s(x,y)=\sum_{w:(x,y)\multimap (z,z)}P[w]\gamma^{len(w)}</script><p>其中$w:(x,y)\multimap (z,z)$表示从顶点$x$开始的随机漫步与从顶点$y$开始的随机漫步第一次相遇于顶点$z$。证明与rooted PageRank基本一致，可以见原论文。</p><p>总结来说，$\gamma$-decaying heuristic函数的思想是，对于远离目标顶点的结构信息通过指数衰减的方式给一个更小的权重，因为它们带来的信息十分有限。</p><h4 id="SEAL框架"><a href="#SEAL框架" class="headerlink" title="SEAL框架"></a>SEAL框架</h4><div align="center"><a href="https://imgchr.com/i/ypCC6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/ypCC6A.png" alt="ypCC6A.png" border="0" width="80%"></a></div><p>这一节就是根据上面的理论分析建立一个用于边预测任务的框架。一个图神经网络的典型输入形式是$(A,X)$，在本论文中，$A$自然地被定义为子图$G^h_{x,y}$的邻接矩阵，子图的获取即来自正样本（已知边）也来自负样本（未知边）。接下来的部分就是介绍论文怎么定义顶点的特征矩阵$X$，它包含三个部分：structural node labels、node embeddings和node attributes。</p><h5 id="Node-labeling"><a href="#Node-labeling" class="headerlink" title="Node labeling"></a>Node labeling</h5><p>跟作者的另一篇论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样，通过给顶点打标签的方式来区别顶点在子图中的不同角色，这么做的意义在另一篇博客说过了这里就不写了，具体打标签的方式为：</p><ul><li>起始顶点$x$与目标顶点$y$的标签都为”1“</li><li>如果两个顶点$i、j$距离起始顶点与目标顶点的距离都相同，那么它们的标签一样</li><li>$(d(i,x),d(i,y))=(a,b)\rightarrow label:a+b$</li></ul><p>将顶点的标签进行one-hot编码后作为结构特征。</p><h5 id="Node-embeddings-Node-attributes"><a href="#Node-embeddings-Node-attributes" class="headerlink" title="Node embeddings + Node attributes"></a>Node embeddings + Node attributes</h5><p>Node attributes一般数据集直接给定，而Node embeddings是通过一个GNN得到，具体做法是：给定正样本$E_p\in E$，负样本$E_n$，$E_p\and E_n=\empty$，在这么一个图$G’=(V,E\and E_n)$上生成embeddings，防止过拟合。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>USAir、NS、PB、Yeast、C.ele、Power、Router、E.coli</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NIPS18一篇使用图神经网络来做图的边预测任务的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>C++常用操作</title>
    <link href="http://Bithub00.com/2021/01/17/c++%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>http://Bithub00.com/2021/01/17/c++常用操作/</id>
    <published>2021-01-17T13:48:25.048Z</published>
    <updated>2021-05-28T09:03:51.235Z</updated>
    
    <content type="html"><![CDATA[<p>记录备查</p><a id="more"></a><h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><ul><li>从字符串中删除子串</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">removeSubstrs</span><span class="params">(<span class="built_in">string</span>&amp; s, <span class="keyword">const</span> <span class="built_in">string</span>&amp; e)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = e.length();</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">string</span>::size_type i = s.find(e); i != <span class="built_in">string</span>::npos; i = s.find(e))</span><br><span class="line">            s.erase(i, n);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>使用STL库实现小顶堆</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">priority_queue &lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt; &gt; pq;</span><br></pre></td></tr></table></figure><ul><li>自定义排序</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> x, y, len;</span><br><span class="line">    Edge(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span> len) : x(x), y(y), len(len)&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;Edge&gt; edges;</span><br><span class="line">sort(edges.begin(), edges.end(), [](Edge a, Edge b) -&gt; <span class="keyword">int</span> &#123;<span class="keyword">return</span> a.len &lt; b.len;&#125;);</span><br></pre></td></tr></table></figure><ul><li>初始化二维vector</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> row = <span class="number">3</span>, col = <span class="number">3</span>;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; matrix(row, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(col, <span class="number">0</span>));</span><br></pre></td></tr></table></figure><ul><li>位运算</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//求二进制中1的个数</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(n != <span class="number">0</span>)&#123;</span><br><span class="line">  n &amp;= n<span class="number">-1</span>;</span><br><span class="line">  count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//除2</span></span><br><span class="line">x &gt;&gt;= <span class="number">1</span>;</span><br><span class="line"><span class="comment">//乘2</span></span><br><span class="line">x &lt;&lt;= <span class="number">1</span>;</span><br><span class="line"><span class="comment">//大写转小写</span></span><br><span class="line">ch |= <span class="string">' '</span>;</span><br><span class="line"><span class="comment">//小写转大写</span></span><br><span class="line">ch &amp;= <span class="string">'_'</span>;</span><br><span class="line"><span class="comment">//第i位是否为1</span></span><br><span class="line">(val &gt;&gt; i) &amp; <span class="number">1</span>;</span><br></pre></td></tr></table></figure><ul><li>分割字符串</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//完全分割</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">split</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; record, <span class="keyword">const</span> <span class="keyword">char</span> delim = <span class="string">'.'</span>)</span> </span>&#123;</span><br><span class="line">        record.clear();</span><br><span class="line">        <span class="function"><span class="built_in">istringstream</span> <span class="title">is</span><span class="params">(s)</span></span>;</span><br><span class="line">        <span class="built_in">string</span> temp;</span><br><span class="line">        <span class="keyword">while</span> (getline(is, temp, delim)) &#123;</span><br><span class="line">            record.push_back(move(temp));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//只分割一次</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">split</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; record, <span class="keyword">const</span> <span class="keyword">char</span> delim = <span class="string">'.'</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//s="www.bithub00.com"➡"bithub00.com"、"com"</span></span><br><span class="line">        record.clear();</span><br><span class="line">        <span class="keyword">int</span> index = s.find(<span class="string">'.'</span>);</span><br><span class="line">  <span class="keyword">while</span>(index &gt; <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">string</span> t = s.substr(index+<span class="number">1</span>);</span><br><span class="line">          record.push_back(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>从句子中读取单词</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">string</span> A = <span class="string">"Data Mining"</span>;</span><br><span class="line"><span class="function"><span class="built_in">istringstream</span> <span class="title">in</span><span class="params">(A)</span></span>;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; v;</span><br><span class="line"><span class="built_in">string</span> t;</span><br><span class="line"><span class="keyword">while</span>(in &gt;&gt; t)&#123;</span><br><span class="line">v.push_back(t);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>求文件行数</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">(<span class="keyword">char</span> *File)</span> </span>&#123;</span><br><span class="line"><span class="function">ifstream <span class="title">myfile</span><span class="params">(File)</span></span>;</span><br><span class="line"><span class="built_in">string</span> line;</span><br><span class="line"><span class="keyword">int</span> lineNum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (getline(myfile, line))</span><br><span class="line">lineNum++;</span><br><span class="line">myfile.clear();</span><br><span class="line">myfile.seekg(<span class="number">0</span>, ios::beg); <span class="comment">//回到文件第一行</span></span><br><span class="line"><span class="keyword">return</span> lineNum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>循环查找子串出现的所有位置</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> pos = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">string</span> str;</span><br><span class="line"><span class="built_in">string</span> substr;</span><br><span class="line"><span class="keyword">while</span>(pos != <span class="built_in">string</span>::npos) &#123;</span><br><span class="line">  pos = str.find(substr, pos);</span><br><span class="line">  pos++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>二分区间查找</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> left = <span class="number">0</span>, right = arr.size()<span class="number">-1</span>;</span><br><span class="line"><span class="keyword">while</span>(left &lt;= right) &#123;</span><br><span class="line">  <span class="keyword">int</span> mid = left + (right-left)/<span class="number">2</span>;</span><br><span class="line">  <span class="keyword">if</span>(arr[mid] &gt; value) right = mid<span class="number">-1</span>;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span>(arr[mid] &lt; value) left = mid+<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    right = mid;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//此时arr[right]就是arr中小于等于value的最大数</span></span><br><span class="line"><span class="keyword">return</span> arr[right];</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录备查&lt;/p&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://Bithub00.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Inductive Matrix Completion Based on Graph Neural Networks[ICLR&#39;20]</title>
    <link href="http://Bithub00.com/2021/01/14/ICMC%5BICLR20%5D/"/>
    <id>http://Bithub00.com/2021/01/14/ICMC[ICLR20]/</id>
    <published>2021-01-14T12:56:15.050Z</published>
    <updated>2021-01-15T02:46:33.394Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR20一篇使用GNN来解决现有矩阵补全方法无法泛化问题的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何让矩阵补全方法中一个数据集得到的embedding，能够迁移到另一个数据集上，同时不依赖额外的信息。</p><p>与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>应用的问题相同，具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。传统的做法是将输入的评分矩阵分解成用户与物品的embedding，通过embedding重构评分矩阵，填补其中的缺失值，从而做出预测，如下图所示：</p><div align="center"><a href="https://imgchr.com/i/sd6O1S" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd6O1S.png" alt="sd6O1S.png" border="0" width="80%"></a></div><p>很多现有方法研究的都是如何得到更好的embedding，但它们都是直推式(transductive)而非启发式(inductive)的，意味着没法迁移，例如MovieLens数据集上得到的embedding就不能直接用于Douban数据集上，需要重新训练一个新的embedding。即使对于同一个数据集而言，如果加入新的评分记录，往往需要整个embedding重新训练。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Enclosing-Subgraph-Extraction"><a href="#Enclosing-Subgraph-Extraction" class="headerlink" title="Enclosing Subgraph Extraction"></a>Enclosing Subgraph Extraction</h4><p>论文的做法是为每一个评分记录提取一个子图，并且训练一个图神经网络来将得到的子图映射为预测评分。要想为评分记录提取子图，首先要将评分矩阵转换为图，转换的方法与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>相同，博客中有具体介绍，这里就不重复说明了。论文中对子图的定义方式为，给定一个评分记录$(u,v)$，表示用户$u$给物品$v$评过分，那么这个评分记录提取的子图由该用户$u$、物品$v$以及它们各自的$h$跳邻域内的顶点构成。为了具体说明是怎么从一个评分记录提取出子图的，我从论文作者的视频中截取了这部分内容，如下图所示：</p><p>假设第一张图中深绿色的方格是缺失值，这里先填入了模型的预测评分，倒退着来说明预测评分是怎么通过子图得到的。我们首先找到这个用户评过分的其它物品，对应于第五个物品的四分与第八个物品的两分，如第二张图所示。下一步是找到为这个物品评过分的其他用户，对应于第三个用户的五分与第四个用户的五分。</p><div align="center"><a href="https://imgchr.com/i/sdfNf1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sdfNf1.png" alt="sdfNf1.png" border="0"></a></div><p>通过图二和图三找到的关系，就可以提取出这个评分记录的子图了，如下图所示：</p><div align="center"><a href="https://imgchr.com/i/sd4urT" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd4urT.png" alt="sd4urT.png" border="0" width="90%"></a></div><p>可以看到，这个提取出的子图能提供许多有用的信息，例如用户平均评分、物品平均评分、物品累计评价次数以及基于路径的结构信息。论文希望通过这种结构信息来找到一些特征，从而做出预测，例如，如果用户$u_0$喜欢一个物品$v_0$，那么对于另一个与他品味相同的用户$u_1$，我们可能发现他也喜欢$v_0$。品味相同可以表示为两个用户都喜欢另一个物品$v_1$，这个特征可以表示为这么一条路径：$u_0\rightarrow_{like}v_1\rightarrow_{liked\ by}u_1\rightarrow_{like}v_0$，如果$u_0$与$v_0$之间存在多条这样的路径，那么我们就可以推测$u_0$喜欢$v_0$。类似这样的结构特征数不胜数。因此，与其人工来手动定义大量这样的启发式特征(heuristics)，不如直接将子图输入一个图神经网络，来自动学习更通用的、更有表达能力的特征。</p><h4 id="Node-Labeling"><a href="#Node-Labeling" class="headerlink" title="Node Labeling"></a>Node Labeling</h4><p>这一步给顶点打标签是为了让子图中的顶点有着不同的角色，例如区分哪个是需要预测的目标用户与目标物品，区分用户顶点与物品顶点。而论文中打标签的方式十分简单：</p><ul><li>目标用户与目标物品分别标记为0和1</li><li>对于$h$跳邻域内的顶点，如果是用户顶点标记为$2h$，物品顶点则标记为$2h+1$</li></ul><p>标记之后，我们就能知道哪个是需要预测的目标用户与目标物品、哪些是用户顶点，因为用户顶点的标签均为偶数，以及邻域内顶点距离目标顶点距离的远近。这些标签将转换为one-hot编码的形式作为图神经网络输入的初始特征$x_0$。</p><p>这一节的最后论文作者还说到了这种标记方式与<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>做法的不同之处。GCMC中同样是将标签转换为one-hot编码的形式作为GNN的初始特征，不同的是它用顶点在整个bipartite graph中的全局id作为它的标签，这等价于将GNN第一层信息传递网络的参数，转换为与每个顶点的全局id相关联的embedding函数，可以理解为一个embedding查找表，输入一个全局id，输出它对应的embedding。这显然是直推式的，对于不在查找表中的id，就无法得到它的embedding。这种情况对应于在小数据集上训练网络得到embedding，然后换到大数据集上，因为大数据集的顶点数量肯定要多于小数据集，这就会使得顶点的全局id范围变大，超出了训练出来的这个embedding查找表的范围。</p><h4 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h4><p>这一步的目的就是训练一个GNN来将提取出的子图映射成预测评分。论文所使用的GNN分为两个部分：信息传递层与池化层。前者的作用是得到子图中各顶点的特征向量，后者是根据得到的特征向量形成子图的一个特征表示。</p><p>信息传递部分使用的是<a href="https://arxiv.org/pdf/1703.06103v4.pdf" target="_blank" rel="noopener">R-GCN</a>：</p><script type="math/tex; mode=display">x_i^{l+1}=W_0^lx_i^l+\sum_{r\in R}\sum_{j\in N_r(i)}\frac{1}{|N_r(i)|}W_r^lx_j^l</script><p>其中$x_i^l$表示第$i$个顶点在第$l$层的特征向量，$N_r(i)$表示评分水平$r$下顶点$i$的邻域，顶点$i$以不同的边权重$r$所连接的顶点$j$用不同的参数矩阵$W_r^l$来进行处理。通过堆叠$L$层网络可以得到顶点$i$的$L$个特征向量，通过拼接的方式得到它最终的特征表示$h_i$：</p><script type="math/tex; mode=display">h_i=\text{concat}(x_i^1,x_i^2,\dots,x_i^L)</script><p>池化部分只选取子图中目标用户与目标顶点的特征向量进行拼接，来得到该子图的特征表示，这么做的原因是这两个顶点携带了最多的信息。</p><script type="math/tex; mode=display">g=\text{concat}(h_u,h_v)</script><p>在得到子图的特征表示后，最后一步是通过一个MLP将它转换为一个预测评分$\hat{r}$：</p><script type="math/tex; mode=display">\hat{r}=w^T\sigma(Wg)</script><h4 id="Adjacent-Rating-Regularization"><a href="#Adjacent-Rating-Regularization" class="headerlink" title="Adjacent Rating Regularization"></a>Adjacent Rating Regularization</h4><p>论文对于信息传递部分使用的R-GCN还提出了一点改进，在原始的R-GCN中，不同的评分水平是独立看待的，彼此之间没有关联，例如对于1、4、5这三个评分，显然地4和5都表示了用户的喜爱而1表示了用户的厌恶，同时4和5的相似程度要大于4和1，但这种次序关系及大小关系在原始的R-GCN中都被丢掉了。因此本论文添加了一个约束来引入这部分丢失的信息，具体做法也很简单，就是使得相邻的评分水平使用的参数矩阵更加相似：</p><script type="math/tex; mode=display">L_{ARR}=\sum_{i=1,2,\dots,|R|-1}||W_{r_i+1}-W_{r_i}||_F^2</script><p>这里假设评分$r_1,r_2,\dots,r_{|R|}$表示了用户喜爱程度的递增，通过这个约束就保留了评分的次序信息，同时可以使得出现次数较少的评分水平可以从相邻的评分水平中迁移信息，来弥补数据不足带来的问题。</p><h4 id="Graph-level-GNN-vs-Node-level-GNN"><a href="#Graph-level-GNN-vs-Node-level-GNN" class="headerlink" title="Graph-level GNN vs Node-level GNN"></a>Graph-level GNN vs Node-level GNN</h4><p>这一节还是在于GCMC作比较。在GCMC中，采用的是顶点层面的图神经网络，它应用于图中的顶点来得到顶点的embedding，再通过embedding得到预测评分，如下右图所示。这么做的缺陷是它独立地学习两个顶点所关联的子树，而忽略了这两棵子树之间可能存在的联系。</p><p><div align="center"><a href="https://imgchr.com/i/swYFij" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/15/swYFij.png" alt="swYFij.png" border="0" width="50%"></a></div></p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、ML-100K、ML-1M</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICLR20一篇使用GNN来解决现有矩阵补全方法无法泛化问题的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Variational Graph Auto-Encoders[NIPS&#39;16]</title>
    <link href="http://Bithub00.com/2021/01/09/VGAE%5BNIPS16%5D/"/>
    <id>http://Bithub00.com/2021/01/09/VGAE[NIPS16]/</id>
    <published>2021-01-09T13:54:14.216Z</published>
    <updated>2021-01-11T04:48:15.109Z</updated>
    
    <content type="html"><![CDATA[<p>NIPS16一篇将变分自编码器迁移到图结构数据上的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在图结构数据上如何使用变分自编码器</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>将已知的图进行编码（图卷积）得到图中顶点向量表示的一个分布，在分布中采样得到顶点的向量表示，然后进行解码重新构建图。</p><h4 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h4><p>因为这篇论文做的是一个迁移的工作，变分自编码器的背景对于理解这篇论文来说十分重要，首先进行介绍。</p><p>变分自编码器是自编码器的一种，一个自编码器由编码器和解码器构成，编码器将输入数据转换为低维向量表示，解码器通过得到的低维向量表示进行重构。</p><div align="center"><a href="https://imgchr.com/i/sl9ZxP" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9ZxP.jpg" alt="sl9ZxP.jpg" border="0" width="80%"></a><a href="https://imgchr.com/i/sl9G2q" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9G2q.jpg" alt="sl9G2q.jpg" border="0" width="65%"></a></div>这种结构的不足之处在于，只能产生与输入数据相似的样本，而无法产生新的样本，低维向量表示必须是有真实样本通过编码器得到的，随机产生的低维向量经过重构几乎不可能得到近似真实的样本。而变分自编码器可以解决这个问题。变分自编码器将输入数据编码为一个分布，而不是一个个低维向量表示，然后从这个分布中随机采样来得到低维向量表示。一般假设这个分布为正态分布，因此编码器的任务就是从输入数据中得到均值$\mu$与方差$\sigma^2$。<div align="center"><a href="https://imgchr.com/i/slCW60" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slCW60.jpg" alt="slCW60.jpg" border="0" width="80%"></a><a href="https://imgchr.com/i/slPZB8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPZB8.jpg" alt="slPZB8.jpg" border="0" width="80%"></a></div>然而，如果是将所有输入数据编码到同一个分布里，从这个分布中随机采样的样本$Z_i$无法与输入样本$X_i$一一对应，会影响模型的学习效果。所以，实际的变分自编码器结构如下图所示，为每一个输入样本学习一个正态分布：<div align="center"><a href="https://imgchr.com/i/slPgED" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPgED.jpg" alt="slPgED.jpg" border="0" width="80%"></a></div>采样时常用"重参数"技巧(reparameterization trick)，从分布$N(\mu,\sigma^2)$中采样一个$Z$相当于从$N(0,1)$中采样一个$\epsilon$使得$Z=\mu+\sigma*\epsilon$。  #### 图变分自编码器介绍完传统的变分自编码器，接下来就是介绍这篇论文的工作，如何将变分自编码器的思想迁移到图上。针对图这个数据结构，输入的数据变为图的邻接矩阵$A$与特征矩阵$X$：  邻接矩阵$A$：<div align="center"><a href="https://imgchr.com/i/slFHhQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFHhQ.jpg" alt="slFHhQ.jpg" border="0" width="60%"></a></div>特征矩阵$X$：<div align="center"><a href="https://imgchr.com/i/slFz7T" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFz7T.jpg" alt="slFz7T.jpg" border="0" width="60%"></a></div><p>接下来的工作与变分自编码器相同，通过编码器（图卷积）学习图中顶点低维向量表示分布的均值$\mu$与方差$\sigma^2$，再通过解码器生成图。</p><div align="center"><a href="https://imgchr.com/i/slk1gA" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slk1gA.jpg" alt="slk1gA.jpg" border="0" width="80%"></a></div><p>编码器采用两层结构的图卷积网络，第一层产生一个低维的特征矩阵：</p><script type="math/tex; mode=display">\bar{X}=\text{GCN}(X,A)=\text{ReLU}(\tilde{A}XW_0)\\\tilde{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>第二层得到分布的均值$\mu$与方差$\sigma^2$：</p><script type="math/tex; mode=display">\mu=\text{GCN}_{\mu}(X,A)=\tilde{A}\bar{X}W_1\\\log\sigma^2=\text{GCN}_{\sigma}(X,A)=\tilde{A}\bar{X}W_1</script><p>将两层网络的表达式合并可以得到编码器的表达式：</p><script type="math/tex; mode=display">\text{GCN}(X,A)=\tilde{A}\text{ReLU}(\tilde{A}XW_0)W_1</script><p>同样地使用重参数技巧来得到低维向量表示$Z=\mu+\sigma*\epsilon$。</p><p>编码器重构出图的邻接矩阵，从而得到一个新的图。之所以使用点积的形式来得到邻接矩阵，原因在于我们希望学习到每个顶点的低维向量表示$z$的相似程度，来更好地重构邻接矩阵。而点积可以计算两个向量之间的cosine相似度，这种距离度量方式不受量纲的影响。因此，重构的邻接矩阵可以学习到各个顶点之间的相似程度。</p><script type="math/tex; mode=display">\hat{A}=\sigma(zz^T)</script><p>损失函数用于衡量生草样本与真是样本之间的差异，但如果只用距离度量作为损失函数，为了让编码器的效果最佳，模型会将方差的值学为0，这样从正态分布中采样出来的就是定值，有利于减小生成样本和真实样本之间的差异。但这样一来，就退化成了普通的自编码器，因此在构建损失函数时，往往还会加入各独立正态分布与标准正态分布的KL散度，来使得各个正态分布逼近标准正态分布：</p><script type="math/tex; mode=display">L=E_{q(Z|X,A)}[\log p(A|Z)]-\text{KL}[q(Z|X,A)||p(Z)],\quad where\quad p(Z)=N(0,1)</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NIPS16一篇将变分自编码器迁移到图结构数据上的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Graph Convolutional Matrix Completion[KDD&#39;18]</title>
    <link href="http://Bithub00.com/2021/01/09/GCMC%5BKDD18%5D/"/>
    <id>http://Bithub00.com/2021/01/09/GCMC[KDD18]/</id>
    <published>2021-01-09T13:17:15.588Z</published>
    <updated>2021-01-09T13:30:55.861Z</updated>
    
    <content type="html"><![CDATA[<p>KDD18一篇将图卷积网络用于矩阵补全问题的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将图卷积网络应用于矩阵补全问题。</p><p>具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。如果将评分矩阵转换为一张图，转换方法在下面有进行介绍，这时矩阵补全问题也可以看成图上的边预测问题。要预测用户对一个物品的评分，就是预测图上两个对应顶点之间相连的边的权重。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文通过一个编码器-解码器的架构来实现从已有评分到特征表示再到预测评分的过程。</p><div align="center"><a href="https://imgchr.com/i/sQUdAS" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUdAS.png" alt="sQUdAS.png" border="0" width="70%"></a></div><h4 id="Bipartite-Graph-Construction"><a href="#Bipartite-Graph-Construction" class="headerlink" title="Bipartite Graph Construction"></a>Bipartite Graph Construction</h4><p>首先是将推荐任务里的评分数据转化为一张图，具体做法是将用户和物品都看作图中的顶点，交互记录看作边，分数作为边的权重，如图所示：</p><div align="center"><a href="https://imgchr.com/i/su9fr4" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/08/su9fr4.png" alt="su9fr4.png" border="0" width="60%"></a></div><h4 id="Graph-Convolutional-Encoder"><a href="#Graph-Convolutional-Encoder" class="headerlink" title="Graph Convolutional Encoder"></a>Graph Convolutional Encoder</h4><p>上一步所构建的图的输入形式为邻接矩阵$A\in \mathbb{R}^{n\times n}$与图中顶点的特征矩阵$X\in \mathbb{R}^{n\times d}$。编码器在这一步的作用就是得到用户与物品的特征表示$A,X^u,X^v\rightarrow U,V$。</p><p>具体编码时，论文将不同的评分水平分开考虑$r\in \{1,2,3,4,5\}$，我的理解是它们类似于处理图像数据时的多个channel。以一个评分水平$r$为例，说明编码得到特征表示的过程。假设用户$u_i$对电影$v_j$评分为$r$，而这部电影的特征向量为$x_j$，那么这部电影对这个用户特征表示的贡献可以表示为下面的式子(1)，相当于对特征向量进行了一个线性变换。</p><div align="center"><a href="https://imgchr.com/i/sQUHnx" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUHnx.png" alt="sQUHnx.png" border="0" width="80%"></a></div>对当前评分水平下所有评过分的电影进行求和，再对所有评分水平求和拼接，经过一个非线性变换，就得到了用户$u_i$的特征表示$h_{u_i}$，物品的做法相同。<div align="center"><a href="https://imgchr.com/i/sQdv6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQdv6A.png" alt="sQdv6A.png" border="0" width="80%"></a></div><div align="center"><a href="https://imgchr.com/i/sQwmmq" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQwmmq.png" alt="sQwmmq.png" border="0" width="80%"></a></div><h4 id="Bilinear-Decoder"><a href="#Bilinear-Decoder" class="headerlink" title="Bilinear Decoder"></a>Bilinear Decoder</h4><p>在分别得到用户与物品的特征表示$U$与$V$后，解码器计算出用户对物品评分为$r$的概率，再对每个评分的概率进行求和，得到最终预测的评分。</p><script type="math/tex; mode=display">\begin{aligned}(P_r)_{ij}&=\frac{\exp(u_i^TQ_rv_j)}{\sum_{s\in R}\exp(u_i^TQ_sv_j)} \\\hat{M}&=\sum_{r\in R}rP_r\end{aligned}</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、MovieLens</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KDD18一篇将图卷积网络用于矩阵补全问题的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks for Social Recommendation[WWW&#39;19]</title>
    <link href="http://Bithub00.com/2020/12/22/GraphRec%5BWWW19%5D/"/>
    <id>http://Bithub00.com/2020/12/22/GraphRec[WWW19]/</id>
    <published>2020-12-22T03:22:36.248Z</published>
    <updated>2021-05-29T14:16:57.583Z</updated>
    
    <content type="html"><![CDATA[<p>WWW19将GNN应用于社会化推荐的一篇论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将GNN应用于社会化推荐任务上。</p><p>面临的挑战有三点：</p><ol><li>在一个社会化推荐任务中，输入的数据包括社会关系图和用户-物品交互图，将两张图的信息都聚合才能得到用户更好的一个表示，而此前的GNN只是在同一张图上对邻域内的信息聚合。</li><li>在用户-物品交互图中，顶点与顶点之间的边也包含更多的信息，除了表示是否交互，还能表示用户对一个物品的偏好（喜爱还是厌恶），而此前的GNN只是将边用来表示是否交互。</li><li>社会关系图中用户之间的纽带有强有弱，显然地，一个用户更可能与强纽带的其它用户有类似的喜好。如果将所有纽带关系都看成一样，会有偏差。</li></ol><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>创新：</p><ul><li>在不同图(user-user graph和user-item graph)上进行信息传递与聚合</li><li>除了捕获user-item间的交互关系，还利用了user对item的评分</li><li>用attention机制表示社交关系的重要性，用户纽带的强与弱</li></ul><div align="center"><a href="https://imgchr.com/i/r0xT1A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/r0xT1A.png" alt="r0xT1A.png" border="0" width="90%"></a></div><p>整个GraphRec框架由三个部分组成，分别为user modeling、item modeling和rating prediction。其中user modeling用来学习用户的特征表示，学习的方式是两个聚合：item aggregation和social aggregation，类似地item modeling用来学习物品的特征表示，学习的方式是一个聚合：user aggregation。</p><h4 id="User-Modeling"><a href="#User-Modeling" class="headerlink" title="User Modeling"></a>User Modeling</h4><h5 id="item-aggregation"><a href="#item-aggregation" class="headerlink" title="item aggregation"></a>item aggregation</h5><div align="center"><a href="https://imgchr.com/i/rBuFzt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBuFzt.png" alt="rBuFzt.png" border="0" width="40%"></a></div><p>item aggregation的目的是通过用户交互过的物品以及对这些物品的倾向，来学习物品侧的用户特征表示，数学表示为：</p><script type="math/tex; mode=display">h_i^I=\sigma(W·Aggre_{items}(\{x_{ia},\forall a\in C(i)\})+b)</script><p>$C(i)$就表示用户交互过的物品的一个集合。这里的$x_{ia}$是一个表示向量，它应该能够同时表示交互关系和用户倾向。论文中的做法是通过一个MLP来结合物品的embedding和倾向的embedding，两者分别用$q_a$和$e_r$表示。倾向的embedding可能很难理解，以五分制评分为例，倾向的embedding表示为$e_r\in \mathbb{R}^d$，其中$r\in \{1,2,3,4,5\}$。</p><script type="math/tex; mode=display">x_{ia}=g_v([q_a\oplus e_r])</script><p>定义好$x_{ia}$后，下一步就是如何选取聚合函数$Aggre$了。论文中使用的是attention机制，来源于<a href="#Graph Attention Networks[ICLR&#39;18]">GAT</a>：</p><script type="math/tex; mode=display">\begin{aligned}h_i^I&=\sigma(W·\Big\{\sum_{a\in C(i)}\alpha_{ia}x_{ia}\Big\}+b) \\\alpha_{ia}'&=w_2^T·\sigma(W_1·[x_{ia}\oplus p_i]+b_1)+b_2 \\\alpha_{ia}&=\frac{\exp(\alpha_{ia}')}{\sum_{a\in C(i)}\exp(\alpha_{ia}')}\end{aligned}</script><p>这里的权重$\alpha_{ia}$考虑了$x_{ia}$和用户$u_i$的embedding $p_i$，使得权重能够与当前用户相关。</p><h5 id="social-aggregation"><a href="#social-aggregation" class="headerlink" title="social aggregation"></a>social aggregation</h5><div align="center"><a href="https://imgchr.com/i/rBK7g1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBK7g1.png" alt="rBK7g1.png" border="0" width="40%"></a></div><p>social aggregation中，同样地使用了attention机制，通过attention机制来选取强纽带的其它用户（表现为聚合时权重更大）并聚合他们的信息，聚合的就是物品侧的用户特征表示。</p><script type="math/tex; mode=display">\begin{aligned}h_i^S&=\sigma(W·\Big\{\sum_{o\in N(i)}\beta_{io}h_o^I\Big\}+b) \\\beta_{io}'&=w_2^T·\sigma(W_1·[h_o^I\oplus p_i]+b_1)+b_2 \\\beta_{io}&=\frac{\exp(\beta_{io}')}{\sum_{o\in N(i)}\exp(\beta_{io}')}\end{aligned}</script><p>这里跟item aggregation基本一模一样，就不多介绍了。</p><p>得到物品侧的用户特征表示$h_i^I$和社交侧的用户特征表示$h_i^S$后，用一个MLP将它们结合，得到用户最终的特征表示：</p><script type="math/tex; mode=display">\begin{aligned}c_1&=[h_i^I\oplus h_i^S] \\c_2&=\sigma(W_2·c_1+b_2) \\&······ \\h_i&=\sigma(W_l·c_{l-1}+b_l)\end{aligned}</script><h4 id="Item-Modeling"><a href="#Item-Modeling" class="headerlink" title="Item Modeling"></a>Item Modeling</h4><h5 id="user-aggregation"><a href="#user-aggregation" class="headerlink" title="user aggregation"></a>user aggregation</h5><div align="center"><a href="https://imgchr.com/i/rBYtjH" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBYtjH.png" alt="rBYtjH.png" border="0" width="50%"></a></div><p>Item modeling与User modeling的做法基本一模一样…公式都是一一对应的：</p><script type="math/tex; mode=display">\begin{aligned}f_{jt}&=g_u([p_t\oplus e_r]) \\z_j&=\sigma(W·\Big\{\sum_{t\in B(j)}\mu_{jt}f_{jt}\Big\}+b) \\\mu_{jt}'&=w_2^T·\sigma(W_1·[f_{jt}\oplus q_j]+b_1)+b_2 \\\mu_{jt}&=\frac{\exp(\mu_{jt}')}{\sum_{a\in C(i)}\exp(\mu_{jt}')}\end{aligned}</script><h4 id="Rating-Prediction"><a href="#Rating-Prediction" class="headerlink" title="Rating Prediction"></a>Rating Prediction</h4><p>最后来到评分预测部分，由上面两个部分我们得到了用户特征表示$h_i$与物品特征表示$z_j$，产生评分用的也是一个MLP：</p><script type="math/tex; mode=display">\begin{aligned}g_1&=[h_i\oplus z_j] \\g_2&=\sigma(W_2·g_1+b_2) \\&······ \\g_{l-1}&=\sigma(W_l·g_{l-1}+b_l) \\r_{ij}&=w^T·g_{l-1}\end{aligned}</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Ciao、Epinions</p><p>在科技论文写作课上对这篇论文进行了分享，这里直接把讲稿和PPT放上来。</p><h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><p><a href="https://imgtu.com/i/2EP9mj" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP9mj.png" alt="2EP9mj.png"></a><br><a href="https://imgtu.com/i/2EPSXQ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPSXQ.png" alt="2EPSXQ.png"></a><br><a href="https://imgtu.com/i/2ECz6g" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2ECz6g.png" alt="2ECz6g.png"></a><br><a href="https://imgtu.com/i/2EPP7n" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPP7n.png" alt="2EPP7n.png"></a><br><a href="https://imgtu.com/i/2EPC0s" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPC0s.png" alt="2EPC0s.png"></a><br><a href="https://imgtu.com/i/2EPkt0" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPkt0.png" alt="2EPkt0.png"></a><br><a href="https://imgtu.com/i/2EPFkq" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPFkq.png" alt="2EPFkq.png"></a><br><a href="https://imgtu.com/i/2EPVpT" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPVpT.png" alt="2EPVpT.png"></a><br><a href="https://imgtu.com/i/2EPAhV" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPAhV.png" alt="2EPAhV.png"></a><br><a href="https://imgtu.com/i/2EPZ1U" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPZ1U.png" alt="2EPZ1U.png"></a><br><a href="https://imgtu.com/i/2EPecF" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPecF.png" alt="2EPecF.png"></a><br><a href="https://imgtu.com/i/2EPmX4" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPmX4.png" alt="2EPmX4.png"></a><br><a href="https://imgtu.com/i/2EPunJ" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPunJ.png" alt="2EPunJ.png"></a><br><a href="https://imgtu.com/i/2EPKB9" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPKB9.png" alt="2EPKB9.png"></a><br><a href="https://imgtu.com/i/2EPM7R" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPM7R.png" alt="2EPM7R.png"></a><br><a href="https://imgtu.com/i/2EPlA1" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPlA1.png" alt="2EPlA1.png"></a><br><a href="https://imgtu.com/i/2EP1tx" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP1tx.png" alt="2EP1tx.png"></a><br><a href="https://imgtu.com/i/2EPG9K" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPG9K.png" alt="2EPG9K.png"></a><br><a href="https://imgtu.com/i/2EP3h6" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP3h6.png" alt="2EP3h6.png"></a><br><a href="https://imgtu.com/i/2EPJ1O" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPJ1O.png" alt="2EPJ1O.png"></a><br><a href="https://imgtu.com/i/2EPtje" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPtje.png" alt="2EPtje.png"></a><br><a href="https://imgtu.com/i/2EPYcD" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPYcD.png" alt="2EPYcD.png"></a><br><a href="https://imgtu.com/i/2EPUnH" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPUnH.png" alt="2EPUnH.png"></a><br><a href="https://imgtu.com/i/2EPaBd" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPaBd.png" alt="2EPaBd.png"></a><br><a href="https://imgtu.com/i/2EPdHA" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPdHA.png" alt="2EPdHA.png"></a><br><a href="https://imgtu.com/i/2EP0AI" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EP0AI.png" alt="2EP0AI.png"></a><br><a href="https://imgtu.com/i/2EPBNt" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPBNt.png" alt="2EPBNt.png"></a><br><a href="https://imgtu.com/i/2EPD4P" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPD4P.png" alt="2EPD4P.png"></a><br><a href="https://imgtu.com/i/2EPy38" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPy38.png" alt="2EPy38.png"></a><br><a href="https://imgtu.com/i/2EPs9f" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EPs9f.png" alt="2EPs9f.png"></a><br><a href="https://imgtu.com/i/2EiqRf" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EiqRf.png" alt="2EiqRf.png"></a><br><a href="https://imgtu.com/i/2EiLz8" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EiLz8.png" alt="2EiLz8.png"></a><br><a href="https://imgtu.com/i/2EibJP" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EibJP.png" alt="2EibJP.png"></a><br><a href="https://imgtu.com/i/2EiHit" target="_blank" rel="noopener"><img src="https://z3.ax1x.com/2021/05/29/2EiHit.png" alt="2EiHit.png"></a></p><h3 id="讲稿"><a href="#讲稿" class="headerlink" title="讲稿"></a>讲稿</h3><p>老师和同学们大家好，今天我们要介绍的是WWW会议19年的一篇论文，基于图神经网络的社会推荐。WWW会议是数据挖掘领域的CCF-A类会议。本次介绍由四个部分组成，分别是背景介绍、论文细节、实验评估以及写作技巧。</p><p>首先是背景部分，进入信息时代，我们被越来越多的信息所淹没，表面上我们有了更多的选择，但反而不知道如何选择。而推荐系统就是能够有效缓解这种“信息超载”现象的一个很好的方法。它希望根据你的历史行为记录，来挖掘你的个人喜好，从而向你推荐可能喜欢的物品。它就像一位了解我们喜好的隐形的朋友，在我们浏览或购物时陪伴左右。实际上推荐系统已经和我们的生活息息相关：不管是QQ音乐的”为你推荐”、淘宝的”猜你喜欢”或者是亚马逊的”推荐购买”，都是为我们个性化推荐的内容。</p><p>因为用户和物品的交互记录很容易以图的形式进行表示，以电影评分为例，如果将用户和电影都看成图上的顶点，而评分记录看成对应顶点间的一条边，自然就形成了一张图。而图神经网络是针对图类型数据的一种神经网络架构，很自然地就想到用图神经网络来解决推荐系统的问题，这也是本文研究的动机。图神经网络根据”相邻的顶点具有相似性“这一假设，通过聚合邻域顶点的信息来将图中顶点映射为特征空间中的向量，使得结构上相似的顶点在特征空间中有相似的特征表示。给定一张由顶点和边组成的图作为输入，通常分为如下两个步骤：邻域聚合和状态更新。</p><p>而社会推荐任务的难点在于，首先，数据往往包含两种类型的图，分别是用户-物品交互记录以及用户间的社交关系，而传统的图神经网络都是在同一张图上进行信息的传递和聚合，如何才能利用社交关系图的信息来帮助推荐相关物品？其次，交互记录还包含了更丰富的信息，例如评分高表示喜爱，评分低表示厌恶，如何将这种偏好也体现在模型的构建中？最后，社交网络中不同的好友对我们的偏好影响程度是不同的，关系越好的朋友向我们推荐的物品我们越可能接受。论文的贡献就是解决了这几个问题。</p><p>论文的架构有两条主线，分别是用户侧和物品侧，得到各自的特征表示后，计算出用户对物品的预测评分。首先是用户侧，因为给定的数据中两张图都和用户有关，我们希望能将两部分的信息都利用起来。对于用户-物品交互图，用上面提到的图神经网络的邻域聚合步骤，用户的特征表示由它交互过的物品的特征表示进行聚合得到，同时还将评分的高低纳入考虑，以引入用户的偏好。这种聚合方式是通过物品来定义用户。论文在这里额外地考虑了一步，对用户交互过的物品，它们对用户偏好的贡献也是不同的，为了表示这种不同，论文使用了attention网络来为每个物品计算出一个权重系数，以自适应地聚合这些物品的信息。attention网络涉及的细节较多，因为时间关系不在这里详细介绍，只需要了解通过attention网络可以得到不同物品相对的重要性程度。</p><p>类似地，不同的好友给一个用户偏好带来的影响也是不同的，所以在社交网络图上聚合邻域好友的信息时，论文同样使用了attention网络来为每个好友计算出一个权重系数，以表示好友对用户的重要程度。到这里，我们就从物品和好友两个角度得到了用户的特征表示，将两部分结合起来就得到了用户的特征表示。</p><p>知道用户侧的做法之后再看物品侧的做法就容易理解多了，因为采用了类似的信息聚合过程。对于一个物品，我们可以得到与它交互过的所有用户，那么该物品的特征表示就由这些用户的特征表示进行聚合得到，不出意外地，这里同样使用了attention网络来表示各个用户的重要性来进行加权聚合。现在，我们得到了用户和物品的特征表示后，剩下最后一步就是预测用户对这个物品的评分，对应这个部分的输入输出。论文在这部分采取的架构是多层感知机，因为时间原因不在这里详细介绍，它可以通过用户和物品的特征向量来预测用户对物品的一个评分，评分高的物品我们就作为候选列表向用户推荐。到这里模型的细节就介绍完了，可以看到主要是两个模块的重复使用，邻域聚合和attention表示重要性。</p><p>实验部分，论文的优化目标是减少预测值与实际值之间的偏差，使用的两个推荐系统数据集描述如下，它们的共同点都是十分稀疏，评分数相对于用户数和物品数要少得多，这也符合现实中的情况，人们往往不愿意给出自己对于物品的意见。结果部分，选取的两个指标MAE和RMSE都是越小代表预测效果越好，论文提出的模型GraphRec在所有比较模型里取得了最好的性能，同样还做了参数实验，探究不同参数的取值对结果的影响。介绍完了论文的各个部分后，最后我们来分析一下这篇论文在写作上有什么值得我们借鉴的地方。</p><p>论文做的好的一个地方是图表部分，正如老师上课所说，不应该只用颜色来进行区分不同的结果，有些学者可能喜欢将论文打印成纸质版进行阅读，这时颜色传递的信息就会被丢失。这里论文除了颜色外，图表中还在不同的数据上使用了不同的底纹，这样当进行黑白打印时，仍然能通过底纹来获取颜色所传递的信息。同时，在正文里引用图表的时候，也会在随后通过文字对图标传达的信息进行说明，这样可以让读者即使不阅读图标也能明白图表的含义，而不是不给出任何信息只是单纯为了让读者去看图表。</p><p>然而，论文中也有我们认为的不足之处，碰巧也是图表部分。图表使用了不同的颜色却没有用图例来对每种颜色的含义进行说明，这让人感觉使用颜色仅仅是为了让论文更好看而已。与之相对比的是同年会议的另一篇论文，解决的也是社会推荐问题，其中的图表部分虽然涉及了更多的颜色，但有图例进行详细的说明。让读者能够清晰地明白每种颜色的含义。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;WWW19将GNN应用于社会化推荐的一篇论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR&#39;19]</title>
    <link href="http://Bithub00.com/2020/12/22/PPNP%5BICLR19%5D/"/>
    <id>http://Bithub00.com/2020/12/22/PPNP[ICLR19]/</id>
    <published>2020-12-22T03:21:34.349Z</published>
    <updated>2021-09-24T15:23:38.052Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR19将PageRank与GNN结合以解决GCN层数无法加深的一篇论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GCN层数增加后性能反而变差，如何加深GCN的层数。</p><p>根据GCN的定义，每一层网络用来捕获一跳邻居的信息，例如一个三层的GCN网络捕获的就是一个顶点三跳邻居以内的信息，而现在如果只能用浅层模型，表示只能捕获有限跳内的邻域信息，而有时候要多几跳才能捕获到有用的信息，例如<a href="#Representation Learning on Graphs with Jumping Knowledge Networks[ICML&#39;18]">JK-Net</a>中的例子。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>这一篇论文的工作其实是接着JK-Net继续往下，在那篇论文中，作者分析了GCN中信息传递这个过程与随机漫步之间的关系，说明一个K层GCN等价于从源顶点x出发到邻域顶点y的一个K步的随机游走，如果取$k\rightarrow\infin$，节点的极限分布与初始的顶点表示无关，只与图的拓扑结构有关，所以会导致层数加深后性能反而变差，因为不同顶点的表示会趋同，无法区分。另一个看待的角度是，因为原始GCN是对所有聚合的信息做平均操作，层数加深之后各个顶点的邻域都变得跟整张图差不多，既然每个顶点的邻域都变得差不多，做的又是平均操作，每个顶点聚合出来的样子就会都差不多。</p><p>论文做法是将预测与传递分离成两个独立的过程，信息传递时采用Personalized PageRank的方式将初始顶点纳入考虑。</p><p>PageRank的定义式：</p><script type="math/tex; mode=display">\pi_{pr}=AD^{-1}\pi_{pr}</script><p>Personalized PageRank的定义式：</p><script type="math/tex; mode=display">\pi_{ppr}=(1-\alpha)D^{-1}A\pi_{ppr}+\alpha I_n \\\pi_{ppr}=\alpha(I_n-(1-\alpha)D^{-1}A)^{-1}I_n</script><p>$\alpha I_n$是因为加入随即转移概率，避免自环和无外链的顶点的影响，$I_n$是因为前半部分是矩阵需要进行转换。</p><p>PPNP对PPR的应用：</p><script type="math/tex; mode=display">\pi_{ppr}(i_x)=(1-\alpha)\hat{A'}\pi_{ppr}(i_x)+\alpha i_x \\\pi_{ppr}(i_x)=\alpha(I_n-(1-\alpha)\hat{A'})^{-1}i_x</script><p>其中$A’=A+I_n$，$\hat{A’}=D’^{-1/2}A’D’^{-1/2}$，这么做的目的是，信息传递时，节点不能丢失自身特征，因此我们通过添加一个自环把节点自身特征加回来，来保证节点同时聚合邻域节点特征和自身特征；传递后，度大的节点的聚合特征值比较大，度小的节点的聚合特征值比较小。特征而项的取值范围不一致会带来影响，因此我们通过归一化消除这个问题。。</p><blockquote><p>Personalized PageRank算法的目标是要计算所有节点相对于用户u的相关度。从用户u对应的节点开始游走，每到一个节点都以α的概率停止游走并从u重新开始，或者以1-α的概率继续游走，从当前节点指向的节点中按照均匀分布随机选择一个节点往下游走。这样经过很多轮游走之后，每个顶点被访问到的概率也会收敛趋于稳定，这个时候我们就可以用概率来进行排名了。</p></blockquote><p>相较于原始的GCN模型，现在根顶点$x$对顶点$y$的影响程度$I(x,y)$，变得与$\pi_{ppr}(i_x)$中的第$y$个元素相关，这个影响程度对于每个根顶点都有不同的取值：</p><script type="math/tex; mode=display">\require{cancel}I(x,y)\propto \prod_{ppr}^{(yx)},~\prod_{ppr}^{(yx)}=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}\cancel{I_{n}}</script><h4 id="PPNP"><a href="#PPNP" class="headerlink" title="PPNP"></a>PPNP</h4><p>经过上面的铺垫与介绍，论文提出的模型PPNP可以表示为：</p><script type="math/tex; mode=display">Z_{PPNP}=\text{softmax}(\alpha(I_n-(1-\alpha)\hat{A'})^{-1}H),~~H=f_{\theta}(X)</script><p><div align="center"><a href="https://imgchr.com/i/ravXN9" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/20/ravXN9.png" alt="ravXN9.png" border="0" width="90%"></a></div></p><h4 id="APPNP"><a href="#APPNP" class="headerlink" title="APPNP"></a>APPNP</h4><p>从前面的构造方式可以看到，矩阵$\prod_{ppr}$将会有$\mathbb{R}^{n\times n}$大小，会带来时间和空间上的复杂度。PPNP中涉及矩阵的求逆运算，因此论文提出了一种近似的计算方法APPNP，计算方式如下：</p><script type="math/tex; mode=display">\begin{aligned}Z^{(0)}&=H=f_{\theta}(X) \\Z^{(k+1)}&=(1-\alpha)\hat{A}Z^{(k)}+\alpha H \\Z^{(K)}&=\text{softmax}\Big((1-\alpha)\hat{A}Z^{(K-1)}+\alpha H\Big)\end{aligned}</script><p>其中$K$为信息传递的跳数或者说是随机漫步的步数，$k\in[0,K-2]$，这样一来就不用构造一个$\mathbb{R}^{n\times n}$的矩阵了。</p><h4 id="PPRGO-KDD’20"><a href="#PPRGO-KDD’20" class="headerlink" title="PPRGO[KDD’20]"></a>PPRGO[KDD’20]</h4><p>APPNP需要在每次梯度更新时进行计算，而$\pi(i)=\Pi_{i,:}^{ppr}$等价于顶点$i$的PPR向量，其中$\Pi^{ppr}=\alpha(I_n-(1-\alpha)D^{-1}A)^{-1}$。所以PPRGO采用了近似计算PPR的方法来改进PPNP。用$\Pi^{(\epsilon)}$来代替$\Pi^{ppr}$：</p><script type="math/tex; mode=display">Z=\text{softmax}(\Pi^{(\epsilon)}H) \\z_i=\text{softmax}(\sum_{j\in N^k(i)}\pi^{(\epsilon)}(i)_jH_j)</script><p>$\pi^{(\epsilon)}(i)$由Backward Search得到，并且对近似矩阵取了一个Top-k操作。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Citeseer、Cora-ML、Pubmed、MS Academic  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICLR19将PageRank与GNN结合以解决GCN层数无法加深的一篇论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>DeepInf - Social Influence Prediction with Deep Learning[KDD&#39;18]</title>
    <link href="http://Bithub00.com/2020/12/22/DeepInf%5BKDD18%5D/"/>
    <id>http://Bithub00.com/2020/12/22/DeepInf[KDD18]/</id>
    <published>2020-12-22T03:18:21.401Z</published>
    <updated>2020-12-22T03:18:21.401Z</updated>
    
    <content type="html"><![CDATA[<p>KDD18一篇将GNN应用于社交网络中用户影响力预测任务的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何在图结构的社交数据中预测顶点的影响力。</p><p>在图中，给定顶点$v$与它的邻域以及一个时间段，通过对开始时各顶点的状态进行建模，来对结束时顶点$v$的状态进行预测（是否被激活）。</p><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><ul><li>邻域：给定图$G=(V,E)$，顶点$v$的邻域定义为$N_v^r=\{u:d(u,v)\le r\}$，是一个顶点集合，不包含顶点$v$自身</li><li>中心网络：由邻域中的顶点及边所组成的网络，以$G_v^r$表示</li><li>用户行为：以$s_v^t$表示，用户对应于图中的顶点，对于一个时刻$t$，如果顶点$v$有产生动作，例如转发、引用等，则$s_v^t=1$</li></ul><p>给定用户$v$的中心网络、邻域中用户的行为集合$S_v^t=\{s_i^t:i\in N_v^r\}$，论文想解决的问题是，在一段时间$Δt$后，对用户$v$的行为的预测：</p><script type="math/tex; mode=display">P(s_v^{t+Δt}|G_v^r,S_v^t)</script><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p><a href="https://imgchr.com/i/BGDfOO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGDfOO.png" alt="BGDfOO.png" border="0"></a></p><p>数据预处理方面，论文通过带重启的随机漫步来为图中的每个顶点$v$获取固定大小$n$的中心网络$G_v^r$，接着使用$\text{DeepWalk}$来得到图中顶点的embedding，最后进行归一化。通过这几个步骤对图中的特征进行提取后，论文还进一步添加了几种人工提取的特征，包括用户是否活跃等等：</p><div align="center"><a href="https://imgchr.com/i/BGyXX6" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGyXX6.png" alt="BGyXX6.png" border="0" width="70%"></a></div><blockquote><p>摘要里说传统的影响力建模方法都是人工提取图中顶点及结构的特征，论文的出发点就是自动学习这种特征表示，结果在预处理的最后还是添加了几种人工提取的特征，这不是自相矛盾吗？</p></blockquote><p>经过上面的步骤后，最后得到包含所有用户特征的一个特征矩阵$H\in \mathbb{R}^{n\times F}$，每一行$h_i^T$表示一个用户的特征，$F$等同于$\text{DeepWalk}$长度加上人工特征长度。</p><h4 id="影响力计算"><a href="#影响力计算" class="headerlink" title="影响力计算"></a>影响力计算</h4><p>这一步纯粹是在套GAT的框架，没什么可以说的，计算如下：</p><script type="math/tex; mode=display">H'=\text{GAT}(H)=g(A_{\text{GAT}}(G)HW^T+b)\\A_{\text{GAT}}(G)=[a_{ij}]_{n\times n}</script><p>其中$W\in \mathbb{R}^{F’\times F}, b\in \mathbb{R}^{F’}$是模型的参数，$a_{ij}$的计算在GAT论文的笔记中有记录，不再赘述。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>OAG、Digg、Twitter、Weibo</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KDD18一篇将GNN应用于社交网络中用户影响力预测任务的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>KGAT - Knowledge Graph Attention Network for Recommendation[KDD&#39;19]</title>
    <link href="http://Bithub00.com/2020/12/22/KGAT%5BKDD19%5D/"/>
    <id>http://Bithub00.com/2020/12/22/KGAT[KDD19]/</id>
    <published>2020-12-22T03:16:39.766Z</published>
    <updated>2020-12-22T03:18:37.048Z</updated>
    
    <content type="html"><![CDATA[<p>KDD19一篇将知识图谱与GNN融合的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在推荐系统中，如何将用户-物品交互信息与物品自身的属性相结合以做出更好的推荐，从另一个角度来说，即如何融合用户-物品交互图与知识图谱</p><div align="center"><a href="https://imgchr.com/i/BnaHGn" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnaHGn.png" alt="BnaHGn.png" border="0" width="65%"></a></div><p>以上面的图为例，在电影推荐场景中，用户对应于观众，物品对应于电影，实体Entities可以有多种含义，例如导演、演员、电影类别等，对应的就会有多种关系，对应图中的$r_1-r_4$。对于用户$u_1$，协同过滤更关注于他的相似用户，即同样看过$i_1$的$u_4$与$u_5$；而有监督学习方法例如因子分解机等会更关注物品之间的联系，例如$i_1$与$i_2$同样有着属性$e_1$，但它无法进一步建模更高阶的关系，例如图中黄色圈内的用户$u_2$与$u_3$观看了同一个导演$e_1$的电影$i_2$，而这名导演$e_1$又作为演员参演了灰色圈内的电影$i_3$与$i_4$。图中上半部分对应于用户-物品交互图，下半部分对应于知识图谱。</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BnDu0f" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnDu0f.png" alt="BnDu0f.png" border="0"></a></p><h4 id="CKG-Embedding-Layer"><a href="#CKG-Embedding-Layer" class="headerlink" title="CKG Embedding Layer"></a>CKG Embedding Layer</h4><p>知识图谱的一般形式可以表示为三元组的集合$\{(h,r,t)\}$，表示头实体$h$与尾实体$t$之间有关系$r$，例如$\text{(Hugh Jackman,ActorOf,Logan)}$表示狼叔是电影罗根的演员，这是一种主动的关系，自然就有逆向的被动关系。而对于用户-物品交互信息来说，通常的表示形式为一个矩阵$R$，$R_{ui}$表示用户$u$与物品$i$的关系，有交互则值为1，否则为0。因此，为了统一两种表示形式，论文中将用户-物品交互信息同样改成三元组的集合$\text$，这样一来得到的统一后的新图称之为Collaborative Knowledge Graph(CKG)。</p><p>第一个步骤是对CKG做embedding，得到图中顶点和边的向量表示形式。论文使用了知识图谱中常用的一个方法$\text{TransR}$，即对于一个三元组$(h,r,t)$，目标为：</p><script type="math/tex; mode=display">e_h^r+e_r\approx e_t^r</script><p>其中$e_h,e_t\in \mathbb{R}d、e_r\in \mathbb{R}k$分别为$h、t、r$的embedding，而$e_h^r,e_t^r$为$e_h、e_t$在$r$所处空间中的投影，损失函数定义为：</p><script type="math/tex; mode=display">g(h,r,t)=||W_re_h+e_r-W_re_t||^2_2</script><p>值越小说明该三元组在知识图谱中更可能存在，即头实体$h$与尾实体$t$之间更可能有关系$r$。经过这一步骤之后，CKG中所有的顶点及边我们都得到了它们的embedding。</p><h4 id="Attentive-Embedding-Propagation-Layers"><a href="#Attentive-Embedding-Propagation-Layers" class="headerlink" title="Attentive Embedding Propagation Layers"></a>Attentive Embedding Propagation Layers</h4><p>第二个步骤直接用的GCN与GAT的想法，在一层embedding propagation layer中，利用图卷积网络在邻域中进行信息传播，利用注意力机制来衡量邻域中各邻居顶点的重要程度。再通过堆叠$l$层来聚合$l$阶邻居顶点的信息。</p><p>在每一层中，首先将顶点$h$的邻域以向量形式表示，系数$\pi(h,r,t)$还会进行$\text{softmax}$归一化：</p><script type="math/tex; mode=display">\begin{aligned}e_{N_h}&=\sum_{(h,r,t)\in N_h}\pi(h,r,t)e_t \\\pi(h,r,t)&=(W_re_t)^T\text{tanh}\big(W_re_h+e_r\big)\end{aligned}</script><p>通过堆叠$l$层来聚合$l$阶邻居顶点的信息：</p><script type="math/tex; mode=display">\begin{aligned}e_h^{(l)}&=f\big( e_h^{(l-1)},e_{N_h}^{(l-1)} \big) \\&=\text{LeakyReLU}\big( W_1(e_h+e_{N_h})\big)+\text{LeakyReLU}\big( W_2(e_h\odot e_{N_h})\big)\end{aligned}</script><p>论文中所使用的聚合函数$f$在GCN与GraphSage的基础上，还额外地引入了第二项中$e_h$与$e_{N_h}$的交互，这使得聚合的过程对于两者之间的相近程度更为敏感，会在更相似的顶点中传播更多的信息。</p><h4 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h4><p>在得到$L$层embedding propagation layer的表示后，使用JK-Net中的LSTM-attention进行聚合，在通过点积的形式给出预测分数：</p><script type="math/tex; mode=display">e_u^*=\text{LSTM-attention}(e_u^{(0)},e_u^{(L)})\\e_i^*=\text{LSTM-attention()}e_i^{(0)}||\dots||e_i^{(L)}\\\hat{y}(u,i)={e_u^*}^Te_i^*</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Amazon-book、Last-FM、Yelp2018</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KDD19一篇将知识图谱与GNN融合的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Session-Based Recommendation with Graph Neural Networks[AAAI&#39;19]</title>
    <link href="http://Bithub00.com/2020/12/22/SRGCN%5BAAAI19%5D/"/>
    <id>http://Bithub00.com/2020/12/22/SRGCN[AAAI19]/</id>
    <published>2020-12-22T03:13:14.168Z</published>
    <updated>2020-12-22T03:13:29.818Z</updated>
    
    <content type="html"><![CDATA[<p>AAAI19一篇将gated GNN应用于序列推荐任务的论文</p><a id="more"></a><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\rightarrow b \rightarrow  c$，只考虑$a\rightarrow b$或者$b\rightarrow c$</p><h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BF3uuT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF3uuT.png" alt="BF3uuT.png" border="0"></a></p><h4 id="Session-Graph-Modeling"><a href="#Session-Graph-Modeling" class="headerlink" title="Session Graph Modeling"></a>Session Graph Modeling</h4><p>将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵：</p><div align="center"><a href="https://imgchr.com/i/BF17nO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF17nO.png" alt="BF17nO.png" border="0" width="80%"></a></div><p>上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\in \mathbb{R}^{n\times 2n}$，其中的一行$A_{s,i:}\in \mathbb{R}^{1\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$：</p><div align="center"><a href="https://imgchr.com/i/BFGCkQ" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFGCkQ.png" alt="BFGCkQ.png" border="0" width="80%"></a></div><h4 id="Node-Representation-Learning"><a href="#Node-Representation-Learning" class="headerlink" title="Node Representation Learning"></a>Node Representation Learning</h4><p>论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。</p><h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><p>一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate：</p><div align="center"><a href="https://imgchr.com/i/BFaaAf" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFaaAf.png" alt="BFaaAf.png" border="0" width="60%"></a></div><p>直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\in \mathbb{R}^{n\times d}$，上一时刻隐层表示$H_{t-1}\in \mathbb{R}^{n\times h}$，重置门与更新门的输出由下式计算得到：</p><script type="math/tex; mode=display">R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)</script><p>式中的$W$与$b$分别为权重与偏置参数。</p><h5 id="Reset-Gate"><a href="#Reset-Gate" class="headerlink" title="Reset Gate"></a>Reset Gate</h5><p>传统RNN网络的隐式状态更新公式为：</p><script type="math/tex; mode=display">H_t=\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)</script><p>如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\tilde{H_t}$称为候选状态：</p><script type="math/tex; mode=display">\tilde{H_t}=\tanh(X_tW_{xh}+(R_t\odot{H_{t-1}})W_{hh}+b_h)</script><h5 id="Update-Gate"><a href="#Update-Gate" class="headerlink" title="Update Gate"></a>Update Gate</h5><p>更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\tilde{H_t}决定$：</p><script type="math/tex; mode=display">H_t=Z_t\odot H_{t-1}+(1-Z_t)\odot \tilde{H_t}</script><p>介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习：</p><p><a href="https://imgchr.com/i/BkiNUU" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BkiNUU.png" alt="BkiNUU.png" border="0"></a></p><p>最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。</p><h4 id="Session-Representation-Generation"><a href="#Session-Representation-Generation" class="headerlink" title="Session Representation Generation"></a>Session Representation Generation</h4><p>现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\text{s}=[v_{s,1},v_{s,2},\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\in \mathbb{R}^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。</p><p>局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣：</p><script type="math/tex; mode=display">s_l=v_n</script><p>全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\text{soft-attention}$机制来计算得到：</p><script type="math/tex; mode=display">\begin{aligned}s_g&=\sum_{i=1}^{n}\alpha_iv_i\\\alpha_i&=q^T\sigma(W_1v_n+W_2v_i+c)\end{aligned}</script><p>最后使用一个$\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量：</p><script type="math/tex; mode=display">s_h=W_3[s_l;s_g]</script><h4 id="Making-Recommendation"><a href="#Making-Recommendation" class="headerlink" title="Making Recommendation"></a>Making Recommendation</h4><p>对于一个待推荐物品$v_i\in V$，计算它在序列$s$中作为下一个被点击物品的概率：</p><script type="math/tex; mode=display">\hat{y_i}=\text{softmax}(s_h^Tv_i)</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Yoochoose、Diginetica</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AAAI19一篇将gated GNN应用于序列推荐任务的论文&lt;/p&gt;
    
    </summary>
    
    
      <category term="图神经网络" scheme="http://Bithub00.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="推荐系统" scheme="http://Bithub00.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
