<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="推荐系统，数据库," />





  <link rel="alternate" href="/atom.xml" title="原力小站" type="application/atom+xml" />






<meta name="description" content="对看过的论文做一个记录">
<meta name="keywords" content="推荐系统，数据库">
<meta property="og:type" content="article">
<meta property="og:title" content="论文学习">
<meta property="og:url" content="http://Bithub00.com/2020/09/25/论文学习/index.html">
<meta property="og:site_name" content="原力小站">
<meta property="og:description" content="对看过的论文做一个记录">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0ePimF.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0Z2M40.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0evf4P.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/04/0JtcnS.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/06/0tja9K.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/06/0tvRaR.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WKA5n.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W1Zo4.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W3X5Q.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WGInP.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WyWOU.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WDBw9.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W6V0g.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0Wy1dH.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W6rnO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/13/0fpKeK.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/13/0fpWwT.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/13/0fpbOx.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/07O1IK.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/07O8PO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/0H5cX6.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/0HbZlj.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/20/BpCLz8.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/20/BpEvLT.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BF3uuT.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BF17nO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BFGCkQ.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BFaaAf.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BkiNUU.png">
<meta property="og:updated_time" content="2020-10-22T14:19:58.854Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="论文学习">
<meta name="twitter:description" content="对看过的论文做一个记录">
<meta name="twitter:image" content="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://Bithub00.com/2020/09/25/论文学习/"/>





  <title>论文学习 | 原力小站</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?43ca6a51990599ac3de948cb708d3909";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/BitHub00" 
      class="github-corner" 
      aria-label="View source on Github">
      <svg width="80" height="80" viewBox="0 0 250 250" 
      style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" 
      aria-hidden="true">
      <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
      <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
      <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
      </svg>
    </a>
      <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}
      </style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">原力小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">扎导的原版正联出了吗？</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Bithub00.com/2020/09/25/论文学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.shuan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="原力小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">论文学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-25T08:27:51+08:00">
                2020-09-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6,586
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>对看过的论文做一个记录<br><a id="more"></a></p>
<h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><h2 id="ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19"><a href="#ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19" class="headerlink" title="ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]"></a>ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>给定一个有向图$G=(V,E)$以及一系列顶点对$(u,v)$,判断两个顶点之间是否连通，对应两种查询情况：  </p>
<ul>
<li>Chained Queries：查询路径上每条边开始于上一条边结束，并且总时间在规定的范围内</li>
<li>Snapshot Queries：对于一个动态变化的图，在给定的时间范围内至少在$c$个快闪图中存在连通</li>
</ul>
<h3 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h3><ol>
<li>以起始顶点和目的顶点各自进行多次固定长度的随机漫步构建两个集合，两个集合的交集非空说明连通</li>
<li>对于随机漫步的长度及次数的选取给出了理论证明<br> 2.1 随机漫步的长度：有向图中最长的最短路径的长度，通过10次的深度优先搜索得到<br> 2.2 随机漫步的次数：类比于扔球问题，给定$n$个篮子和数量相等的红球与蓝球，需要扔多少个球来保证有一个篮子中同时有红球与蓝球的概率高？红球可以看作起始顶点$u$可以到达的顶点，蓝球可以看作目的顶点$v$可以到达的顶点</li>
<li>模型的一个假设前提是构建的两个反向的随机漫步的平稳分布应尽可能接近，这对应于正向随机漫步选定一个出边的概率与反向随机漫步选定一个入边的概率相等，而这个概率恰等于顶点度的倒数。使用同配性(assortativity)作为这两个平稳分布接近程度的指标。</li>
</ol>
<h2 id="Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems"><a href="#Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems" class="headerlink" title="Accelerating Exact Inner Product Retrieval by CPU-GPU Systems"></a>Accelerating Exact Inner Product Retrieval by CPU-GPU Systems</h2><h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FEXIPRO[SIGMOD’17]中的对IPR问题的求解较慢，可以使用GPU进行并行加速。</p>
<h4 id="IPR问题"><a href="#IPR问题" class="headerlink" title="IPR问题"></a>IPR问题</h4><p>给定一个用户矩阵$Q\in R^{d\times m}$以及一个物品矩阵$P\in R^{d\times n}$，对于$Q$中的每一个用户$q$，返回内积$q^TP$中的前k个$q^Tp$对应的物品列表$p$</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="行文逻辑"><a href="#行文逻辑" class="headerlink" title="行文逻辑"></a>行文逻辑</h4><p>作者首先画出四个数据集上，SeqScan与FEXIPRO中两个步骤（内积计算与Top-k物品获取）的运行时间占比，发现内积计算占了总开销的90%以上，促使他提出方法加速这一步骤。接下来介绍GPU加速CPU程序的流程，提出了第一个改进方法，即分batch将矩阵送入GPU并行地计算内积。下一步同样地画出它各个步骤的运行时间占比，发现现在top-k物品的获取以及将内积结果从GPU内存复制到CPU内存这两个步骤变成了时间开销的大头。于是顺着分析结果提出了两个改进方法针对性地减小这两个步骤的时间开销。</p>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>前后提出了三个改进方法：GPU-IP、GPU-IPR、GPU-IPRO，分别为：<br>GPU-IP：1<br>GPU-IPR：1+2<br>GPU-IRPO：1+2+3</p>
<ol>
<li><p>并行计算$Q^TP$，并且提出了一种新的矩阵分割方法以充分利用GPU内存，从而加速内积的计算  </p>
<blockquote>
<p>给定GPU内存为$M$，各自选取用户矩阵与物品矩阵的子集$Q_s\in Q,P_s\in P$使得$Size(Q_s^TP_s)\le M$，论文的做法是取$Q_s=Q$，通过$Size(Q^TP_s)=M$来选取$P_s$的大小</p>
</blockquote>
</li>
<li>为每一个用户指定最佳的内积数量$g_s$为1024，从这1024个计算结果中返回top-k，减少了待排序的数据规模<blockquote>
<p>内积数量会严重影响下一步的Bitonic排序的性能。选取的依据是它应该满足每一个线程组的共享内存大小因为它会在GPU缓存层级关系中带来最小的缓存访问延迟(The size of $g_s$ should fit in the shared<br>memory of each threads group as it incurs minimum cache access latency in GPU cache hierarchy.)</p>
</blockquote>
</li>
<li><p>提出了一种剪枝方法来提前结束计算进程，减少了许多内积计算<br>假设用户$u$与其第$k$大的物品的内积为$S_k$，且$||q||\cdot||p||\le S_k$，则有  $q^Tp \le ||q||\cdot||p||\le S_k$，因为目的是得到top-k物品，满足上述不等式的物品已经被排除在top-k之外，不需要送入下一次迭代进行内积计算</p>
<blockquote>
<p>使用这种剪枝方法后，在四个数据集的前10次迭代中，分别减少了98.88%、76.61%、88.69%以及1.57%的用户数量。</p>
</blockquote>
</li>
</ol>
<h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1><h2 id="Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17"><a href="#Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17" class="headerlink" title="Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]"></a>Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]</h2><h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将神经网络应用在图结构数据上？</p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>给定以下输入：  </p>
<ol>
<li>图中顶点的特征矩阵，维度为$N\times D$，其中$N$为顶点数量，$D$为特征数量  </li>
<li>图的结构信息，如邻接矩阵$A$  </li>
</ol>
<p>输出：  </p>
<ol>
<li>图中顶点的特征表示$Z$，维度为$N\times F$，其中$F$为特征数量</li>
</ol>
<p>如果套用神经网络模型，每一层可以用一个非线性函数进行表示：</p>
<script type="math/tex; mode=display">
H^{(l+1)}=f(H^{(l)},A)</script><p>其中$H^{(0)}=X,H^{(L)}=Z$，问题在于如何选取函数$f(.,.)$</p>
<h3 id="做法及创新-1"><a href="#做法及创新-1" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>对于函数$f(.,.)$的选取，论文中提出了一种可能的函数形式：  </p>
<script type="math/tex; mode=display">
f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})</script><p>其中$\hat{A}=A+I$，因为与矩阵$A$相乘表示对于每个顶点，我们对除了自身外所有邻居顶点的特征向量进行求和，因此加上单位矩阵是为了引入自环。而正则化是避免与矩阵$A$相乘改变特征向量的规模。</p>
<p>更一般地，使用邻域信息的图神经网络形式可以概括为：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\sigma\bigg(W_l·\text{AGGREGATE}\bigg(\{h_u^{(l-1)},\forall u\in N(v)\} \bigg)\bigg)</script><p>其中$W_l$是第$l$层网络的权重矩阵，$\text{AGGREGATE}$是与特定模型相关的聚合函数，$h_v^{(l)}$是顶点$v$在第$l$层的隐层特征表示。论文中只是用了一个两层网络就达到了很好的效果。</p>
<p>将论文所提出的函数改写为上述形式，即为：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\text{ReLU}\Big(W_l·\sum_{u\in N(v)}(deg(v)deg(u))^{-1/2}h_u^{(l-1)}\Big)</script><p>其中$deg(u)$为顶点$u$的度。</p>
<h2 id="Neural-Graph-Collaborative-Filtering-SIGIR’19"><a href="#Neural-Graph-Collaborative-Filtering-SIGIR’19" class="headerlink" title="Neural Graph Collaborative Filtering[SIGIR’19]"></a>Neural Graph Collaborative Filtering[SIGIR’19]</h2><h3 id="解决的问题-3"><a href="#解决的问题-3" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在现有的推荐模型中，用户和物品的embedding只考虑了它们自身的特征，没有考虑用户-物品的交互信息</p>
<h3 id="做法及创新-2"><a href="#做法及创新-2" class="headerlink" title="做法及创新"></a>做法及创新</h3><script type="math/tex; mode=display">
\hat{y}_{NGCF}(u,i)={e^*_u}^Te^*_i \\
e^*_u = e_u^{(0)}||\dotsb||e_u^{(L)} \\
e^*_i = e_i^{(0)}||\dotsb||e_i^{(L)} \\
e_u^{(l)}=LeakyReLU(m^{(l)}_{u\leftarrow u}+\sum_{i\in N_u}m^{(l)}_{u\leftarrow i}) \\
\begin{cases}
m^{(l)}_{u\leftarrow i}=p_{ui}(W_1^{(l)}e_i^{(l-1)}+W_2^{(l)}(e_i^{(l-1)}\odot e_u^{(l-1)})) \\\\
m^{(l)}_{u\leftarrow u}=W_1^{(l)}e_u^{(l-1)}
\end{cases} \\
m_{u\leftarrow i}=\frac{1}{\sqrt{|N_u||N_i|}}(W_1e_i+W_2(e_i\odot e_u))</script><ol>
<li><p>通过堆叠$l$层embedding传播层，一个用户（物品）可以获得它的$l$跳邻居所传播的信息，如下图所示，通过这种方法来建模用户-物品交互信息中的高阶connectivity，下图展示的是一个三阶的例子:</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg" width="400" height="200" alt="0ZY5mF.jpg" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0ePimF.png" width="68%" alt="0ePimF.png" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0Z2M40.jpg" width="400" height="200" alt="0Z2M40.jpg" border="0">
</div>
</li>
<li><p>传统GCN推荐方法中，message embedding只考虑物品embedding$e_i$，论文中将用户embedding与物品embedding的交互也纳入考虑，解释为“This makes the message dependent on the affinity between $e_i$ and<br>$e_u$, e.g., passing more messages from the similar items.”</p>
</li>
<li><p>两个层面上的dropout：message &amp; node dropout。前者表示在第$l$层传播层中，只有部分信息会对最后的表示有贡献；后者表示在第$l$层传播层中，随机地丢弃一些顶点。</p>
</li>
</ol>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-book</p>
<h2 id="LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20"><a href="#LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20" class="headerlink" title="LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]"></a>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]</h2><h3 id="解决的问题-4"><a href="#解决的问题-4" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在协同过滤中，图卷积网络中的特征转换与非线性激活对提升模型表现贡献很小，甚至有负面影响。</p>
<blockquote>
<p>在半监督顶点分类问题中，每个顶点有充分的语义特征作为输入，例如一篇文章的标题与摘要词，这种情况下加入多层的非线性特征转换能够有助于学习特征。而在协同过滤任务中，每个顶点（用户或商品）没有这么充分的语义特征，因此没有多大的作用。</p>
</blockquote>
<h3 id="做法及创新-3"><a href="#做法及创新-3" class="headerlink" title="做法及创新"></a>做法及创新</h3><div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0evf4P.png" alt="0evf4P.png" width="80%" border="0">
</div>

<script type="math/tex; mode=display">
\hat{y}_{ui}=e_u^Te_i \\
e_u=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_i=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_u^{(k+1)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(k)} \\
e_i^{(k+1)}=\sum_{i\in N_i}\frac{1}{\sqrt{|N_i||N_u|}}e_u^{(k)}</script><ol>
<li><p>仅考虑图卷积网络中的neighborhood aggregation，通过在用户-物品交互网络中线性传播来学习用户和物品的embedding，再通过加权和将各层学习的embedding作为最后的embedding</p>
</li>
<li><p>通过减少不必要的架构，相较于NGCF大大减少了需要训练的参数量。唯一需要训练的模型参数是第0层的embedding，即$e_u^{(0)}$与$e_i^{(0)}$，当它们两个给定后，后续层的embedding可以通过传播规则直接进行计算</p>
<blockquote>
<p>以加权和的方式结合各层的embedding等价于带自连接的图卷积</p>
</blockquote>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
E^{(K)}&=(A+I)E^{(K-1)}=(A+I)^KE^{(0)}\\
&=C_K^0E^{(0)}+C_K^1AE^{(0)}+C_K^2A^2E^{(0)}+\dots+C_K^KA^KE^{(0)}
\end{aligned}</script><ol>
<li>模型的可解释性更强，以二层网络为例:</li>
</ol>
<script type="math/tex; mode=display">
e_u^{(2)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(1)}=\sum_{i\in N_u}\frac{1}{|N_i|}\sum_{v\in N_i}\frac{1}{\sqrt{|N_u||N_v|}}e_v^{(0)}</script><p>如果另一个用户$v$与目标用户$u$有关联，则影响以下面的系数表示：</p>
<script type="math/tex; mode=display">
c_{v\rightarrow u}=\frac{1}{\sqrt{|N_u||N_v|}}\sum_{i\in N_u\cap N_v}\frac{1}{|N_i|}</script><p>可解释为:</p>
<ul>
<li>共同交互过的物品越多系数越大 $i\in N_u\cap N_v$</li>
<li>物品流行度越低系数越大$\frac{1}{|N_i|}$</li>
<li>用户$v$越不活跃系数越大$\frac{1}{|N_v|}$</li>
</ul>
<h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-Book</p>
<h2 id="Simplifying-Graph-Convolutional-Networks-PMLR’19"><a href="#Simplifying-Graph-Convolutional-Networks-PMLR’19" class="headerlink" title="Simplifying Graph Convolutional Networks[PMLR’19]"></a>Simplifying Graph Convolutional Networks[PMLR’19]</h2><h3 id="解决的问题-5"><a href="#解决的问题-5" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>图卷积网络中可能引入了一些不必要的复杂性及冗余的计算</p>
<h3 id="做法及创新-4"><a href="#做法及创新-4" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><img src="https://s1.ax1x.com/2020/10/04/0JtcnS.png" alt="0JtcnS.png" border="0">  </p>
<ol>
<li>移除图卷积网络各层之间的非线性关系，合并各层之间的权重矩阵</li>
</ol>
<h4 id="原始图卷积网络"><a href="#原始图卷积网络" class="headerlink" title="原始图卷积网络"></a>原始图卷积网络</h4><p>对于一个输入的图，图卷积网络利用多层网络为每个顶点的特征$x_i$学习一个新的特征表示，随即输入一个线性分类器。对第$k$层网络，输入为$H^{(k-1)}$，输出为$H^{(k)}$，其中$H^{(0)}=X$。一个$K$层的图卷积网络等价于对图中每个顶点的特征向量$x_i$应用一个$K$层感知机，不同之处在于顶点的隐层表示local averaging：</p>
<script type="math/tex; mode=display">
h_i^{(k)}\leftarrow \frac{1}{d_i+1}h_i^{(k-1)}+\sum^n_{j=1}\frac{a_{ij}}{\sqrt{(d_i+1)(d_j+1)}}h_j^{(k-1)}</script><p>矩阵形式：</p>
<script type="math/tex; mode=display">
S=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>其中$A=A+I$，则隐层表示用矩阵的形式表示为：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow SH^{(k-1)}</script><p>Local averaging：this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes  </p>
<p>$\Theta^{(k)}$为第$K$层网络的权重矩阵：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow \text{ReLU}(H^{(k)}\Theta^{(k)})</script><p>$Y\in R^{n\times C}$，$y_{ic}$表示第$i$个顶点属于类别$C$的概率</p>
<script type="math/tex; mode=display">
Y_{GCN}=\text{softmax}(SH^{(K-1)}\Theta^{(K)})</script><h4 id="简化图卷积网络"><a href="#简化图卷积网络" class="headerlink" title="简化图卷积网络"></a>简化图卷积网络</h4><blockquote>
<p>在传统的多层感知机中，多层网络可以提高模型的表现力，是因为这样引入了特征之间的层级关系，例如第二层网络的特征是以第一层网络为基础构建的。而在图卷积网络中，这还有另外一层含义，在每一层中顶点的隐层表示都是以一跳的邻居进行平均，经过$K$层之后，一个顶点就能获得$K$跳邻居的特征信息。这类似于在卷积网路中网络的深度提升了特征的receptive field。</p>
</blockquote>
<p>保留local averaging，移除了非线性激活函数：</p>
<script type="math/tex; mode=display">
Y=\text{softmax}(S^KX\Theta^{(1)}\dots \Theta^{(K)})</script><p>其中$S^K$可以预先进行计算，大大减少了模型的训练时间</p>
<p>论文中证明了简化后的图卷积网络等价于谱空间的一个低通滤波器，它通过的低频信号对应于图中平滑后的特征</p>
<h3 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、Reddit</p>
<h2 id="Inductive-Representation-Learning-on-Large-Graphs-NIPS’17"><a href="#Inductive-Representation-Learning-on-Large-Graphs-NIPS’17" class="headerlink" title="Inductive Representation Learning on Large Graphs[NIPS’17]"></a>Inductive Representation Learning on Large Graphs[NIPS’17]</h2><h3 id="解决的问题-6"><a href="#解决的问题-6" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>对于学习图上顶点的embedding，现有的方法多为直推式学习，学习目标是直接生成当前顶点的embedding，不能泛化到未知顶点上</p>
<h3 id="做法及创新-5"><a href="#做法及创新-5" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出一种归纳式学习方法GrdaphSAGE，不为每个顶点学习单独的embedding，而是学习一种聚合函数$\text{AGGREGATE}$，从一个顶点的局部邻域聚合特征信息，为未知的顶点直接生成embedding，因此旧的顶点只要邻域发生变化也能得到一个新的embedding</p>
<blockquote>
<p>GCN不是归纳式，因为每次迭代会用到整个图的邻接矩阵$A$；而GraphSAGE可以对GCN做了精简，每次迭代只抽样取直接相连的邻居</p>
</blockquote>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol>
<li>给定顶点$v$及其特征$x_v$,作为它的初始表示$h_v^0=x_v$。</li>
<li>计算邻域向量$h^k_{N(v)}=\text{AGGREGATE}({h_u^{(k-1)}}, \forall u\in N(v))$，当前层顶点的邻居从上一层采样，且邻居个数固定，非所有邻居，这样每个顶点和采样后邻居的个数都相同，可以直接拼成一个batch送到GPU中进行批训练</li>
<li>将邻域向量与自身上一层的表示拼接，通过非线性激活函数$\sigma$后作为这一层的表示$h_v^k=\sigma(W^k\text{CONCAT}(h_v^{(k-1)},h^k_{N(v)})$</li>
<li>标准化 $h_v^k=h_v^k/||h_v^k||_2$</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/06/0tja9K.png" alt="0tja9K.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/06/0tvRaR.jpg" alt="0tvRaR.jpg" width="50%" border="0">
</div>

<h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><ol>
<li>MEAN</li>
</ol>
<script type="math/tex; mode=display">
h_v^k=\sigma(W·\text{MEAN}(\{h_v^{k-1}\}\cup\{h_u^{k-1},\forall u\in N(v) \})</script><ol>
<li>LSTM</li>
<li>Pooling<br>GraphSAGE采用的max-pooling策略能够隐式地选取领域中重要的顶点：</li>
</ol>
<script type="math/tex; mode=display">
\text{AGGREGATE}_k^{pool}=\text{max}(\{\sigma(W_{pool}h_u^k + b),\forall u\in N(v)\})</script><h3 id="数据集-3"><a href="#数据集-3" class="headerlink" title="数据集"></a>数据集</h3><p>BioGRID、Reddit</p>
<h2 id="Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16"><a href="#Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16" class="headerlink" title="Learning Convolutional Neural Networks for Graphs[ICML’16]"></a>Learning Convolutional Neural Networks for Graphs[ICML’16]</h2><h3 id="解决的问题-7"><a href="#解决的问题-7" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积神经网络都是应用在图像数据上，如何将它有效地应用于图类型的数据上。</p>
<p>对于图像数据，应用一个卷积神经网络可以看成将receptive field（图中为$3\times3$）以固定的步长将图像遍历，因为图像中像素点的排列有一定的次序，receptive field的移动顺序总是从上到下，从左到右。这也唯一地决定了receptive field对一个像素点的遍历方式以及它如何被映射到向量空间中。</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WKA5n.png" alt="0WKA5n.png" border="0" width="65%">
</div>

<p>然而对于图结构数据这种隐式的结构特征很多时候是缺失的，而且当给定不止一张图时，各个图之间的顶点没有必然的联系。因此，在将卷积神经网络应用在图数据上时，需要解决下面两个问题：</p>
<ol>
<li><p>决定邻域中顶点的产生次序</p>
</li>
<li><p>计算一个将图映射到向量空间的映射方法</p>
</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/12/0W1Zo4.png" alt="0W1Zo4.png" border="0" width="80%"></p>
<h3 id="做法及创新-6"><a href="#做法及创新-6" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出方法的流程如下：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0W3X5Q.png" alt="0W3X5Q.png" border="0" width="60%"></p>
<h4 id="Node-Sequence-Selection"><a href="#Node-Sequence-Selection" class="headerlink" title="Node Sequence Selection"></a>Node Sequence Selection</h4><p>从图中选取固定数量$w$的顶点，它类比于图像的宽度，而选出的顶点就是卷积操作中小矩形的中心顶点。$w$就是在这个图上所做的卷积操作的个数。如下图所示，$w=6$，代表需要从图中选择6个顶点做卷积操作。论文中选取顶点的方式为$\text{DFS}$，关键点在于图标签函数$l$，这个函数的作用是决定选取顶点的次序，可以选区的函数为between centrality与WL算法等等</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WGInP.png" alt="0WGInP.png" border="0"></p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WyWOU.png" alt="0WyWOU.png" border="0"></p>
<h4 id="Neighborhood-Assembly"><a href="#Neighborhood-Assembly" class="headerlink" title="Neighborhood Assembly"></a>Neighborhood Assembly</h4><p>选取完顶点后，下一步是为它们构建receptive field，类似于第一张图中的$3\times3$矩阵。选取的方式为，以顶点$v$为中心，通过$\text{BFS}$添加领域顶点，直到满足receptive field长度$k$：</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WDBw9.png" alt="0WDBw9.png" border="0" width="60%">
</div>

<p><img src="https://s1.ax1x.com/2020/10/12/0W6V0g.png" alt="0W6V0g.png" border="0" width="80%"></p>
<h4 id="Graph-Normalization"><a href="#Graph-Normalization" class="headerlink" title="Graph Normalization"></a>Graph Normalization</h4><p>在选取了满足数量的邻域顶点后，下一步是通过图标签函数$l$为这些顶点赋予一个次序，目的在于将无序的领域映射为一个有序的向量：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0Wy1dH.png" alt="0Wy1dH.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0W6rnO.png" alt="0W6rnO.png" border="0" width="60%">
</div>

<h4 id="Convolutional-Architecture"><a href="#Convolutional-Architecture" class="headerlink" title="Convolutional Architecture"></a>Convolutional Architecture</h4><p>最后一步就是应用卷积层提取特征，顶点和边的属性对应于传统图像CNN中的channel：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpKeK.png" alt="0fpKeK.png" border="0" width="75%"></p>
<p>假设顶点特征的数目为$a_v$，边的特征个数为$a_e$，$w$为选取的顶点个数，$k$为receptive field中的顶点个数，则对于输入的一系列图中的每一个，可以得到两个张量维度分别为$(w,k,a_v)、(w,k,k,a_e)$，可以变换为$(wk,a_v)、(wk^2,a_e)$，其中$a_v$与$a_e$可以看成是传统图像卷积中channel的个数，对它们做一维的卷积操作，第一个的receptive field的大小为$k$，第二个的receptive field的大小为$k^2$。</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpWwT.png" alt="0fpWwT.png" border="0" width="75%"></p>
<p>整体卷积结构：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpbOx.png" alt="0fpbOx.png" border="0" width="75%"></p>
<h3 id="数据集-4"><a href="#数据集-4" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI、D&amp;D</p>
<h2 id="Graph-Attention-Networks-ICLR’18"><a href="#Graph-Attention-Networks-ICLR’18" class="headerlink" title="Graph Attention Networks[ICLR’18]"></a>Graph Attention Networks[ICLR’18]</h2><h3 id="解决的问题-8"><a href="#解决的问题-8" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将卷积神经网络应用于图类型的数据上。</p>
<h3 id="做法及创新-7"><a href="#做法及创新-7" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><div align="center">
<img src="https://s1.ax1x.com/2020/10/16/07O1IK.png" alt="1" border="0" width="70%">
<img src="https://s1.ax1x.com/2020/10/16/07O8PO.png" alt="2" border="0" width="70%">
</div>
给定一个含$n$个顶点的图，其中顶点的特征构成的集合为$(\overrightarrow{h_1},\overrightarrow{h_2},\dots,\overrightarrow{h_n})$，$\overrightarrow{h_i}\in R^F$邻接矩阵为$A$。一个图卷积层根据已有的顶点特征和图的结构来计算一个新的特征集合$(\overrightarrow{h_1'},\overrightarrow{h_2'},\dots,\overrightarrow{h_n'})$，$\overrightarrow{h_i'}\in R^{F'}$

每个图卷积层首先会进行特征转换，以特征矩阵$W$表示，$W\in R^{F'\times F}$它将特征向量线性转换为$\overrightarrow{g_i}=W\overrightarrow{h_i}$，再将新得到的特征向量以某种方式进行结合。为了利用邻域的信息，一种典型的做法如下：
$$
\overrightarrow{h_i}'=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}\overrightarrow{g_j}\bigg)
$$
其中$N_i$表示顶点$i$的邻域（典型的构造方式是选取直接相连的顶点，包括自身），$\alpha_{ij}$表示顶点$j$的特征对于顶点$i$的重要程度，也可以看成一种权重。

现有的做法都是显式地定义$\alpha_{ij}$，本文的创新之处在于使用attention机制隐式地定义$\alpha_{ij}$。所使用的attention机制定义为$a:R^{F'}\times R^{F'} \rightarrow R$，以一个权重向量$\overrightarrow{a}\in R^{2F'}$表示，对应于论文中的self-attention，它具体的定义如下：

#### 算法流程

##### Self-attention

1. 基于顶点的特征计算系数$e_{ij}$
$$
e_{ij}=a(W\overrightarrow{h_i},W\overrightarrow{h_j})
$$

2. 以顶点的邻域将上一步计算得到的系数正则化，这么做能引入图的结构信息：
$$
\alpha_{ij}=\text{softmax}_j(e_{ij})=\frac{\exp(e_{ij})}{\sum_{k\in N_i}\exp(e_{ik})}
$$

<img src="https://s1.ax1x.com/2020/10/16/0H5cX6.png" alt="0H5cX6.png" border="0" width="30%">

3. 以上一步得到的系数$\alpha_{ij}$作为顶点$j$的特征对顶点$i$的重要程度，将领域中各顶点的特征做一个线性组合以作为顶点$i$最终输出的特征表示：
$$
\overrightarrow{h_i'}=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}W\overrightarrow{h_j}\bigg)
$$

#####  Multi-head attention

为了稳定self-attention的学习过程，论文引入了multi-head attention，即由$K$个相互独立的self-attention得到各自的特征，再进行拼接：

<img src="https://s1.ax1x.com/2020/10/16/0HbZlj.png" alt="0HbZlj.png" border="0" width="60%">
$$
\overrightarrow{h_i'}=\Vert_{k=1}^K\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)
$$
其中$\alpha_{ij}^k$是第$k$个attention机制$(a^k)$计算出来的正则化系数，$W^k$是对应的将输入进行线性转化的权重矩阵。论文选取的拼接操作为求平均：
$$
\overrightarrow{h_i'}=\sigma\bigg(\frac{1}{K}\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)
$$

### 数据集
Cora、Citeseer、Pubmed、PPI

## Representation Learning on Graphs with Jumping Knowledge Networks[ICML'18]
### 解决的问题
当图卷积网络GCN的层数超过两层时模型的表现会变差，这使得GCN只能作为浅层模型使用，且在对邻域节点的信息进行聚合时，即使同样是采用$k$层网络来聚合$k$跳邻居的信息，有着不同局部结构的顶点获得的信息也可能完全不同，以下图为例：

<div align="center">
<a href="https://imgchr.com/i/BpCLz8" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpCLz8.jpg" alt="BpCLz8.jpg" border="0" width="70%"></a>
</div>

<p>图$(a)$中的顶点位于核心区域，因此采用$4$层网络把几乎整个图的信息都进行聚合了，而不是它的邻域，这会导致过度平滑，而图$(b)$中顶点位于图边缘的一个树状结构中，采取同样的$4$层网络只囊括了一小部分顶点的信息，只有在第$5$层囊括了核心顶点之后才有效地囊括了更多顶点的信息。</p>
<p>所以，对于处于核心区域的顶点，GCN中每多一层即每多一次卷积操作，节点的表达会更倾向全局，这导致核心区域的很多顶点的表示到最后没有区分性。对于这样的顶点应该减少GCN的层数来让顶点更倾向局部从而在表示上可以区分；而处于边缘的顶点，即使更新多次，聚合的信息也寥寥无几，对于这样的顶点应该增加GCN的层数，来学习到更充分的信息。因此，对于不同的顶点应该选取不同的层数，传统做法对于所有顶点都用一个值会带来偏差。</p>
<h3 id="做法及创新-8"><a href="#做法及创新-8" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>通过Layer aggregation来让顶点最后的表示自适应地聚合不同层的信息，局部还是全部，让模型自己来学习：</p>
<div align="center">
<a href="https://imgchr.com/i/BpEvLT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpEvLT.jpg" alt="BpEvLT.jpg" border="0" width="50%"></a>
</div>

<p>论文的重点在于最后的Layer aggregation层，可选的三种操作为：Concat、Max-pooing以及LSTM-attn。</p>
<ol>
<li><p>Concat</p>
<p>将各层的表示直接拼接在一起，送入Linear层。对于小数据集及结构单一的图这种聚合方式会更好，因为它们不需要顶点在聚合邻域的顶点信息时具有什么自适应性。</p>
</li>
<li><p>Max-pooling</p>
<p>选取各层的表示中包含信息量最多的作为顶点的最终表示，在多层结构中，低层聚合更多局部信息，而高层会聚合更多全局信息，因此对于核心区域内的顶点可能会选取高层表示而边缘顶点选取低层表示。</p>
</li>
<li><p>LSTM-attention</p>
<p>对于各层的表示，attention机制通过计算一个系数$s_v^{(l)}$来表示各层表示的重要性，其中$\sum_ls_v^{(l)}=1$，顶点最终的表示就是各层表示的一个加权和：$\sum_ls_v^{(l)}·h_v^{(l)}$。</p>
<blockquote>
<p>$s_v^{(l)}$的计算：将$k$层网络各层的表示$h_v^{(1)},\dots,h_v^{(k)}$输入一个双向LSTM中，同时生成各层$l$的前向LSTM与反向LSTM的隐式特征，分别表示为$f_v^{(l)}、b_v^{(l)}$，拼接后将$|f_v^{(l)}||b_v^{(l)}|$送入一个Linear层，将Linear层的结果进行Softmax归一化操作就得到了系数$s_v^{l}$。</p>
</blockquote>
</li>
</ol>
<h3 id="数据集-5"><a href="#数据集-5" class="headerlink" title="数据集"></a>数据集</h3><p>Citeseer、Cora、Reddit、PPI</p>
<h2 id="Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19"><a href="#Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19" class="headerlink" title="Session-Based Recommendation with Graph Neural Networks[AAAI’19]"></a>Session-Based Recommendation with Graph Neural Networks[AAAI’19]</h2><h3 id="解决的问题-9"><a href="#解决的问题-9" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\rightarrow b \rightarrow  c$，只考虑$a\rightarrow b$或者$b\rightarrow c$</p>
<h3 id="做法及创新-9"><a href="#做法及创新-9" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BF3uuT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF3uuT.png" alt="BF3uuT.png" border="0"></a></p>
<h4 id="Session-Graph-Modeling"><a href="#Session-Graph-Modeling" class="headerlink" title="Session Graph Modeling"></a>Session Graph Modeling</h4><p>将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵：</p>
<div align="center">
<a href="https://imgchr.com/i/BF17nO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF17nO.png" alt="BF17nO.png" border="0" width="80%"></a>
</div>

<p>上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\in R^{n\times 2n}$，其中的一行$A_{s,i:}\in R^{1\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$：</p>
<div align="center">
<a href="https://imgchr.com/i/BFGCkQ" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFGCkQ.png" alt="BFGCkQ.png" border="0" width="80%"></a>
</div>

<h4 id="Node-Representation-Learning"><a href="#Node-Representation-Learning" class="headerlink" title="Node Representation Learning"></a>Node Representation Learning</h4><p>论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。</p>
<h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><p>一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate：</p>
<div align="center">
<a href="https://imgchr.com/i/BFaaAf" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFaaAf.png" alt="BFaaAf.png" border="0" width="60%"></a>
</div>

<p>直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\in R^{n\times d}$，上一时刻隐层表示$H_{t-1}\in R^{n\times h}$，重置门与更新门的输出由下式计算得到：</p>
<script type="math/tex; mode=display">
R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\
Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)</script><p>式中的$W$与$b$分别为权重与偏置参数。</p>
<h5 id="Reset-Gate"><a href="#Reset-Gate" class="headerlink" title="Reset Gate"></a>Reset Gate</h5><p>传统RNN网络的隐式状态更新公式为：</p>
<script type="math/tex; mode=display">
H_t=\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)</script><p>如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\tilde{H_t}$称为候选状态：</p>
<script type="math/tex; mode=display">
\tilde{H_t}=\tanh(X_tW_{xh}+(R_t\odot{H_{t-1}})W_{hh}+b_h)</script><h5 id="Update-Gate"><a href="#Update-Gate" class="headerlink" title="Update Gate"></a>Update Gate</h5><p>更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\tilde{H_t}决定$：</p>
<script type="math/tex; mode=display">
H_t=Z_t\odot H_{t-1}+(1-Z_t)\odot \tilde{H_t}</script><p>介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习：</p>
<p><a href="https://imgchr.com/i/BkiNUU" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BkiNUU.png" alt="BkiNUU.png" border="0"></a></p>
<p>最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。</p>
<h4 id="Session-Representation-Generation"><a href="#Session-Representation-Generation" class="headerlink" title="Session Representation Generation"></a>Session Representation Generation</h4><p>现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\text{s}=[v_{s,1},v_{s,2},\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\in R^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。</p>
<p>局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣：</p>
<script type="math/tex; mode=display">
s_l=v_n</script><p>全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\text{soft-attention}$机制来计算得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_g&=\sum_{i=1}^{n}\alpha_iv_i\\
\alpha_i&=q^T\sigma(W_1v_n+W_2v_i+c)
\end{aligned}</script><p>最后使用一个$\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量：</p>
<script type="math/tex; mode=display">
s_h=W_3[s_l;s_g]</script><h4 id="Making-Recommendation"><a href="#Making-Recommendation" class="headerlink" title="Making Recommendation"></a>Making Recommendation</h4><p>对于一个待推荐物品$v_i\in V$，计算它在序列$s$中作为下一个被点击物品的概率：</p>
<script type="math/tex; mode=display">
\hat{y_i}=\text{softmax}(s_h^Tv_i)</script><h3 id="数据集-6"><a href="#数据集-6" class="headerlink" title="数据集"></a>数据集</h3><p>Yoochoose、Diginetica</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/推荐系统，数据库/" rel="tag">#推荐系统，数据库</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/08/25/d2l/" rel="next" title="d2l学习笔记">
                <i class="fa fa-chevron-left"></i> d2l学习笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.jpg"
                alt="Mr.shuan" />
            
              <p class="site-author-name" itemprop="name">Mr.shuan</p>
              <p class="site-description motion-element" itemprop="description">May 4th be with you</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://joaquinchou.com/" title="喵语小站" target="_blank">喵语小站</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#数据库"><span class="nav-number">1.</span> <span class="nav-text">数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19"><span class="nav-number">1.1.</span> <span class="nav-text">ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题"><span class="nav-number">1.1.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创新之处"><span class="nav-number">1.1.2.</span> <span class="nav-text">创新之处</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems"><span class="nav-number">1.2.</span> <span class="nav-text">Accelerating Exact Inner Product Retrieval by CPU-GPU Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#IPR问题"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">IPR问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新"><span class="nav-number">1.2.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#行文逻辑"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">行文逻辑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#贡献"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">贡献</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图神经网络"><span class="nav-number">2.</span> <span class="nav-text">图神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17"><span class="nav-number">2.1.</span> <span class="nav-text">Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-2"><span class="nav-number">2.1.1.</span> <span class="nav-text">解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题描述"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">问题描述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-1"><span class="nav-number">2.1.2.</span> <span class="nav-text">做法及创新</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Graph-Collaborative-Filtering-SIGIR’19"><span class="nav-number">2.2.</span> <span class="nav-text">Neural Graph Collaborative Filtering[SIGIR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-3"><span class="nav-number">2.2.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-2"><span class="nav-number">2.2.2.</span> <span class="nav-text">做法及创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集"><span class="nav-number">2.2.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20"><span class="nav-number">2.3.</span> <span class="nav-text">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-4"><span class="nav-number">2.3.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-3"><span class="nav-number">2.3.2.</span> <span class="nav-text">做法及创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-1"><span class="nav-number">2.3.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simplifying-Graph-Convolutional-Networks-PMLR’19"><span class="nav-number">2.4.</span> <span class="nav-text">Simplifying Graph Convolutional Networks[PMLR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-5"><span class="nav-number">2.4.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-4"><span class="nav-number">2.4.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原始图卷积网络"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">原始图卷积网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简化图卷积网络"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">简化图卷积网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-2"><span class="nav-number">2.4.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inductive-Representation-Learning-on-Large-Graphs-NIPS’17"><span class="nav-number">2.5.</span> <span class="nav-text">Inductive Representation Learning on Large Graphs[NIPS’17]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-6"><span class="nav-number">2.5.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-5"><span class="nav-number">2.5.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法流程"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#聚合函数"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">聚合函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-3"><span class="nav-number">2.5.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16"><span class="nav-number">2.6.</span> <span class="nav-text">Learning Convolutional Neural Networks for Graphs[ICML’16]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-7"><span class="nav-number">2.6.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-6"><span class="nav-number">2.6.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Node-Sequence-Selection"><span class="nav-number">2.6.2.1.</span> <span class="nav-text">Node Sequence Selection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neighborhood-Assembly"><span class="nav-number">2.6.2.2.</span> <span class="nav-text">Neighborhood Assembly</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-Normalization"><span class="nav-number">2.6.2.3.</span> <span class="nav-text">Graph Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convolutional-Architecture"><span class="nav-number">2.6.2.4.</span> <span class="nav-text">Convolutional Architecture</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-4"><span class="nav-number">2.6.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Attention-Networks-ICLR’18"><span class="nav-number">2.7.</span> <span class="nav-text">Graph Attention Networks[ICLR’18]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-8"><span class="nav-number">2.7.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-7"><span class="nav-number">2.7.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图卷积"><span class="nav-number">2.7.2.1.</span> <span class="nav-text">图卷积</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-8"><span class="nav-number">2.7.3.</span> <span class="nav-text">做法及创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-5"><span class="nav-number">2.7.4.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19"><span class="nav-number">2.8.</span> <span class="nav-text">Session-Based Recommendation with Graph Neural Networks[AAAI’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-9"><span class="nav-number">2.8.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-9"><span class="nav-number">2.8.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Session-Graph-Modeling"><span class="nav-number">2.8.2.1.</span> <span class="nav-text">Session Graph Modeling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Node-Representation-Learning"><span class="nav-number">2.8.2.2.</span> <span class="nav-text">Node Representation Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GRU"><span class="nav-number">2.8.2.2.1.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reset-Gate"><span class="nav-number">2.8.2.2.2.</span> <span class="nav-text">Reset Gate</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Update-Gate"><span class="nav-number">2.8.2.2.3.</span> <span class="nav-text">Update Gate</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Session-Representation-Generation"><span class="nav-number">2.8.2.3.</span> <span class="nav-text">Session Representation Generation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Making-Recommendation"><span class="nav-number">2.8.2.4.</span> <span class="nav-text">Making Recommendation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-6"><span class="nav-number">2.8.3.</span> <span class="nav-text">数据集</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr.shuan</span>

  
</div>


  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_pv">原力小站已到访<span id="busuanzi_value_site_pv"></span>人次</span>
  <span class="post-meta-divider">|</span>
  <span id="busuanzi_container_site_uv">欢迎第<span id="busuanzi_value_site_uv"></span>位绝地武士








<div class="theme-info">
  <span class="post-meta-divider">|</span>
  <span class="post-count">小站全站共77.2k字</span>
</div>
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  


</body>
</html>
