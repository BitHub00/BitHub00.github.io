<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="推荐系统," />





  <link rel="alternate" href="/atom.xml" title="原力小站" type="application/atom+xml" />






<meta name="description" content="对看过的论文做一个记录">
<meta name="keywords" content="推荐系统">
<meta property="og:type" content="article">
<meta property="og:title" content="论文学习">
<meta property="og:url" content="http://Bithub00.com/2020/09/25/论文学习/index.html">
<meta property="og:site_name" content="原力小站">
<meta property="og:description" content="对看过的论文做一个记录">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0ePimF.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0Z2M40.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/09/29/0evf4P.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/04/0JtcnS.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/06/0tja9K.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/06/0tvRaR.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WKA5n.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W1Zo4.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W3X5Q.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WGInP.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WyWOU.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0WDBw9.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W6V0g.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0Wy1dH.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/12/0W6rnO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/13/0fpKeK.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/13/0fpWwT.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/13/0fpbOx.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/07O1IK.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/07O8PO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/0H5cX6.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/16/0HbZlj.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/20/BpCLz8.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/20/BpEvLT.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BF3uuT.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BF17nO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BFGCkQ.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BFaaAf.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/22/BkiNUU.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/26/BnaHGn.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/26/BnDu0f.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/29/BGDfOO.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/10/29/BGyXX6.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/12/20/ravXN9.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/12/21/r0xT1A.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/12/21/rBuFzt.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/12/21/rBK7g1.png">
<meta property="og:image" content="https://s3.ax1x.com/2020/12/21/rBYtjH.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/09/sQUdAS.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/08/su9fr4.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/09/sQUHnx.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/09/sQdv6A.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/09/sQwmmq.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/sl9ZxP.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/sl9G2q.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/slCW60.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/slPZB8.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/slPgED.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/slFHhQ.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/slFz7T.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/10/slk1gA.jpg">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/14/sd6O1S.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/14/sdfNf1.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/14/sd4urT.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/15/swYFij.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/28/ypCC6A.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/29/yiECBF.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/28/y9x4KI.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/29/yiFHv8.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/29/yimVaD.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/01/31/yVPBNQ.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/02/05/yGIv5D.png">
<meta property="og:image" content="https://s3.ax1x.com/2021/02/05/yGoirt.png">
<meta property="og:updated_time" content="2021-05-19T06:28:17.524Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="论文学习">
<meta name="twitter:description" content="对看过的论文做一个记录">
<meta name="twitter:image" content="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://Bithub00.com/2020/09/25/论文学习/"/>





  <title>论文学习 | 原力小站</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?43ca6a51990599ac3de948cb708d3909";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/BitHub00" 
      class="github-corner" 
      aria-label="View source on Github">
      <svg width="80" height="80" viewBox="0 0 250 250" 
      style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" 
      aria-hidden="true">
      <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
      <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
      <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
      </svg>
    </a>
      <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}
      </style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">原力小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">扎导的原版正联出了吗？</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://Bithub00.com/2020/09/25/论文学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.shuan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="原力小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">论文学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-25T08:27:51+08:00">
                2020-09-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2020/09/25/论文学习/" class="leancloud_visitors" data-flag-title="论文学习">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  20,685
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>对看过的论文做一个记录</p>
<a id="more"></a>
<h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><h2 id="ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19"><a href="#ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19" class="headerlink" title="ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]"></a>ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>给定一个有向图$G=(V,E)$以及一系列顶点对$(u,v)$,判断两个顶点之间是否连通，对应两种查询情况：  </p>
<ul>
<li>Chained Queries：查询路径上每条边开始于上一条边结束，并且总时间在规定的范围内</li>
<li>Snapshot Queries：对于一个动态变化的图，在给定的时间范围内至少在$c$个快闪图中存在连通</li>
</ul>
<h3 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h3><ol>
<li>以起始顶点和目的顶点各自进行多次固定长度的随机漫步构建两个集合，两个集合的交集非空说明连通</li>
<li>对于随机漫步的长度及次数的选取给出了理论证明<br> 2.1 随机漫步的长度：有向图中最长的最短路径的长度，通过10次的深度优先搜索得到<br> 2.2 随机漫步的次数：类比于扔球问题，给定$n$个篮子和数量相等的红球与蓝球，需要扔多少个球来保证有一个篮子中同时有红球与蓝球的概率高？红球可以看作起始顶点$u$可以到达的顶点，蓝球可以看作目的顶点$v$可以到达的顶点</li>
<li>模型的一个假设前提是构建的两个反向的随机漫步的平稳分布应尽可能接近，这对应于正向随机漫步选定一个出边的概率与反向随机漫步选定一个入边的概率相等，而这个概率恰等于顶点度的倒数。使用同配性(assortativity)作为这两个平稳分布接近程度的指标。</li>
</ol>
<h2 id="Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems-SIGIR’19"><a href="#Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems-SIGIR’19" class="headerlink" title="Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR’19]"></a>Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR’19]</h2><h3 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>FEXIPRO[SIGMOD’17]中的对IPR问题的求解较慢，可以使用GPU进行并行加速。</p>
<h4 id="IPR问题"><a href="#IPR问题" class="headerlink" title="IPR问题"></a>IPR问题</h4><p>给定一个用户矩阵$Q\in \mathbb{R}^{d\times m}$以及一个物品矩阵$P\in \mathbb{R}^{d\times n}$，对于$Q$中的每一个用户$q$，返回内积$q^TP$中的前k个$q^Tp$对应的物品列表$p$</p>
<h3 id="做法及创新"><a href="#做法及创新" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="行文逻辑"><a href="#行文逻辑" class="headerlink" title="行文逻辑"></a>行文逻辑</h4><p>作者首先画出四个数据集上，SeqScan与FEXIPRO中两个步骤（内积计算与Top-k物品获取）的运行时间占比，发现内积计算占了总开销的90%以上，促使他提出方法加速这一步骤。接下来介绍GPU加速CPU程序的流程，提出了第一个改进方法，即分batch将矩阵送入GPU并行地计算内积。下一步同样地画出它各个步骤的运行时间占比，发现现在top-k物品的获取以及将内积结果从GPU内存复制到CPU内存这两个步骤变成了时间开销的大头。于是顺着分析结果提出了两个改进方法针对性地减小这两个步骤的时间开销。</p>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>前后提出了三个改进方法：GPU-IP、GPU-IPR、GPU-IPRO，分别为：<br>GPU-IP：1<br>GPU-IPR：1+2<br>GPU-IRPO：1+2+3</p>
<ol>
<li><p>并行计算$Q^TP$，并且提出了一种新的矩阵分割方法以充分利用GPU内存，从而加速内积的计算  </p>
<blockquote>
<p>给定GPU内存为$M$，各自选取用户矩阵与物品矩阵的子集$Q_s\in Q,P_s\in P$使得$Size(Q_s^TP_s)\le M$，论文的做法是取$Q_s=Q$，通过$Size(Q^TP_s)=M$来选取$P_s$的大小</p>
</blockquote>
</li>
<li>为每一个用户指定最佳的内积数量$g_s$为1024，从这1024个计算结果中返回top-k，减少了待排序的数据规模<blockquote>
<p>内积数量会严重影响下一步的Bitonic排序的性能。选取的依据是它应该满足每一个线程组的共享内存大小因为它会在GPU缓存层级关系中带来最小的缓存访问延迟(The size of $g_s$ should fit in the shared<br>memory of each threads group as it incurs minimum cache access latency in GPU cache hierarchy.)</p>
</blockquote>
</li>
<li><p>提出了一种剪枝方法来提前结束计算进程，减少了许多内积计算<br>假设用户$u$与其第$k$大的物品的内积为$S_k$，且$||q||\cdot||p||\le S_k$，则有  $q^Tp \le ||q||\cdot||p||\le S_k$，因为目的是得到top-k物品，满足上述不等式的物品已经被排除在top-k之外，不需要送入下一次迭代进行内积计算</p>
<blockquote>
<p>使用这种剪枝方法后，在四个数据集的前10次迭代中，分别减少了98.88%、76.61%、88.69%以及1.57%的用户数量。</p>
</blockquote>
</li>
</ol>
<h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1><h2 id="Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17"><a href="#Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17" class="headerlink" title="Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]"></a>Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]</h2><h3 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将神经网络应用在图结构数据上？</p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>给定以下输入：  </p>
<ol>
<li>图中顶点的特征矩阵$H\in \mathbb{R}^{n\times F}$，其中$n$为顶点数量，$F$为特征数量  </li>
<li>图的结构信息，如邻接矩阵$A$  </li>
</ol>
<p>输出：  </p>
<ol>
<li>图中顶点新的的特征表示$H’\in \mathbb{R}^{n\times F’}$，即</li>
</ol>
<script type="math/tex; mode=display">
H'=\text{GCN(H)}=g(AHW^T+b)</script><p>如果套用神经网络模型，每一层可以用一个非线性函数进行表示：</p>
<script type="math/tex; mode=display">
H^{(l+1)}=f(H^{(l)},A)</script><p>其中$H^{(0)}=X,H^{(L)}=Z$，问题在于如何选取函数$f(.,.)$</p>
<h3 id="做法及创新-1"><a href="#做法及创新-1" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>对于函数$f(.,.)$的选取，论文中提出了一种可能的函数形式：  </p>
<script type="math/tex; mode=display">
f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}) \\</script><p>其中$\hat{A}=A+I$，因为与矩阵$A$相乘表示对于每个顶点，我们对除了自身外所有邻居顶点的特征向量进行求和，因此加上单位矩阵是为了引入自环。而正则化是避免与矩阵$A$相乘改变特征向量的规模。实际在论文中只使用两层网络就达到了很好的效果，表示为：</p>
<script type="math/tex; mode=display">
Z_{\text{GCN}}=\text{softmax}\big(\hat{A'}\text{ReLU}\big(\hat{A'}XW_0\big)W_1\big)</script><p>其中$W_0、W_1$为这两层网络的参数，$\hat{A’}=\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$，$Z\in \mathbb{R}^{n\times c}$为预测的顶点标签，$c$为类别数目，毕竟论文解决的就是一个分类问题。</p>
<p>更一般地，<a href="https://arxiv.org/abs/1810.00826v3" target="_blank" rel="noopener">GIN</a>将使用邻域信息的图神经网络形式概括为：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\text{COMBINE}^{(l)}(h_v^{(l-1)},a_v^{(l)}),\quad a_v^{(l)}=\text{AGGREGATE}^{(l)}(\{h_u^{(l-1)}:u\in N(v) \})</script><p>其中$W_l$是第$l$层网络的权重矩阵，$\text{AGGREGATE}$是与特定模型相关的聚合函数，$h_v^{(l)}$是顶点$v$在第$l$层的隐层特征表示。论文中使用的网络形式如下，只是用了一个两层网络就达到了很好的效果：</p>
<script type="math/tex; mode=display">
h_v^{(l)}=\text{ReLU}\Big(W_l·\text{MEAN}\big\{h_u^{(l-1)}:u\in N(v)\cup\{v\} \big\}\Big)</script><p><a href="https://papers.nips.cc/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf" target="_blank" rel="noopener">AS-GCN</a>中对这篇论文的模型形式描述如下：</p>
<script type="math/tex; mode=display">
h_{v_i}^{(l)}=\sigma\Big(W_l·\sum_{j=1}^Na(v_i,u_j)·h_{u_j}^{(l-1)}\Big),i=1,\dots,N</script><p>这里$A=(a(v_i,u_j))\in \mathbb{R}^{N\times N}$对应前面一种写法的正则化邻接矩阵$\hat{A’}$，表面上看对于顶点$v_i$，需要考虑将图中剩下的所有顶点的上一时刻的隐层表示做加权和，来作为它当前时刻的隐层表示，因为$j$的取值范围为$[1,N]$，$N$就是图中顶点的数量。但实际上，大多数顶点因为与$v_i$并无边相连，所以邻接矩阵中对应的值为0，意味着在加权和中的权重为0，相当于加权和时只会考虑有边相连的顶点，这同样是考虑邻域，只不过跟上面那种写法不同。</p>
<h2 id="Neural-Graph-Collaborative-Filtering-SIGIR’19"><a href="#Neural-Graph-Collaborative-Filtering-SIGIR’19" class="headerlink" title="Neural Graph Collaborative Filtering[SIGIR’19]"></a>Neural Graph Collaborative Filtering[SIGIR’19]</h2><h3 id="解决的问题-3"><a href="#解决的问题-3" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在现有的推荐模型中，用户和物品的embedding只考虑了它们自身的特征，没有考虑用户-物品的交互信息</p>
<h3 id="做法及创新-2"><a href="#做法及创新-2" class="headerlink" title="做法及创新"></a>做法及创新</h3><script type="math/tex; mode=display">
\hat{y}_{NGCF}(u,i)={e^*_u}^Te^*_i \\
e^*_u = e_u^{(0)}||\dotsb||e_u^{(L)} \\
e^*_i = e_i^{(0)}||\dotsb||e_i^{(L)} \\
e_u^{(l)}=LeakyReLU(m^{(l)}_{u\leftarrow u}+\sum_{i\in N_u}m^{(l)}_{u\leftarrow i}) \\
\begin{cases}
m^{(l)}_{u\leftarrow i}=p_{ui}(W_1^{(l)}e_i^{(l-1)}+W_2^{(l)}(e_i^{(l-1)}\odot e_u^{(l-1)})) \\\\
m^{(l)}_{u\leftarrow u}=W_1^{(l)}e_u^{(l-1)}
\end{cases} \\
m_{u\leftarrow i}=\frac{1}{\sqrt{|N_u||N_i|}}(W_1e_i+W_2(e_i\odot e_u))</script><ol>
<li><p>通过堆叠$l$层embedding传播层，一个用户（物品）可以获得它的$l$跳邻居所传播的信息，如下图所示，通过这种方法来建模用户-物品交互信息中的高阶connectivity，下图展示的是一个三阶的例子:</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0ZY5mF.jpg" width="400" height="200" alt="0ZY5mF.jpg" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0ePimF.png" width="68%" alt="0ePimF.png" border="0">
<img src="https://s1.ax1x.com/2020/09/29/0Z2M40.jpg" width="400" height="200" alt="0Z2M40.jpg" border="0">
</div>
</li>
<li><p>传统GCN推荐方法中，message embedding只考虑物品embedding$e_i$，论文中将用户embedding与物品embedding的交互也纳入考虑，解释为“This makes the message dependent on the affinity between $e_i$ and $e_u$, e.g., passing more messages from the similar items.”</p>
</li>
<li><p>两个层面上的dropout：message &amp; node dropout。前者表示在第$l$层传播层中，只有部分信息会对最后的表示有贡献；后者表示在第$l$层传播层中，随机地丢弃一些顶点。</p>
</li>
</ol>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-book</p>
<h2 id="LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20"><a href="#LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20" class="headerlink" title="LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]"></a>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]</h2><h3 id="解决的问题-4"><a href="#解决的问题-4" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在协同过滤中，图卷积网络中的特征转换与非线性激活对提升模型表现贡献很小，甚至有负面影响。</p>
<blockquote>
<p>在半监督顶点分类问题中，每个顶点有充分的语义特征作为输入，例如一篇文章的标题与摘要词，这种情况下加入多层的非线性特征转换能够有助于学习特征。而在协同过滤任务中，每个顶点（用户或商品）没有这么充分的语义特征，因此没有多大的作用。</p>
</blockquote>
<h3 id="做法及创新-3"><a href="#做法及创新-3" class="headerlink" title="做法及创新"></a>做法及创新</h3><div align="center">
<img src="https://s1.ax1x.com/2020/09/29/0evf4P.png" alt="0evf4P.png" width="80%" border="0">
</div>

<script type="math/tex; mode=display">
\hat{y}_{ui}=e_u^Te_i \\
e_u=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_i=\sum_{k=0}^K\alpha_ke_u^{(k)} \\
e_u^{(k+1)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(k)} \\
e_i^{(k+1)}=\sum_{i\in N_i}\frac{1}{\sqrt{|N_i||N_u|}}e_u^{(k)}</script><ol>
<li><p>仅考虑图卷积网络中的neighborhood aggregation，通过在用户-物品交互网络中线性传播来学习用户和物品的embedding，再通过加权和将各层学习的embedding作为最后的embedding</p>
</li>
<li><p>通过减少不必要的架构，相较于NGCF大大减少了需要训练的参数量。唯一需要训练的模型参数是第0层的embedding，即$e_u^{(0)}$与$e_i^{(0)}$，当它们两个给定后，后续层的embedding可以通过传播规则直接进行计算</p>
<blockquote>
<p>以加权和的方式结合各层的embedding等价于带自连接的图卷积</p>
</blockquote>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
E^{(K)}&=(A+I)E^{(K-1)}=(A+I)^KE^{(0)}\\
&=C_K^0E^{(0)}+C_K^1AE^{(0)}+C_K^2A^2E^{(0)}+\dots+C_K^KA^KE^{(0)}
\end{aligned}</script><ol>
<li>模型的可解释性更强，以二层网络为例:</li>
</ol>
<script type="math/tex; mode=display">
e_u^{(2)}=\sum_{i\in N_u}\frac{1}{\sqrt{|N_u||N_i|}}e_i^{(1)}=\sum_{i\in N_u}\frac{1}{|N_i|}\sum_{v\in N_i}\frac{1}{\sqrt{|N_u||N_v|}}e_v^{(0)}</script><p>如果另一个用户$v$与目标用户$u$有关联，则影响以下面的系数表示：</p>
<script type="math/tex; mode=display">
c_{v\rightarrow u}=\frac{1}{\sqrt{|N_u||N_v|}}\sum_{i\in N_u\cap N_v}\frac{1}{|N_i|}</script><p>可解释为:</p>
<ul>
<li>共同交互过的物品越多系数越大 $i\in N_u\cap N_v$</li>
<li>物品流行度越低系数越大$\frac{1}{|N_i|}$</li>
<li>用户$v$越不活跃系数越大$\frac{1}{|N_v|}$</li>
</ul>
<h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h3><p>Gowalla、Yelp2018、Amazon-Book</p>
<h2 id="Simplifying-Graph-Convolutional-Networks-PMLR’19"><a href="#Simplifying-Graph-Convolutional-Networks-PMLR’19" class="headerlink" title="Simplifying Graph Convolutional Networks[PMLR’19]"></a>Simplifying Graph Convolutional Networks[PMLR’19]</h2><h3 id="解决的问题-5"><a href="#解决的问题-5" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>图卷积网络中可能引入了一些不必要的复杂性及冗余的计算</p>
<h3 id="做法及创新-4"><a href="#做法及创新-4" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><img src="https://s1.ax1x.com/2020/10/04/0JtcnS.png" alt="0JtcnS.png" border="0">  </p>
<ol>
<li>移除图卷积网络各层之间的非线性关系，合并各层之间的权重矩阵</li>
</ol>
<h4 id="原始图卷积网络"><a href="#原始图卷积网络" class="headerlink" title="原始图卷积网络"></a>原始图卷积网络</h4><p>对于一个输入的图，图卷积网络利用多层网络为每个顶点的特征$x_i$学习一个新的特征表示，随即输入一个线性分类器。对第$k$层网络，输入为$H^{(k-1)}$，输出为$H^{(k)}$，其中$H^{(0)}=X$。一个$K$层的图卷积网络等价于对图中每个顶点的特征向量$x_i$应用一个$K$层感知机，不同之处在于顶点的隐层表示local averaging：</p>
<script type="math/tex; mode=display">
h_i^{(k)}\leftarrow \frac{1}{d_i+1}h_i^{(k-1)}+\sum^n_{j=1}\frac{a_{ij}}{\sqrt{(d_i+1)(d_j+1)}}h_j^{(k-1)}</script><p>矩阵形式：</p>
<script type="math/tex; mode=display">
S=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>其中$A=A+I$，则隐层表示用矩阵的形式表示为：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow SH^{(k-1)}</script><p>Local averaging：this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes  </p>
<p>$\Theta^{(k)}$为第$K$层网络的权重矩阵：</p>
<script type="math/tex; mode=display">
H^{(k)}\leftarrow \text{ReLU}(H^{(k)}\Theta^{(k)})</script><p>$Y\in \mathbb{R}^{n\times C}$，$y_{ic}$表示第$i$个顶点属于类别$C$的概率</p>
<script type="math/tex; mode=display">
Y_{GCN}=\text{softmax}(SH^{(K-1)}\Theta^{(K)})</script><h4 id="简化图卷积网络"><a href="#简化图卷积网络" class="headerlink" title="简化图卷积网络"></a>简化图卷积网络</h4><blockquote>
<p>在传统的多层感知机中，多层网络可以提高模型的表现力，是因为这样引入了特征之间的层级关系，例如第二层网络的特征是以第一层网络为基础构建的。而在图卷积网络中，这还有另外一层含义，在每一层中顶点的隐层表示都是以一跳的邻居进行平均，经过$K$层之后，一个顶点就能获得$K$跳邻居的特征信息。这类似于在卷积网路中网络的深度提升了特征的receptive field。</p>
</blockquote>
<p>保留local averaging，移除了非线性激活函数：</p>
<script type="math/tex; mode=display">
Y=\text{softmax}(S^KX\Theta^{(1)}\dots \Theta^{(K)})</script><p>其中$S^K$可以预先进行计算，大大减少了模型的训练时间</p>
<p>论文中证明了简化后的图卷积网络等价于谱空间的一个低通滤波器，它通过的低频信号对应于图中平滑后的特征</p>
<h3 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、Reddit</p>
<h2 id="Inductive-Representation-Learning-on-Large-Graphs-NIPS’17"><a href="#Inductive-Representation-Learning-on-Large-Graphs-NIPS’17" class="headerlink" title="Inductive Representation Learning on Large Graphs[NIPS’17]"></a>Inductive Representation Learning on Large Graphs[NIPS’17]</h2><h3 id="解决的问题-6"><a href="#解决的问题-6" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>对于学习图上顶点的embedding，现有的方法多为直推式学习，学习目标是直接生成当前顶点的embedding，不能泛化到未知顶点上</p>
<h3 id="做法及创新-5"><a href="#做法及创新-5" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出一种归纳式学习方法GrdaphSAGE，不为每个顶点学习单独的embedding，而是学习一种聚合函数$\text{AGGREGATE}$，从一个顶点的局部邻域聚合特征信息，为未知的顶点直接生成embedding，因此旧的顶点只要邻域发生变化也能得到一个新的embedding</p>
<blockquote>
<p>GCN不是归纳式，因为每次迭代会用到整个图的邻接矩阵$A$；而GraphSAGE可以对GCN做了精简，每次迭代只抽样取直接相连的邻居</p>
</blockquote>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol>
<li>给定顶点$v$及其特征$x_v$,作为它的初始表示$h_v^0=x_v$。</li>
<li>计算邻域向量$h^k_{N(v)}=\text{AGGREGATE}({h_u^{(k-1)}}, \forall u\in N(v))$，当前层顶点的邻居从上一层采样，且邻居个数固定，非所有邻居，这样每个顶点和采样后邻居的个数都相同，可以直接拼成一个batch送到GPU中进行批训练</li>
<li>将邻域向量与自身上一层的表示拼接，通过非线性激活函数$\sigma$后作为这一层的表示$h_v^k=\sigma(W^k\text{CONCAT}(h_v^{(k-1)},h^k_{N(v)})$</li>
<li>标准化 $h_v^k=h_v^k/||h_v^k||_2$</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/06/0tja9K.png" alt="0tja9K.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/06/0tvRaR.jpg" alt="0tvRaR.jpg" width="50%" border="0">
</div>

<h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><ol>
<li>MEAN</li>
</ol>
<script type="math/tex; mode=display">
h_v^k=\sigma(W·\text{MEAN}(\{h_v^{k-1}\}\cup\{h_u^{k-1},\forall u\in N(v) \})</script><ol>
<li>LSTM</li>
<li>Pooling<br>GraphSAGE采用的max-pooling策略能够隐式地选取领域中重要的顶点：</li>
</ol>
<script type="math/tex; mode=display">
h^k_{N(v)}=\text{AGGREGATE}_k^{pool}=\text{max}(\{\sigma(W_{pool}h_u^k + b),\forall u\in N(v)\})</script><h3 id="数据集-3"><a href="#数据集-3" class="headerlink" title="数据集"></a>数据集</h3><p>BioGRID、Reddit</p>
<h2 id="Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16"><a href="#Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16" class="headerlink" title="Learning Convolutional Neural Networks for Graphs[ICML’16]"></a>Learning Convolutional Neural Networks for Graphs[ICML’16]</h2><h3 id="解决的问题-7"><a href="#解决的问题-7" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>卷积神经网络都是应用在图像数据上，如何将它有效地应用于图类型的数据上。</p>
<p>对于图像数据，应用一个卷积神经网络可以看成将receptive field（图中为$3\times3$）以固定的步长将图像遍历，因为图像中像素点的排列有一定的次序，receptive field的移动顺序总是从上到下，从左到右。这也唯一地决定了receptive field对一个像素点的遍历方式以及它如何被映射到向量空间中。</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WKA5n.png" alt="0WKA5n.png" border="0" width="65%">
</div>

<p>然而对于图结构数据这种隐式的结构特征很多时候是缺失的，而且当给定不止一张图时，各个图之间的顶点没有必然的联系。因此，在将卷积神经网络应用在图数据上时，需要解决下面两个问题：</p>
<ol>
<li><p>决定邻域中顶点的产生次序</p>
</li>
<li><p>计算一个将图映射到向量空间的映射方法</p>
</li>
</ol>
<p><img src="https://s1.ax1x.com/2020/10/12/0W1Zo4.png" alt="0W1Zo4.png" border="0" width="80%"></p>
<h3 id="做法及创新-6"><a href="#做法及创新-6" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>论文提出方法的流程如下：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0W3X5Q.png" alt="0W3X5Q.png" border="0" width="60%"></p>
<h4 id="Node-Sequence-Selection"><a href="#Node-Sequence-Selection" class="headerlink" title="Node Sequence Selection"></a>Node Sequence Selection</h4><p>从图中选取固定数量$w$的顶点，它类比于图像的宽度，而选出的顶点就是卷积操作中小矩形的中心顶点。$w$就是在这个图上所做的卷积操作的个数。如下图所示，$w=6$，代表需要从图中选择6个顶点做卷积操作。论文中选取顶点的方式为$\text{DFS}$，关键点在于图标签函数$l$，这个函数的作用是决定选取顶点的次序，可以选区的函数为between centrality与WL算法等等</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WGInP.png" alt="0WGInP.png" border="0"></p>
<p><img src="https://s1.ax1x.com/2020/10/12/0WyWOU.png" alt="0WyWOU.png" border="0"></p>
<h4 id="Neighborhood-Assembly"><a href="#Neighborhood-Assembly" class="headerlink" title="Neighborhood Assembly"></a>Neighborhood Assembly</h4><p>选取完顶点后，下一步是为它们构建receptive field，类似于第一张图中的$3\times3$矩阵。选取的方式为，以顶点$v$为中心，通过$\text{BFS}$添加领域顶点，直到满足receptive field长度$k$：</p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0WDBw9.png" alt="0WDBw9.png" border="0" width="60%">
</div>

<p><img src="https://s1.ax1x.com/2020/10/12/0W6V0g.png" alt="0W6V0g.png" border="0" width="80%"></p>
<h4 id="Graph-Normalization"><a href="#Graph-Normalization" class="headerlink" title="Graph Normalization"></a>Graph Normalization</h4><p>在选取了满足数量的邻域顶点后，下一步是通过图标签函数$l$为这些顶点赋予一个次序，目的在于将无序的领域映射为一个有序的向量：</p>
<p><img src="https://s1.ax1x.com/2020/10/12/0Wy1dH.png" alt="0Wy1dH.png" border="0"></p>
<div align="center">
<img src="https://s1.ax1x.com/2020/10/12/0W6rnO.png" alt="0W6rnO.png" border="0" width="60%">
</div>

<h4 id="Convolutional-Architecture"><a href="#Convolutional-Architecture" class="headerlink" title="Convolutional Architecture"></a>Convolutional Architecture</h4><p>最后一步就是应用卷积层提取特征，顶点和边的属性对应于传统图像CNN中的channel：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpKeK.png" alt="0fpKeK.png" border="0" width="75%"></p>
<p>假设顶点特征的数目为$a_v$，边的特征个数为$a_e$，$w$为选取的顶点个数，$k$为receptive field中的顶点个数，则对于输入的一系列图中的每一个，可以得到两个张量维度分别为$(w,k,a_v)、(w,k,k,a_e)$，可以变换为$(wk,a_v)、(wk^2,a_e)$，其中$a_v$与$a_e$可以看成是传统图像卷积中channel的个数，对它们做一维的卷积操作，第一个的receptive field的大小为$k$，第二个的receptive field的大小为$k^2$。</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpWwT.png" alt="0fpWwT.png" border="0" width="75%"></p>
<p>整体卷积结构：</p>
<p><img src="https://s1.ax1x.com/2020/10/13/0fpbOx.png" alt="0fpbOx.png" border="0" width="75%"></p>
<h3 id="数据集-4"><a href="#数据集-4" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI、D&amp;D</p>
<h2 id="Graph-Attention-Networks-ICLR’18"><a href="#Graph-Attention-Networks-ICLR’18" class="headerlink" title="Graph Attention Networks[ICLR’18]"></a>Graph Attention Networks[ICLR’18]</h2><h3 id="解决的问题-8"><a href="#解决的问题-8" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将attention机制应用于图类型的数据上。</p>
<h3 id="做法及创新-7"><a href="#做法及创新-7" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><div align="center">
<img src="https://s1.ax1x.com/2020/10/16/07O1IK.png" alt="1" border="0" width="60%">
<img src="https://s1.ax1x.com/2020/10/16/07O8PO.png" alt="2" border="0" width="60%">
</div>

<p>给定一个含$n$个顶点的图，其中顶点的特征构成的集合为$(\overrightarrow{h_1},\overrightarrow{h_2},\dots,\overrightarrow{h_n})$，$\overrightarrow{h_i}\in \mathbb{R}^F$且邻接矩阵为$A$。一个图卷积层根据已有的顶点特征和图的结构来计算一个新的特征集合$(\overrightarrow{h_1’},\overrightarrow{h_2’},\dots,\overrightarrow{h_n’})$，$\overrightarrow{h_i’}\in \mathbb{R}^{F’}$</p>
<p>每个图卷积层首先会进行特征转换，以特征矩阵$W$表示，$W\in \mathbb{R}^{F’\times F}$它将特征向量线性转换为$\overrightarrow{g_i}=W\overrightarrow{h_i}$，再将新得到的特征向量以某种方式进行结合。为了利用邻域的信息，一种典型的做法如下：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_i}'=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}\overrightarrow{g_j}\bigg)</script><p>其中$N_i$表示顶点$i$的邻域（典型的构造方式是选取直接相连的顶点，包括自身），$\alpha_{ij}$表示顶点$j$的特征对于顶点$i$的重要程度，也可以看成一种权重。</p>
<p>现有的做法都是显式地定义$\alpha_{ij}$，本文的创新之处在于使用attention机制隐式地定义$\alpha_{ij}$。所使用的attention机制定义为$a:R^{F’}\times \mathbb{R}^{F’} \rightarrow \mathbb{R}$，以一个权重向量$\overrightarrow{a}\in \mathbb{R}^{2F’}$表示，对应于论文中的self-attention。  </p>
<h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><ol>
<li>基于顶点的特征计算系数$e_{ij}$</li>
</ol>
<script type="math/tex; mode=display">
e_{ij}=a(W\overrightarrow{h_i},W\overrightarrow{h_j})</script><ol>
<li>以顶点的邻域将上一步计算得到的系数正则化，这么做能引入图的结构信息：</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_{ij}&=\text{softmax}_j(e_{ij})=\frac{\exp(e_{ij})}{\sum_{k\in N_i}\exp(e_{ik})}\\
&=\frac{\exp(\text{LeakyReLU}(\overrightarrow{a}^T[W\overrightarrow{h}_i||W\overrightarrow{h}_j]))}{\sum_{k\in N_i}\exp(\text{LeakyReLU}(\overrightarrow{a}^T[W\overrightarrow{h}_i||W\overrightarrow{h}_k]))}
\end{aligned}</script><p><img src="https://s1.ax1x.com/2020/10/16/0H5cX6.png" alt="0H5cX6.png" border="0" width="30%"></p>
<blockquote>
<p>次序不变性：给定$(i,j),(i,k),(i’,j),(i’,k)$表示两个顶点间的关系，可以为边或自环。$a$为对应的attention系数，如果$a_{ij}&gt;a_{ik}$，则有$a_{i’j}&gt;a_{i’k}$</p>
</blockquote>
<p>​    <a href="https://dl.acm.org/doi/10.1145/3219819.3220077" target="_blank" rel="noopener">DeepInf</a>中给出了证明：</p>
<p>​    将权重向量$\overrightarrow{a}\in \mathbb{R}^{2F’}$重写为$\overrightarrow{a}=[p^T，q^T]$，则有</p>
<script type="math/tex; mode=display">
e_{ij}=\text{LeakyReLU}(p^TWh_i+q^TWh_j)</script><p>​    由softmax与LeakyReLU的单调性可知，因为$a_{ij}&gt;a_{ik}$，有$q^TWh_j&gt;q^TWh_k$，类似地就可以得到$a_{i’j}&gt;a_{i’k}$。</p>
<p>​    这意味着，即使每个顶点都只关注于自己的邻域，但得到的attention系数却具有全局性。</p>
<ol>
<li>以上一步得到的系数$\alpha_{ij}$作为顶点$j$的特征对顶点$i$的重要程度，将领域中各顶点的特征做一个线性组合以作为顶点$i$最终输出的特征表示：</li>
</ol>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}W\overrightarrow{h_j}\bigg)</script><h4 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h4><p>为了稳定self-attention的学习过程，论文引入了multi-head attention，即由$K$个相互独立的self-attention得到各自的特征，再进行拼接：</p>
<p><img src="https://s1.ax1x.com/2020/10/16/0HbZlj.png" alt="0HbZlj.png" border="0" width="60%"></p>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\Vert_{k=1}^K\sigma\bigg(\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)</script><p>其中$\alpha_{ij}^k$是第$k$个attention机制$(a^k)$计算出来的正则化系数，$W^k$是对应的将输入进行线性转化的权重矩阵。论文选取的拼接操作为求平均：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_i'}=\sigma\bigg(\frac{1}{K}\sum_{k=1}^K\sum_{j\in N_i}\alpha_{ij}^kW^k\overrightarrow{h_j}\bigg)</script><h3 id="数据集-5"><a href="#数据集-5" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed、PPI</p>
<h2 id="Representation-Learning-on-Graphs-with-Jumping-Knowledge-Networks-ICML’18"><a href="#Representation-Learning-on-Graphs-with-Jumping-Knowledge-Networks-ICML’18" class="headerlink" title="Representation Learning on Graphs with Jumping Knowledge Networks[ICML’18]"></a>Representation Learning on Graphs with Jumping Knowledge Networks[ICML’18]</h2><h3 id="解决的问题-9"><a href="#解决的问题-9" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>当图卷积网络GCN的层数超过两层时模型的表现会变差，这使得GCN只能作为浅层模型使用，且在对邻域节点的信息进行聚合时，即使同样是采用$k$层网络来聚合$k$跳邻居的信息，有着不同局部结构的顶点获得的信息也可能完全不同，以下图为例：</p>
<div align="center">
<a href="https://imgchr.com/i/BpCLz8" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpCLz8.jpg" alt="BpCLz8.jpg" border="0" width="70%"></a>
</div>

<p>图$(a)$中的顶点位于核心区域，因此采用$4$层网络把几乎整个图的信息都进行聚合了，而不是它的邻域，这会导致过度平滑，而图$(b)$中顶点位于图边缘的一个树状结构中，采取同样的$4$层网络只囊括了一小部分顶点的信息，只有在第$5$层囊括了核心顶点之后才有效地囊括了更多顶点的信息。</p>
<p>所以，对于处于核心区域的顶点，GCN中每多一层即每多一次卷积操作，节点的表达会更倾向全局，这导致核心区域的很多顶点的表示到最后没有区分性。对于这样的顶点应该减少GCN的层数来让顶点更倾向局部从而在表示上可以区分；而处于边缘的顶点，即使更新多次，聚合的信息也寥寥无几，对于这样的顶点应该增加GCN的层数，来学习到更充分的信息。因此，对于不同的顶点应该选取不同的层数，传统做法对于所有顶点都用一个值会带来偏差。</p>
<h3 id="做法及创新-8"><a href="#做法及创新-8" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>理论部分，论文主要讨论的问题是，在一个$k$层的GCN中，顶点$x$对顶点$y$的影响程度，即顶点$x$输入特征的改变，会对顶点$y$在最后一层得到的表示产生多大的变化，也可以说是顶点$y$对于顶点$x$有多敏感。假设输入的特征为$X\in \mathbb{R}^{n\times f}$，输出的预测标签为$Z\in \mathbb{R}^{n\times c}$，其中$n$为图中顶点数目，$c$为类别数目，$f$为特征数目，则这种影响程度可以表示为$I(x,y)=\sum_i\sum_j\frac{\partial Z_{yi}}{\partial X_{xj}}$。</p>
<p>更特别地，论文证明了这个影响程度与从顶点$x$开始的$k$步随机漫步的分布有关，如果对$k$取极限$k\rightarrow \infty$，则随机漫步的分布会收敛到$P_{lim}(\rightarrow y)$。详细论证过程可见原文。这说明，结果与随机漫步的的起始顶点$x$没有关系，通过这种方法来得到$x$的邻域信息是不适用的。</p>
<p>另一种说法是，一个$k$层的图卷积网络等同于一个$k$阶的多项式过滤器，其中的系数是预先确定的<a href="https://arxiv.org/abs/2007.02133v1" target="_blank" rel="noopener">SDC</a>。这么一个过滤器与随机漫步类似，最终会收敛到一个静态向量，从而导致过度平滑。</p>
<p>实践部分，论文提出JK-Net，通过Layer aggregation来让顶点最后的表示自适应地聚合不同层的信息，局部还是全部，让模型自己来学习：</p>
<div align="center">
<a href="https://imgchr.com/i/BpEvLT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/20/BpEvLT.jpg" alt="BpEvLT.jpg" border="0" width="50%"></a>
</div>

<p>论文的重点在于最后的Layer aggregation层，可选的三种操作为：Concat、Max-pooing以及LSTM-attn。</p>
<ol>
<li><p>Concat</p>
<p>将各层的表示直接拼接在一起，送入Linear层。对于小数据集及结构单一的图这种聚合方式会更好，因为它们不需要顶点在聚合邻域的顶点信息时具有什么自适应性。</p>
</li>
<li><p>Max-pooling</p>
<p>选取各层的表示中包含信息量最多的作为顶点的最终表示，在多层结构中，低层聚合更多局部信息，而高层会聚合更多全局信息，因此对于核心区域内的顶点可能会选取高层表示而边缘顶点选取低层表示。</p>
</li>
<li><p>LSTM-attention</p>
<p>对于各层的表示，attention机制通过计算一个系数$s_v^{(l)}$来表示各层表示的重要性，其中$\sum_ls_v^{(l)}=1$，顶点最终的表示就是各层表示的一个加权和：$\sum_ls_v^{(l)}·h_v^{(l)}$。</p>
<blockquote>
<p>$s_v^{(l)}$的计算：将$k$层网络各层的表示$h_v^{(1)},\dots,h_v^{(k)}$输入一个双向LSTM中，同时生成各层$l$的前向LSTM与反向LSTM的隐式特征，分别表示为$f_v^{(l)}、b_v^{(l)}$，拼接后将$|f_v^{(l)}||b_v^{(l)}|$送入一个Linear层，将Linear层的结果进行Softmax归一化操作就得到了系数$s_v^{l}$。</p>
</blockquote>
</li>
</ol>
<h3 id="数据集-6"><a href="#数据集-6" class="headerlink" title="数据集"></a>数据集</h3><p>Citeseer、Cora、Reddit、PPI</p>
<h2 id="Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19"><a href="#Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19" class="headerlink" title="Session-Based Recommendation with Graph Neural Networks[AAAI’19]"></a>Session-Based Recommendation with Graph Neural Networks[AAAI’19]</h2><h3 id="解决的问题-10"><a href="#解决的问题-10" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在序列推荐任务中，现有的方法很难在每条序列中取得准确的用户embedding，因为得到的序列数据往往是匿名的，且序列中记录的点击数据所透露出来的用户行为信息有限。同时，序列中物品间的关系虽然常被证实有效，但现有的方法往往只考虑一阶的前后连续关系，即对于$a\rightarrow b \rightarrow  c$，只考虑$a\rightarrow b$或者$b\rightarrow c$</p>
<h3 id="做法及创新-9"><a href="#做法及创新-9" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BF3uuT" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF3uuT.png" alt="BF3uuT.png" border="0"></a></p>
<h4 id="Session-Graph-Modeling"><a href="#Session-Graph-Modeling" class="headerlink" title="Session Graph Modeling"></a>Session Graph Modeling</h4><p>将每条序列$s$表示成一个有向图，并对图中的边进行正则化，具体做法为边的出现次数除以边起始顶点的出度。以序列$s=[v_1,v_2,v_3,v_2,v_4]$为例构建一个有向图，得到邻接矩阵：</p>
<div align="center">
<a href="https://imgchr.com/i/BF17nO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BF17nO.png" alt="BF17nO.png" border="0" width="80%"></a>
</div>

<p>上面的邻接矩阵以考虑顶点的出边并以出度正则化，类似地可以考虑顶点的入边并以入度正则化，将得到的两种邻接矩阵进行拼接，得到论文中提到的连接矩阵$A_s\in \mathbb{R}^{n\times 2n}$，其中的一行$A_{s,i:}\in \mathbb{R}^{1\times 2n}$对应于所构建的有向图中的一个顶点$v_{s,i}$：</p>
<div align="center">
<a href="https://imgchr.com/i/BFGCkQ" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFGCkQ.png" alt="BFGCkQ.png" border="0" width="80%"></a>
</div>

<h4 id="Node-Representation-Learning"><a href="#Node-Representation-Learning" class="headerlink" title="Node Representation Learning"></a>Node Representation Learning</h4><p>论文使用gated GNN来学习图中顶点的表示，为了类比地说明各式的具体含义，首先对Gated Recurrent Units（GRU）进行介绍，它是循环神经网络中的一个概念。</p>
<h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><p>一个典型的GRU如下所示，输入为上一时刻的隐层表示$H_{t-1}$及当前时刻的表示$X_t$，包含一个重置门Reset Gate和一个更新门Update Gate：</p>
<div align="center">
<a href="https://imgchr.com/i/BFaaAf" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BFaaAf.png" alt="BFaaAf.png" border="0" width="60%"></a>
</div>

<p>直观的来说，重置门决定有多少历史信息被保留，而更新门决定利用多少当前时刻$X_t$的信息。给定当前时刻输入$X_t\in \mathbb{R}^{n\times d}$，上一时刻隐层表示$H_{t-1}\in \mathbb{R}^{n\times h}$，重置门与更新门的输出由下式计算得到：</p>
<script type="math/tex; mode=display">
R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\
Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)</script><p>式中的$W$与$b$分别为权重与偏置参数。</p>
<h5 id="Reset-Gate"><a href="#Reset-Gate" class="headerlink" title="Reset Gate"></a>Reset Gate</h5><p>传统RNN网络的隐式状态更新公式为：</p>
<script type="math/tex; mode=display">
H_t=\tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_h)</script><p>如果我们需要减少历史信息带来的影响，可以将$H_{t-1}$与$R_t$逐元素相乘。如果$R_t$中的元素接近于1，得到的结果就是传统的RNN，如果$R_t$中的结果接近于0，得到的结果就是以$X_t$作为输入的MLP，计算出来的$\tilde{H_t}$称为候选状态：</p>
<script type="math/tex; mode=display">
\tilde{H_t}=\tanh(X_tW_{xh}+(R_t\odot{H_{t-1}})W_{hh}+b_h)</script><h5 id="Update-Gate"><a href="#Update-Gate" class="headerlink" title="Update Gate"></a>Update Gate</h5><p>更新门决定新的隐式状态$H_t$多大程度上与上一时刻$H_{t-1}$相同，以及重置门得到的候选状态$\tilde{H_t}$中有多少信息可以被利用，如果$Z_t$中的元素接近于1，将主要保留历史信息，当前时刻$X_t$的信息基本被忽略，这相当于跳过了时刻$t$；当$Z_t$中的元素接近于0时，$H_t$将主要由$\tilde{H_t}决定$：</p>
<script type="math/tex; mode=display">
H_t=Z_t\odot H_{t-1}+(1-Z_t)\odot \tilde{H_t}</script><p>介绍完了GRU的基本概念，接下来是论文中的方法，可以类比地进行学习：</p>
<p><a href="https://imgchr.com/i/BkiNUU" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/22/BkiNUU.png" alt="BkiNUU.png" border="0"></a></p>
<p>最主要的不同之处在公式$(1)$，它用于在连接矩阵$A_s$的约束下进行不同顶点间的信息传播，具体来说，它提取了邻域的隐向量并将它们作为GNN的输入。</p>
<h4 id="Session-Representation-Generation"><a href="#Session-Representation-Generation" class="headerlink" title="Session Representation Generation"></a>Session Representation Generation</h4><p>现有的做法都假设每条序列中的用户都有一个独特的隐式表示，而论文中提出的方法不对这个隐式向量做任何假设，相反，它用序列中顶点的表示来作为序列的表示，而顶点的表示正是上一步将所有序列构建的图送入gated GNN学习得到的。给定一个序列$\text{s}=[v_{s,1},v_{s,2},\dots,v_{s,n}]$，这一步的目的是得到它的embedding向量$s\in \mathbb{R}^d$。为了结合用户的长期偏好与当前兴趣，生成的embedding向量也有局部和全局两部分组成。</p>
<p>局部embedding向量的构造非常简单，就是最后一个点击过的物品的表示，因为最后一个点击过的物品就表明了用户当前的兴趣：</p>
<script type="math/tex; mode=display">
s_l=v_n</script><p>全局embedding向量的构造需要将所有顶点的表示都聚合进来，论文的做法是做一个线性加权，权重使用$\text{soft-attention}$机制来计算得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_g&=\sum_{i=1}^{n}\alpha_iv_i\\
\alpha_i&=q^T\sigma(W_1v_n+W_2v_i+c)
\end{aligned}</script><p>最后使用一个$\text{Linear}$层来将局部与全局embedding向量进行结合得到最终的序列embedding向量：</p>
<script type="math/tex; mode=display">
s_h=W_3[s_l;s_g]</script><h4 id="Making-Recommendation"><a href="#Making-Recommendation" class="headerlink" title="Making Recommendation"></a>Making Recommendation</h4><p>对于一个待推荐物品$v_i\in V$，计算它在序列$s$中作为下一个被点击物品的概率：</p>
<script type="math/tex; mode=display">
\hat{y_i}=\text{softmax}(s_h^Tv_i)</script><h3 id="数据集-7"><a href="#数据集-7" class="headerlink" title="数据集"></a>数据集</h3><p>Yoochoose、Diginetica</p>
<h2 id="KGAT-Knowledge-Graph-Attention-Network-for-Recommendation-KDD’19"><a href="#KGAT-Knowledge-Graph-Attention-Network-for-Recommendation-KDD’19" class="headerlink" title="KGAT: Knowledge Graph Attention Network for Recommendation[KDD’19]"></a>KGAT: Knowledge Graph Attention Network for Recommendation[KDD’19]</h2><h3 id="解决的问题-11"><a href="#解决的问题-11" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在推荐系统中，如何将用户-物品交互信息与物品自身的属性相结合以做出更好的推荐，从另一个角度来说，即如何融合用户-物品交互图与知识图谱</p>
<div align="center">
<a href="https://imgchr.com/i/BnaHGn" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnaHGn.png" alt="BnaHGn.png" border="0" width="65%"></a>
</div>

<p>以上面的图为例，在电影推荐场景中，用户对应于观众，物品对应于电影，实体Entities可以有多种含义，例如导演、演员、电影类别等，对应的就会有多种关系，对应图中的$r_1-r_4$。对于用户$u_1$，协同过滤更关注于他的相似用户，即同样看过$i_1$的$u_4$与$u_5$；而有监督学习方法例如因子分解机等会更关注物品之间的联系，例如$i_1$与$i_2$同样有着属性$e_1$，但它无法进一步建模更高阶的关系，例如图中黄色圈内的用户$u_2$与$u_3$观看了同一个导演$e_1$的电影$i_2$，而这名导演$e_1$又作为演员参演了灰色圈内的电影$i_3$与$i_4$。图中上半部分对应于用户-物品交互图，下半部分对应于知识图谱。</p>
<h3 id="做法及创新-10"><a href="#做法及创新-10" class="headerlink" title="做法及创新"></a>做法及创新</h3><p><a href="https://imgchr.com/i/BnDu0f" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/26/BnDu0f.png" alt="BnDu0f.png" border="0"></a></p>
<h4 id="CKG-Embedding-Layer"><a href="#CKG-Embedding-Layer" class="headerlink" title="CKG Embedding Layer"></a>CKG Embedding Layer</h4><p>知识图谱的一般形式可以表示为三元组的集合$\{(h,r,t)\}$，表示头实体$h$与尾实体$t$之间有关系$r$，例如$\text{(Hugh Jackman,ActorOf,Logan)}$表示狼叔是电影罗根的演员，这是一种主动的关系，自然就有逆向的被动关系。而对于用户-物品交互信息来说，通常的表示形式为一个矩阵$R$，$R_{ui}$表示用户$u$与物品$i$的关系，有交互则值为1，否则为0。因此，为了统一两种表示形式，论文中将用户-物品交互信息同样改成三元组的集合$\text$，这样一来得到的统一后的新图称之为Collaborative Knowledge Graph(CKG)。</p>
<p>第一个步骤是对CKG做embedding，得到图中顶点和边的向量表示形式。论文使用了知识图谱中常用的一个方法$\text{TransR}$，即对于一个三元组$(h,r,t)$，目标为：</p>
<script type="math/tex; mode=display">
e_h^r+e_r\approx e_t^r</script><p>其中$e_h,e_t\in \mathbb{R}d、e_r\in \mathbb{R}k$分别为$h、t、r$的embedding，而$e_h^r,e_t^r$为$e_h、e_t$在$r$所处空间中的投影，损失函数定义为：</p>
<script type="math/tex; mode=display">
g(h,r,t)=||W_re_h+e_r-W_re_t||^2_2</script><p>值越小说明该三元组在知识图谱中更可能存在，即头实体$h$与尾实体$t$之间更可能有关系$r$。经过这一步骤之后，CKG中所有的顶点及边我们都得到了它们的embedding。</p>
<h4 id="Attentive-Embedding-Propagation-Layers"><a href="#Attentive-Embedding-Propagation-Layers" class="headerlink" title="Attentive Embedding Propagation Layers"></a>Attentive Embedding Propagation Layers</h4><p>第二个步骤直接用的GCN与GAT的想法，在一层embedding propagation layer中，利用图卷积网络在邻域中进行信息传播，利用注意力机制来衡量邻域中各邻居顶点的重要程度。再通过堆叠$l$层来聚合$l$阶邻居顶点的信息。</p>
<p>在每一层中，首先将顶点$h$的邻域以向量形式表示，系数$\pi(h,r,t)$还会进行$\text{softmax}$归一化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_{N_h}&=\sum_{(h,r,t)\in N_h}\pi(h,r,t)e_t \\
\pi(h,r,t)&=(W_re_t)^T\text{tanh}\big(W_re_h+e_r\big)
\end{aligned}</script><p>通过堆叠$l$层来聚合$l$阶邻居顶点的信息：</p>
<script type="math/tex; mode=display">
\begin{aligned}
e_h^{(l)}&=f\big( e_h^{(l-1)},e_{N_h}^{(l-1)} \big) \\
&=\text{LeakyReLU}\big( W_1(e_h+e_{N_h})\big)+\text{LeakyReLU}\big( W_2(e_h\odot e_{N_h})\big)
\end{aligned}</script><p>论文中所使用的聚合函数$f$在GCN与GraphSage的基础上，还额外地引入了第二项中$e_h$与$e_{N_h}$的交互，这使得聚合的过程对于两者之间的相近程度更为敏感，会在更相似的顶点中传播更多的信息。</p>
<h4 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h4><p>在得到$L$层embedding propagation layer的表示后，使用JK-Net中的LSTM-attention进行聚合，在通过点积的形式给出预测分数：</p>
<script type="math/tex; mode=display">
e_u^*=\text{LSTM-attention}(e_u^{(0)},e_u^{(L)})\\e_i^*=\text{LSTM-attention()}e_i^{(0)}||\dots||e_i^{(L)}\\
\hat{y}(u,i)={e_u^*}^Te_i^*</script><h3 id="数据集-8"><a href="#数据集-8" class="headerlink" title="数据集"></a>数据集</h3><p>Amazon-book、Last-FM、Yelp2018</p>
<h2 id="DeepInf-Social-Influence-Prediction-with-Deep-Learning-KDD’18"><a href="#DeepInf-Social-Influence-Prediction-with-Deep-Learning-KDD’18" class="headerlink" title="DeepInf: Social Influence Prediction with Deep Learning[KDD’18]"></a>DeepInf: Social Influence Prediction with Deep Learning[KDD’18]</h2><h3 id="解决的问题-12"><a href="#解决的问题-12" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何在图结构的社交数据中预测顶点的影响力。</p>
<p>在图中，给定顶点$v$与它的邻域以及一个时间段，通过对开始时各顶点的状态进行建模，来对结束时顶点$v$的状态进行预测（是否被激活）。</p>
<h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><ul>
<li>邻域：给定图$G=(V,E)$，顶点$v$的邻域定义为$N_v^r=\{u:d(u,v)\le r\}$，是一个顶点集合，不包含顶点$v$自身</li>
<li>中心网络：由邻域中的顶点及边所组成的网络，以$G_v^r$表示</li>
<li>用户行为：以$s_v^t$表示，用户对应于图中的顶点，对于一个时刻$t$，如果顶点$v$有产生动作，例如转发、引用等，则$s_v^t=1$</li>
</ul>
<p>给定用户$v$的中心网络、邻域中用户的行为集合$S_v^t=\{s_i^t:i\in N_v^r\}$，论文想解决的问题是，在一段时间$Δt$后，对用户$v$的行为的预测：</p>
<script type="math/tex; mode=display">
P(s_v^{t+Δt}|G_v^r,S_v^t)</script><h3 id="做法及创新-11"><a href="#做法及创新-11" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p><a href="https://imgchr.com/i/BGDfOO" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGDfOO.png" alt="BGDfOO.png" border="0"></a></p>
<p>数据预处理方面，论文通过带重启的随机漫步来为图中的每个顶点$v$获取固定大小$n$的中心网络$G_v^r$，接着使用$\text{DeepWalk}$来得到图中顶点的embedding，最后进行归一化。通过这几个步骤对图中的特征进行提取后，论文还进一步添加了几种人工提取的特征，包括用户是否活跃等等：</p>
<div align="center">
<a href="https://imgchr.com/i/BGyXX6" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/10/29/BGyXX6.png" alt="BGyXX6.png" border="0" width="70%"></a>
</div>

<blockquote>
<p>摘要里说传统的影响力建模方法都是人工提取图中顶点及结构的特征，论文的出发点就是自动学习这种特征表示，结果在预处理的最后还是添加了几种人工提取的特征，这不是自相矛盾吗？</p>
</blockquote>
<p>经过上面的步骤后，最后得到包含所有用户特征的一个特征矩阵$H\in \mathbb{R}^{n\times F}$，每一行$h_i^T$表示一个用户的特征，$F$等同于$\text{DeepWalk}$长度加上人工特征长度。</p>
<h4 id="影响力计算"><a href="#影响力计算" class="headerlink" title="影响力计算"></a>影响力计算</h4><p>这一步纯粹是在套GAT的框架，没什么可以说的，计算如下：</p>
<script type="math/tex; mode=display">
H'=\text{GAT}(H)=g(A_{\text{GAT}}(G)HW^T+b)\\
A_{\text{GAT}}(G)=[a_{ij}]_{n\times n}</script><p>其中$W\in \mathbb{R}^{F’\times F}, b\in \mathbb{R}^{F’}$是模型的参数，$a_{ij}$的计算在GAT论文的笔记中有记录，不再赘述。</p>
<h3 id="数据集-9"><a href="#数据集-9" class="headerlink" title="数据集"></a>数据集</h3><p>OAG、Digg、Twitter、Weibo</p>
<h2 id="Predict-then-Propagate-Graph-Neural-Networks-meet-Personalized-PageRank-ICLR’19"><a href="#Predict-then-Propagate-Graph-Neural-Networks-meet-Personalized-PageRank-ICLR’19" class="headerlink" title="Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR’19]"></a>Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR’19]</h2><h3 id="解决的问题-13"><a href="#解决的问题-13" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GCN层数增加后性能反而变差，如何加深GCN的层数。</p>
<p>根据GCN的定义，每一层网络用来捕获一跳邻居的信息，例如一个三层的GCN网络捕获的就是一个顶点三跳邻居以内的信息，而现在如果只能用浅层模型，表示只能捕获有限跳内的邻域信息，而有时候要多几跳才能捕获到有用的信息，例如<a href="#Representation Learning on Graphs with Jumping Knowledge Networks[ICML&#39;18]">JK-Net</a>中的例子。</p>
<h3 id="做法及创新-12"><a href="#做法及创新-12" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>这一篇论文的工作其实是接着JK-Net继续往下，在那篇论文中，作者分析了GCN中信息传递这个过程与随机漫步之间的关系，论证了当层数加深之后，GCN会收敛到这个随机漫步的极限分布，而这个极限分布只与图的全局属性有关，没有把随机漫步的起始顶点，或者说是GCN中从邻域中传递和聚合信息的根顶点考虑在内，这么一来，层数加深之后每个顶点聚合出来的样子都差不多，无法区分从而导致性能变差，另一个看待的角度是，因为原始GCN是对所有聚合的信息做平均操作，层数加深之后各个顶点的邻域都变得跟整张图差不多，既然每个顶点的邻域都变得差不多，做的又是平均操作，每个顶点聚合出来的样子就会都差不多。</p>
<p>论文提出的解决办法是引入PageRank的思想，这也是从JK-Net中的结论观察出来的。JK-Net中所说的GCN会收敛到的极限分布的计算方法如下：</p>
<script type="math/tex; mode=display">
\pi_{lim}=\hat{A}\pi_{lim}</script><p>而PageRank的计算方法如下：</p>
<script type="math/tex; mode=display">
\pi_{pr}=A_{rw}\pi_{pr}</script><p>其中$A_{rw}=AD^{-1}$，两个计算方法明显地相似，区别在于，PageRank中邻接矩阵$A$没有考虑根顶点自身，而极限分布的计算里$\hat{A}$是引入了自环的。而Personalized PageRank通过引入自环而考虑了根顶点自身，论文的想法就是将随机漫步的极限分布用Personalized PageRank来代替，它的计算方法为：</p>
<script type="math/tex; mode=display">
\pi_{ppr}(i_x)=(1-\alpha)\hat{A}\pi_{ppr}(i_x)+\alpha i_x \\
\rightarrow \pi_{ppr}(i_x)=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}i_x</script><p>其中$i_x$是一个one_hot指示向量，用来从根顶点重新启动。</p>
<blockquote>
<p>Personalized PageRank算法的目标是要计算所有节点相对于用户u的相关度。从用户u对应的节点开始游走，每到一个节点都以α的概率停止游走并从u重新开始，或者以1-α的概率继续游走，从当前节点指向的节点中按照均匀分布随机选择一个节点往下游走。这样经过很多轮游走之后，每个顶点被访问到的概率也会收敛趋于稳定，这个时候我们就可以用概率来进行排名了。</p>
</blockquote>
<p>相较于原始的GCN模型，现在根顶点$x$对顶点$y$的影响程度$I(x,y)$，变得与$\pi_{ppr}(i_x)$中的第$y$个元素相关，这个影响程度对于每个根顶点都有不同的取值：</p>
<script type="math/tex; mode=display">
\require{cancel}
I(x,y)\propto \prod_{ppr}^{(yx)},\prod_{ppr}^{(yx)}=\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}\cancel{I_{n}}</script><h4 id="PPNP"><a href="#PPNP" class="headerlink" title="PPNP"></a>PPNP</h4><p>经过上面的铺垫与介绍，论文提出的模型PPNP可以表示为：</p>
<script type="math/tex; mode=display">
Z_{PPNP}=\text{softmax}\Big(\alpha\Big(I_n-(1-\alpha)\hat{A}\Big)^{-1}H\Big),H_{i,:}=f_{\theta}(X_i,:)</script><p>其中$X$为特征矩阵，$f_{\theta}$是一个参数为$\theta$的神经网络，用来产生预测类别$H\in \mathbb{R}^{n\times c}$。</p>
<div align="center">
<a href="https://imgchr.com/i/ravXN9" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/20/ravXN9.png" alt="ravXN9.png" border="0" width="90%"></a>
</div>
由公式和图中都可以看到，PPNP其实是由两部分组成，左边的神经网络与右边的信息传递网络，神经网络部分就类似于在[GCN](#Semi-Supervised Classification with Graph Convolutional Network [ICLR'17])中介绍的，输入顶点特征与图的结构信息（邻接矩阵），输出顶点新的特征表示。信息传递网络部分，在PPNP中通过它来得到预测标签，而原始GCN的做法是$Z_{GCN}=\text{softmax}(\hat{A}HW)$，其中$W$是每层网络的参数。

#### APPNP

从前面的构造方式可以看到，矩阵$\prod_{ppr}$将会有$\mathbb{R}^{n\times n}$大小，会带来时间和空间上的复杂度。因此论文提出了一种近似的计算方法APPNP，计算方式如下：
$$
\begin{aligned}
Z^{(0)}&=H=f_{\theta}(X) \\
Z^{(k+1)}&=(1-\alpha)\hat{A}Z^{(k)}+\alpha H \\
Z^{(K)}&=\text{softmax}\Big((1-\alpha)\hat{A}Z^{(K-1)}+\alpha H\Big)
\end{aligned}
$$
其中$K$为信息传递的跳数或者说是随机漫步的步数，$k\in[0,K-2]$，这样一来就不用构造一个$\mathbb{R}^{n\times n}$的矩阵了。（不知道为什么...）  

### 数据集

Citeseer、Cora-ML、Pubmed、MS Academic  

## Graph Neural Networks for Social Recommendation[WWW'19]

### 解决的问题

如何将GNN应用于社会化推荐任务上。

面临的挑战有三点：

1. 在一个社会化推荐任务中，输入的数据包括社会关系图和用户-物品交互图，将两张图的信息都聚合才能得到用户更好的一个表示，而此前的GNN只是在同一张图上对邻域内的信息聚合。
2. 在用户-物品交互图中，顶点与顶点之间的边也包含更多的信息，除了表示是否交互，还能表示用户对一个物品的偏好（喜爱还是厌恶），而此前的GNN只是将边用来表示是否交互。
3. 社会关系图中用户之间的纽带有强有弱，显然地，一个用户更可能与强纽带的其它用户有类似的喜好。如果将所有纽带关系都看成一样，会有偏差。

### 做法及创新

创新：

* 在不同图(user-user graph和user-item graph)上进行信息传递与聚合
* 除了捕获user-item间的交互关系，还利用了user对item的评分
* 用attention机制表示社交关系的重要性，用户纽带的强与弱

<div align="center">
<a href="https://imgchr.com/i/r0xT1A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/r0xT1A.png" alt="r0xT1A.png" border="0" width="90%"></a>
</div>
整个GraphRec框架由三个部分组成，分别为user modeling、item modeling和rating prediction。其中user modeling用来学习用户的特征表示，学习的方式是两个聚合：item aggregation和social aggregation，类似地item modeling用来学习物品的特征表示，学习的方式是一个聚合：user aggregation。

#### User Modeling

##### item aggregation

<div align="center">
<a href="https://imgchr.com/i/rBuFzt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBuFzt.png" alt="rBuFzt.png" border="0" width="40%"></a>
</div>

<p>item aggregation的目的是通过用户交互过的物品以及对这些物品的倾向，来学习物品侧的用户特征表示，数学表示为：</p>
<script type="math/tex; mode=display">
h_i^I=\sigma(W·Aggre_{items}(\{x_{ia},\forall a\in C(i)\})+b)</script><p>$C(i)$就表示用户交互过的物品的一个集合。这里的$x_{ia}$是一个表示向量，它应该能够同时表示交互关系和用户倾向。论文中的做法是通过一个MLP来结合物品的embedding和倾向的embedding，两者分别用$q_a$和$e_r$表示。倾向的embedding可能很难理解，以五分制评分为例，倾向的embedding表示为$e_r\in \mathbb{R}^d$，其中$r\in \{1,2,3,4,5\}$。</p>
<script type="math/tex; mode=display">
x_{ia}=g_v([q_a\oplus e_r])</script><p>定义好$x_{ia}$后，下一步就是如何选取聚合函数$Aggre$了。论文中使用的是attention机制，来源于<a href="#Graph Attention Networks[ICLR&#39;18]">GAT</a>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_i^I&=\sigma(W·\Big\{\sum_{a\in C(i)}
\alpha_{ia}x_{ia}\Big\}+b) \\
\alpha_{ia}'&=w_2^T·\sigma(W_1·[x_{ia}\oplus p_i]+b_1)+b_2 \\
\alpha_{ia}&=\frac{\exp(\alpha_{ia}')}{\sum_{a\in C(i)}\exp(\alpha_{ia}')}
\end{aligned}</script><p>这里的权重$\alpha_{ia}$考虑了$x_{ia}$和用户$u_i$的embedding $p_i$，使得权重能够与当前用户相关。</p>
<h5 id="social-aggregation"><a href="#social-aggregation" class="headerlink" title="social aggregation"></a>social aggregation</h5><div align="center">
<a href="https://imgchr.com/i/rBK7g1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBK7g1.png" alt="rBK7g1.png" border="0" width="40%"></a>
</div>
social aggregation中，同样地使用了attention机制，通过attention机制来选取强纽带的其它用户（表现为聚合时权重更大）并聚合他们的信息，聚合的就是物品侧的用户特征表示。
$$
\begin{aligned}
h_i^S&=\sigma(W·\Big\{\sum_{o\in N(i)}
\beta_{io}h_o^I\Big\}+b) \\
\beta_{io}'&=w_2^T·\sigma(W_1·[h_o^I\oplus p_i]+b_1)+b_2 \\
\beta_{io}&=\frac{\exp(\beta_{io}')}{\sum_{o\in N(i)}\exp(\beta_{io}')}
\end{aligned}
$$
这里跟item aggregation基本一模一样，就不多介绍了。

得到物品侧的用户特征表示$h_i^I$和社交侧的用户特征表示$h_i^S$后，用一个MLP将它们结合，得到用户最终的特征表示：
$$
\begin{aligned}
c_1&=[h_i^I\oplus h_i^S] \\
c_2&=\sigma(W_2·c_1+b_2) \\
&······ \\
h_i&=\sigma(W_l·c_{l-1}+b_l)
\end{aligned}
$$


#### Item Modeling

##### user aggregation

<div align="center">
<a href="https://imgchr.com/i/rBYtjH" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/21/rBYtjH.png" alt="rBYtjH.png" border="0" width="50%"></a>
</div>
Item modeling与User modeling的做法基本一模一样...公式都是一一对应的：
$$
\begin{aligned}
f_{jt}&=g_u([p_t\oplus e_r]) \\
z_j&=\sigma(W·\Big\{\sum_{t\in B(j)}
\mu_{jt}f_{jt}\Big\}+b) \\
\mu_{jt}'&=w_2^T·\sigma(W_1·[f_{jt}\oplus q_j]+b_1)+b_2 \\
\mu_{jt}&=\frac{\exp(\mu_{jt}')}{\sum_{a\in C(i)}\exp(\mu_{jt}')}
\end{aligned}
$$


#### Rating Prediction

最后来到评分预测部分，由上面两个部分我们得到了用户特征表示$h_i$与物品特征表示$z_j$，产生评分用的也是一个MLP：
$$
\begin{aligned}
g_1&=[h_i\oplus z_j] \\
g_2&=\sigma(W_2·g_1+b_2) \\
&······ \\
g_{l-1}&=\sigma(W_l·g_{l-1}+b_l) \\
r_{ij}&=w^T·g_{l-1}
\end{aligned}
$$

### 数据集

Ciao、Epinions

## Graph Convolutional Matrix Completion[KDD'18]

### 解决的问题

如何将图卷积网络应用于矩阵补全问题。

具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。如果将评分矩阵转换为一张图，转换方法在下面有进行介绍，这时矩阵补全问题也可以看成图上的边预测问题。要预测用户对一个物品的评分，就是预测图上两个对应顶点之间相连的边的权重。

### 做法及创新

论文通过一个编码器-解码器的架构来实现从已有评分到特征表示再到预测评分的过程。

<div align="center">
<a href="https://imgchr.com/i/sQUdAS" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUdAS.png" alt="sQUdAS.png" border="0" width="70%"></a>
</div>

<h4 id="Bipartite-Graph-Construction"><a href="#Bipartite-Graph-Construction" class="headerlink" title="Bipartite Graph Construction"></a>Bipartite Graph Construction</h4><p>首先是将推荐任务里的评分数据转化为一张图，具体做法是将用户和物品都看作图中的顶点，交互记录看作边，分数作为边的权重，如图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/su9fr4" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/08/su9fr4.png" alt="su9fr4.png" border="0" width="60%"></a>
</div>

<h4 id="Graph-Convolutional-Encoder"><a href="#Graph-Convolutional-Encoder" class="headerlink" title="Graph Convolutional Encoder"></a>Graph Convolutional Encoder</h4><p>上一步所构建的图的输入形式为邻接矩阵$A\in \mathbb{R}^{n\times n}$与图中顶点的特征矩阵$X\in \mathbb{R}^{n\times d}$。编码器在这一步的作用就是得到用户与物品的特征表示$A,X^u,X^v\rightarrow U,V$。</p>
<p>具体编码时，论文将不同的评分水平分开考虑$r\in \{1,2,3,4,5\}$，我的理解是它们类似于处理图像数据时的多个channel。以一个评分水平$r$为例，说明编码得到特征表示的过程。假设用户$u_i$对电影$v_j$评分为$r$，而这部电影的特征向量为$x_j$，那么这部电影对这个用户特征表示的贡献可以表示为下面的式子(1)，相当于对特征向量进行了一个线性变换。</p>
<div align="center">
<a href="https://imgchr.com/i/sQUHnx" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQUHnx.png" alt="sQUHnx.png" border="0" width="80%"></a>
</div>
对当前评分水平下所有评过分的电影进行求和，再对所有评分水平求和拼接，经过一个非线性变换，就得到了用户$u_i$的特征表示$h_{u_i}$，物品的做法相同。

<div align="center">
<a href="https://imgchr.com/i/sQdv6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQdv6A.png" alt="sQdv6A.png" border="0" width="80%"></a>
</div>

<div align="center">
<a href="https://imgchr.com/i/sQwmmq" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/09/sQwmmq.png" alt="sQwmmq.png" border="0" width="80%"></a>
</div>

<h4 id="Bilinear-Decoder"><a href="#Bilinear-Decoder" class="headerlink" title="Bilinear Decoder"></a>Bilinear Decoder</h4><p>在分别得到用户与物品的特征表示$U$与$V$后，解码器计算出用户对物品评分为$r$的概率，再对每个评分的概率进行求和，得到最终预测的评分。</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P_r)_{ij}&=\frac{\exp(u_i^TQ_rv_j)}{\sum_{s\in R}\exp(u_i^TQ_sv_j)} \\
\hat{M}&=\sum_{r\in R}rP_r
\end{aligned}</script><h3 id="数据集-10"><a href="#数据集-10" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、MovieLens</p>
<h2 id="Variational-Graph-Auto-Encoders-NIPS’16"><a href="#Variational-Graph-Auto-Encoders-NIPS’16" class="headerlink" title="Variational Graph Auto-Encoders[NIPS’16]"></a>Variational Graph Auto-Encoders[NIPS’16]</h2><h3 id="解决的问题-14"><a href="#解决的问题-14" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>在图结构数据上如何使用变分自编码器</p>
<h3 id="做法及创新-13"><a href="#做法及创新-13" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>将已知的图进行编码（图卷积）得到图中顶点向量表示的一个分布，在分布中采样得到顶点的向量表示，然后进行解码重新构建图。</p>
<h4 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h4><p>因为这篇论文做的是一个迁移的工作，变分自编码器的背景对于理解这篇论文来说十分重要，首先进行介绍。</p>
<p>变分自编码器是自编码器的一种，一个自编码器由编码器和解码器构成，编码器将输入数据转换为低维向量表示，解码器通过得到的低维向量表示进行重构。</p>
<div align="center">
<a href="https://imgchr.com/i/sl9ZxP" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9ZxP.jpg" alt="sl9ZxP.jpg" border="0" width="80%"></a>
<a href="https://imgchr.com/i/sl9G2q" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/sl9G2q.jpg" alt="sl9G2q.jpg" border="0" width="65%"></a>
</div>

<p>这种结构的不足之处在于，只能产生与输入数据相似的样本，而无法产生新的样本，低维向量表示必须是有真实样本通过编码器得到的，随机产生的低维向量经过重构几乎不可能得到近似真实的样本。而变分自编码器可以解决这个问题。</p>
<p>变分自编码器将输入数据编码为一个分布，而不是一个个低维向量表示，然后从这个分布中随机采样来得到低维向量表示。一般假设这个分布为正态分布，因此编码器的任务就是从输入数据中得到均值$\mu$与方差$\sigma^2$。</p>
<div align="center">
<a href="https://imgchr.com/i/slCW60" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slCW60.jpg" alt="slCW60.jpg" border="0" width="80%"></a>
<a href="https://imgchr.com/i/slPZB8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPZB8.jpg" alt="slPZB8.jpg" border="0" width="80%"></a>
</div>

<p>然而，如果是将所有输入数据编码到同一个分布里，从这个分布中随机采样的样本$Z_i$无法与输入样本$X_i$一一对应，会影响模型的学习效果。所以，实际的变分自编码器结构如下图所示，为每一个输入样本学习一个正态分布：</p>
<div align="center">
<a href="https://imgchr.com/i/slPgED" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slPgED.jpg" alt="slPgED.jpg" border="0" width="80%"></a>
</div>

<p>采样时常用”重参数”技巧(reparameterization trick)，从分布$N(\mu,\sigma^2)$中采样一个$Z$相当于从$N(0,1)$中采样一个$\epsilon$使得$Z=\mu+\sigma*\epsilon$。</p>
<h4 id="图变分自编码器"><a href="#图变分自编码器" class="headerlink" title="图变分自编码器"></a>图变分自编码器</h4><p>介绍完传统的变分自编码器，接下来就是介绍这篇论文的工作，如何将变分自编码器的思想迁移到图上。</p>
<p>针对图这个数据结构，输入的数据变为图的邻接矩阵$A$与特征矩阵$X$：<br>邻接矩阵$A$：</p>
<div align="center">
<a href="https://imgchr.com/i/slFHhQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFHhQ.jpg" alt="slFHhQ.jpg" border="0" width="60%"></a>
</div>

<p>特征矩阵$X$：</p>
<div align="center">
<a href="https://imgchr.com/i/slFz7T" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slFz7T.jpg" alt="slFz7T.jpg" border="0" width="60%"></a>
</div>


<p>接下来的工作与变分自编码器相同，通过编码器（图卷积）学习图中顶点低维向量表示分布的均值$\mu$与方差$\sigma^2$，再通过解码器生成图。</p>
<div align="center">
<a href="https://imgchr.com/i/slk1gA" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/10/slk1gA.jpg" alt="slk1gA.jpg" border="0" width="80%"></a>
</div>

<p>编码器采用两层结构的图卷积网络，第一层产生一个低维的特征矩阵：</p>
<script type="math/tex; mode=display">
\bar{X}=\text{GCN}(X,A)=\text{ReLU}(\tilde{A}XW_0)\\
\tilde{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>第二层得到分布的均值$\mu$与方差$\sigma^2$：</p>
<script type="math/tex; mode=display">
\mu=\text{GCN}_{\mu}(X,A)=\tilde{A}\bar{X}W_1\\
\log\sigma^2=\text{GCN}_{\sigma}(X,A)=\tilde{A}\bar{X}W_1</script><p>将两层网络的表达式合并可以得到编码器的表达式：</p>
<script type="math/tex; mode=display">
\text{GCN}(X,A)=\tilde{A}\text{ReLU}(\tilde{A}XW_0)W_1</script><p>同样地使用重参数技巧来得到低维向量表示$Z=\mu+\sigma*\epsilon$。  </p>
<p>编码器重构出图的邻接矩阵，从而得到一个新的图。之所以使用点积的形式来得到邻接矩阵，原因在于我们希望学习到每个顶点的低维向量表示$z$的相似程度，来更好地重构邻接矩阵。而点积可以计算两个向量之间的cosine相似度，这种距离度量方式不受量纲的影响。因此，重构的邻接矩阵可以学习到各个顶点之间的相似程度。</p>
<script type="math/tex; mode=display">
\hat{A}=\sigma(zz^T)</script><p>损失函数用于衡量生草样本与真是样本之间的差异，但如果只用距离度量作为损失函数，为了让编码器的效果最佳，模型会将方差的值学为0，这样从正态分布中采样出来的就是定值，有利于减小生成样本和真实样本之间的差异。但这样一来，就退化成了普通的自编码器，因此在构建损失函数时，往往还会加入各独立正态分布与标准正态分布的KL散度，来使得各个正态分布逼近标准正态分布：</p>
<script type="math/tex; mode=display">
L=E_{q(Z|X,A)}[\log p(A|Z)]-\text{KL}[q(Z|X,A)||p(Z)],\quad where\quad p(Z)=N(0,1)</script><h3 id="数据集-11"><a href="#数据集-11" class="headerlink" title="数据集"></a>数据集</h3><p>Cora、Citeseer、Pubmed</p>
<h2 id="Inductive-Matrix-Completion-Based-on-Graph-Neural-Networks-ICLR’20"><a href="#Inductive-Matrix-Completion-Based-on-Graph-Neural-Networks-ICLR’20" class="headerlink" title="Inductive Matrix Completion Based on Graph Neural Networks[ICLR’20]"></a>Inductive Matrix Completion Based on Graph Neural Networks[ICLR’20]</h2><h3 id="解决的问题-15"><a href="#解决的问题-15" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何让矩阵补全方法中一个数据集得到的embedding，能够迁移到另一个数据集上，同时不依赖额外的信息。</p>
<p>与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>应用的问题相同，具体地，这篇论文做的是推荐系统方向下的矩阵补全问题，给定一个评分矩阵，如何根据已有的评分记录来预测用户对其他物品的评分。传统的做法是将输入的评分矩阵分解成用户与物品的embedding，通过embedding重构评分矩阵，填补其中的缺失值，从而做出预测，如下图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/sd6O1S" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd6O1S.png" alt="sd6O1S.png" border="0" width="80%"></a>
</div>


<p>很多现有方法研究的都是如何得到更好的embedding，但它们都是直推式(transductive)而非启发式(inductive)的，意味着没法迁移，例如MovieLens数据集上得到的embedding就不能直接用于Douban数据集上，需要重新训练一个新的embedding。即使对于同一个数据集而言，如果加入新的评分记录，往往需要整个embedding重新训练。</p>
<h3 id="做法及创新-14"><a href="#做法及创新-14" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="Enclosing-Subgraph-Extraction"><a href="#Enclosing-Subgraph-Extraction" class="headerlink" title="Enclosing Subgraph Extraction"></a>Enclosing Subgraph Extraction</h4><p>论文的做法是为每一个评分记录提取一个子图，并且训练一个图神经网络来将得到的子图映射为预测评分。要想为评分记录提取子图，首先要将评分矩阵转换为图，转换的方法与另一篇论文<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>相同，博客中有具体介绍，这里就不重复说明了。论文中对子图的定义方式为，给定一个评分记录$(u,v)$，表示用户$u$给物品$v$评过分，那么这个评分记录提取的子图由该用户$u$、物品$v$以及它们各自的$h$跳邻域内的顶点构成。为了具体说明是怎么从一个评分记录提取出子图的，我从论文作者的视频中截取了这部分内容，如下图所示：</p>
<p>假设第一张图中深绿色的方格是缺失值，这里先填入了模型的预测评分，倒退着来说明预测评分是怎么通过子图得到的。我们首先找到这个用户评过分的其它物品，对应于第五个物品的四分与第八个物品的两分，如第二张图所示。下一步是找到为这个物品评过分的其他用户，对应于第三个用户的五分与第四个用户的五分。</p>
<div align="center">
<a href="https://imgchr.com/i/sdfNf1" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sdfNf1.png" alt="sdfNf1.png" border="0"></a>
</div>


<p>通过图二和图三找到的关系，就可以提取出这个评分记录的子图了，如下图所示：</p>
<div align="center">
<a href="https://imgchr.com/i/sd4urT" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/14/sd4urT.png" alt="sd4urT.png" border="0" width="90%"></a>
</div>


<p>可以看到，这个提取出的子图能提供许多有用的信息，例如用户平均评分、物品平均评分、物品累计评价次数以及基于路径的结构信息。论文希望通过这种结构信息来找到一些特征，从而做出预测，例如，如果用户$u_0$喜欢一个物品$v_0$，那么对于另一个与他品味相同的用户$u_1$，我们可能发现他也喜欢$v_0$。品味相同可以表示为两个用户都喜欢另一个物品$v_1$，这个特征可以表示为这么一条路径：$u_0\rightarrow_{like}v_1\rightarrow_{liked\ by}u_1\rightarrow_{like}v_0$，如果$u_0$与$v_0$之间存在多条这样的路径，那么我们就可以推测$u_0$喜欢$v_0$。类似这样的结构特征数不胜数。因此，与其人工来手动定义大量这样的启发式特征(heuristics)，不如直接将子图输入一个图神经网络，来自动学习更通用的、更有表达能力的特征。</p>
<h4 id="Node-Labeling"><a href="#Node-Labeling" class="headerlink" title="Node Labeling"></a>Node Labeling</h4><p>这一步给顶点打标签是为了让子图中的顶点有着不同的角色，例如区分哪个是需要预测的目标用户与目标物品，区分用户顶点与物品顶点。而论文中打标签的方式十分简单：</p>
<ul>
<li>目标用户与目标物品分别标记为0和1</li>
<li>对于$h$跳邻域内的顶点，如果是用户顶点标记为$2h$，物品顶点则标记为$2h+1$</li>
</ul>
<p>标记之后，我们就能知道哪个是需要预测的目标用户与目标物品、哪些是用户顶点，因为用户顶点的标签均为偶数，以及邻域内顶点距离目标顶点距离的远近。这些标签将转换为one-hot编码的形式作为图神经网络输入的初始特征$x_0$。</p>
<p>这一节的最后论文作者还说到了这种标记方式与<a href="http://www.bithub00.com/2021/01/09/GCMC[KDD18]/" target="_blank" rel="noopener">GCMC</a>做法的不同之处。GCMC中同样是将标签转换为one-hot编码的形式作为GNN的初始特征，不同的是它用顶点在整个bipartite graph中的全局id作为它的标签，这等价于将GNN第一层信息传递网络的参数，转换为与每个顶点的全局id相关联的embedding函数，可以理解为一个embedding查找表，输入一个全局id，输出它对应的embedding。这显然是直推式的，对于不在查找表中的id，就无法得到它的embedding。这种情况对应于在小数据集上训练网络得到embedding，然后换到大数据集上，因为大数据集的顶点数量肯定要多于小数据集，这就会使得顶点的全局id范围变大，超出了训练出来的这个embedding查找表的范围。</p>
<h4 id="Graph-Neural-Network"><a href="#Graph-Neural-Network" class="headerlink" title="Graph Neural Network"></a>Graph Neural Network</h4><p>这一步的目的就是训练一个GNN来将提取出的子图映射成预测评分。论文所使用的GNN分为两个部分：信息传递层与池化层。前者的作用是得到子图中各顶点的特征向量，后者是根据得到的特征向量形成子图的一个特征表示。</p>
<p>信息传递部分使用的是<a href="https://arxiv.org/pdf/1703.06103v4.pdf" target="_blank" rel="noopener">R-GCN</a>：</p>
<script type="math/tex; mode=display">
x_i^{l+1}=W_0^lx_i^l+\sum_{r\in R}\sum_{j\in N_r(i)}\frac{1}{|N_r(i)|}W_r^lx_j^l</script><p>其中$x_i^l$表示第$i$个顶点在第$l$层的特征向量，$N_r(i)$表示评分水平$r$下顶点$i$的邻域，顶点$i$以不同的边权重$r$所连接的顶点$j$用不同的参数矩阵$W_r^l$来进行处理。通过堆叠$L$层网络可以得到顶点$i$的$L$个特征向量，通过拼接的方式得到它最终的特征表示$h_i$：</p>
<script type="math/tex; mode=display">
h_i=\text{concat}(x_i^1,x_i^2,\dots,x_i^L)</script><p>池化部分只选取子图中目标用户与目标顶点的特征向量进行拼接，来得到该子图的特征表示，这么做的原因是这两个顶点携带了最多的信息。</p>
<script type="math/tex; mode=display">
g=\text{concat}(h_u,h_v)</script><p>在得到子图的特征表示后，最后一步是通过一个MLP将它转换为一个预测评分$\hat{r}$：</p>
<script type="math/tex; mode=display">
\hat{r}=w^T\sigma(Wg)</script><h4 id="Adjacent-Rating-Regularization"><a href="#Adjacent-Rating-Regularization" class="headerlink" title="Adjacent Rating Regularization"></a>Adjacent Rating Regularization</h4><p>论文对于信息传递部分使用的R-GCN还提出了一点改进，在原始的R-GCN中，不同的评分水平是独立看待的，彼此之间没有关联，例如对于1、4、5这三个评分，显然地4和5都表示了用户的喜爱而1表示了用户的厌恶，同时4和5的相似程度要大于4和1，但这种次序关系及大小关系在原始的R-GCN中都被丢掉了。因此本论文添加了一个约束来引入这部分丢失的信息，具体做法也很简单，就是使得相邻的评分水平使用的参数矩阵更加相似：</p>
<script type="math/tex; mode=display">
L_{ARR}=\sum_{i=1,2,\dots,|R|-1}||W_{r_i+1}-W_{r_i}||_F^2</script><p>这里假设评分$r_1,r_2,\dots,r_{|R|}$表示了用户喜爱程度的递增，通过这个约束就保留了评分的次序信息，同时可以使得出现次数较少的评分水平可以从相邻的评分水平中迁移信息，来弥补数据不足带来的问题。</p>
<h4 id="Graph-level-GNN-vs-Node-level-GNN"><a href="#Graph-level-GNN-vs-Node-level-GNN" class="headerlink" title="Graph-level GNN vs Node-level GNN"></a>Graph-level GNN vs Node-level GNN</h4><p>这一节还是在于GCMC作比较。在GCMC中，采用的是顶点层面的图神经网络，它应用于图中的顶点来得到顶点的embedding，再通过embedding得到预测评分，如下右图所示。这么做的缺陷是它独立地学习两个顶点所关联的子树，而忽略了这两棵子树之间可能存在的联系。</p>
<div align="center">
<a href="https://imgchr.com/i/swYFij" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/15/swYFij.png" alt="swYFij.png" border="0" width="50%"></a>
</div>

<h3 id="数据集-12"><a href="#数据集-12" class="headerlink" title="数据集"></a>数据集</h3><p>Flixster、Douban、YahooMusic、ML-100K、ML-1M</p>
<h2 id="Link-Prediction-Based-on-Graph-Neural-Networks-NIPS’18"><a href="#Link-Prediction-Based-on-Graph-Neural-Networks-NIPS’18" class="headerlink" title="Link Prediction Based on Graph Neural Networks[NIPS’18]"></a>Link Prediction Based on Graph Neural Networks[NIPS’18]</h2><h3 id="解决的问题-16"><a href="#解决的问题-16" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何能自动而非人工定义的方式来学习图中的结构信息，从而进行边预测。</p>
<p>边预测任务就是预测图中的两个顶点是否有可能有边相连。一种常用的方法为启发式方法(heuristic)，它根据定义的顶点相似度来判断这条边存在的概率有多大。几种定义相似度的方法可以根据需要使用的邻居顶点的跳数来分类，例如common neighbors与preferential attachment是一阶的，因为它们只需要一跳邻居的信息，而Adamic-Adar和resource allocation为二阶，Katz、rooted PageRank与SimRank是更高阶的相似度。</p>
<p>这种启发式方法的缺点在于，边存在的概率很大程度依赖于定义的顶点相似度。例如选取common neighbors这个相似度，在社交网络可能是成立的，因为如果两个人有很多共同的朋友，他们两个确实更有可能认识，但是在蛋白质交互网络截然相反，有越多相同邻居顶点的蛋白质反而越不可能建立联系。所以，与其预先定义一种相似度，不如根据网络的特点自动的学习出来。</p>
<p>另一个挑战是，高阶的相似度相较于低阶相似度往往能带来更好的表现，但是随着阶数越高，每个顶点所形成的子图会越来越逼近完整的图，这样会带来过高的时间复杂度与空间复杂度。本文的另一个贡献就在于，定义了一种逼近的方式，不需要$h$阶的子图也能近似的获取$h$阶子图中包含的信息，之间的误差有理论上限。</p>
<h3 id="做法及创新-15"><a href="#做法及创新-15" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>同ICLR20的论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样（毕竟是同一个作者），论文对子图的定义方式为，给定一对顶点$(x,y)$，它的子图为顶点$x$与$y$不高于$h$阶的邻域的一个并集，数学描述如下，也就是与顶点$x$或$y$的距离小于等于$h$所构成的点的集合：</p>
<blockquote>
<p>给定一个图$G=(V,E)$，以及图上两个顶点$x、y$，它的$h$阶围绕子图(enclosing subgraph)$G^h_{x,y}$为图$G$的一个子图，满足$\{i|d(i,x)\le h\ or\ d(i,y)\le h\}$.</p>
</blockquote>
<p>接下来是定义一个$\gamma$-decaying heuristic函数，它用来逼近$h$阶子图的信息而不需要实际计算$h$阶子图：</p>
<script type="math/tex; mode=display">
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>其中$\gamma$是一个位于$(0,1)$的衰减因子，$\eta$是一个正的常数或一个上界为常数的函数。因为这里的求和从1到$\infin$，接下来的定理说明可以用有限项去逼近$H(x,y)$，误差随着$h$的增加而指数下降：</p>
<blockquote>
<p>定理一：</p>
<p>如果函数$f(x,y,l)$满足：</p>
<ol>
<li>$f(x,y,l)\le \lambda^l$，其中$\lambda &lt;\frac{1}{\gamma}$</li>
<li>对于$l=1,2,\dots,g(h)$，$f(x,y,l)$能够从$h$阶子图$G^h_{x,y}$中计算得到，其中$g(h)=ah+b$，$a,b\in \N,\ a&gt;0$</li>
</ol>
</blockquote>
<p>证明的方法很容易理解：</p>
<blockquote>
<p>逼近项为：</p>
<script type="math/tex; mode=display">
\tilde{H}(x,y)=\eta\sum_{l=1}^{g(h)}\gamma^lf(x,y,l)</script><p>计算差值可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
|H(x,y)-\tilde{H}(x,y)|&=\eta\sum_{l=g(h)+1}^{\infin}\gamma^lf(x,y,l)\\
&\le \eta\sum_{l=ah+b+1}^{\infin}\gamma^l\lambda^l\\
&=\eta\frac{(\gamma \lambda)^{ah+b+1}}{1-\gamma \lambda}
\end{aligned}</script></blockquote>
<p>第一个不等式是根据定理一的第一个条件，最后一个等号是根据等比数列的求和公式，当项数$n\rightarrow \infin$且$q\in(0,1)$时，结果为$\frac{a_1}{1-q}$。</p>
<p>到这里可能还是不知道这个$H(x,y)$和图中$h$阶的信息有什么关系，下面就通过Katz、rooted PageRank和SimRank三个高阶相似度来具体说明怎么使用：</p>
<p>在说明之前，先介绍一个引理，接下来会用到，证明起来也很直观：</p>
<blockquote>
<p>顶点$x$与$y$之间任意一条长度$l$满足$l\le2h+1$的路径都被包含在子图$G^h_{x,y}$中</p>
</blockquote>
<p>证明：</p>
<blockquote>
<p>即证明给定一条长度为$l$的路径$w=<x,v_1,\dots,v_{l-1},y>$中的每一个顶点都在子图中。取其中任意一个顶点$v_i$，满足$d(v_i,x)\ge h$且$d(v_i,y)\ge h$，根据子图$G^h_{x,y}$的定义它不在其中。那么有：</x,v_1,\dots,v_{l-1},y></p>
<script type="math/tex; mode=display">
2h+1\ge l=|<x,v_1,\dots,v_i>|+|<v_i,\dots,v_{l-1},y>|\ge d(v_i,x)+d(v_i,y)=2h+2</script><p>矛盾，不等号是因为$d(x,y)$就是表示两个顶点之间的最短路径，所以有$d(v_i,x)&lt;h$或$d(v_i,y)&lt;h$，则顶点$v_i$在子图$G^h_{x,y}$中。</p>
</blockquote>
<h4 id="Katz-index"><a href="#Katz-index" class="headerlink" title="Katz index"></a>Katz index</h4><p>给定一对顶点$(x,y)$，Katz index定义为：</p>
<script type="math/tex; mode=display">
\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}</script><p>其中$\text{walk}^{<l>}(x,y)$是这两个顶点之间长度为$l$的路径构成的集合，$A^l$是邻接矩阵的$l$次幂。从表达式可以看到，长度越长的路径在计算时会被$\beta^l$衰减的越多$(0&lt;\beta&lt;1)$，短路径有更大的权重。</l></p>
<p>对比两式可以发现：</p>
<script type="math/tex; mode=display">
\text{Katz}_{x,y}=\sum_{l=1}^{\infin}\beta^l|\text{walks}^{<l>}(x,y)|=\sum_{l=1}^{\infin}\beta^l[A^l]_{x,y}\\
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>Katz index是论文中定义的$\gamma$-decaying heuristic函数的一种特殊形式，取$\eta=1,\gamma=\beta$，$f(x,y,l)=|\text{walks}^{<l>}(x,y)|=[A^l]_{x,y}$。根据引理，只要取长度小于2h+1的路径，其中的顶点就会全部被子图给包含，这也就满足了定理一的第2个“可计算”条件。对于第一个条件，可以通过数学归纳法说明Katz index的表达式同样满足：</l></p>
<blockquote>
<p>给定任意的顶点$i、j$，$[A^l]_{i,j}$的上限为$d^l$，其中$d$是网络中的最大顶点度</p>
</blockquote>
<p>数学归纳法证明：</p>
<blockquote>
<p>当$l=1$时，$A_{i,j}$退化成了顶点的度，那显然有$A_{i,j}\le d$成立。假设$k=l$时也成立$[A^l]_{i,j}\le d^l$，当$k=l+1$时：</p>
<script type="math/tex; mode=display">
[A^{l+1}]_{i,j}=\sum_{k=1}^{|V|}[A^l]_{i,k}A_{k,j}\le d^l\sum_{k=1}^{|V|}A_{k,j}\le d^ld=d^{l+1}</script></blockquote>
<p>第一个等式就是矩阵乘法的定义，因为$[A^{l+1}]$的含义就是$l+1$个邻接矩阵$A$相乘。因此，对比定理一的第一个条件，我们只要取$\lambda=d$，$d$满足$d&lt;\frac{1}{\beta}$就能够成立，这样一来两个条件都被满足了，这说明Katz index能够很好地从$h$阶子图中近似。</p>
<h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><p>rooted PageRank来源于这篇论文<a href="https://dl.acm.org/doi/10.1145/511446.511513" target="_blank" rel="noopener">Topic-sensitive PageRank</a>，它通过迭代计算PageRank向量$\pi_x$来得到某一点相对于其它顶点的相似度。具体来说，它计算一个从顶点$x$开始的随机漫步的平稳分布，这个随机漫步以概率$\alpha$移动到任一邻居上或以概率$1-\alpha$回到顶点$x$。这个平稳分布满足：</p>
<script type="math/tex; mode=display">
\pi_x=\alpha P\pi_x+(1-\alpha)e_x</script><p>其中$[\pi_x]_i$表示在这个平稳分布下漫步到顶点$i$的概率，$P$为转移矩阵，其中$P_{i,j}=\frac{1}{|\Gamma(v_j)|}$，这里的$\Gamma(v_j)$表示顶点$v_j$的一跳邻居构成的集合。如果一个顶点与五个顶点相连，那它转移到其中任意一个顶点的概率就是$\frac{1}{5}$。</p>
<p>rooted PageRank应用于边预测任务时，用来得到一对顶点$(x,y)$的分数，以$[\pi_x]_y$或$[\pi_x]_y+[\pi_y]_x$（对称）表示，分数越高越有可能有边相连。</p>
<p>接下来就要说明rooted PageRank如何能够同样以论文中提出的$\gamma$-decaying heuristic函数进行表示。根据<a href="http://infolab.stanford.edu/~glenj/spws.pdf" target="_blank" rel="noopener">inverse P-distance理论</a>，$[\pi_x]_y$能够等价地改写为：</p>
<script type="math/tex; mode=display">
[\pi_x]_y=(1-\alpha)\sum_{w:x\leadsto y}P[w]\alpha^{len(w)}</script><p> 这里的求和范围$w:x\leadsto y$表示所有从$x$开始结束于$y$的路径，$P[w]$定义为$\prod_{i=0}^{k-1}\frac{1}{|\Gamma(v_i)|}$，$k$是路径长度，$v_i$是路径中的顶点，通过这条路径来从$x$到$y$的概率就是漫步到路径中每一个顶点的概率的连乘。</p>
<p>接下来就是证明这个形式满足定理一的两个条件：</p>
<blockquote>
<p>首先进一步改写：</p>
<script type="math/tex; mode=display">
[\pi_x]_y=(1-\alpha)\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]\alpha^l\\
H(x,y)=\eta\sum_{l=1}^{\infin}\gamma^lf(x,y,l)</script><p>对比：取$\gamma=\alpha,\eta=(1-\alpha),f(x,y,l)=\sum_{l=1}^{\infin}\sum_{W:x\leadsto y\\len(w)=l}P[w]$。因为这时候$f(x,y,l)$表示一个随机漫步恰好以$l$步从顶点$x$漫步到$y$的概率，有$\sum_{z\in V}f(x,z,l)=1$，则$f(x,y,l)\le1&lt;\frac{1}{\alpha}$，这样就满足了定理一，而根据引理，只要取长度小于等于2h+1的路径，路径中的点就会被全部包含在子图中，也就满足了第二个”可计算“条件。</p>
</blockquote>
<h4 id="SimRank"><a href="#SimRank" class="headerlink" title="SimRank"></a>SimRank</h4><p>SimRank的核心思想是，如果两个顶点的邻域相似，那它们也相似：</p>
<script type="math/tex; mode=display">
s(x,y)=\gamma \frac{\sum_{a\in\Gamma(x)}\sum_{b\in \Gamma(y)}s(a,b)}{|\Gamma(x)|·|\Gamma(y)|}</script><p>它有一个<a href="https://dl.acm.org/doi/10.1145/775047.775126" target="_blank" rel="noopener">等价定义形式</a>：</p>
<script type="math/tex; mode=display">
s(x,y)=\sum_{w:(x,y)\multimap (z,z)}P[w]\gamma^{len(w)}</script><p>其中$w:(x,y)\multimap (z,z)$表示从顶点$x$开始的随机漫步与从顶点$y$开始的随机漫步第一次相遇于顶点$z$。证明与rooted PageRank基本一致，可以见原论文。</p>
<p>总结来说，$\gamma$-decaying heuristic函数的思想是，对于远离目标顶点的结构信息通过指数衰减的方式给一个更小的权重，因为它们带来的信息十分有限。</p>
<h4 id="SEAL框架"><a href="#SEAL框架" class="headerlink" title="SEAL框架"></a>SEAL框架</h4><div align="center">
<a href="https://imgchr.com/i/ypCC6A" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/ypCC6A.png" alt="ypCC6A.png" border="0" width="80%"></a>
</div>


<p>这一节就是根据上面的理论分析建立一个用于边预测任务的框架。一个图神经网络的典型输入形式是$(A,X)$，在本论文中，$A$自然地被定义为子图$G^h_{x,y}$的邻接矩阵，子图的获取即来自正样本（已知边）也来自负样本（未知边）。接下来的部分就是介绍论文怎么定义顶点的特征矩阵$X$，它包含三个部分：structural node labels、node embeddings和node attributes。</p>
<h5 id="Node-labeling"><a href="#Node-labeling" class="headerlink" title="Node labeling"></a>Node labeling</h5><p>跟作者的另一篇论文<a href="http://www.bithub00.com/2021/01/14/ICMC[ICLR20]/" target="_blank" rel="noopener">ICMC</a>一样，通过给顶点打标签的方式来区别顶点在子图中的不同角色，这么做的意义在另一篇博客说过了这里就不写了，具体打标签的方式为：</p>
<ul>
<li>起始顶点$x$与目标顶点$y$的标签都为”1“</li>
<li>如果两个顶点$i、j$距离起始顶点与目标顶点的距离都相同，那么它们的标签一样</li>
<li>$(d(i,x),d(i,y))=(a,b)\rightarrow label:a+b$</li>
</ul>
<p>将顶点的标签进行one-hot编码后作为结构特征。</p>
<h5 id="Node-embeddings-Node-attributes"><a href="#Node-embeddings-Node-attributes" class="headerlink" title="Node embeddings + Node attributes"></a>Node embeddings + Node attributes</h5><p>Node attributes一般数据集直接给定，而Node embeddings是通过一个GNN得到，具体做法是：给定正样本$E_p\in E$，负样本$E_n$，$E_p\and E_n=\empty$，在这么一个图$G’=(V,E\and E_n)$上生成embeddings，防止过拟合。</p>
<h3 id="数据集-13"><a href="#数据集-13" class="headerlink" title="数据集"></a>数据集</h3><p>USAir、NS、PB、Yeast、C.ele、Power、Router、E.coli</p>
<h2 id="GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training-KDD’20"><a href="#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training-KDD’20" class="headerlink" title="GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD’20]"></a>GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD’20]</h2><h3 id="解决的问题-17"><a href="#解决的问题-17" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>如何将自监督学习的思想应用与图表示学习，通过预训练图神经网络从而仅需要微调就可以应用于新的数据集。</p>
<p>图表示学习目前受到了广泛关注，但目前绝大多数的图表示学习方法都是针对特定领域的图进行学习和建模，训练出的图神经网络难以迁移。</p>
<h3 id="做法及创新-16"><a href="#做法及创新-16" class="headerlink" title="做法及创新"></a>做法及创新</h3><h4 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h4><p>对比学习是自监督学习思想的一种典型框架，一个典型的例子如下图所示：</p>
<div align="center">
  <a href="https://imgchr.com/i/yiECBF" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiECBF.png" alt="yiECBF.png" border="0" width="80%"></a>
</div>

<p>对比学习的思想是：尽管我们已经见过钞票很多次，能够轻易地分辨出一张钞票，我们也很少能画出一张完美无缺的钞票。<strong>表示学习算法不需要关注到样本的每一个细节，只要学到的特征能够将用来区分其它样本即可</strong>。不需要模型能够生成一匹栩栩如生的马之后它才能去分辨一张图片里的动物是不是马，这就是对比学习和生成对抗网络的一个区别。</p>
<p>既然是表示学习，核心就是通过一个函数把样本$x$转换成特征表示$f(x)$，而对比学习作为一种表示学习方法，它的思想是满足下面这个式子：</p>
<script type="math/tex; mode=display">
s(f(x),f(x^+))\gg s(f(x),f(x^-))</script><p>使得类似样本之间的相似度要远大于非类似样本之间的相似度，这样才能够进行区分。</p>
<h4 id="图表示学习"><a href="#图表示学习" class="headerlink" title="图表示学习"></a>图表示学习</h4><p>具体到论文的图表示学习任务中，论文的一个重要假设是，具有典型性的图结构在不同的网络之间是普遍存在而且可以迁移的（Representative graph structural patterns are universal and transferable across networks）。受对比学习在计算机视觉和自然语言处理领域的成功应用，论文想把对比学习（contrastive learning）的思想放在图表示学习中。通过预训练一个图神经网络，它能够很好地区分这些典型性的图结构，这样它的表现就不会仅仅局限于某个特定的数据集。</p>
<div align="center">
<a href="https://imgchr.com/i/y9x4KI" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/28/y9x4KI.png" alt="y9x4KI.png" border="0" width="80%"></a>
</div>

<p>论文首先将现有工作对顶点相似度的衡量分为了三类：</p>
<ol>
<li><p>邻域相似度</p>
<p>核心思想：越近的两个顶点之间相似度越高，包括有Jaccard、RWR、SimRank以及LINE、DeepWalk、node2vec。</p>
</li>
<li><p>结构相似度</p>
<p>核心思想：有相似的局部结构的两个顶点之间相似度更高。不同于邻域相似度，结构相似度不需要两个顶点之间有路径相连。常用的局部结构包括vertex degree、structural diversity、structural hole、k-core、motif等。</p>
</li>
<li><p>属性相似度</p>
<p>当数据集中顶点有许多标签信息时，可以将标签作为顶点的特征来衡量它们之间的相似度。</p>
</li>
</ol>
<p>在对比学习中，给定一个查询表示$q$以及一个包含$K+1$个键表示${k_0,\dots,k_K}$的字典，我们希望找到一个能与$q$匹配的键$k_+$。所以，论文优化的损失函数来自于InfoNCE：</p>
<script type="math/tex; mode=display">
L=-\log \frac{\exp(q^Tk_+\tau)}{\sum_{i=0}^K\exp(q^Tk_i/\tau)}</script><p>其中$f_q、f_k$是两个图神经网络，分别将样本$x^q$和$x^k$转换为低维表示$q$与$k$。</p>
<h4 id="正负样本获取"><a href="#正负样本获取" class="headerlink" title="正负样本获取"></a>正负样本获取</h4><p>因为查询和键可以是任意形式，具体到本论文里，定义每一个样本都是一个从特定顶点的$r$阶邻居网络中采样的子图，这里的子图定义和其它论文一致：$S_v=\{u:d(u,v)&lt;r \}$，距离顶点$v$最短路径距离小于$r$的顶点构成的集合。既然是最短路径，给定$r$那么这个集合也基本确定了，这种情况下得到的子图数量有限，在计算机视觉领域，当输入用于训练的图片数量有限时，往往会使用反转、旋转等方式对图片进行变换，以扩充训练图片的数量，这里论文也想采取类似的做法，对得到的子图$x$进行变换，来得到对比学习中的类似$x^+$与非类似样本$x^-$，具体做法如下：</p>
<ol>
<li><strong>带重启动的随机漫步</strong>。首先从子图的中心顶点$v$开始随机漫步，每一步时都有一定概率重新回到中心顶点，而漫步到任一邻居顶点的概率与当前顶点的出度有关。</li>
<li><strong>子图推演</strong>。随机漫步可以得到一系列顶点，它们构成的集合记为$\tilde{S_v}$，所形成的子图记作$\tilde{G_v}$，它就可以看作子图$S_v$的一个变换。</li>
<li><strong>匿名化</strong>。重新定义$\tilde{G_v}$中的顶点的标签，将$\{1,2,\dots,|\tilde{S_v} |\}$的顺序随机打乱作为重新定义后的标签。</li>
</ol>
<div align="center">
    <a href="https://imgchr.com/i/yiFHv8" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yiFHv8.png" alt="yiFHv8.png" border="0" width="70%"></a>
</div>

<p>论文对于每个子图都进行两次上述变换，而变换后的子图显然会与原子图相似，这样就有了一组相似的子图$(x^q,x^{k_+})$。要得到不相似的子图也很容易，不是同一个子图变换得到的子图就定义为不相似：$(x^q,x^k),k\not =k_+$。在上图的例子中，$x^q$和$x^{k_0}$是从红色的中心顶点采样得到的子图，我们认为它是一对正样本，而$x^{k_1}$和$x^{k_2}$作为从蓝色的中心顶点采样得到的子图，则被作为负样本。在变换时之所以要做最后一步，是为了防止图神经网络在判断两个子图是否相似时，仅仅是通过判断对应顶点的标签是不是一样，这样显然没有学到任何有用的结构信息。这里有一个小结论：</p>
<blockquote>
<p>绝大多数图神经网络对于输入图中顶点的顺序的随机扰动有稳定性</p>
</blockquote>
<p>现在有了正样本和负样本，下一步就是训练一个图神经网络对它们加以区分了，论文选取的是GIN。这就是自监督学习的思想，对比学习就是这种思想的一种典型框架。因为现有的图神经网络框架都需要额外的顶点特征作为输入，论文提出了一种位置embedding来作为其中特征：$I-D^{-1/2}AD^{-1/2}=U\Lambda U^T$，矩阵$U$中排序靠前的特征向量作为embedding。其它特征还包括顶点度的one-hot编码和中心顶点的指示向量。</p>
<h4 id="模型学习"><a href="#模型学习" class="headerlink" title="模型学习"></a>模型学习</h4><p>在模型学习时采用了何凯明组的MoCo框架的思想：</p>
<div align="center">
  <a href="https://imgchr.com/i/yimVaD" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/29/yimVaD.png" alt="yimVaD.png" border="0" width="80%"></a>
</div>

<p>在对比学习中，我们需要维护一个大小为$K$的字典和编码器，要计算上面定义的损失函数，理想的情况是把所有负样本加入字典中进行计算，这会导致$K$很大字典难以维护。在MoCo的方法中，为了增大字典大小$K$，需要维护一个负样本的队列，队列中包含此前训练过的batch的样本作为负样本。在更新参数时，只有$q$的编码器图神经网络$f_q$中的参数通过反向传播进行更新，而$k$的编码器$f_k$中的值通过一种动量法进行更新：$\theta_k\leftarrow m\theta_k+(1-m)\theta_q$。</p>
<h3 id="数据集-14"><a href="#数据集-14" class="headerlink" title="数据集"></a>数据集</h3><p>Academia、DBLP(SNAP)、DBLP(NetRep)、IMDB、Facebook、LiveJournal</p>
<h2 id="How-Powerful-are-Graph-Neural-Networks-ICLR’19"><a href="#How-Powerful-are-Graph-Neural-Networks-ICLR’19" class="headerlink" title="How Powerful are Graph Neural Networks?[ICLR’19]"></a>How Powerful are Graph Neural Networks?[ICLR’19]</h2><h3 id="解决的问题-18"><a href="#解决的问题-18" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>GNN性能表现好的原因是什么？</p>
<h4 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h4><ol>
<li>证明了GNN的性能上限是Weisfeiler-Lehman (WL) test，最多只和它一样有效</li>
<li>给出了GNN在什么条件下能够和WL test一样有效</li>
<li>指明了主流GNN框架如GCN、GraphSage无法区分的图结构，以及它们能够区分的图结构的特点</li>
<li>提出了一个简单有效的框架GIN，能够与WL test一样有效</li>
</ol>
<h3 id="做法及创新-17"><a href="#做法及创新-17" class="headerlink" title="做法及创新"></a>做法及创新</h3><p>首先是介绍现有GNN框架的做法及图同构测试的定义，还有WL test的做法。</p>
<h4 id="GNN与WL-test"><a href="#GNN与WL-test" class="headerlink" title="GNN与WL test"></a>GNN与WL test</h4><p>论文认为主流的GNN框架可以分为下面这三步：</p>
<ol>
<li><p><strong>Aggregate</strong>：聚合邻域内的信息</p>
<script type="math/tex; mode=display">
a_v^{(k)}=\text{AGGREGATE}^{(k)}(\{h_u^{(k-1)}:u\in N(v) \})</script></li>
<li><p><strong>Combine</strong>：将聚合后的邻域信息与当前顶点信息结合</p>
<script type="math/tex; mode=display">
h_v^{(k)}=\text{COMBINE}^{(k)}(h_v^{(k-1)},a_v^{(k)})</script></li>
<li><p><strong>Readout</strong>：通过图中的每个顶点的表示得到图的表示</p>
<script type="math/tex; mode=display">
h_G=\text{READOUT}({h_v^{(K)}|v\in G})</script></li>
</ol>
<p>图同构测试就是判断两张图是否在拓扑结构上相同。而WL test的做法是迭代地进行以下步骤：</p>
<ul>
<li>聚合顶点及其邻域的标签信息</li>
<li>将聚合后的标签集合哈希成唯一的新标签</li>
</ul>
<p>如果经过若干次迭代后，两张图中的顶点的标签出现了不同则判断为不同构。基于WL test有一种核函数被提出以计算图之间的相似性。直观上来说，如下图所示，一个顶点在第$k$次迭代时的标签，实际表示了一颗以该顶点为根顶点高度为$k$的子树。</p>
<div align="center">
  <a href="https://imgchr.com/i/yVPBNQ" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/01/31/yVPBNQ.png" alt="yVPBNQ.png" border="0" width="80%"></a>
</div>


<p>而GNN同样是通过迭代地更新图中每个顶点的特征向量来捕捉图的结构信息以其周围顶点的特征，这里的结构特征同样可以是上图的根子树rooted subtree。如果给每个顶点的特征向量一个唯一的标签例如{a,b,c,…}，那一个顶点的邻域中所有顶点的特征向量可以构成一个Multiset，它的定义基本和C++中的Multiset一样，是一个Set的同时里面的元素还可以重复例如{a,a,b,c}。论文中对Multiset给出的数学定义是：$X=(S,m)$，其中$S$由Multiset中的非重复元素构成，$m$表示$S$中的元素在$X$中的频数。</p>
<p>直观上来说，一个有效的GNN应该只有在两个顶点对应的根子树结构相同，且其中对应顶点的特征向量也相同时，才将这两个顶点在特征空间中映射成相同的表示。也就是永远不会将不同的两个Multiset映射成同一个特征表示（因为Multiset中的顶点也是根子树中的顶点，既然它们都是通过聚合邻域得到的）。这也就意味着GNN中使用的聚合函数必须是单射的，对值域内的每一个$y$，存在最多一个定义域内的$x$使得$f(x)=y$。有下面这么一个引理：</p>
<blockquote>
<p>设$G_1$和$G_2$是两个非同构图，如果一个图神经网络$A:G\rightarrow \mathbb{R}^d$将$G_1$和$G_2$映射成不同的embedding，那么WL test同样会判断这两个图为非同构。</p>
</blockquote>
<p>引理表明在图区分任务上，一个图神经网络的表现最多和WL test一样好。而一样好的条件是，这个图神经网络的邻居聚合函数和图表示函数都是单射的。这里的一个局限是，函数考虑的定义域和值域都是离散集合。</p>
<p>图神经网络相较于WL test的另一个好处是，WL test输入的顶点特征向量都是one-hot编码，这无法捕捉到子树之间的结构相似度：</p>
<blockquote>
<p>Note that node feature vectors in the WL test are essentially one-hot encodings and thus cannot capture the similarity between subtrees. In contrast, a GNN satisfying the criteria in Theorem 3 generalizes the WL test by learning to embed the subtrees to low-dimensional space. This enables GNNs to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures.</p>
</blockquote>
<h4 id="图同构网络GIN"><a href="#图同构网络GIN" class="headerlink" title="图同构网络GIN"></a>图同构网络GIN</h4><p>基于上面介绍的引理和结论，论文提出的GIN框架如下：</p>
<script type="math/tex; mode=display">
h_v^{(k)}=\text{MLP}^{(k)}\Big((1+\epsilon^{(k)})·h_v^{(k-1)}+\sum_{u\in N(v)}h_u^{(k-1)} \Big)</script><p>对比一开始论文给出的GNN主流框架，可以看到是Aggregate函数选取了求和函数，Combine函数选取了MLP+(1+$\epsilon$)的形式。常见的Aggregate函数包括求和Sum、最大值Max和平均值Mean，论文花了一部分篇幅来说明求和相较于其他两个函数的好处：</p>
<div align="center">
  <a href="https://imgchr.com/i/yGIv5D" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGIv5D.png" alt="yGIv5D.png" border="0" width="80%"></a>
  <a href="https://imgchr.com/i/yGoirt" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2021/02/05/yGoirt.png" alt="yGoirt.png" border="0" width="80%"></a>
</div>

<p>上面两幅图分别说明这几种聚合函数的特点及何种场景下会导致误差。第一幅图中即使减少了顶点的数量但对于取平均和最大值函数来说得到的信息保持不变，第二幅图也是想说明同样的问题，例如取平均，两个一样的顶点与三个一样的顶点取平均出来结构都是一样的，但它们分别对应的局部结构是不相同的。</p>
<p>对于顶点分类及边预测这类下游任务，只要得到顶点的embedding即可。而对于图分类任务，还需要根据所有顶点的embedding来得到图的一个表示，也就是前面提到的主流GNN框架做法的第三步Readout函数。论文的做法类似于<a href="http://www.bithub00.com/2020/12/22/JK-Net[ICML18]/" target="_blank" rel="noopener">JK-Net</a>，将所有层的表示都考虑进来，不过没有具体说是怎么做的。</p>
<p>最后，论文还探讨了那些不满足上面定理的GNN框架如GCN、GraphSAGE等，这些框架都采用一层感知机如ReLU来将Multiset映射成特征表示，而不像论文的做法采用多层感知机，而ReLU存在将不同的Multiset表示成同一种特征表示的情况，即$\exist X_1 \not=X_2,\ s.t. \ \sum_{x\in X_1}\text{ReLU}(Wx)=\sum_{x\in X_2}\text{ReLU}(Wx)$。论文中直接给了一个简单的例子：$X_1=\{1,1,1,1,1\},X_2=\{2,3\}$，因为有$\sum_{x\in X_1}\text{ReLU}(Wx)=\text{ReLU}(W\sum_{x\in X_1}x)$。</p>
<h3 id="数据集-15"><a href="#数据集-15" class="headerlink" title="数据集"></a>数据集</h3><p>MUTAG、PTC、NCI1、PROTEINS、COLLAB、IMDB-BINARY、IMDB-MULTI、REDDIT-BINARY、REDDIT-MULTI5K</p>
<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/推荐系统/" rel="tag">#推荐系统</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/08/25/d2l/" rel="next" title="d2l学习笔记">
                <i class="fa fa-chevron-left"></i> d2l学习笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/12/22/ARROW[ICDE19]/" rel="prev" title="ARROW - Approximating Reachability using Random walks Over Web-scale Graphs[ICDE'19]">
                ARROW - Approximating Reachability using Random walks Over Web-scale Graphs[ICDE'19] <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.jpg"
                alt="Mr.shuan" />
            
              <p class="site-author-name" itemprop="name">Mr.shuan</p>
              <p class="site-description motion-element" itemprop="description">May 4th be with you</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">58</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://joaquinchou.com/" title="喵语小站" target="_blank">喵语小站</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#数据库"><span class="nav-number">1.</span> <span class="nav-text">数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ARROW-Approximating-Reachability-using-Random-walks-Over-Web-scale-Graphs-ICDE’19"><span class="nav-number">1.1.</span> <span class="nav-text">ARROW: Approximating Reachability using Random walks Over Web-scale Graphs[ICDE’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题"><span class="nav-number">1.1.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创新之处"><span class="nav-number">1.1.2.</span> <span class="nav-text">创新之处</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Accelerating-Exact-Inner-Product-Retrieval-by-CPU-GPU-Systems-SIGIR’19"><span class="nav-number">1.2.</span> <span class="nav-text">Accelerating Exact Inner Product Retrieval by CPU-GPU Systems[SIGIR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#IPR问题"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">IPR问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新"><span class="nav-number">1.2.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#行文逻辑"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">行文逻辑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#贡献"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">贡献</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图神经网络"><span class="nav-number">2.</span> <span class="nav-text">图神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Semi-Supervised-Classification-with-Graph-Convolutional-Network-ICLR’17"><span class="nav-number">2.1.</span> <span class="nav-text">Semi-Supervised Classification with Graph Convolutional Network [ICLR’17]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-2"><span class="nav-number">2.1.1.</span> <span class="nav-text">解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题描述"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">问题描述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-1"><span class="nav-number">2.1.2.</span> <span class="nav-text">做法及创新</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Graph-Collaborative-Filtering-SIGIR’19"><span class="nav-number">2.2.</span> <span class="nav-text">Neural Graph Collaborative Filtering[SIGIR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-3"><span class="nav-number">2.2.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-2"><span class="nav-number">2.2.2.</span> <span class="nav-text">做法及创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集"><span class="nav-number">2.2.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LightGCN-Simplifying-and-Powering-Graph-Convolution-Network-for-Recommendation-SIGIR’20"><span class="nav-number">2.3.</span> <span class="nav-text">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation[SIGIR’20]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-4"><span class="nav-number">2.3.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-3"><span class="nav-number">2.3.2.</span> <span class="nav-text">做法及创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-1"><span class="nav-number">2.3.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simplifying-Graph-Convolutional-Networks-PMLR’19"><span class="nav-number">2.4.</span> <span class="nav-text">Simplifying Graph Convolutional Networks[PMLR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-5"><span class="nav-number">2.4.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-4"><span class="nav-number">2.4.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原始图卷积网络"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">原始图卷积网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简化图卷积网络"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">简化图卷积网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-2"><span class="nav-number">2.4.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inductive-Representation-Learning-on-Large-Graphs-NIPS’17"><span class="nav-number">2.5.</span> <span class="nav-text">Inductive Representation Learning on Large Graphs[NIPS’17]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-6"><span class="nav-number">2.5.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-5"><span class="nav-number">2.5.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法流程"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#聚合函数"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">聚合函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-3"><span class="nav-number">2.5.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learning-Convolutional-Neural-Networks-for-Graphs-ICML’16"><span class="nav-number">2.6.</span> <span class="nav-text">Learning Convolutional Neural Networks for Graphs[ICML’16]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-7"><span class="nav-number">2.6.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-6"><span class="nav-number">2.6.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Node-Sequence-Selection"><span class="nav-number">2.6.2.1.</span> <span class="nav-text">Node Sequence Selection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neighborhood-Assembly"><span class="nav-number">2.6.2.2.</span> <span class="nav-text">Neighborhood Assembly</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-Normalization"><span class="nav-number">2.6.2.3.</span> <span class="nav-text">Graph Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convolutional-Architecture"><span class="nav-number">2.6.2.4.</span> <span class="nav-text">Convolutional Architecture</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-4"><span class="nav-number">2.6.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Attention-Networks-ICLR’18"><span class="nav-number">2.7.</span> <span class="nav-text">Graph Attention Networks[ICLR’18]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-8"><span class="nav-number">2.7.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-7"><span class="nav-number">2.7.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图卷积"><span class="nav-number">2.7.2.1.</span> <span class="nav-text">图卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-attention"><span class="nav-number">2.7.2.2.</span> <span class="nav-text">Self-attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-head-attention"><span class="nav-number">2.7.2.3.</span> <span class="nav-text">Multi-head attention</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-5"><span class="nav-number">2.7.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Representation-Learning-on-Graphs-with-Jumping-Knowledge-Networks-ICML’18"><span class="nav-number">2.8.</span> <span class="nav-text">Representation Learning on Graphs with Jumping Knowledge Networks[ICML’18]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-9"><span class="nav-number">2.8.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-8"><span class="nav-number">2.8.2.</span> <span class="nav-text">做法及创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-6"><span class="nav-number">2.8.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-Based-Recommendation-with-Graph-Neural-Networks-AAAI’19"><span class="nav-number">2.9.</span> <span class="nav-text">Session-Based Recommendation with Graph Neural Networks[AAAI’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-10"><span class="nav-number">2.9.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-9"><span class="nav-number">2.9.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Session-Graph-Modeling"><span class="nav-number">2.9.2.1.</span> <span class="nav-text">Session Graph Modeling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Node-Representation-Learning"><span class="nav-number">2.9.2.2.</span> <span class="nav-text">Node Representation Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GRU"><span class="nav-number">2.9.2.2.1.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reset-Gate"><span class="nav-number">2.9.2.2.2.</span> <span class="nav-text">Reset Gate</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Update-Gate"><span class="nav-number">2.9.2.2.3.</span> <span class="nav-text">Update Gate</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Session-Representation-Generation"><span class="nav-number">2.9.2.3.</span> <span class="nav-text">Session Representation Generation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Making-Recommendation"><span class="nav-number">2.9.2.4.</span> <span class="nav-text">Making Recommendation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-7"><span class="nav-number">2.9.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KGAT-Knowledge-Graph-Attention-Network-for-Recommendation-KDD’19"><span class="nav-number">2.10.</span> <span class="nav-text">KGAT: Knowledge Graph Attention Network for Recommendation[KDD’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-11"><span class="nav-number">2.10.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-10"><span class="nav-number">2.10.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CKG-Embedding-Layer"><span class="nav-number">2.10.2.1.</span> <span class="nav-text">CKG Embedding Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Attentive-Embedding-Propagation-Layers"><span class="nav-number">2.10.2.2.</span> <span class="nav-text">Attentive Embedding Propagation Layers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-Prediction"><span class="nav-number">2.10.2.3.</span> <span class="nav-text">Model Prediction</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-8"><span class="nav-number">2.10.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepInf-Social-Influence-Prediction-with-Deep-Learning-KDD’18"><span class="nav-number">2.11.</span> <span class="nav-text">DeepInf: Social Influence Prediction with Deep Learning[KDD’18]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-12"><span class="nav-number">2.11.1.</span> <span class="nav-text">解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题定义"><span class="nav-number">2.11.1.1.</span> <span class="nav-text">问题定义</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-11"><span class="nav-number">2.11.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据预处理"><span class="nav-number">2.11.2.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#影响力计算"><span class="nav-number">2.11.2.2.</span> <span class="nav-text">影响力计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-9"><span class="nav-number">2.11.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Predict-then-Propagate-Graph-Neural-Networks-meet-Personalized-PageRank-ICLR’19"><span class="nav-number">2.12.</span> <span class="nav-text">Predict then Propagate Graph Neural Networks meet Personalized PageRank[ICLR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-13"><span class="nav-number">2.12.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-12"><span class="nav-number">2.12.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PPNP"><span class="nav-number">2.12.2.1.</span> <span class="nav-text">PPNP</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#social-aggregation"><span class="nav-number">2.12.2.1.1.</span> <span class="nav-text">social aggregation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bipartite-Graph-Construction"><span class="nav-number">2.12.2.2.</span> <span class="nav-text">Bipartite Graph Construction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-Convolutional-Encoder"><span class="nav-number">2.12.2.3.</span> <span class="nav-text">Graph Convolutional Encoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bilinear-Decoder"><span class="nav-number">2.12.2.4.</span> <span class="nav-text">Bilinear Decoder</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-10"><span class="nav-number">2.12.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Variational-Graph-Auto-Encoders-NIPS’16"><span class="nav-number">2.13.</span> <span class="nav-text">Variational Graph Auto-Encoders[NIPS’16]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-14"><span class="nav-number">2.13.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-13"><span class="nav-number">2.13.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#变分自编码器"><span class="nav-number">2.13.2.1.</span> <span class="nav-text">变分自编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图变分自编码器"><span class="nav-number">2.13.2.2.</span> <span class="nav-text">图变分自编码器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-11"><span class="nav-number">2.13.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inductive-Matrix-Completion-Based-on-Graph-Neural-Networks-ICLR’20"><span class="nav-number">2.14.</span> <span class="nav-text">Inductive Matrix Completion Based on Graph Neural Networks[ICLR’20]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-15"><span class="nav-number">2.14.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-14"><span class="nav-number">2.14.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Enclosing-Subgraph-Extraction"><span class="nav-number">2.14.2.1.</span> <span class="nav-text">Enclosing Subgraph Extraction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Node-Labeling"><span class="nav-number">2.14.2.2.</span> <span class="nav-text">Node Labeling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-Neural-Network"><span class="nav-number">2.14.2.3.</span> <span class="nav-text">Graph Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adjacent-Rating-Regularization"><span class="nav-number">2.14.2.4.</span> <span class="nav-text">Adjacent Rating Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Graph-level-GNN-vs-Node-level-GNN"><span class="nav-number">2.14.2.5.</span> <span class="nav-text">Graph-level GNN vs Node-level GNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-12"><span class="nav-number">2.14.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Link-Prediction-Based-on-Graph-Neural-Networks-NIPS’18"><span class="nav-number">2.15.</span> <span class="nav-text">Link Prediction Based on Graph Neural Networks[NIPS’18]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-16"><span class="nav-number">2.15.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-15"><span class="nav-number">2.15.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Katz-index"><span class="nav-number">2.15.2.1.</span> <span class="nav-text">Katz index</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PageRank"><span class="nav-number">2.15.2.2.</span> <span class="nav-text">PageRank</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SimRank"><span class="nav-number">2.15.2.3.</span> <span class="nav-text">SimRank</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SEAL框架"><span class="nav-number">2.15.2.4.</span> <span class="nav-text">SEAL框架</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Node-labeling"><span class="nav-number">2.15.2.4.1.</span> <span class="nav-text">Node labeling</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Node-embeddings-Node-attributes"><span class="nav-number">2.15.2.4.2.</span> <span class="nav-text">Node embeddings + Node attributes</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-13"><span class="nav-number">2.15.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GCC-Graph-Contrastive-Coding-for-Graph-Neural-Network-Pre-Training-KDD’20"><span class="nav-number">2.16.</span> <span class="nav-text">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training[KDD’20]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-17"><span class="nav-number">2.16.1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-16"><span class="nav-number">2.16.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#对比学习"><span class="nav-number">2.16.2.1.</span> <span class="nav-text">对比学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图表示学习"><span class="nav-number">2.16.2.2.</span> <span class="nav-text">图表示学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正负样本获取"><span class="nav-number">2.16.2.3.</span> <span class="nav-text">正负样本获取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型学习"><span class="nav-number">2.16.2.4.</span> <span class="nav-text">模型学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-14"><span class="nav-number">2.16.3.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-Powerful-are-Graph-Neural-Networks-ICLR’19"><span class="nav-number">2.17.</span> <span class="nav-text">How Powerful are Graph Neural Networks?[ICLR’19]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决的问题-18"><span class="nav-number">2.17.1.</span> <span class="nav-text">解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#贡献："><span class="nav-number">2.17.1.1.</span> <span class="nav-text">贡献：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#做法及创新-17"><span class="nav-number">2.17.2.</span> <span class="nav-text">做法及创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GNN与WL-test"><span class="nav-number">2.17.2.1.</span> <span class="nav-text">GNN与WL test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图同构网络GIN"><span class="nav-number">2.17.2.2.</span> <span class="nav-text">图同构网络GIN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集-15"><span class="nav-number">2.17.3.</span> <span class="nav-text">数据集</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#知识图谱"><span class="nav-number">3.</span> <span class="nav-text">知识图谱</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr.shuan</span>

  
</div>


  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_pv">原力小站已到访<span id="busuanzi_value_site_pv"></span>人次</span>
  <span class="post-meta-divider">|</span>
  <span id="busuanzi_container_site_uv">欢迎第<span id="busuanzi_value_site_uv"></span>位绝地武士








<div class="theme-info">
  <span class="post-meta-divider">|</span>
  <span class="post-count">小站全站共125.0k字</span>
</div>
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("yfqHuQnnOVtQFUYkBt8hCuPk-gzGzoHsz", "04fMIxrudywlX8NlYX71hCnM");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  


</body>
</html>
